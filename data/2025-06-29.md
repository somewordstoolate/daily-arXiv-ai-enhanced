<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 39]
- [cs.AI](#cs.AI) [Total: 13]
- [cs.LG](#cs.LG) [Total: 77]
- [q-fin.PM](#q-fin.PM) [Total: 1]
- [cs.NE](#cs.NE) [Total: 2]
- [cs.IR](#cs.IR) [Total: 2]
- [math.OC](#math.OC) [Total: 2]
- [quant-ph](#quant-ph) [Total: 1]
- [stat.ML](#stat.ML) [Total: 9]
- [cs.SI](#cs.SI) [Total: 1]
- [cs.CV](#cs.CV) [Total: 30]
- [physics.geo-ph](#physics.geo-ph) [Total: 1]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 1]
- [math.DS](#math.DS) [Total: 1]
- [eess.AS](#eess.AS) [Total: 2]
- [math.NA](#math.NA) [Total: 1]
- [q-bio.CB](#q-bio.CB) [Total: 1]
- [cs.CR](#cs.CR) [Total: 4]
- [cs.HC](#cs.HC) [Total: 2]
- [cs.SE](#cs.SE) [Total: 6]
- [cs.RO](#cs.RO) [Total: 3]
- [stat.ME](#stat.ME) [Total: 1]
- [cs.DC](#cs.DC) [Total: 2]
- [eess.IV](#eess.IV) [Total: 4]
- [q-bio.QM](#q-bio.QM) [Total: 1]
- [cs.SD](#cs.SD) [Total: 6]
- [q-bio.BM](#q-bio.BM) [Total: 2]
- [physics.med-ph](#physics.med-ph) [Total: 1]
- [cs.GR](#cs.GR) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Towards Probabilistic Question Answering Over Tabular Data](https://arxiv.org/abs/2506.20747)
*Chen Shen,Sajjadur Rahman,Estevam Hruschka*

Main category: cs.CL

TL;DR: 论文提出了LUCARIO基准和框架，用于处理表格数据上的概率性问题，结合贝叶斯网络和LLMs，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有QA系统（如NL2SQL）在处理表格数据的事实性问题时表现良好，但在需要不确定性推理的概率性问题上表现不足。

Method: 通过从表格中推导贝叶斯网络，将自然语言查询转化为概率查询，并利用大语言模型生成最终答案。

Result: 实验结果表明，该方法在基线模型上有显著改进，展示了混合符号-神经推理的优势。

Conclusion: 该研究为表格数据上的概率性QA提供了新方法，证明了混合推理的有效性。

Abstract: Current approaches for question answering (QA) over tabular data, such as
NL2SQL systems, perform well for factual questions where answers are directly
retrieved from tables. However, they fall short on probabilistic questions
requiring reasoning under uncertainty. In this paper, we introduce a new
benchmark LUCARIO and a framework for probabilistic QA over large tabular data.
Our method induces Bayesian Networks from tables, translates natural language
queries into probabilistic queries, and uses large language models (LLMs) to
generate final answers. Empirical results demonstrate significant improvements
over baselines, highlighting the benefits of hybrid symbolic-neural reasoning.

</details>


### [2] [Multi-lingual Functional Evaluation for Large Language Models](https://arxiv.org/abs/2506.20793)
*Victor Ojewale,Inioluwa Deborah Raji,Suresh Venkatasubramanian*

Main category: cs.CL

TL;DR: 论文提出新的多语言功能基准测试CL-GSM Symbolic和CL-IFEval，揭示静态基准测试与功能性能间的差异及模型跨语言鲁棒性差异。


<details>
  <summary>Details</summary>
Motivation: 现有静态多语言基准测试（如Belebele、M-MMLU和M-GSM）难以充分评估模型在多语言环境中的实际性能和鲁棒性，因此需要开发更贴近实际功能的多语言评估方法。

Method: 通过将英语功能基准测试模板翻译为五种不同资源水平的语言（法语、西班牙语、印地语、阿拉伯语和约鲁巴语），构建了跨语言功能基准测试CL-GSM Symbolic和CL-IFEval。

Result: 研究发现静态基准与功能性能的关联性差异显著（如M-GSM与CL-GSM Symbolic在英/法/西语中性能下降24%/17%/18%），且模型在不同语言间的鲁棒性波动较大（如阿拉伯语和英语表现最稳定）。

Conclusion: 静态基准测试无法全面反映模型功能性能，需结合跨语言功能评估；不同语言的资源水平显著影响模型表现，未来需针对性优化低资源语言能力。

Abstract: Multi-lingual competence in large language models is often evaluated via
static data benchmarks such as Belebele, M-MMLU and M-GSM. However, these
evaluations often fail to provide an adequate understanding of the practical
performance and robustness of models across multi-lingual settings. In
response, we create multi-lingual functional benchmarks -- Cross-Lingual Grade
School Math Symbolic (CL-GSM Symbolic) and Cross-Lingual Instruction-Following
Eval (CL-IFEval)-- by translating existing functional benchmark templates from
English to five additional languages that span the range of resources available
for NLP: French, Spanish, Hindi, Arabic and Yoruba. Our results reveal that
some static multi-lingual benchmarks capture functional performance much more
closely than others (i.e. across models, there is a 24%, 17% and 18% decrease
in performance between M-GSM and CL-GSM Symbolic in English, French and Spanish
respectively; similarly there's a 15 - 24% performance drop across languages
between Belebele and CL-IFEval, and only a 0.5% to 3% performance drop between
M-MMLU and CL-IFEval). Similarly, we find that model robustness across
languages varies significantly, with certain languages (eg. Arabic, English)
being the most consistently well performing across evaluation iterations.

</details>


### [3] [The Ideation-Execution Gap: Execution Outcomes of LLM-Generated versus Human Research Ideas](https://arxiv.org/abs/2506.20803)
*Chenglei Si,Tatsunori Hashimoto,Diyi Yang*

Main category: cs.CL

TL;DR: 研究发现，虽然LLM生成的科研想法在构思阶段被认为更具新颖性，但实际执行后效果不如人类专家想法，揭示了当前LLM在生成有效科研想法上的局限性。


<details>
  <summary>Details</summary>
Motivation: 探讨大型语言模型（LLM）生成的科研想法是否能在实际执行后产生优于人类专家想法的研究成果，以验证LLM在科研中的实际效用。

Method: 招募43名专家研究人员，随机分配执行LLM生成或人类专家撰写的科研想法，每人投入超100小时实施并撰写4页短文，最后由NLP专家盲审评分。

Result: 执行后，LLM生成想法在所有评估指标（新颖性、兴奋度、有效性、整体评价）上得分显著下降（p < 0.05），部分指标甚至出现人类想法反超LLM的情况。

Conclusion: 当前LLM生成的科研想法存在‘构思-执行差距’，其实际效果低于人类专家想法，突显了仅凭构思阶段评估研究想法的局限性。

Abstract: Large Language Models (LLMs) have shown promise in accelerating the
scientific research pipeline. A key capability for this process is the ability
to generate novel research ideas, and prior studies have found settings in
which LLM-generated research ideas were judged as more novel than human-expert
ideas. However, a good idea should not simply appear to be novel, it should
also result in better research after being executed. To test whether
AI-generated ideas lead to better research outcomes, we conduct an execution
study by recruiting 43 expert researchers to execute randomly-assigned ideas,
either written by experts or generated by an LLM. Each expert spent over 100
hours implementing the idea and wrote a 4-page short paper to document the
experiments. All the executed projects are then reviewed blindly by expert NLP
researchers. Comparing the review scores of the same ideas before and after
execution, the scores of the LLM-generated ideas decrease significantly more
than expert-written ideas on all evaluation metrics (novelty, excitement,
effectiveness, and overall; p < 0.05), closing the gap between LLM and human
ideas observed at the ideation stage. When comparing the aggregated review
scores from the execution study, we even observe that for many metrics there is
a flip in rankings where human ideas score higher than LLM ideas. This
ideation-execution gap highlights the limitations of current LLMs in generating
truly effective research ideas and the challenge of evaluating research ideas
in the absence of execution outcomes.

</details>


### [4] [MultiFinRAG: An Optimized Multimodal Retrieval-Augmented Generation (RAG) Framework for Financial Question Answering](https://arxiv.org/abs/2506.20821)
*Chinmay Gondhalekar,Urjitkumar Patel,Fang-Chun Yeh*

Main category: cs.CL

TL;DR: MultiFinRAG框架通过多模态提取和分层回退策略，显著提升金融文档问答的准确性。


<details>
  <summary>Details</summary>
Motivation: 传统大型语言模型在处理金融文档的多模态内容时存在局限性，如令牌限制、布局丢失和跨模态上下文碎片化，难以实现跨模态联合推理。

Method: MultiFinRAG首先对表格和图像进行批量多模态提取，生成结构化JSON和文本摘要，然后通过模态感知相似性阈值进行嵌入和索引，最后采用分层回退策略动态选择上下文。

Result: 在涉及文本、表格、图像和跨模态推理的复杂金融问答任务中，MultiFinRAG比ChatGPT-4o（免费版）的准确率高出19个百分点。

Conclusion: MultiFinRAG在普通硬件上实现了高效的跨模态推理，显著提升了金融问答的准确性。

Abstract: Financial documents--such as 10-Ks, 10-Qs, and investor presentations--span
hundreds of pages and combine diverse modalities, including dense narrative
text, structured tables, and complex figures. Answering questions over such
content often requires joint reasoning across modalities, which strains
traditional large language models (LLMs) and retrieval-augmented generation
(RAG) pipelines due to token limitations, layout loss, and fragmented
cross-modal context. We introduce MultiFinRAG, a retrieval-augmented generation
framework purpose-built for financial QA. MultiFinRAG first performs multimodal
extraction by grouping table and figure images into batches and sending them to
a lightweight, quantized open-source multimodal LLM, which produces both
structured JSON outputs and concise textual summaries. These outputs, along
with narrative text, are embedded and indexed with modality-aware similarity
thresholds for precise retrieval. A tiered fallback strategy then dynamically
escalates from text-only to text+table+image contexts when necessary, enabling
cross-modal reasoning while reducing irrelevant context. Despite running on
commodity hardware, MultiFinRAG achieves 19 percentage points higher accuracy
than ChatGPT-4o (free-tier) on complex financial QA tasks involving text,
tables, images, and combined multimodal reasoning.

</details>


### [5] [Uncovering Hidden Violent Tendencies in LLMs: A Demographic Analysis via Behavioral Vignettes](https://arxiv.org/abs/2506.20822)
*Quintin Myers,Yanjun Gao*

Main category: cs.CL

TL;DR: 该研究首次使用社会心理学工具评估大语言模型（LLMs）对日常冲突的反应，发现其暴力倾向存在表面与内部偏好不一致及人口统计学偏差。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs被提议用于检测和应对网络暴力内容，但其在道德模糊现实场景中的推理能力尚未充分研究。

Method: 采用经过验证的社会心理学工具（VBVQ问卷），通过基于人口统计特征的角色提示，在零样本设置下评估六个不同背景开发的LLMs。

Result: 发现：(1) LLMs表面文本生成与内部暴力偏好存在差异；(2) 其暴力倾向在不同人口统计维度上呈现与现有社会科学研究相矛盾的偏差。

Conclusion: LLMs在暴力内容处理上存在潜在偏见，其行为模式与社会科学共识不符，需进一步优化对齐。

Abstract: Large language models (LLMs) are increasingly proposed for detecting and
responding to violent content online, yet their ability to reason about morally
ambiguous, real-world scenarios remains underexamined. We present the first
study to evaluate LLMs using a validated social science instrument designed to
measure human response to everyday conflict, namely the Violent Behavior
Vignette Questionnaire (VBVQ). To assess potential bias, we introduce
persona-based prompting that varies race, age, and geographic identity within
the United States. Six LLMs developed across different geopolitical and
organizational contexts are evaluated under a unified zero-shot setting. Our
study reveals two key findings: (1) LLMs surface-level text generation often
diverges from their internal preference for violent responses; (2) their
violent tendencies vary across demographics, frequently contradicting
established findings in criminology, social science, and psychology.

</details>


### [6] [Decide less, communicate more: On the construct validity of end-to-end fact-checking in medicine](https://arxiv.org/abs/2506.20876)
*Sebastian Joseph,Lily Chen,Barry Wei,Michael Mackert,Iain J. Marshall,Paul Pu Liang,Ramez Kouzy,Byron C. Wallace,Junyi Jessy Li*

Main category: cs.CL

TL;DR: 该研究探讨了自动事实核查系统在医学领域应用不足的原因，揭示了医学事实核查面临的三大挑战，并提出应将其视为交互式沟通问题而非端到端流程。


<details>
  <summary>Details</summary>
Motivation: 由于医学决策的高风险性和医学文献的复杂性，公众往往缺乏足够的医学素养来正确理解医学信息。尽管自动事实核查技术在公共卫生和医学领域的应用潜力巨大，但这些系统在实际中仍未得到广泛使用。

Method: 研究通过首次调查临床专家如何通过综合医学证据来验证社交媒体上的真实医学声明，寻找医学事实核查的上限。

Result: 研究发现医学事实核查面临三大根本性挑战：难以将现实中的声明与临床试验形式的科学证据联系起来；未明确声明的歧义性与意图不匹配；以及真实性标签的主观性。

Conclusion: 研究认为，医学事实核查应被视为一个交互式沟通问题来对待和评估，而非一个端到端的过程。

Abstract: Technological progress has led to concrete advancements in tasks that were
regarded as challenging, such as automatic fact-checking. Interest in adopting
these systems for public health and medicine has grown due to the high-stakes
nature of medical decisions and challenges in critically appraising a vast and
diverse medical literature. Evidence-based medicine connects to every
individual, and yet the nature of it is highly technical, rendering the medical
literacy of majority users inadequate to sufficiently navigate the domain. Such
problems with medical communication ripens the ground for end-to-end
fact-checking agents: check a claim against current medical literature and
return with an evidence-backed verdict. And yet, such systems remain largely
unused. To understand this, we present the first study examining how clinical
experts verify real claims from social media by synthesizing medical evidence.
In searching for this upper-bound, we reveal fundamental challenges in
end-to-end fact-checking when applied to medicine: Difficulties connecting
claims in the wild to scientific evidence in the form of clinical trials;
ambiguities in underspecified claims mixed with mismatched intentions; and
inherently subjective veracity labels. We argue that fact-checking should be
approached and evaluated as an interactive communication problem, rather than
an end-to-end process.

</details>


### [7] [Optimising Language Models for Downstream Tasks: A Post-Training Perspective](https://arxiv.org/abs/2506.20917)
*Zhengyan Shi*

Main category: cs.CL

TL;DR: 该论文提出了一系列方法，旨在更高效、更鲁棒地使语言模型适应下游任务，包括从未标记数据中提取任务相关知识、参数高效的微调方法、改进的监督微调方法以及新的评估基准。


<details>
  <summary>Details</summary>
Motivation: 当前语言模型在适应特定任务时面临挑战，如未充分利用未标记数据、在小规模任务数据上过拟合以及计算成本高，这些问题限制了其在现实语言任务中的应用。

Method: 论文提出了四种方法：1) 从未标记数据中提取任务相关知识的持续预训练技术；2) 参数高效的微调方法，降低计算成本；3) 改进的监督微调方法，提升模型在稀缺标注数据下的表现；4) 开发新的评估方法和基准，如多跳空间推理任务。

Result: 通过广泛的实证研究，这些方法显著提升了语言模型的鲁棒性、效率和泛化能力，使其更适应广泛的应用场景。

Conclusion: 这些进展标志着向更鲁棒、更高效的语言模型迈出了重要一步，为实现人工通用智能的目标更近了一步。

Abstract: Language models (LMs) have demonstrated remarkable capabilities in NLP, yet
adapting them efficiently and robustly to specific tasks remains challenging.
As their scale and complexity grow, fine-tuning LMs on labelled data often
underutilizes available unlabelled data, leads to overfitting on small
task-specific sets, and imposes significant computational costs. These
limitations hamper their application to the open-ended landscape of real-world
language tasks.
  This thesis proposes a series of methods to better adapt LMs to downstream
applications. First, we explore strategies for extracting task-relevant
knowledge from unlabelled data, introducing a novel continued pre-training
technique that outperforms state-of-the-art semi-supervised approaches. Next,
we present a parameter-efficient fine-tuning method that substantially reduces
memory and compute costs while maintaining competitive performance. We also
introduce improved supervised fine-tuning methods that enable LMs to better
follow instructions, especially when labelled data is scarce, enhancing their
performance across a range of NLP tasks, including open-ended generation.
Finally, we develop new evaluation methods and benchmarks, such as multi-hop
spatial reasoning tasks, to assess LM capabilities and adaptation more
comprehensively.
  Through extensive empirical studies across diverse NLP tasks, our results
demonstrate that these approaches substantially improve LM robustness,
efficiency, and generalization, making them more adaptable to a broad range of
applications. These advances mark a significant step towards more robust and
efficient LMs, bringing us closer to the goal of artificial general
intelligence.

</details>


### [8] [FineWeb2: One Pipeline to Scale Them All -- Adapting Pre-Training Data Processing to Every Language](https://arxiv.org/abs/2506.20920)
*Guilherme Penedo,Hynek Kydlíček,Vinko Sabolčec,Bettina Messmer,Negar Foroutan,Amir Hossein Kargaran,Colin Raffel,Martin Jaggi,Leandro Von Werra,Thomas Wolf*

Main category: cs.CL

TL;DR: 本文提出了一种自动适应多语言的预训练数据集构建流程FineWeb2，通过优化过滤和去重方法，提升了非英语语料库的质量，并发布了包含1000多种语言的20TB数据集。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型的预训练需要大量高质量多语言文本数据，但非英语语料的过滤和去重流程难以适配多种语言，导致多语言模型性能受限。

Method: 基于FineWeb设计自动化多语言数据清洗流程，通过九种语言的实验验证方法有效性，并提出考虑重复次数和质量的数据集平衡策略。

Result: 新流程构建的非英语语料库优于现有数据集，最终扩展至1000多种语言，发布包含50亿文档的20TB数据集FineWeb2及相关代码。

Conclusion: 提出的自动化多语言数据处理流程能有效提升模型性能，大规模数据集FineWeb2的发布将促进多语言LLM的发展。

Abstract: Pre-training state-of-the-art large language models (LLMs) requires vast
amounts of clean and diverse text data. While the open development of large
high-quality English pre-training datasets has seen substantial recent
progress, training performant multilingual LLMs remains a challenge, in large
part due to the inherent difficulty of tailoring filtering and deduplication
pipelines to a large number of languages. In this work, we introduce a new
pre-training dataset curation pipeline based on FineWeb that can be
automatically adapted to support any language. We extensively ablate our
pipeline design choices on a set of nine diverse languages, guided by a set of
meaningful and informative evaluation tasks that were chosen through a novel
selection process based on measurable criteria. Ultimately, we show that our
pipeline can be used to create non-English corpora that produce more performant
models than prior datasets. We additionally introduce a straightforward and
principled approach to rebalance datasets that takes into consideration both
duplication count and quality, providing an additional performance uplift.
Finally, we scale our pipeline to over 1000 languages using almost 100 Common
Crawl snapshots to produce FineWeb2, a new 20 terabyte (5 billion document)
multilingual dataset which we release along with our pipeline, training, and
evaluation codebases.

</details>


### [9] [KaLM-Embedding-V2: Superior Training Techniques and Data Inspire A Versatile Embedding Model](https://arxiv.org/abs/2506.20923)
*Xinping Zhao,Xinshuo Hu,Zifei Shan,Shouzheng Huang,Yao Zhou,Zetian Sun,Zhenyu Liu,Dongfang Li,Xinyuan Wei,Qian Chen,Youcheng Pan,Yang Xiang,Meishan Zhang,Haofen Wang,Jun Yu,Baotian Hu,Min Zhang*

Main category: cs.CL

TL;DR: KaLM-Embedding-V2是一种多功能、紧凑的嵌入模型，通过创新的训练技术和数据策略，在通用文本嵌入任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 为了提升文本嵌入模型的性能和泛化能力，同时保持模型的紧凑性，研究者提出了KaLM-Embedding-V2，旨在通过改进架构和训练策略，超越同类模型。

Method: 方法包括：(1) 使用双向Transformer和均值池化生成固定长度嵌入；(2) 多阶段训练流程：预训练、微调和模型参数平均；(3) 引入焦点式重加权机制和在线硬负样本混合策略；(4) 收集大量多样化数据进行训练。

Result: 在MTEB中英文基准测试中，KaLM-Embedding-V2显著优于同类尺寸模型，并与更大模型竞争，为紧凑型嵌入模型设定了新标准。

Conclusion: KaLM-Embedding-V2通过创新的训练策略和架构优化，实现了在紧凑模型尺寸下的卓越性能，为文本嵌入领域提供了高效解决方案。

Abstract: In this paper, we propose KaLM-Embedding-V2, a versatile and compact
embedding model, which achieves impressive performance in general-purpose text
embedding tasks by leveraging superior training techniques and data. Our key
innovations include: (1) To better align the architecture with representation
learning, we remove the causal attention mask and adopt a fully bidirectional
transformer with simple yet effective mean-pooling to produce fixed-length
embeddings; (2) We employ a multi-stage training pipeline: (i) pre-training on
large-scale weakly supervised open-source corpora; (ii) fine-tuning on
high-quality retrieval and non-retrieval datasets; and (iii) model-soup
parameter averaging for robust generalization. Besides, we introduce a
focal-style reweighting mechanism that concentrates learning on difficult
samples and an online hard-negative mixing strategy to continuously enrich hard
negatives without expensive offline mining; (3) We collect over 20 categories
of data for pre-training and 100 categories of data for fine-tuning, to boost
both the performance and generalization of the embedding model. Extensive
evaluations on the Massive Text Embedding Benchmark (MTEB) Chinese and English
show that our model significantly outperforms others of comparable size, and
competes with 3x, 14x, 18x, and 26x larger embedding models, setting a new
standard for a versatile and compact embedding model with less than 1B
parameters.

</details>


### [10] [Can Gradient Descent Simulate Prompting?](https://arxiv.org/abs/2506.20989)
*Eric Zhang,Leshem Choshen,Jacob Andreas*

Main category: cs.CL

TL;DR: 该论文提出了一种通过元训练使梯度更新模拟提示效果的方法，提升了微调的表现。


<details>
  <summary>Details</summary>
Motivation: 研究如何通过梯度更新使语言模型的微调能像提示一样有效，解决微调在泛化和逻辑推理上的不足。

Method: 采用基于梯度的元学习方法，利用语言模型自身的提示预测作为目标，无需真实标签。

Result: 梯度下降训练后，模型在某些任务上表现接近甚至优于提示方法，如反转诅咒任务和文本问答。

Conclusion: 适当的初始化下，梯度下降具有强大表达能力，为长上下文建模和梯度学习泛化能力提供了新思路。

Abstract: There are two primary ways of incorporating new information into a language
model (LM): changing its prompt or changing its parameters, e.g. via
fine-tuning. Parameter updates incur no long-term storage cost for model
changes. However, for many model updates, prompting is significantly more
effective: prompted models can generalize robustly from single examples and
draw logical inferences that do not occur under standard fine-tuning. Can
models be modified so that fine-tuning does emulate prompting? This paper
describes a method for meta-training LMs such that gradient updates emulate the
effects of conditioning on new information. Our approach uses tools from
gradient-based meta-learning but uses an LM's own prompted predictions as
targets, eliminating the need for ground-truth labels. Subsequent gradient
descent training recovers some (and occasionally all) of prompted model
performance -- showing improvement on the ``reversal curse'' tasks, and
answering questions about text passages after a single gradient update. These
results suggest that, with appropriate initialization, gradient descent can be
surprisingly expressive. Our results suggest new avenues for long-context
modeling and offer insight into the generalization capabilities of
gradient-based learning.

</details>


### [11] [SAC: A Framework for Measuring and Inducing Personality Traits in LLMs with Dynamic Intensity Control](https://arxiv.org/abs/2506.20993)
*Adithya Chittem,Aishna Shrivastava,Sai Tarun Pendela,Jagat Sesh Challa,Dhruv Kumar*

Main category: cs.CL

TL;DR: 该论文提出了一种改进大语言模型（LLM）个性表达的方法，通过扩展16PF模型和动态控制特质强度，使LLM的个性表现更细腻和可控。


<details>
  <summary>Details</summary>
Motivation: 现有LLM个性模型多依赖粗糙的'大五'人格框架，且缺乏对特质强度的控制机制，难以实现细腻的个性表达。

Method: 扩展机器性格量表（MPI）至16PF模型，开发特定属性控制（SAC）框架，采用形容词锚定和五维度强度因子进行动态调控。

Result: 连续强度调控比二元切换更稳定，目标特质变化会系统性影响相关特质，表明LLM具有多维人格结构的内在表征。

Conclusion: 该方法为医疗、教育等领域提供了更拟人化的人机交互可能，推动社交机器人的发展。

Abstract: Large language models (LLMs) have gained significant traction across a wide
range of fields in recent years. There is also a growing expectation for them
to display human-like personalities during interactions. To meet this
expectation, numerous studies have proposed methods for modelling LLM
personalities through psychometric evaluations. However, most existing models
face two major limitations: they rely on the Big Five (OCEAN) framework, which
only provides coarse personality dimensions, and they lack mechanisms for
controlling trait intensity. In this paper, we address this gap by extending
the Machine Personality Inventory (MPI), which originally used the Big Five
model, to incorporate the 16 Personality Factor (16PF) model, allowing
expressive control over sixteen distinct traits. We also developed a structured
framework known as Specific Attribute Control (SAC) for evaluating and
dynamically inducing trait intensity in LLMs. Our method introduces
adjective-based semantic anchoring to guide trait intensity expression and
leverages behavioural questions across five intensity factors:
\textit{Frequency}, \textit{Depth}, \textit{Threshold}, \textit{Effort}, and
\textit{Willingness}. Through experimentation, we find that modelling intensity
as a continuous spectrum yields substantially more consistent and controllable
personality expression compared to binary trait toggling. Moreover, we observe
that changes in target trait intensity systematically influence closely related
traits in psychologically coherent directions, suggesting that LLMs internalize
multi-dimensional personality structures rather than treating traits in
isolation. Our work opens new pathways for controlled and nuanced human-machine
interactions in domains such as healthcare, education, and interviewing
processes, bringing us one step closer to truly human-like social machines.

</details>


### [12] [Large Language Models Acing Chartered Accountancy](https://arxiv.org/abs/2506.21031)
*Jatin Gupta,Akhil Sharma,Saransh Singhania,Mohammad Adnan,Sakshi Deo,Ali Imam Abidi,Keshav Gupta*

Main category: cs.CL

TL;DR: 论文介绍了CA-Ben基准测试，评估了六种大型语言模型在印度特许会计师考试内容上的表现，发现Claude 3.5 Sonnet和GPT-4o表现最佳，但在数值计算和法律解释方面仍有挑战。


<details>
  <summary>Details</summary>
Motivation: 研究动机是评估大型语言模型在特定金融领域知识（如印度特许会计师考试内容）上的应用能力，填补现有研究的空白。

Method: 方法是通过CA-Ben基准测试，使用印度特许会计师协会的考试题目构建数据集，对六种主流大型语言模型进行标准化评估。

Result: 结果显示，Claude 3.5 Sonnet和GPT-4o在概念和法律推理方面表现最优，但在数值计算和法律解释方面存在明显不足。

Conclusion: 结论指出当前大型语言模型在金融领域的优势与局限，建议未来通过混合推理和检索增强生成方法改进定量分析和法律解释能力。

Abstract: Advanced intelligent systems, particularly Large Language Models (LLMs), are
significantly reshaping financial practices through advancements in Natural
Language Processing (NLP). However, the extent to which these models
effectively capture and apply domain-specific financial knowledge remains
uncertain. Addressing a critical gap in the expansive Indian financial context,
this paper introduces CA-Ben, a Chartered Accountancy benchmark specifically
designed to evaluate the financial, legal, and quantitative reasoning
capabilities of LLMs. CA-Ben comprises structured question-answer datasets
derived from the rigorous examinations conducted by the Institute of Chartered
Accountants of India (ICAI), spanning foundational, intermediate, and advanced
CA curriculum stages. Six prominent LLMs i.e. GPT 4o, LLAMA 3.3 70B, LLAMA 3.1
405B, MISTRAL Large, Claude 3.5 Sonnet, and Microsoft Phi 4 were evaluated
using standardized protocols. Results indicate variations in performance, with
Claude 3.5 Sonnet and GPT-4o outperforming others, especially in conceptual and
legal reasoning. Notable challenges emerged in numerical computations and legal
interpretations. The findings emphasize the strengths and limitations of
current LLMs, suggesting future improvements through hybrid reasoning and
retrieval-augmented generation methods, particularly for quantitative analysis
and accurate legal interpretation.

</details>


### [13] [A Semi-supervised Scalable Unified Framework for E-commerce Query Classification](https://arxiv.org/abs/2506.21049)
*Chunyuan Yuan,Chong Zhang,Zheng Fang,Ming Pang,Xue Jiang,Changping Peng,Zhangang Lin,Ching Law*

Main category: cs.CL

TL;DR: 该论文提出了一种半监督可扩展的统一框架（SSUF），用于解决电商查询分类中的信息不足和标签依赖问题，通过知识增强、标签增强和结构增强模块提升分类效果。


<details>
  <summary>Details</summary>
Motivation: 电商查询通常简短且缺乏上下文，现有方法依赖用户点击行为构建训练样本，导致马太效应，且缺乏统一框架，算法优化效率低。

Method: SSUF框架包含知识增强模块（利用世界知识增强查询表示）、标签增强模块（利用标签语义和半监督信号减少对后验标签的依赖）和结构增强模块（基于复杂标签关系增强标签表示），各模块高度可插拔。

Result: 离线和在线A/B实验表明，SSUF显著优于现有最先进模型。

Conclusion: SSUF通过统一框架和增强模块有效解决了电商查询分类中的信息不足和标签依赖问题，提升了分类性能。

Abstract: Query classification, including multiple subtasks such as intent and category
prediction, is vital to e-commerce applications. E-commerce queries are usually
short and lack context, and the information between labels cannot be used,
resulting in insufficient prior information for modeling. Most existing
industrial query classification methods rely on users' posterior click behavior
to construct training samples, resulting in a Matthew vicious cycle.
Furthermore, the subtasks of query classification lack a unified framework,
leading to low efficiency for algorithm optimization.
  In this paper, we propose a novel Semi-supervised Scalable Unified Framework
(SSUF), containing multiple enhanced modules to unify the query classification
tasks. The knowledge-enhanced module uses world knowledge to enhance query
representations and solve the problem of insufficient query information. The
label-enhanced module uses label semantics and semi-supervised signals to
reduce the dependence on posterior labels. The structure-enhanced module
enhances the label representation based on the complex label relations. Each
module is highly pluggable, and input features can be added or removed as
needed according to each subtask. We conduct extensive offline and online A/B
experiments, and the results show that SSUF significantly outperforms the
state-of-the-art models.

</details>


### [14] [MT2-CSD: A New Dataset and Multi-Semantic Knowledge Fusion Method for Conversational Stance Detection](https://arxiv.org/abs/2506.21053)
*Fuqiang Niu,Genan Dai,Yisha Lu,Jiayu Liao,Xiang Li,Hu Huang,Bowen Zhang*

Main category: cs.CL

TL;DR: 该论文提出了一个名为MT2-CSD的大规模数据集和LLM-CRAN模型，用于改进社交媒体中的多目标、多轮对话立场检测。


<details>
  <summary>Details</summary>
Motivation: 传统立场检测研究多针对单一样本，难以模拟真实社交媒体中的多方讨论场景，主要原因是缺乏真实反映社交媒体互动动态的数据集。

Method: 论文引入MT2-CSD数据集，并提出LLM-CRAN模型，利用大语言模型的推理能力增强对话理解。

Result: 实验结果表明，LLM-CRAN在MT2-CSD数据集上的表现显著优于基线模型。

Conclusion: MT2-CSD数据集和LLM-CRAN模型为对话立场检测提供了新的挑战和解决方案。

Abstract: In the realm of contemporary social media, automatic stance detection is
pivotal for opinion mining, as it synthesizes and examines user perspectives on
contentious topics to uncover prevailing trends and sentiments. Traditional
stance detection research often targets individual instances, thereby limiting
its capacity to model multi-party discussions typical in real social media
scenarios. This shortcoming largely stems from the scarcity of datasets that
authentically capture the dynamics of social media interactions, hindering
advancements in conversational stance detection. In this paper, we introduce
MT2-CSD, a comprehensive dataset for multi-target, multi-turn conversational
stance detection. To the best of our knowledge, MT2-CSD is the largest dataset
available for this purpose, comprising 24,457 annotated instances and
exhibiting the greatest conversational depth, thereby presenting new challenges
for stance detection. To address these challenges, we propose the Large
Language model enhanced Conversational Relational Attention Network (LLM-CRAN),
which exploits the reasoning capabilities of LLMs to improve conversational
understanding. We conduct extensive experiments to evaluate the efficacy of
LLM-CRAN on the MT2-CSD dataset. The experimental results indicate that
LLM-CRAN significantly outperforms strong baseline models in the task of
conversational stance detection.

</details>


### [15] [DALR: Dual-level Alignment Learning for Multimodal Sentence Representation Learning](https://arxiv.org/abs/2506.21096)
*Kang He,Yuzhe Ding. Haining Wang,Fei Li,Chong Teng,Donghong Ji*

Main category: cs.CL

TL;DR: 本文提出DALR方法，通过双层次对齐学习解决多模态句子表示中的跨模态偏差和模态内语义分歧问题，提升了表示质量。


<details>
  <summary>Details</summary>
Motivation: 现有方法在图像与文本对齐上较为粗糙，存在跨模态偏差和模态内语义分歧问题，影响了句子表示的质量。

Method: 提出DALR方法，包含一致性学习模块实现细粒度跨模态对齐，并结合排序蒸馏与全局模态内对齐学习来捕捉句子间的复杂关系。

Result: 在语义文本相似性（STS）和迁移（TR）任务上的实验验证了DALR的有效性，其性能优于现有基线方法。

Conclusion: DALR通过双层次对齐学习有效提升了多模态句子表示的质量，实验证明了其优越性。

Abstract: Previous multimodal sentence representation learning methods have achieved
impressive performance. However, most approaches focus on aligning images and
text at a coarse level, facing two critical challenges:cross-modal misalignment
bias and intra-modal semantic divergence, which significantly degrade sentence
representation quality. To address these challenges, we propose DALR
(Dual-level Alignment Learning for Multimodal Sentence Representation). For
cross-modal alignment, we propose a consistency learning module that softens
negative samples and utilizes semantic similarity from an auxiliary task to
achieve fine-grained cross-modal alignment. Additionally, we contend that
sentence relationships go beyond binary positive-negative labels, exhibiting a
more intricate ranking structure. To better capture these relationships and
enhance representation quality, we integrate ranking distillation with global
intra-modal alignment learning. Comprehensive experiments on semantic textual
similarity (STS) and transfer (TR) tasks validate the effectiveness of our
approach, consistently demonstrating its superiority over state-of-the-art
baselines.

</details>


### [16] [ComRAG: Retrieval-Augmented Generation with Dynamic Vector Stores for Real-time Community Question Answering in Industry](https://arxiv.org/abs/2506.21098)
*Qinwen Chen,Wenbiao Tao,Zhiwei Zhu,Mingfan Xi,Liangzhong Guo,Yuan Wang,Wei Wang,Yunshi Lan*

Main category: cs.CL

TL;DR: ComRAG框架通过结合静态知识和动态历史QA对，提升了工业CQA平台的实时性能和效率。


<details>
  <summary>Details</summary>
Motivation: 社区问答平台作为重要知识库，现有方法未能充分利用外部知识和动态历史交互，且缺乏适合工业部署的记忆机制。

Method: 提出ComRAG框架，采用基于质心的记忆机制整合静态知识与动态历史QA对，优化检索、生成和存储效率。

Result: 在三个工业CQA数据集上，ComRAG在向量相似度提升25.9%、延迟降低8.7%-23.3%、存储块增长率从20.23%降至2.06%。

Conclusion: ComRAG显著提升了工业CQA的实时性能与效率，验证了其框架设计的有效性。

Abstract: Community Question Answering (CQA) platforms can be deemed as important
knowledge bases in community, but effectively leveraging historical
interactions and domain knowledge in real-time remains a challenge. Existing
methods often underutilize external knowledge, fail to incorporate dynamic
historical QA context, or lack memory mechanisms suited for industrial
deployment. We propose ComRAG, a retrieval-augmented generation framework for
real-time industrial CQA that integrates static knowledge with dynamic
historical QA pairs via a centroid-based memory mechanism designed for
retrieval, generation, and efficient storage. Evaluated on three industrial CQA
datasets, ComRAG consistently outperforms all baselines--achieving up to 25.9%
improvement in vector similarity, reducing latency by 8.7% to 23.3%, and
lowering chunk growth from 20.23% to 2.06% over iterations.

</details>


### [17] [Progtuning: Progressive Fine-tuning Framework for Transformer-based Language Models](https://arxiv.org/abs/2506.21119)
*Xiaoshuang Ji,Zhendong Zhao,Xiaojun Chen,Xin Zhao,Zeyao Liu*

Main category: cs.CL

TL;DR: Progtuning是一种结合渐进学习的新型微调框架，通过逐步减少更新的Transformer块数量，优化资源分配并减少约25%的更新参数，同时保持竞争力。


<details>
  <summary>Details</summary>
Motivation: 随着模型规模增大，全参数微调成本高昂。现有参数高效微调方法仍需更新相同数量的参数，忽略了Transformer块间贡献不均的问题，导致计算资源分配低效。

Method: 提出Progtuning框架，基于贡献度逐步减少更新的Transformer块数量，实现资源优化分配。

Result: Progtuning减少约25%的更新参数，仍保持竞争力，且与参数高效微调方法兼容，在多种适应场景中表现优异。

Conclusion: Progtuning通过渐进学习有效优化微调过程，显著降低计算成本，同时保持模型性能，具有广泛适应性。

Abstract: Fine-tuning is a promising technique for leveraging Transformer-based
language models in downstream tasks. As model sizes continue to grow, updating
all model parameters becomes increasingly costly. Parameter-efficient
fine-tuning methods effectively address this issue by selectively updating a
small subset of parameters. However, fine-tuning and most existing
parameter-efficient fine-tuning methods require updating the same number of
parameters as the initial size, ignoring the unequal contribution across
Transformer blocks and leading to extremely inefficient allocation of computing
resources. In this paper, we propose Progtuning, the novel fine-tuning
framework combined with progressive learning for Transformer-based language
models. Specifically, Progtuning progressively reduces the number of updated
transformer blocks based on the contribution. Remarkably, Progtuning optimizes
resource allocation and reduces the number of updated parameters by
approximately 25\%, while still maintaining competitive performance. And it
also exhibits high adaptability with parameter-efficient fine-tuning methods,
demonstrating excellent performance across various adaptation scenarios.

</details>


### [18] [Compressed and Smooth Latent Space for Text Diffusion Modeling](https://arxiv.org/abs/2506.21170)
*Viacheslav Meshchaninov,Egor Chimbulatov,Alexander Shabalin,Aleksandr Abramov,Dmitry Vetrov*

Main category: cs.CL

TL;DR: Cosmos提出了一种基于压缩潜在空间的扩散模型文本生成方法，在保持生成质量的同时显著提升推理速度。


<details>
  <summary>Details</summary>
Motivation: 自回归语言模型存在解码速度慢和全局连贯性差的问题，扩散模型虽具并行生成优势，但受限于词元级表示的高维度。

Method: 通过同时训练自编码器学习压缩潜在空间，结合预训练语言编码器的冻结激活实现语义对齐，支持基于扰动的数据增强。

Result: 在故事生成等4项任务中，Cosmos以8倍压缩率实现媲美词元级扩散模型的质量，推理速度提升2倍以上。

Conclusion: Cosmos证明了潜在空间压缩对文本扩散模型的有效性，为高质量并行文本生成提供了新范式。

Abstract: Autoregressive language models dominate modern text generation, yet their
sequential nature introduces fundamental limitations: decoding is slow, and
maintaining global coherence remains challenging. Diffusion models offer a
promising alternative by enabling parallel generation and flexible control;
however, their application to text generation is hindered by the high
dimensionality of token-level representations. We introduce Cosmos, a novel
approach to text generation that operates entirely in a compressed, smooth
latent space tailored specifically for diffusion. This space is learned using
an autoencoder trained simultaneously for token-level reconstruction and
alignment with frozen activations from a pretrained language encoder, providing
robust semantic grounding and enabling effective perturbation-based
augmentations. Empirically, we demonstrate that text representations can be
compressed by $8\times$ while maintaining generation quality comparable to
token-level diffusion models. Furthermore, increasing the latent sequence
length allows Cosmos to surpass both diffusion-based and autoregressive
baselines. We evaluate Cosmos on four diverse generative tasks including story
generation, question generation, summarization, and detoxification and compare
it with various generative paradigms. Cosmos achieves comparable or superior
generation quality while offering more than $2\times$ faster inference.

</details>


### [19] [CBF-AFA: Chunk-Based Multi-SSL Fusion for Automatic Fluency Assessment](https://arxiv.org/abs/2506.20243)
*Papa Séga Wade,Mihai Andries,Ioannis Kanellos,Thierry Moudenc*

Main category: cs.CL

TL;DR: 提出一种基于分块的自监督学习融合方法，通过CNN-BiLSTM框架提升非母语者口语流利度评估的准确性。


<details>
  <summary>Details</summary>
Motivation: 自动流利度评估（AFA）在捕捉非母语者语音节奏、停顿和不流畅方面仍具挑战性，需更精细的时序分析和多特征融合方法。

Method: 结合Wav2Vec2/HuBERT/WavLM的自监督模型，使用Silero-VAD分块，加权融合声学/语言特征，并加入分块级流利度标记，通过CNN-BiLSTM建模局部和长期依赖。

Result: 在Speechocean762和Avalinguo数据集上，F1分数分别提升2.8和4.2，Pearson相关系数提升6.2和4.0，超越单模型和Pyannote.audio基线。

Conclusion: 分块多模型融合能有效评估流利度，但需进一步研究对不规则韵律方言的泛化能力。

Abstract: Automatic fluency assessment (AFA) remains challenging, particularly in
capturing speech rhythm, pauses, and disfluencies in non-native speakers. We
introduce a chunk-based approach integrating self-supervised learning (SSL)
models (Wav2Vec2, HuBERT, and WavLM) selected for their complementary strengths
in phonetic, prosodic, and noisy speech modeling, with a hierarchical
CNN-BiLSTM framework. Speech is segmented into breath-group chunks using Silero
voice activity detection (Silero-VAD), enabling fine-grained temporal analysis
while mitigating over-segmentation artifacts. SSL embeddings are fused via a
learnable weighted mechanism, balancing acoustic and linguistic features, and
enriched with chunk-level fluency markers (e.g., speech rate, pause durations,
n-gram repetitions). The CNN-BiLSTM captures local and long-term dependencies
across chunks. Evaluated on Avalinguo and Speechocean762, our approach improves
F1-score by 2.8 and Pearson correlation by 6.2 points over single SSL baselines
on Speechocean762, with gains of 4.2 F1-score and 4.0 Pearson points on
Avalinguo, surpassing Pyannote.audio-based segmentation baselines. These
findings highlight chunk-based multi-SSL fusion for robust fluency evaluation,
though future work should explore generalization to dialects with irregular
prosody.

</details>


### [20] [Maintaining MTEB: Towards Long Term Usability and Reproducibility of Embedding Benchmarks](https://arxiv.org/abs/2506.21182)
*Isaac Chung,Imene Kerboua,Marton Kardos,Roman Solomatin,Kenneth Enevoldsen*

Main category: cs.CL

TL;DR: 本文讨论了如何通过工程实践确保MTEB基准测试的可重现性和可扩展性，包括持续集成、数据集验证和社区贡献处理。


<details>
  <summary>Details</summary>
Motivation: MTEB已成为文本嵌入模型的标准评估平台，但需要确保其持续的可重现性和可扩展性，以保持其在领域内的相关性和质量。

Method: 通过维护稳健的持续集成流程、自动化测试执行、评估结果的泛化能力，并设计增强可重现性和可用性的方案。

Result: 这些工程实践成功扩展了MTEB，使其更全面，同时保持了质量和相关性。

Conclusion: 本文的经验为面临类似挑战的基准测试维护者提供了有价值的见解，特别是在确保机器学习评估框架的可重现性和可用性方面。

Abstract: The Massive Text Embedding Benchmark (MTEB) has become a standard evaluation
platform for text embedding models. While previous work has established the
core benchmark methodology, this paper focuses on the engineering aspects that
ensure MTEB's continued reproducibility and extensibility. We present our
approach to maintaining robust continuous integration pipelines that validate
dataset integrity, automate test execution, and assess benchmark results'
generalizability. We detail the design choices that collectively enhance
reproducibility and usability. Furthermore, we discuss our strategies for
handling community contributions and extending the benchmark with new tasks and
datasets. These engineering practices have been instrumental in scaling MTEB to
become more comprehensive while maintaining quality and, ultimately, relevance
to the field. Our experiences offer valuable insights for benchmark maintainers
facing similar challenges in ensuring reproducibility and usability in machine
learning evaluation frameworks. The MTEB repository is available at:
https://github.com/embeddings-benchmark/mteb

</details>


### [21] [Prompt-Guided Turn-Taking Prediction](https://arxiv.org/abs/2506.21191)
*Koji Inoue,Mikey Elmers,Yahui Fu,Zi Haur Pang,Divesh Lala,Keiko Ochi,Tatsuya Kawahara*

Main category: cs.CL

TL;DR: 提出了一种通过文本提示动态控制对话轮换预测的新模型，基于Transformer架构，实验证明其提高了预测准确性并能根据提示调整轮换时机。


<details>
  <summary>Details</summary>
Motivation: 现有的对话轮换预测模型缺乏直观和显式的控制方式，无法动态适应不同对话伙伴和情境。

Method: 基于Transformer的语音活动投影（VAP）模型，将文本提示嵌入到通道和跨通道Transformer中，利用大语言模型生成合成提示数据。

Result: 模型在950多小时的人-人对话数据上验证，预测准确性提高，并能根据文本提示有效调整轮换时机。

Conclusion: 该模型通过文本提示实现了对话轮换的动态控制，为对话系统和机器人提供了更灵活的交互方式。

Abstract: Turn-taking prediction models are essential components in spoken dialogue
systems and conversational robots. Recent approaches leverage transformer-based
architectures to predict speech activity continuously and in real-time. In this
study, we propose a novel model that enables turn-taking prediction to be
dynamically controlled via textual prompts. This approach allows intuitive and
explicit control through instructions such as "faster" or "calmer" adapting
dynamically to conversational partners and contexts. The proposed model builds
upon a transformer-based voice activity projection (VAP) model, incorporating
textual prompt embeddings into both channel-wise transformers and a
cross-channel transformer. We evaluated the feasibility of our approach using
over 950 hours of human-human spoken dialogue data. Since textual prompt data
for the proposed approach was not available in existing datasets, we utilized a
large language model (LLM) to generate synthetic prompt sentences. Experimental
results demonstrated that the proposed model improved prediction accuracy and
effectively varied turn-taking timing behaviors according to the textual
prompts.

</details>


### [22] [Enhancing Automatic Term Extraction with Large Language Models via Syntactic Retrieval](https://arxiv.org/abs/2506.21222)
*Yongchan Chun,Minhyuk Kim,Dongjun Kim,Chanjun Park,Heuiseok Lim*

Main category: cs.CL

TL;DR: 该论文提出了一种基于检索的提示策略，利用句法相似性在少样本设置下进行自动术语抽取（ATE），并在多个专业基准测试中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLM）在多种自然语言处理任务中表现出色，但其在自动术语抽取（ATE）中的应用尚未充分探索。论文旨在探索如何利用LLM提升ATE任务的表现。

Method: 论文提出了一种基于检索的提示策略，通过句法相似性而非语义相似性选择少样本示例，这种方法与领域无关，并能更可靠地指导术语边界的识别。

Result: 在三个专业ATE基准测试上的实验表明，句法检索方法提高了F1分数，尤其是在查询句子与检索示例之间存在词汇重叠时表现更佳。

Conclusion: 研究结果表明，在将LLM应用于术语抽取任务时，句法线索的重要性不可忽视，基于句法的检索方法能够有效提升性能。

Abstract: Automatic Term Extraction (ATE) identifies domain-specific expressions that
are crucial for downstream tasks such as machine translation and information
retrieval. Although large language models (LLMs) have significantly advanced
various NLP tasks, their potential for ATE has scarcely been examined. We
propose a retrieval-based prompting strategy that, in the few-shot setting,
selects demonstrations according to \emph{syntactic} rather than semantic
similarity. This syntactic retrieval method is domain-agnostic and provides
more reliable guidance for capturing term boundaries. We evaluate the approach
in both in-domain and cross-domain settings, analyzing how lexical overlap
between the query sentence and its retrieved examples affects performance.
Experiments on three specialized ATE benchmarks show that syntactic retrieval
improves F1-score. These findings highlight the importance of syntactic cues
when adapting LLMs to terminology-extraction tasks.

</details>


### [23] [Agent-RewardBench: Towards a Unified Benchmark for Reward Modeling across Perception, Planning, and Safety in Real-World Multimodal Agents](https://arxiv.org/abs/2506.21252)
*Tianyi Men,Zhuoran Jin,Pengfei Cao,Yubo Chen,Kang Liu,Jun Zhao*

Main category: cs.CL

TL;DR: 该论文提出了Agent-RewardBench，一个用于评估多模态大语言模型（MLLMs）奖励建模能力的基准，以解决现有代理在自我纠正和泛化方面的不足。


<details>
  <summary>Details</summary>
Motivation: 随着多模态大语言模型的发展，多模态代理在现实任务中展现出潜力，但由于缺乏外部反馈，这些代理在自我纠正和泛化方面存在困难。因此，需要建立一个针对代理的奖励基准来指导奖励模型的选择。

Method: 论文提出了Agent-RewardBench，该基准具有三个关键特点：（1）多维度评估和真实场景覆盖；（2）步骤级奖励评估；（3）适当难度和高质量数据。

Result: 实验表明，即使是目前最先进的多模态模型在奖励建模方面的表现也有限，凸显了进行专门训练的必要性。

Conclusion: Agent-RewardBench为评估和改进多模态代理的奖励建模能力提供了一个有效的工具，并指出了未来研究的方向。

Abstract: As Multimodal Large Language Models (MLLMs) advance, multimodal agents show
promise in real-world tasks like web navigation and embodied intelligence.
However, due to limitations in a lack of external feedback, these agents
struggle with self-correction and generalization. A promising approach is to
use reward models as external feedback, but there is no clear on how to select
reward models for agents. Thus, there is an urgent need to build a reward bench
targeted at agents. To address these challenges, we propose Agent-RewardBench,
a benchmark designed to evaluate reward modeling ability in MLLMs. The
benchmark is characterized by three key features: (1) Multiple dimensions and
real-world agent scenarios evaluation. It covers perception, planning, and
safety with 7 scenarios; (2) Step-level reward evaluation. It allows for the
assessment of agent capabilities at the individual steps of a task, providing a
more granular view of performance during the planning process; and (3)
Appropriately difficulty and high-quality. We carefully sample from 10 diverse
models, difficulty control to maintain task challenges, and manual verification
to ensure the integrity of the data. Experiments demonstrate that even
state-of-the-art multimodal models show limited performance, highlighting the
need for specialized training in agent reward modeling. Code is available at
github.

</details>


### [24] [Cat and Mouse -- Can Fake Text Generation Outpace Detector Systems?](https://arxiv.org/abs/2506.21274)
*Andrea McGlinchey,Peter J Barclay*

Main category: cs.CL

TL;DR: 大型语言模型能生成逼真的假文本，但研究发现简单分类器仍能有效检测，且模型欺骗性提升可能趋于平缓。


<details>
  <summary>Details</summary>
Motivation: 探讨大型语言模型生成假文本的能力是否会超越检测器的识别能力，以及这种对抗是否会持续升级。

Method: 通过统计分类器检测古典侦探小说风格的假文本，比较不同版本模型（如Gemini和GPT）的欺骗性变化。

Result: Gemini在0.5版本升级后欺骗性增强，而GPT未表现类似趋势，表明检测假文本仍具可行性。

Conclusion: 即使模型规模扩大，可靠检测假文本仍可能实现，但新架构可能提升模型欺骗性。

Abstract: Large language models can produce convincing "fake text" in domains such as
academic writing, product reviews, and political news. Many approaches have
been investigated for the detection of artificially generated text. While this
may seem to presage an endless "arms race", we note that newer LLMs use ever
more parameters, training data, and energy, while relatively simple classifiers
demonstrate a good level of detection accuracy with modest resources. To
approach the question of whether the models' ability to beat the detectors may
therefore reach a plateau, we examine the ability of statistical classifiers to
identify "fake text" in the style of classical detective fiction. Over a 0.5
version increase, we found that Gemini showed an increased ability to generate
deceptive text, while GPT did not. This suggests that reliable detection of
fake text may remain feasible even for ever-larger models, though new model
architectures may improve their deceptiveness

</details>


### [25] [Double-Checker: Enhancing Reasoning of Slow-Thinking LLMs via Self-Critical Fine-Tuning](https://arxiv.org/abs/2506.21285)
*Xin Xu,Tianhao Chen,Fan Zhang,Wanlong Liu,Pengxiang Li,Ajay Kumar Jaiswal,Yuchen Yan,Jishan Hu,Yang Wang,Hao Chen,Shiwei Liu,Shizhe Diao,Can Yang,Lu Yin*

Main category: cs.CL

TL;DR: Double-Checker框架通过自我批判和迭代优化提升大语言模型的推理能力，显著提高AIME基准测试性能。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在生成信息性批判和优化解决方案方面能力有限，需要一种方法来增强其自我批判和迭代优化的能力。

Method: 通过微调1,730个自我批判实例，Double-Checker框架使大语言模型能够在推理过程中迭代批判和优化其输出。

Result: Double-Checker将AIME基准测试的pass@1性能从4.4%提升至18.2%，显著增强了大语言模型的推理能力。

Conclusion: Double-Checker框架为开发更具信任和有效性的大语言模型提供了一种有前景的方向。

Abstract: While slow-thinking large language models (LLMs) exhibit reflection-like
reasoning, commonly referred to as the "aha moment:, their ability to generate
informative critiques and refine prior solutions remains limited. In this
paper, we introduce Double-Checker, a principled framework designed to enhance
the reasoning capabilities of slow-thinking LLMs by fostering explicit
self-critique and iterative refinement of their previous solutions. By
fine-tuning on our curated 1,730 self-critical instances, Double-Checker
empowers long-CoT LLMs to iteratively critique and refine their outputs during
inference until they evaluate their solutions as correct under self-generated
critiques. We validate the efficacy of Double-Checker across a comprehensive
suite of reasoning benchmarks, demonstrating that iterative self-critique
significantly enhances the reasoning capabilities of long-CoT LLMs. Notably,
our Double-Checker increases the pass@1 performance on challenging AIME
benchmarks from 4.4% to 18.2% compared to the original long-CoT LLMs. These
results highlight a promising direction for developing more trustworthy and
effective LLMs capable of structured self-critique.

</details>


### [26] [Small Encoders Can Rival Large Decoders in Detecting Groundedness](https://arxiv.org/abs/2506.21288)
*Istabrak Abbes,Gabriele Prato,Quentin Fournier,Fernando Rodriguez,Alaa Boukhary,Adam Elwood,Sarath Chandar*

Main category: cs.CL

TL;DR: 该论文提出了一种轻量级方法，用于检测查询是否基于给定文档，以减少大语言模型生成答案时的资源消耗和延迟。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）在缺乏上下文信息时容易产生不可靠的回答，影响事实一致性和可信度。因此，需要在生成答案前检测查询是否基于提供的文档。

Method: 使用轻量级的编码器模型（如RoBERTa和NomicBERT），在精选数据集上进行微调，以检测查询的groundedness。

Result: 这些轻量级模型在groundedness检测上的准确性与最先进的LLMs（如Llama3 8B和GPT4o）相当，同时显著降低了推理延迟。

Conclusion: 轻量级模型可以有效检测查询的groundedness，减少资源消耗和延迟，提升大语言模型的效率和可靠性。

Abstract: Augmenting large language models (LLMs) with external context significantly
improves their performance in natural language processing (NLP) tasks. However,
LLMs struggle to answer queries reliably when the provided context lacks
information, often resorting to ungrounded speculation or internal knowledge.
Groundedness - generating responses strictly supported by the context - is
essential for ensuring factual consistency and trustworthiness. This study
focuses on detecting whether a given query is grounded in a document provided
in context before the costly answer generation by LLMs. Such a detection
mechanism can significantly reduce both inference time and resource
consumption. We show that lightweight, task specific encoder models such as
RoBERTa and NomicBERT, fine-tuned on curated datasets, can achieve accuracy
comparable to state-of-the-art LLMs, such as Llama3 8B and GPT4o, in
groundedness detection while reducing inference latency by orders of magnitude.
The code is available at : https://github.com/chandarlab/Hallucinate-less

</details>


### [27] [Detecting Referring Expressions in Visually Grounded Dialogue with Autoregressive Language Models](https://arxiv.org/abs/2506.21294)
*Bram Willemsen,Gabriel Skantze*

Main category: cs.CL

TL;DR: 本文探讨了仅使用文本的自回归语言模型在视觉对话中指代表达抽取任务中的有效性，发现语言上下文对任务至关重要，但指出该任务本质上是多模态的。


<details>
  <summary>Details</summary>
Motivation: 研究目的是探索仅依赖语言上下文（而非视觉信息）能否有效检测视觉对话中具有视觉指涉物的提及表达。

Method: 采用预训练大语言模型（LLM），通过参数高效微调进行粗粒度标注，利用下一词预测划分指代表达的文本边界。

Result: 实验表明：即使使用中等规模LLM和小数据集，纯文本方法仍能有效工作，凸显语言上下文的重要性。

Conclusion: 虽然纯文本方法有效，但该任务本质是多模态问题，单模态方法存在根本性局限。

Abstract: In this paper, we explore the use of a text-only, autoregressive language
modeling approach for the extraction of referring expressions from visually
grounded dialogue. More specifically, the aim is to investigate the extent to
which the linguistic context alone can inform the detection of mentions that
have a (visually perceivable) referent in the visual context of the
conversation. To this end, we adapt a pretrained large language model (LLM) to
perform a relatively course-grained annotation of mention spans in unfolding
conversations by demarcating mention span boundaries in text via next-token
prediction. Our findings indicate that even when using a moderately sized LLM,
relatively small datasets, and parameter-efficient fine-tuning, a text-only
approach can be effective, highlighting the relative importance of the
linguistic context for this task. Nevertheless, we argue that the task
represents an inherently multimodal problem and discuss limitations fundamental
to unimodal approaches.

</details>


### [28] [Structuralist Approach to AI Literary Criticism: Leveraging Greimas Semiotic Square for Large Language Models](https://arxiv.org/abs/2506.21360)
*Fangzhou Dong,Yifan Zeng,Yingpeng Sang,Hong Shen*

Main category: cs.CL

TL;DR: 该论文提出GLASS框架，基于格雷马斯符号学方阵，提升大语言模型在深度文学分析中的能力，并创建首个相关数据集。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在理解和生成文本方面表现出色，但在对思想深刻、叙事复杂的作品提供专业文学批评方面存在困难。

Method: 提出GLASS框架，基于格雷马斯符号学方阵（GSS），构建结构化分析框架，并创建首个GSS文学批评数据集，包含48部作品的详细分析。

Result: GLASS框架在多个作品和大语言模型上的表现与专家批评相比显示出高性能，并应用于39部经典作品，产生原创且高质量的分析。

Conclusion: 该研究为文学研究和教育提供了基于AI的工具，揭示了文学参与的认知机制。

Abstract: Large Language Models (LLMs) excel in understanding and generating text but
struggle with providing professional literary criticism for works with profound
thoughts and complex narratives. This paper proposes GLASS (Greimas Literary
Analysis via Semiotic Square), a structured analytical framework based on
Greimas Semiotic Square (GSS), to enhance LLMs' ability to conduct in-depth
literary analysis. GLASS facilitates the rapid dissection of narrative
structures and deep meanings in narrative works. We propose the first dataset
for GSS-based literary criticism, featuring detailed analyses of 48 works. Then
we propose quantitative metrics for GSS-based literary criticism using the
LLM-as-a-judge paradigm. Our framework's results, compared with expert
criticism across multiple works and LLMs, show high performance. Finally, we
applied GLASS to 39 classic works, producing original and high-quality analyses
that address existing research gaps. This research provides an AI-based tool
for literary research and education, offering insights into the cognitive
mechanisms underlying literary engagement.

</details>


### [29] [Leveraging LLM-Assisted Query Understanding for Live Retrieval-Augmented Generation](https://arxiv.org/abs/2506.21384)
*Guanting Dong,Xiaoxi Li,Yuyao Zhang,Mengjie Deng*

Main category: cs.CL

TL;DR: Omni-RAG是一个新颖的框架，旨在提升实时开放域环境中检索增强生成（RAG）系统的鲁棒性和有效性，通过LLM辅助的查询理解和多模块处理复杂、嘈杂的查询。


<details>
  <summary>Details</summary>
Motivation: 现实中的RAG系统在处理嘈杂、模糊且包含多意图的用户查询时面临挑战，现有系统通常在较干净的数据上训练或评估，难以应对复杂输入。

Method: Omni-RAG采用LLM辅助的查询理解，包括三个关键模块：深度查询理解与分解、意图感知的知识检索、以及重排序与生成，以处理复杂查询。

Result: Omni-RAG旨在通过其框架填补当前RAG能力与实际应用需求之间的差距，特别是在处理复杂和嘈杂查询方面。

Conclusion: Omni-RAG通过其创新的多模块设计，为实时开放域环境中的RAG系统提供了更鲁棒和有效的解决方案。

Abstract: Real-world live retrieval-augmented generation (RAG) systems face significant
challenges when processing user queries that are often noisy, ambiguous, and
contain multiple intents. While RAG enhances large language models (LLMs) with
external knowledge, current systems typically struggle with such complex
inputs, as they are often trained or evaluated on cleaner data. This paper
introduces Omni-RAG, a novel framework designed to improve the robustness and
effectiveness of RAG systems in live, open-domain settings. Omni-RAG employs
LLM-assisted query understanding to preprocess user inputs through three key
modules: (1) Deep Query Understanding and Decomposition, which utilizes LLMs
with tailored prompts to denoise queries (e.g., correcting spelling errors) and
decompose multi-intent queries into structured sub-queries; (2) Intent-Aware
Knowledge Retrieval, which performs retrieval for each sub-query from a corpus
(i.e., FineWeb using OpenSearch) and aggregates the results; and (3) Reranking
and Generation, where a reranker (i.e., BGE) refines document selection before
a final response is generated by an LLM (i.e., Falcon-10B) using a
chain-of-thought prompt. Omni-RAG aims to bridge the gap between current RAG
capabilities and the demands of real-world applications, such as those
highlighted by the SIGIR 2025 LiveRAG Challenge, by robustly handling complex
and noisy queries.

</details>


### [30] [Domain Knowledge-Enhanced LLMs for Fraud and Concept Drift Detection](https://arxiv.org/abs/2506.21443)
*Ali Şenol,Garima Agrawal,Huan Liu*

Main category: cs.CL

TL;DR: 该论文提出了一种结合领域知识的LLM框架，用于检测动态平台上的欺诈对话和概念漂移，显著提高了分类准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 由于语言模式的演变和概念漂移（CD），动态平台上检测欺诈对话变得越来越困难。LLMs在自然语言任务中表现优异，但在高风险场景中易受上下文模糊和幻觉影响。

Method: 论文提出了一个领域知识增强的LLM框架，包含三个主要组件：1）检测虚假对话的DK-LLM模块；2）检测语义漂移的OCDD单元；3）分类漂移性质的第二个DK-LLM模块。

Result: 实验表明，该系统在虚假评论数据集和SEConvo对话数据集上表现优异，LLaMA实现达到了98%的分类准确率，显著优于零样本基线。

Conclusion: 结合领域知识和漂移检测显著提升了高风险NLP应用的性能、可解释性和鲁棒性。

Abstract: Detecting deceptive conversations on dynamic platforms is increasingly
difficult due to evolving language patterns and Concept Drift (CD)-i.e.,
semantic or topical shifts that alter the context or intent of interactions
over time. These shifts can obscure malicious intent or mimic normal dialogue,
making accurate classification challenging. While Large Language Models (LLMs)
show strong performance in natural language tasks, they often struggle with
contextual ambiguity and hallucinations in risk-sensitive scenarios. To address
these challenges, we present a Domain Knowledge (DK)-Enhanced LLM framework
that integrates pretrained LLMs with structured, task-specific insights to
perform fraud and concept drift detection. The proposed architecture consists
of three main components: (1) a DK-LLM module to detect fake or deceptive
conversations; (2) a drift detection unit (OCDD) to determine whether a
semantic shift has occurred; and (3) a second DK-LLM module to classify the
drift as either benign or fraudulent. We first validate the value of domain
knowledge using a fake review dataset and then apply our full framework to
SEConvo, a multiturn dialogue dataset that includes various types of fraud and
spam attacks. Results show that our system detects fake conversations with high
accuracy and effectively classifies the nature of drift. Guided by structured
prompts, the LLaMA-based implementation achieves 98% classification accuracy.
Comparative studies against zero-shot baselines demonstrate that incorporating
domain knowledge and drift awareness significantly improves performance,
interpretability, and robustness in high-stakes NLP applications.

</details>


### [31] [Text2Cypher Across Languages: Evaluating Foundational Models Beyond English](https://arxiv.org/abs/2506.21445)
*Makbule Gulcin Ozsoy,William Tai*

Main category: cs.CL

TL;DR: 该论文研究了基础大语言模型在多语言Text2Cypher任务中的表现，发现性能因语言不同而异，提示翻译对结果影响较小，呼吁更多多语言查询生成的包容性评估和开发。


<details>
  <summary>Details</summary>
Motivation: 当前大多数自然语言接口研究仅关注英语，其他语言的评估有限。本文旨在探究基础LLMs在多语言Text2Cypher任务中的性能差异，并促进多语言查询生成的包容性发展。

Method: 通过将英文问题翻译成西班牙语和土耳其语并保留原始Cypher查询，创建并发布了一个多语言测试集。使用标准化提示和指标评估多个基础模型。

Result: 结果显示性能呈现一致模式：英语最高，西班牙语次之，土耳其语最低。这归因于训练数据可用性和语言特性的差异。提示翻译对评估指标影响较小。

Conclusion: 研究强调了在多语言查询生成中进行更包容性评估和开发的必要性。未来工作包括模式本地化和跨多种语言的微调。

Abstract: Recent advances in large language models have enabled natural language
interfaces that translate user questions into database queries, such as
Text2SQL, Text2SPARQL, and Text2Cypher. While these interfaces enhance database
accessibility, most research today focuses solely on English, with limited
evaluation in other languages. This paper investigates the performance of
foundational LLMs on the Text2Cypher task across multiple languages. We create
and release a multilingual test set by translating English questions into
Spanish and Turkish while preserving the original Cypher queries, enabling fair
cross-lingual comparison. We evaluate multiple foundational models using
standardized prompts and metrics. Our results show a consistent performance
pattern: highest on English, then Spanish, and lowest on Turkish. We attribute
this to differences in training data availability and linguistic
characteristics. Additionally, we explore the impact of translating task
prompts into Spanish and Turkish. Results show little to no change in
evaluation metrics, suggesting prompt translation has minor impact. Our
findings highlight the need for more inclusive evaluation and development in
multilingual query generation. Future work includes schema localization and
fine-tuning across diverse languages.

</details>


### [32] [Aligning Spoken Dialogue Models from User Interactions](https://arxiv.org/abs/2506.21463)
*Anne Wu,Laurent Mazaré,Neil Zeghidour,Alexandre Défossez*

Main category: cs.CL

TL;DR: 提出了一种新颖的偏好对齐框架，用于通过用户交互改进实时语音对话模型，利用大规模偏好数据集和离线对齐方法优化模型表现。


<details>
  <summary>Details</summary>
Motivation: 当前偏好学习方法主要针对文本语言模型，无法直接适应实时语音交互的复杂性（如打断、插话等），因此需要专门针对语音对话的偏好对齐方法。

Method: 构建了包含15万对偏好标注的大规模语音对话数据集，结合AI反馈，利用离线对齐方法微调全双工自回归语音到语音模型。

Result: 实验表明，通用对话的反馈能有效提升语音对话模型的事实性、安全性和上下文对齐性。人类评估验证了模型在多轮对话中的改进效果。

Conclusion: 研究表明，在实时语音对话系统中，合理平衡多种动态因素（如打断、上下文等）对实现自然交互至关重要。

Abstract: We propose a novel preference alignment framework for improving spoken
dialogue models on real-time conversations from user interactions. Current
preference learning methods primarily focus on text-based language models, and
are not directly suited to the complexities of real-time speech interactions,
with richer dynamics (e.g. interruption, interjection) and no explicit
segmentation between speaker turns.We create a large-scale dataset of more than
150,000 preference pairs from raw multi-turn speech conversations, annotated
with AI feedback, to cover preferences over both linguistic content and
temporal context variations. We leverage offline alignment methods to finetune
a full-duplex autoregressive speech-to-speech model. Extensive experiments
demonstrate that feedback on generic conversations can be consistently
effective in improving spoken dialogue models to produce more factual, safer
and more contextually aligned interactions. We deploy the finetuned model and
conduct holistic human evaluations to assess the impact beyond single-turn
conversations. Our findings shed light on the importance of a well-calibrated
balance among various dynamics, crucial for natural real-time speech dialogue
systems.

</details>


### [33] [TopK Language Models](https://arxiv.org/abs/2506.21468)
*Ryosuke Takahashi,Tatsuro Inaba,Kentaro Inui,Benjamin Heinzerling*

Main category: cs.CL

TL;DR: 论文提出了一种改进的Transformer架构，通过引入TopK激活函数，使模型的隐藏状态等同于TopK稀疏自编码器的潜在特征，从而在保持模型性能的同时提高了可解释性。


<details>
  <summary>Details</summary>
Motivation: 稀疏自编码器（SAEs）在分析和解释基于Transformer的语言模型（LMs）的激活空间时存在不足，如训练条件和架构选择影响特征学习，以及特征稳定性不足导致难以比较不同检查点的特征。这些问题限制了SAEs的实用性和内部有效性。

Method: 论文提出了一种改进的Transformer架构，通过在选定层中引入TopK激活函数，使模型的隐藏状态等同于TopK稀疏自编码器的潜在特征。这种方法消除了事后训练的需求，同时提供了与SAEs相当的可解释性。

Result: 实验表明，TopK LMs学习的稀疏表示能够通过目标神经元干预成功引导，并促进跨检查点和层的神经元形成过程的详细分析。TopK LMs在模型大小、计算效率和可解释性之间提供了良好的权衡。

Conclusion: TopK LMs为理解语言模型如何学习和表示概念提供了稳定可靠的工具，有望显著推动未来模型可解释性和可控性的研究。

Abstract: Sparse autoencoders (SAEs) have become an important tool for analyzing and
interpreting the activation space of transformer-based language models (LMs).
However, SAEs suffer several shortcomings that diminish their utility and
internal validity. Since SAEs are trained post-hoc, it is unclear if the
failure to discover a particular concept is a failure on the SAE's side or due
to the underlying LM not representing this concept. This problem is exacerbated
by training conditions and architecture choices affecting which features an SAE
learns. When tracing how LMs learn concepts during training, the lack of
feature stability also makes it difficult to compare SAEs features across
different checkpoints. To address these limitations, we introduce a
modification to the transformer architecture that incorporates a TopK
activation function at chosen layers, making the model's hidden states
equivalent to the latent features of a TopK SAE. This approach eliminates the
need for post-hoc training while providing interpretability comparable to SAEs.
The resulting TopK LMs offer a favorable trade-off between model size,
computational efficiency, and interpretability. Despite this simple
architectural change, TopK LMs maintain their original capabilities while
providing robust interpretability benefits. Our experiments demonstrate that
the sparse representations learned by TopK LMs enable successful steering
through targeted neuron interventions and facilitate detailed analysis of
neuron formation processes across checkpoints and layers. These features make
TopK LMs stable and reliable tools for understanding how language models learn
and represent concepts, which we believe will significantly advance future
research on model interpretability and controllability.

</details>


### [34] [Bridging Offline and Online Reinforcement Learning for LLMs](https://arxiv.org/abs/2506.21495)
*Jack Lanchantin,Angelica Chen,Janice Lan,Xian Li,Swarnadeep Saha,Tianlu Wang,Jing Xu,Ping Yu,Weizhe Yuan,Jason E Weston,Sainbayar Sukhbaatar,Ilia Kulikov*

Main category: cs.CL

TL;DR: 该研究探讨了强化学习方法在微调大型语言模型时的有效性，比较了离线、半在线和全在线模式在可验证和不可验证任务中的表现，发现在线和半在线方法表现相似且优于离线方法。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于探索强化学习方法在不同训练模式（离线、半在线、全在线）下对大型语言模型微调的效果，特别是在可验证和不可验证任务中的应用。

Method: 研究方法包括在可验证数学任务和不可验证指令跟随任务上进行实验，比较在线和半在线直接偏好优化（DPO）和组奖励策略优化（GRPO）目标，并分析训练动态和超参数选择策略。

Result: 研究结果显示，在线和半在线方法在性能和收敛性上表现相似，且均显著优于离线方法。同时，多任务处理可验证和不可验证奖励能提升两类任务的性能。

Conclusion: 研究结论表明，在线和半在线强化学习方法在微调大型语言模型时表现优异，且多任务处理能进一步提升模型性能。

Abstract: We investigate the effectiveness of reinforcement learning methods for
finetuning large language models when transitioning from offline to semi-online
to fully online regimes for both verifiable and non-verifiable tasks. Our
experiments cover training on verifiable math as well as non-verifiable
instruction following with a set of benchmark evaluations for both. Across
these settings, we extensively compare online and semi-online Direct Preference
Optimization and Group Reward Policy Optimization objectives, and surprisingly
find similar performance and convergence between these variants, which all
strongly outperform offline methods. We provide a detailed analysis of the
training dynamics and hyperparameter selection strategies to achieve optimal
results. Finally, we show that multi-tasking with verifiable and non-verifiable
rewards jointly yields improved performance across both task types.

</details>


### [35] [Enhancing User Engagement in Socially-Driven Dialogue through Interactive LLM Alignments](https://arxiv.org/abs/2506.21497)
*Jiashuo Wang,Kaitao Song,Chunpu Xu,Changhe Song,Yang Xiao,Dongsheng Li,Lili Qiu,Wenjie Li*

Main category: cs.CL

TL;DR: 该论文提出了一种通过未来对话信号优化交互式大语言模型（LLM）以提升用户参与度的方法，并在情感支持和劝导对话场景中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有研究虽能优化知识推理或对话行为规划，但用户参与度与知识/对话行为的关系不明确，难以保证社交对话中的用户参与。因此需要更直接的指标（如用户反应）来对齐模型。

Method: 1) 用用户模拟器与目标LLM交互；2) 通过i×MCTS算法探索对话路径；3) 收集高低质量对话数据；4) 采用直接偏好优化（DPO）对齐模型。

Result: 在情感支持和劝导对话场景中，该方法显著提升了交互式LLM的用户参与度。

Conclusion: 利用未来对话信号作为奖励机制，能有效引导交互式LLM学习提升用户参与度的策略，为社交对话系统提供了新优化思路。

Abstract: Enhancing user engagement through interactions plays an essential role in
socially-driven dialogues. While prior works have optimized models to reason
over relevant knowledge or plan a dialogue act flow, the relationship between
user engagement and knowledge or dialogue acts is subtle and does not guarantee
user engagement in socially-driven dialogues. To this end, we enable
interactive LLMs to learn user engagement by leveraging signals from the future
development of conversations. Specifically, we adopt a more direct and relevant
indicator of user engagement, i.e., the user's reaction related to dialogue
intention after the interaction, as a reward to align interactive LLMs. To
achieve this, we develop a user simulator to interact with target interactive
LLMs and explore interactions between the user and the interactive LLM system
via \textit{i$\times$MCTS} (\textit{M}onte \textit{C}arlo \textit{T}ree
\textit{S}earch for \textit{i}nteraction). In this way, we collect a dataset
containing pairs of higher and lower-quality experiences using
\textit{i$\times$MCTS}, and align interactive LLMs for high-level user
engagement by direct preference optimization (DPO) accordingly. Experiments
conducted on two socially-driven dialogue scenarios (emotional support
conversations and persuasion for good) demonstrate that our method effectively
enhances user engagement in interactive LLMs.

</details>


### [36] [skLEP: A Slovak General Language Understanding Benchmark](https://arxiv.org/abs/2506.21508)
*Marek Šuppa,Andrej Ridzik,Daniel Hládek,Tomáš Javůrek,Viktória Ondrejová,Kristína Sásiková,Martin Tamajka,Marián Šimko*

Main category: cs.CL

TL;DR: 研究者推出了首个斯洛伐克语自然语言理解基准测试skLEP，包含九项任务，并评估了多种语言模型。


<details>
  <summary>Details</summary>
Motivation: 为填补斯洛伐克语在自然语言理解评估方面的空白，研究者创建了首个全面基准测试skLEP。

Method: 通过编译新数据集和翻译英语资源构建skLEP，并系统评估了多种预训练语言模型。

Result: 发布了完整的基准数据、开源工具包和公开排行榜，以促进斯洛伐克语NLU研究的可重复性和未来发展。

Conclusion: skLEP为斯洛伐克语NLU研究提供了重要工具和资源，有望推动该领域的进一步研究。

Abstract: In this work, we introduce skLEP, the first comprehensive benchmark
specifically designed for evaluating Slovak natural language understanding
(NLU) models. We have compiled skLEP to encompass nine diverse tasks that span
token-level, sentence-pair, and document-level challenges, thereby offering a
thorough assessment of model capabilities. To create this benchmark, we curated
new, original datasets tailored for Slovak and meticulously translated
established English NLU resources. Within this paper, we also present the first
systematic and extensive evaluation of a wide array of Slovak-specific,
multilingual, and English pre-trained language models using the skLEP tasks.
Finally, we also release the complete benchmark data, an open-source toolkit
facilitating both fine-tuning and evaluation of models, and a public
leaderboard at https://github.com/slovak-nlp/sklep in the hopes of fostering
reproducibility and drive future research in Slovak NLU.

</details>


### [37] [Potemkin Understanding in Large Language Models](https://arxiv.org/abs/2506.21521)
*Marina Mancoridis,Bec Weeks,Keyon Vafa,Sendhil Mullainathan*

Main category: cs.CL

TL;DR: 论文质疑当前用基准测试评估大语言模型（LLM）能力的合理性，提出若模型错误与人类不同，则其理解仅为假象（Potemkin理解），并通过实验证明这种现象普遍存在。


<details>
  <summary>Details</summary>
Motivation: 当前LLM通过基准测试（如AP考试）评估能力，但若其错误模式与人类不同，则测试结果可能仅反映虚假理解，而非真实能力。论文旨在验证这种‘Potemkin理解’是否存在及其普遍性。

Method: 提出两种量化方法：1）在三个领域设计专用基准测试；2）通用方法计算Potemkin现象的下限。通过分析模型回答与人类理解的差异，检测内部概念表征的不一致性。

Result: 发现Potemkin现象在模型、任务和领域中普遍存在，且错误不仅源于理解偏差，更反映概念表征的深层内在矛盾。

Conclusion: 依赖与人类标准对齐的基准测试可能高估LLM真实能力，需开发更严格的评估方法以区分真实理解与表面模仿。

Abstract: Large language models (LLMs) are regularly evaluated using benchmark
datasets. But what justifies making inferences about an LLM's capabilities
based on its answers to a curated set of questions? This paper first introduces
a formal framework to address this question. The key is to note that the
benchmarks used to test LLMs -- such as AP exams -- are also those used to test
people. However, this raises an implication: these benchmarks are only valid
tests if LLMs misunderstand concepts in ways that mirror human
misunderstandings. Otherwise, success on benchmarks only demonstrates potemkin
understanding: the illusion of understanding driven by answers irreconcilable
with how any human would interpret a concept. We present two procedures for
quantifying the existence of potemkins: one using a specially designed
benchmark in three domains, the other using a general procedure that provides a
lower-bound on their prevalence. We find that potemkins are ubiquitous across
models, tasks, and domains. We also find that these failures reflect not just
incorrect understanding, but deeper internal incoherence in concept
representations.

</details>


### [38] ["What's Up, Doc?": Analyzing How Users Seek Health Information in Large-Scale Conversational AI Datasets](https://arxiv.org/abs/2506.21532)
*Akshay Paruchuri,Maryam Aziz,Rohit Vartak,Ayman Ali,Best Uchehara,Xin Liu,Ishan Chatterjee,Monica Agrawal*

Main category: cs.CL

TL;DR: 该论文分析了11K条真实用户与LLM的医疗健康对话，揭示了用户行为模式及潜在风险，强调需改进医疗对话AI。


<details>
  <summary>Details</summary>
Motivation: 随着越来越多人通过聊天机器人获取医疗信息，这些对话的性质和风险尚未被充分研究，因此需要系统性分析。

Method: 通过过滤大规模对话数据集，构建了HealthChat-11K数据集，并结合临床分类法分析21个医疗专科的用户交互行为。

Result: 研究发现用户存在信息不完整、情感化提问、诱导性提问等行为，凸显了LLM在医疗支持能力上的不足。

Conclusion: 需优化LLM的医疗对话能力，以减少误导风险并提升信息准确性。

Abstract: People are increasingly seeking healthcare information from large language
models (LLMs) via interactive chatbots, yet the nature and inherent risks of
these conversations remain largely unexplored. In this paper, we filter
large-scale conversational AI datasets to achieve HealthChat-11K, a curated
dataset of 11K real-world conversations composed of 25K user messages. We use
HealthChat-11K and a clinician-driven taxonomy for how users interact with LLMs
when seeking healthcare information in order to systematically study user
interactions across 21 distinct health specialties. Our analysis reveals
insights into the nature of how and why users seek health information, such as
common interactions, instances of incomplete context, affective behaviors, and
interactions (e.g., leading questions) that can induce sycophancy, underscoring
the need for improvements in the healthcare support capabilities of LLMs
deployed as conversational AI. Code and artifacts to retrieve our analyses and
combine them into a curated dataset can be found here:
https://github.com/yahskapar/HealthChat

</details>


### [39] [Data Efficacy for Language Model Training](https://arxiv.org/abs/2506.21545)
*Yalun Dai,Yangyu Huang,Xin Zhang,Wenshan Wu,Chong Li,Wenhui Lu,Shijie Cao,Li Dong,Scarlett Li*

Main category: cs.CL

TL;DR: 该论文提出了数据效能（Data Efficacy）的概念，并通过DELT范式优化语言模型训练数据的组织方式，包括数据评分、选择和排序，以提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 当前研究主要集中在数据效率（Data Efficiency），即通过选择最优数据子集来提升模型性能。然而，数据组织方式对性能的影响尚未充分探索，因此论文提出数据效能的概念，旨在通过优化数据组织来最大化模型表现。

Method: 论文提出了DELT范式，包含数据评分（如LQS）、数据选择和数据排序（如FO）。LQS从梯度一致性角度评估数据的可学习性和质量，FO则解决模型遗忘和数据分布偏差问题。

Result: 实验表明，DELT的不同实例均能不同程度提升模型性能，且不增加数据规模和模型大小。其中，LQS与FO的组合效果最佳。数据效能与数据效率可以同时实现。

Conclusion: 数据效能是语言模型训练中一个具有潜力的基础研究方向，通过优化数据组织方式可以显著提升模型性能。

Abstract: Data is fundamental to the training of language models (LM). Recent research
has been dedicated to data efficiency, which aims to maximize performance by
selecting a minimal or optimal subset of training data. Techniques such as data
filtering, sampling, and selection play a crucial role in this area. To
complement it, we define Data Efficacy, which focuses on maximizing performance
by optimizing the organization of training data and remains relatively
underexplored. This work introduces a general paradigm, DELT, for considering
data efficacy in LM training, which highlights the significance of training
data organization. DELT comprises three components: Data Scoring, Data
Selection, and Data Ordering. Among these components, we design
Learnability-Quality Scoring (LQS), as a new instance of Data Scoring, which
considers both the learnability and quality of each data sample from the
gradient consistency perspective. We also devise Folding Ordering (FO), as a
novel instance of Data Ordering, which addresses issues such as model
forgetting and data distribution bias. Comprehensive experiments validate the
data efficacy in LM training, which demonstrates the following: Firstly,
various instances of the proposed DELT enhance LM performance to varying
degrees without increasing the data scale and model size. Secondly, among these
instances, the combination of our proposed LQS for data scoring and Folding for
data ordering achieves the most significant improvement. Lastly, data efficacy
can be achieved together with data efficiency by applying data selection.
Therefore, we believe that data efficacy is a promising foundational area in LM
training.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [40] [The Singapore Consensus on Global AI Safety Research Priorities](https://arxiv.org/abs/2506.20702)
*Yoshua Bengio,Tegan Maharaj,Luke Ong,Stuart Russell,Dawn Song,Max Tegmark,Lan Xue,Ya-Qin Zhang,Stephen Casper,Wan Sie Lee,Sören Mindermann,Vanessa Wilfred,Vidhisha Balachandran,Fazl Barez,Michael Belinsky,Imane Bello,Malo Bourgon,Mark Brakel,Siméon Campos,Duncan Cass-Beggs,Jiahao Chen,Rumman Chowdhury,Kuan Chua Seah,Jeff Clune,Juntao Dai,Agnes Delaborde,Nouha Dziri,Francisco Eiras,Joshua Engels,Jinyu Fan,Adam Gleave,Noah Goodman,Fynn Heide,Dan Hendrycks,Cyrus Hodes,Bryan Low Kian Hsiang,Minlie Huang,Sami Jawhar,Wang Jingyu,Adam Tauman Kalai,Meindert Kamphuis,Mohan Kankanhalli,Subhash Kantamneni,Mathias Bonde Kirk,Thomas Kwa,Jeffrey Ladish,Kwok-Yan Lam,Wan Lee Sie,Taewhi Lee,Xiaojian Li,Jiajun Liu,Chaochao Lu,Yifan Mai,Richard Mallah,Julian Michael,Nick Moës,Simon Möller,Kihyuk Nam,Kwan Yee Ng,Mark Nitzberg,Besmira Nushi,Seán O hÉigeartaigh,Alejandro Ortega,Pierre Peigné,James Petrie,Benjamin Prud'Homme,Reihaneh Rabbany,Nayat Sanchez-Pi,Sarah Schwettmann,Buck Shlegeris,Saad Siddiqui,Aradhana Sinha,Martín Soto,Cheston Tan,Dong Ting,Robert Trager,Brian Tse,Anthony Tung K. H.,Vanessa Wilfred,John Willes,Denise Wong,Wei Xu,Rongwu Xu,Yi Zeng,HongJiang Zhang,Djordje Žikelić*

Main category: cs.AI

TL;DR: 2025年新加坡AI安全会议旨在通过汇集全球AI科学家，识别和综合AI安全研究重点，构建可信的AI生态系统。


<details>
  <summary>Details</summary>
Motivation: 随着AI能力的快速提升和自主性的增强，确保AI的安全性、可靠性和可信性成为重要议题。构建可信的AI生态系统有助于人们更自信地接受AI，同时为创新提供最大空间并避免反弹。

Method: 会议采用深度防御模型，将AI安全研究领域分为三类：开发可信AI系统的挑战（开发）、评估其风险的挑战（评估）以及部署后监控和干预的挑战（控制）。

Result: 报告基于由Yoshua Bengio主持、33国政府支持的国际AI安全报告，提出了AI安全研究的优先领域和框架。

Conclusion: 通过国际合作和深度防御模型，可以更有效地应对AI安全挑战，推动可信AI生态系统的发展。

Abstract: Rapidly improving AI capabilities and autonomy hold significant promise of
transformation, but are also driving vigorous debate on how to ensure that AI
is safe, i.e., trustworthy, reliable, and secure. Building a trusted ecosystem
is therefore essential -- it helps people embrace AI with confidence and gives
maximal space for innovation while avoiding backlash.
  The "2025 Singapore Conference on AI (SCAI): International Scientific
Exchange on AI Safety" aimed to support research in this space by bringing
together AI scientists across geographies to identify and synthesise research
priorities in AI safety. This resulting report builds on the International AI
Safety Report chaired by Yoshua Bengio and backed by 33 governments. By
adopting a defence-in-depth model, this report organises AI safety research
domains into three types: challenges with creating trustworthy AI systems
(Development), challenges with evaluating their risks (Assessment), and
challenges with monitoring and intervening after deployment (Control).

</details>


### [41] [MAGPIE: A dataset for Multi-AGent contextual PrIvacy Evaluation](https://arxiv.org/abs/2506.20737)
*Gurusha Juneja,Alon Albalak,Wenyue Hua,William Yang Wang*

Main category: cs.AI

TL;DR: 当前LLM智能体在上下文隐私理解和多轮对话中保护用户隐私方面表现不佳，新基准MAGPIE揭示了其缺陷。


<details>
  <summary>Details</summary>
Motivation: 随着基于LLM的智能体在协作任务中的广泛应用，隐私保护变得至关重要。现有评估基准主要针对单轮简单任务，无法全面评估复杂场景下的隐私保护能力。

Method: 提出了MAGPIE基准，包含158个高风险场景，评估LLM在(a)理解上下文隐私数据和(b)协作时不侵犯用户隐私的能力。

Result: 实验表明，包括GPT-4o和Claude-2.7-Sonnet在内的当前模型对上下文隐私理解不足，错误分类私有数据的比例分别为25.2%和43.6%。在多轮对话中，即使有明确隐私指令，泄露隐私的比例也高达59.9%和50.5%。

Conclusion: 当前模型在上下文隐私保护和协作任务解决方面表现不佳，需要进一步改进以同时满足这两个需求。

Abstract: The proliferation of LLM-based agents has led to increasing deployment of
inter-agent collaboration for tasks like scheduling, negotiation, resource
allocation etc. In such systems, privacy is critical, as agents often access
proprietary tools and domain-specific databases requiring strict
confidentiality. This paper examines whether LLM-based agents demonstrate an
understanding of contextual privacy. And, if instructed, do these systems
preserve inference time user privacy in non-adversarial multi-turn
conversation. Existing benchmarks to evaluate contextual privacy in LLM-agents
primarily assess single-turn, low-complexity tasks where private information
can be easily excluded. We first present a benchmark - MAGPIE comprising 158
real-life high-stakes scenarios across 15 domains. These scenarios are designed
such that complete exclusion of private data impedes task completion yet
unrestricted information sharing could lead to substantial losses. We then
evaluate the current state-of-the-art LLMs on (a) their understanding of
contextually private data and (b) their ability to collaborate without
violating user privacy. Empirical experiments demonstrate that current models,
including GPT-4o and Claude-2.7-Sonnet, lack robust understanding of contextual
privacy, misclassifying private data as shareable 25.2\% and 43.6\% of the
time. In multi-turn conversations, these models disclose private information in
59.9\% and 50.5\% of cases even under explicit privacy instructions.
Furthermore, multi-agent systems fail to complete tasks in 71\% of scenarios.
These results underscore that current models are not aligned towards both
contextual privacy preservation and collaborative task-solving.

</details>


### [42] [Dynamic Context-Aware Prompt Recommendation for Domain-Specific AI Applications](https://arxiv.org/abs/2506.20815)
*Xinye Tang,Haijun Zhai,Chaitanya Belwal,Vineeth Thayanithi,Philip Baumann,Yogesh K Roy*

Main category: cs.AI

TL;DR: 提出了一种动态上下文感知的提示推荐系统，用于提升领域特定AI应用中用户提示的质量。


<details>
  <summary>Details</summary>
Motivation: LLM应用高度依赖用户提示的质量，而领域特定应用中设计高质量提示尤为困难。

Method: 结合上下文查询分析、检索增强知识基础、分层技能组织和自适应技能排序，动态生成相关且可操作的提示建议。

Result: 在真实数据集上的实验表明，该方法在自动和专家评估中均表现出高实用性和相关性。

Conclusion: 该系统通过动态选择和排序相关技能，结合预定义和自适应模板，有效提升了提示的质量和实用性。

Abstract: LLM-powered applications are highly susceptible to the quality of user
prompts, and crafting high-quality prompts can often be challenging especially
for domain-specific applications. This paper presents a novel dynamic
context-aware prompt recommendation system for domain-specific AI applications.
Our solution combines contextual query analysis, retrieval-augmented knowledge
grounding, hierarchical skill organization, and adaptive skill ranking to
generate relevant and actionable prompt suggestions.
  The system leverages behavioral telemetry and a two-stage hierarchical
reasoning process to dynamically select and rank relevant skills, and
synthesizes prompts using both predefined and adaptive templates enhanced with
few-shot learning. Experiments on real-world datasets demonstrate that our
approach achieves high usefulness and relevance, as validated by both automated
and expert evaluations.

</details>


### [43] [Beyond Reactive Safety: Risk-Aware LLM Alignment via Long-Horizon Simulation](https://arxiv.org/abs/2506.20949)
*Chenkai Sun,Denghui Zhang,ChengXiang Zhai,Heng Ji*

Main category: cs.AI

TL;DR: 论文提出一个框架评估语言模型建议的宏观社会影响，并创建数据集测试模型对间接危害的预见能力，结果显示其方法在安全基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 随着语言模型代理在公共政策、医疗等高风险社会决策中的影响力增长，需确保其建议能产生积极影响，因此需要理解其建议的深远影响。

Method: 提出概念验证框架，模拟模型生成建议在宏观社会系统中的传播，并引入包含100个间接危害场景的数据集测试模型预见能力。

Result: 新数据集上性能提升超20%，在现有安全基准（AdvBench等）上平均胜率超70%。

Conclusion: 该方法为构建更安全的语言模型代理提供了有前景的方向。

Abstract: Given the growing influence of language model-based agents on high-stakes
societal decisions, from public policy to healthcare, ensuring their beneficial
impact requires understanding the far-reaching implications of their
suggestions. We propose a proof-of-concept framework that projects how
model-generated advice could propagate through societal systems on a
macroscopic scale over time, enabling more robust alignment. To assess the
long-term safety awareness of language models, we also introduce a dataset of
100 indirect harm scenarios, testing models' ability to foresee adverse,
non-obvious outcomes from seemingly harmless user prompts. Our approach
achieves not only over 20% improvement on the new dataset but also an average
win rate exceeding 70% against strong baselines on existing safety benchmarks
(AdvBench, SafeRLHF, WildGuardMix), suggesting a promising direction for safer
agents.

</details>


### [44] [Unveiling Causal Reasoning in Large Language Models: Reality or Mirage?](https://arxiv.org/abs/2506.21215)
*Haoang Chi,He Li,Wenjing Yang,Feng Liu,Long Lan,Xiaoguang Ren,Tongliang Liu,Bo Han*

Main category: cs.AI

TL;DR: 论文指出当前大语言模型仅具备浅层因果推理能力，提出结合通用知识与目标导向提示的G^2-Reasoner方法，显著提升了模型在新颖和反事实场景中的因果推理表现。


<details>
  <summary>Details</summary>
Motivation: 探索大语言模型是否具备人类水平的真实因果推理能力，现有证据表明其仅能进行浅层（Level-1）推理，缺乏深层（Level-2）人类式因果推理机制。

Method: 通过分析Transformer的自回归机制揭示其非因果性，构建新基准CausalProbe-2024验证模型局限，并提出融合通用知识与目标提示的G^2-Reasoner方法。

Result: LLMs在CausalProbe-2024上表现显著下降，证实其仅具备Level-1能力；G^2-Reasoner使模型在新颖和反事实场景中的因果推理能力得到显著提升。

Conclusion: 结合知识引导与目标驱动的推理框架是推动LLMs向Level-2因果推理迈进的有效路径，为强人工智能发展提供了新思路。

Abstract: Causal reasoning capability is critical in advancing large language models
(LLMs) toward strong artificial intelligence. While versatile LLMs appear to
have demonstrated capabilities in understanding contextual causality and
providing responses that obey the laws of causality, it remains unclear whether
they perform genuine causal reasoning akin to humans. However, current evidence
indicates the contrary. Specifically, LLMs are only capable of performing
shallow (level-1) causal reasoning, primarily attributed to the causal
knowledge embedded in their parameters, but they lack the capacity for genuine
human-like (level-2) causal reasoning. To support this hypothesis,
methodologically, we delve into the autoregression mechanism of
transformer-based LLMs, revealing that it is not inherently causal.
Empirically, we introduce a new causal Q&A benchmark called CausalProbe-2024,
whose corpora are fresh and nearly unseen for the studied LLMs. The LLMs
exhibit a significant performance drop on CausalProbe-2024 compared to earlier
benchmarks, indicating the fact that they primarily engage in level-1 causal
reasoning. To bridge the gap towards level-2 causal reasoning, we draw
inspiration from the fact that human reasoning is usually facilitated by
general knowledge and intended goals. We propose G^2-Reasoner, a method that
incorporates general knowledge and goal-oriented prompts into LLMs' causal
reasoning processes. Experiments demonstrate that G^2-Reasoner significantly
enhances LLMs' causal reasoning capability, particularly in fresh and
counterfactual contexts. This work sheds light on a new path for LLMs to
advance towards genuine causal reasoning, going beyond level-1 and making
strides towards level-2.

</details>


### [45] [World-aware Planning Narratives Enhance Large Vision-Language Model Planner](https://arxiv.org/abs/2506.21230)
*Junhao Shi,Zhaoye Fei,Siyin Wang,Qipeng Guo,Jingjing Gong,Xipeng QIu*

Main category: cs.AI

TL;DR: 论文提出WAP框架，通过四种认知能力增强大视觉语言模型的环境理解能力，显著提升了任务成功率。


<details>
  <summary>Details</summary>
Motivation: 现有的大视觉语言模型在复杂场景和陌生环境中的多步目标规划上表现不佳，主要依赖与环境无关的模仿学习，缺乏对环境的深入理解。

Method: 提出World-Aware Planning Narrative Enhancement (WAP)框架，通过视觉外观建模、空间推理、功能抽象和语法 grounding 四种认知能力，结合课程学习，仅使用原始视觉观察来训练和评估模型。

Result: 在EB-ALFRED基准测试中，Qwen2.5-VL模型的任务成功率提升了60.7%，尤其在常识推理（+60.0）和长时规划（+70.0）方面表现突出，甚至超越了GPT-4o和Claude-3.5-Sonnet等专有系统。

Conclusion: WAP框架通过增强环境理解能力，显著提升了大视觉语言模型在复杂任务中的表现，展示了其在具身规划任务中的潜力。

Abstract: Large Vision-Language Models (LVLMs) show promise for embodied planning tasks
but struggle with complex scenarios involving unfamiliar environments and
multi-step goals. Current approaches rely on environment-agnostic imitation
learning that disconnects instructions from environmental contexts, causing
models to struggle with context-sensitive instructions and rely on
supplementary cues rather than visual reasoning during long-horizon
interactions. In this work, we propose World-Aware Planning Narrative
Enhancement (WAP), a framework that infuses LVLMs with comprehensive
environmental understanding through four cognitive capabilities (visual
appearance modeling, spatial reasoning, functional abstraction, and syntactic
grounding) while developing and evaluating models using only raw visual
observations through curriculum learning. Evaluations on the EB-ALFRED
benchmark demonstrate substantial improvements, with Qwen2.5-VL achieving a
60.7 absolute improvement in task success rates, particularly in commonsense
reasoning (+60.0) and long-horizon planning (+70.0). Notably, our enhanced
open-source models outperform proprietary systems like GPT-4o and
Claude-3.5-Sonnet by a large margin.

</details>


### [46] [IXAII: An Interactive Explainable Artificial Intelligence Interface for Decision Support Systems](https://arxiv.org/abs/2506.21310)
*Pauline Speckmann,Mario Nadj,Christian Janiesch*

Main category: cs.AI

TL;DR: 本文提出了一种名为IXAII的交互式可解释智能系统，通过整合多种可解释AI方法并提供个性化视图，增强了AI的透明度和用户体验。


<details>
  <summary>Details</summary>
Motivation: 现有的可解释AI方法多为静态且忽视用户视角，限制了其对目标受众的有效性。因此，作者开发了IXAII系统，旨在通过交互性和多视角解释来弥补这一不足。

Method: IXAII系统整合了LIME、SHAP、Anchors和DiCE四种可解释AI方法，并为五类用户群体提供定制化视图，使用户能够控制解释的内容和格式。

Result: 通过专家和普通用户的访谈评估，IXAII系统因其多样化的解释和可视化选项，被认为能有效提升AI的透明度。

Conclusion: IXAII系统通过结合可解释AI方法、交互性和实际应用，为AI解释实践和人机交互提供了新的视角。

Abstract: Although several post-hoc methods for explainable AI have been developed,
most are static and neglect the user perspective, limiting their effectiveness
for the target audience. In response, we developed the interactive explainable
intelligent system called IXAII that offers explanations from four explainable
AI methods: LIME, SHAP, Anchors, and DiCE. Our prototype provides tailored
views for five user groups and gives users agency over the explanations'
content and their format. We evaluated IXAII through interviews with experts
and lay users. Our results indicate that IXAII, which provides different
explanations with multiple visualization options, is perceived as helpful to
increase transparency. By bridging the gaps between explainable AI methods,
interactivity, and practical implementation, we provide a novel perspective on
AI explanation practices and human-AI interaction.

</details>


### [47] [Active Inference AI Systems for Scientific Discovery](https://arxiv.org/abs/2506.21329)
*Karthik Duraisamy*

Main category: cs.AI

TL;DR: 论文提出AI驱动科学发现需解决三个关键差距（抽象、推理、现实），并设计主动推理AI系统架构，强调人类判断的不可或缺性。


<details>
  <summary>Details</summary>
Motivation: 当前AI系统在科学发现中存在架构局限、推理脆弱及与现实脱节的问题，需通过填补三大差距推动进步。

Method: 构建主动推理AI系统：结合因果自监督模型、贝叶斯约束的符号规划、持续增长的知识图谱，并通过仿真与实验闭环优化表征。

Result: 提出一种内外部验证交织的AI架构，使科学发现源于反事实推理与实证的互动，同时确立人类在不确定性中的核心作用。

Conclusion: AI科学发现需系统性整合因果推理与实证验证，人类判断应作为永久性组件嵌入架构以处理模糊反馈。

Abstract: The rapid evolution of artificial intelligence has led to expectations of
transformative scientific discovery, yet current systems remain fundamentally
limited by their operational architectures, brittle reasoning mechanisms, and
their separation from experimental reality. Building on earlier work, we
contend that progress in AI-driven science now depends on closing three
fundamental gaps -- the abstraction gap, the reasoning gap, and the reality gap
-- rather than on model size/data/test time compute. Scientific reasoning
demands internal representations that support simulation of actions and
response, causal structures that distinguish correlation from mechanism, and
continuous calibration. We define active inference AI systems for scientific
discovery as those that (i) maintain long-lived research memories grounded in
causal self-supervised foundation models, (ii) symbolic or neuro-symbolic
planners equipped with Bayesian guardrails, (iii) grow persistent knowledge
graphs where thinking generates novel conceptual nodes, reasoning establishes
causal edges, and real-world interaction prunes false connections while
strengthening verified pathways, and (iv) refine their internal representations
through closed-loop interaction with both high-fidelity simulators and
automated laboratories - an operational loop where mental simulation guides
action and empirical surprise reshapes understanding. In essence, we outline an
architecture where discovery arises from the interplay between internal models
that enable counterfactual reasoning and external validation that grounds
hypotheses in reality. It is also argued that the inherent ambiguity in
feedback from simulations and experiments, and underlying uncertainties makes
human judgment indispensable, not as a temporary scaffold but as a permanent
architectural component.

</details>


### [48] [TableMoE: Neuro-Symbolic Routing for Structured Expert Reasoning in Multimodal Table Understanding](https://arxiv.org/abs/2506.21393)
*Junwen Zhang,Pu Chen,Yin Zhang*

Main category: cs.AI

TL;DR: TableMoE提出了一种神经符号混合专家架构，用于处理复杂多模态表格数据，显著提升了现有模型的性能。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的表格数据因结构复杂、符号密集和视觉退化等问题，现有多模态大语言模型在处理此类数据时表现不佳。

Method: TableMoE采用神经符号路由机制，动态将表格元素路由到专门专家处理，并引入大规模对齐数据集进行预训练。

Result: TableMoE在多个挑战性基准测试中显著超越现有最先进模型，验证了其核心组件的有效性。

Conclusion: 通过神经符号推理的整合，TableMoE展示了在多模态表格理解中的可解释性和鲁棒性。

Abstract: Multimodal understanding of tables in real-world contexts is challenging due
to the complexity of structure, symbolic density, and visual degradation (blur,
skew, watermarking, incomplete structures or fonts, multi-span or
hierarchically nested layouts). Existing multimodal large language models
(MLLMs) struggle with such WildStruct conditions, resulting in limited
performance and poor generalization. To address these challenges, we propose
TableMoE, a neuro-symbolic Mixture-of-Connector-Experts (MoCE) architecture
specifically designed for robust, structured reasoning over multimodal table
data. TableMoE features an innovative Neuro-Symbolic Routing mechanism, which
predicts latent semantic token roles (e.g., header, data cell, axis, formula)
and dynamically routes table elements to specialized experts (Table-to-HTML,
Table-to-JSON, Table-to-Code) using a confidence-aware gating strategy informed
by symbolic reasoning graphs. To facilitate effective alignment-driven
pretraining, we introduce the large-scale TableMoE-Align dataset, consisting of
1.2M table-HTML-JSON-code quadruples across finance, science, biomedicine and
industry, utilized exclusively for model pretraining. For evaluation, we curate
and release four challenging WildStruct benchmarks: WMMFinQA, WMMTatQA,
WMMTabDialog, and WMMFinanceMath, designed specifically to stress-test models
under real-world multimodal degradation and structural complexity. Experimental
results demonstrate that TableMoE significantly surpasses existing
state-of-the-art models. Extensive ablation studies validate each core
component, emphasizing the critical role of Neuro-Symbolic Routing and
structured expert alignment. Through qualitative analyses, we further showcase
TableMoE's interpretability and enhanced robustness, underscoring the
effectiveness of integrating neuro-symbolic reasoning for multimodal table
understanding.

</details>


### [49] [Spatial Mental Modeling from Limited Views](https://arxiv.org/abs/2506.21458)
*Baiqiao Yin,Qineng Wang,Pingyue Zhang,Jianshu Zhang,Kangrui Wang,Zihan Wang,Jieyu Zhang,Keshigeyan Chandrasegaran,Han Liu,Ranjay Krishna,Saining Xie,Manling Li,Jiajun Wu,Li Fei-Fei*

Main category: cs.AI

TL;DR: 论文提出MindCube基准测试，揭示现有视觉语言模型在构建空间心理模型上的不足，并通过‘先映射后推理’方法显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 研究视觉语言模型是否能像人类一样从少量视角想象完整场景，构建空间心理模型以推理布局、视角和运动。

Method: 使用MindCube基准测试评估模型，提出‘先映射后推理’方法，结合认知地图生成和强化学习。

Result: 模型准确率从37.8%提升至70.7%，验证了空间心理模型构建对理解不可见空间的重要性。

Conclusion: 通过结构化空间表征和灵活推理过程，显著提升了模型对不可观察空间的理解能力。

Abstract: Can Vision Language Models (VLMs) imagine the full scene from just a few
views, like humans do? Humans form spatial mental models, internal
representations of unseen space, to reason about layout, perspective, and
motion. Our new MindCube benchmark with 21,154 questions across 3,268 images
exposes this critical gap, where existing VLMs exhibit near-random performance.
Using MindCube, we systematically evaluate how well VLMs build robust spatial
mental models through representing positions (cognitive mapping), orientations
(perspective-taking), and dynamics (mental simulation for "what-if" movements).
We then explore three approaches to help VLMs approximate spatial mental
models, including unseen intermediate views, natural language reasoning chains,
and cognitive maps. The significant improvement comes from a synergistic
approach, "map-then-reason", that jointly trains the model to first generate a
cognitive map and then reason upon it. By training models to reason over these
internal maps, we boosted accuracy from 37.8% to 60.8% (+23.0%). Adding
reinforcement learning pushed performance even further to 70.7% (+32.9%). Our
key insight is that such scaffolding of spatial mental models, actively
constructing and utilizing internal structured spatial representations with
flexible reasoning processes, significantly improves understanding of
unobservable space.

</details>


### [50] [Ad-Hoc Human-AI Coordination Challenge](https://arxiv.org/abs/2506.21490)
*Tin Dizdarević,Ravi Hammond,Tobias Gessler,Anisoara Calinescu,Jonathan Cook,Matteo Gallici,Andrei Lupu,Jakob Nicolaus Foerster*

Main category: cs.AI

TL;DR: 论文提出AH2AC2挑战，通过人类代理解决Hanabi游戏中人机协作评估难题，开源有限数据集并建立基线结果。


<details>
  <summary>Details</summary>
Motivation: 现实应用中AI与人类无缝协作至关重要，但现有评估方法成本高且难以复现。Hanabi游戏因信息不完整、沟通受限等特点成为理想测试平台，但人类评估限制了其应用。

Method: 构建大规模人类数据集上的代理模型（human proxy agents），作为廉价、可复现的评估伙伴；开源3,079局游戏数据以促进数据高效方法研究，并通过受控系统评估代理。

Result: 建立了双人和三人Hanabi场景的基线结果，代理模型在AH2AC2框架下实现了稳定评估。

Conclusion: AH2AC2通过人类代理和有限数据集解决了人机协作评估的瓶颈，为未来研究提供了可扩展的基准平台。

Abstract: Achieving seamless coordination between AI agents and humans is crucial for
real-world applications, yet it remains a significant open challenge. Hanabi is
a cooperative card game featuring imperfect information, constrained
communication, theory of mind requirements, and coordinated action -- making it
an ideal testbed for human-AI coordination. However, its use for human-AI
interaction has been limited by the challenges of human evaluation. In this
work, we introduce the Ad-Hoc Human-AI Coordination Challenge (AH2AC2) to
overcome the constraints of costly and difficult-to-reproduce human
evaluations. We develop \textit{human proxy agents} on a large-scale human
dataset that serve as robust, cheap, and reproducible human-like evaluation
partners in AH2AC2. To encourage the development of data-efficient methods, we
open-source a dataset of 3,079 games, deliberately limiting the amount of
available human gameplay data. We present baseline results for both two- and
three- player Hanabi scenarios. To ensure fair evaluation, we host the proxy
agents through a controlled evaluation system rather than releasing them
publicly. The code is available at
\href{https://github.com/FLAIROx/ah2ac2}{https://github.com/FLAIROx/ah2ac2}.

</details>


### [51] [Mind2Web 2: Evaluating Agentic Search with Agent-as-a-Judge](https://arxiv.org/abs/2506.21506)
*Boyu Gou,Zanming Huang,Yuting Ning,Yu Gu,Michael Lin,Weijian Qi,Andrei Kopanev,Botao Yu,Bernal Jiménez Gutiérrez,Yiheng Shu,Chan Hee Song,Jiaman Wu,Shijie Chen,Hanane Nour Moussa,Tianshu Zhang,Jian Xie,Yifei Li,Tianci Xue,Zeyi Liao,Kai Zhang,Boyuan Zheng,Zhaowei Cai,Viktor Rozgic,Morteza Ziyadi,Huan Sun,Yu Su*

Main category: cs.AI

TL;DR: 论文介绍了Mind2Web 2基准测试，用于评估自主网络搜索系统的性能，并提出了一种新的Agent-as-a-Judge评估框架。


<details>
  <summary>Details</summary>
Motivation: 现有的评估基准和方法无法满足自主网络搜索系统日益增长的复杂性和开放性需求，因此需要开发新的评估工具和方法。

Method: 提出了Mind2Web 2基准测试，包含130个真实、高质量、长周期的任务，并设计了基于树形结构的Agent-as-a-Judge框架来自动评估答案正确性和来源归属。

Result: 评估了九种前沿自主搜索系统和人类表现，最佳系统OpenAI Deep Research已达到人类表现的50-70%，且耗时减半。

Conclusion: Mind2Web 2为下一代自主搜索系统的开发和评估提供了严格的基础，展示了巨大的潜力。

Abstract: Agentic search such as Deep Research systems, where large language models
autonomously browse the web, synthesize information, and return comprehensive
citation-backed answers, represents a major shift in how users interact with
web-scale information. While promising greater efficiency and cognitive
offloading, the growing complexity and open-endedness of agentic search have
outpaced existing evaluation benchmarks and methodologies, which largely assume
short search horizons and static answers. In this paper, we introduce Mind2Web
2, a benchmark of 130 realistic, high-quality, and long-horizon tasks that
require real-time web browsing and extensive information synthesis, constructed
with over 1,000 hours of human labor. To address the challenge of evaluating
time-varying and complex answers, we propose a novel Agent-as-a-Judge
framework. Our method constructs task-specific judge agents based on a
tree-structured rubric design to automatically assess both answer correctness
and source attribution. We conduct a comprehensive evaluation of nine frontier
agentic search systems and human performance, along with a detailed error
analysis to draw insights for future development. The best-performing system,
OpenAI Deep Research, can already achieve 50-70% of human performance while
spending half the time, showing a great potential. Altogether, Mind2Web 2
provides a rigorous foundation for developing and benchmarking the next
generation of agentic search systems.

</details>


### [52] [PsyLite Technical Report](https://arxiv.org/abs/2506.21536)
*Fangjun Ding,Renyu Zhang,Xinyu Feng,Chengye Xie,Zheng Zhang,Yanting Zhang*

Main category: cs.AI

TL;DR: 本文提出PsyLite，一个基于InternLM2.5-7B-chat的轻量级心理咨询大语言模型代理，通过两阶段训练策略提升模型能力，并在资源受限环境中实现高效部署。


<details>
  <summary>Details</summary>
Motivation: 现有AI心理咨询模型在对话安全、场景处理及轻量化部署方面存在不足，需改进以满足心理健康领域的需求。

Method: 采用混合蒸馏数据微调和ORPO偏好优化的两阶段训练策略，结合条件RAG技术增强对话安全与用户体验。

Result: PsyLite在多项评估中超越基线模型，心理咨询专业性提升47.6%，对话安全提升2.4%，且仅需5GB内存运行。

Conclusion: PsyLite为资源受限环境下的心理咨询应用提供了高效、安全的解决方案。

Abstract: With the rapid development of digital technology, AI-driven psychological
counseling has gradually become an important research direction in the field of
mental health. However, existing models still have deficiencies in dialogue
safety, detailed scenario handling, and lightweight deployment. To address
these issues, this study proposes PsyLite, a lightweight psychological
counseling large language model agent developed based on the base model
InternLM2.5-7B-chat. Through a two-stage training strategy (hybrid distillation
data fine-tuning and ORPO preference optimization), PsyLite enhances the
model's deep-reasoning ability, psychological counseling ability, and safe
dialogue ability. After deployment using Ollama and Open WebUI, a custom
workflow is created with Pipelines. An innovative conditional RAG is designed
to introduce crosstalk humor elements at appropriate times during psychological
counseling to enhance user experience and decline dangerous requests to
strengthen dialogue safety. Evaluations show that PsyLite outperforms the
baseline models in the Chinese general evaluation (CEval), psychological
counseling professional evaluation (CPsyCounE), and dialogue safety evaluation
(SafeDialBench), particularly in psychological counseling professionalism
(CPsyCounE score improvement of 47.6\%) and dialogue safety (\safe{} score
improvement of 2.4\%). Additionally, the model uses quantization technology
(GGUF q4\_k\_m) to achieve low hardware deployment (5GB memory is sufficient
for operation), providing a feasible solution for psychological counseling
applications in resource-constrained environments.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [53] [Progressive Size-Adaptive Federated Learning: A Comprehensive Framework for Heterogeneous Multi-Modal Data Systems](https://arxiv.org/abs/2506.20685)
*Sajid Hussain,Muhammad Sohail,Nauman Ali Khan,Naima Iltaf,Ihtesham ul Islam*

Main category: cs.LG

TL;DR: 本文提出了一种基于数据集大小特性的自适应联邦学习框架SAFL，通过多模态数据实验揭示了数据集大小对联邦学习效果的影响，并展示了其在通信效率和性能上的优势。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习研究主要关注模型异构性和聚合技术，而忽略了数据集大小特性对训练动态的根本影响。本文旨在填补这一空白，探索数据特性如何驱动联邦学习策略。

Method: 提出Size-Based Adaptive Federated Learning (SAFL)框架，通过系统组织异构多模态数据的联邦学习，基于数据集大小特性进行渐进式训练。

Result: 实验表明：1)联邦学习在1000-1500样本范围内效果最佳；2)结构化数据性能显著优于非结构化数据；3)超过2000样本的大数据集会导致性能下降。SAFL平均准确率达87.68%，通信效率高。

Conclusion: SAFL框架为理解数据特性如何影响联邦学习提供了理论和实践指导，填补了该领域的关键空白，并为实际部署提供了高效解决方案。

Abstract: Federated Learning (FL) has emerged as a transformative paradigm for
distributed machine learning while preserving data privacy. However, existing
approaches predominantly focus on model heterogeneity and aggregation
techniques, largely overlooking the fundamental impact of dataset size
characteristics on federated training dynamics. This paper introduces
Size-Based Adaptive Federated Learning (SAFL), a novel progressive training
framework that systematically organizes federated learning based on dataset
size characteristics across heterogeneous multi-modal data. Our comprehensive
experimental evaluation across 13 diverse datasets spanning 7 modalities
(vision, text, time series, audio, sensor, medical vision, and multimodal)
reveals critical insights: 1) an optimal dataset size range of 1000-1500
samples for federated learning effectiveness; 2) a clear modality performance
hierarchy with structured data (time series, sensor) significantly
outperforming unstructured data (text, multimodal); and 3) systematic
performance degradation for large datasets exceeding 2000 samples. SAFL
achieves an average accuracy of 87.68% across all datasets, with structured
data modalities reaching 99%+ accuracy. The framework demonstrates superior
communication efficiency, reducing total data transfer to 7.38 GB across 558
communications while maintaining high performance. Our real-time monitoring
framework provides unprecedented insights into system resource utilization,
network efficiency, and training dynamics. This work fills critical gaps in
understanding how data characteristics should drive federated learning
strategies, providing both theoretical insights and practical guidance for
real-world FL deployments in neural network and learning systems.

</details>


### [54] [E-ABIN: an Explainable module for Anomaly detection in BIological Networks](https://arxiv.org/abs/2506.20693)
*Ugo Lomoio,Tommaso Mazza,Pierangelo Veltri,Pietro Hiram Guzzi*

Main category: cs.LG

TL;DR: 论文介绍了E-ABIN，一个用于生物网络中异常检测的可解释框架，结合了机器学习和图深度学习技术，应用于膀胱癌和乳糜泻的研究。


<details>
  <summary>Details</summary>
Motivation: 随着大规模组学数据的增加，需要能够处理复杂基因表达数据并提供可解释结果的强大分析框架。当前基因异常检测方法多限于单一数据集且缺乏易用的图形界面。

Method: E-ABIN结合了支持向量机、随机森林、图自编码器（GAEs）和图对抗属性网络（GAANs）等算法，构建了一个统一且用户友好的平台。

Result: 通过膀胱癌和乳糜泻的案例研究，E-ABIN有效识别了与生物学相关的异常，并提供了对疾病机制的深入理解。

Conclusion: E-ABIN是一个通用且可解释的生物网络异常检测框架，具有高预测准确性和良好的可解释性，适用于复杂疾病研究。

Abstract: The increasing availability of large-scale omics data calls for robust
analytical frameworks capable of handling complex gene expression datasets
while offering interpretable results. Recent advances in artificial
intelligence have enabled the identification of aberrant molecular patterns
distinguishing disease states from healthy controls. Coupled with improvements
in model interpretability, these tools now support the identification of genes
potentially driving disease phenotypes. However, current approaches to gene
anomaly detection often remain limited to single datasets and lack accessible
graphical interfaces. Here, we introduce E-ABIN, a general-purpose, explainable
framework for Anomaly detection in Biological Networks. E-ABIN combines
classical machine learning and graph-based deep learning techniques within a
unified, user-friendly platform, enabling the detection and interpretation of
anomalies from gene expression or methylation-derived networks. By integrating
algorithms such as Support Vector Machines, Random Forests, Graph Autoencoders
(GAEs), and Graph Adversarial Attributed Networks (GAANs), E-ABIN ensures a
high predictive accuracy while maintaining interpretability. We demonstrate the
utility of E-ABIN through case studies of bladder cancer and coeliac disease,
where it effectively uncovers biologically relevant anomalies and offers
insights into disease mechanisms.

</details>


### [55] [On Context-Content Uncertainty Principle](https://arxiv.org/abs/2506.20699)
*Xin Li*

Main category: cs.LG

TL;DR: 论文提出了上下文-内容不确定性原理（CCUP），认为不确定性下的推理受上下文与内容间熵不对称性支配，并构建了分层计算框架，推导出四个层级的操作原则，为大脑和机器如何通过递归结构-特异性对齐最小化不确定性提供了统一理论。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于理解大脑和机器如何在不确定性条件下进行高效推理，通过探索上下文与内容之间的熵不对称性，提出一个统一的计算框架来解释和优化推理过程。

Method: 方法包括构建基于CCUP的分层计算框架，从基础熵最小化出发，逐步推导出四个层级的操作原则，并通过形式等价定理、依赖关系分析和计算模拟验证其有效性。

Result: 结果表明，CCUP框架能够显著提升推理效率，其分层原则（如核心推理约束、资源分配、时间引导学习和空间层级组合）为不确定性最小化提供了可操作路径，并通过模拟验证了理论优势。

Conclusion: 结论指出，CCUP不仅为理解大脑和机器的推理机制提供了统一理论基础，还揭示了通过内容导向的循环一致性熵梯度解析实现结构-特异性对齐的普适性机制。

Abstract: The Context-Content Uncertainty Principle (CCUP) proposes that inference
under uncertainty is governed by an entropy asymmetry between context and
content: high-entropy contexts must be interpreted through alignment with
low-entropy, structured content. In this paper, we develop a layered
computational framework that derives operational principles from this
foundational asymmetry. At the base level, CCUP formalizes inference as
directional entropy minimization, establishing a variational gradient that
favors content-first structuring. Building upon this, we identify four
hierarchical layers of operational principles: (\textbf{L1}) \emph{Core
Inference Constraints}, including structure-before-specificity, asymmetric
inference flow, cycle-consistent bootstrapping, and conditional compression,
all shown to be mutually reducible; (\textbf{L2}) \emph{Resource Allocation
Principles}, such as precision-weighted attention, asymmetric learning rates,
and attractor-based memory encoding; (\textbf{L3}) \emph{Temporal Bootstrapping
Dynamics}, which organize learning over time via structure-guided curricula;
and (\textbf{L4}) \emph{Spatial Hierarchical Composition}, which integrates
these mechanisms into self-organizing cycles of memory, inference, and
planning. We present formal equivalence theorems, a dependency lattice among
principles, and computational simulations demonstrating the efficiency gains of
CCUP-aligned inference. This work provides a unified theoretical foundation for
understanding how brains and machines minimize uncertainty through recursive
structure-specificity alignment. The brain is not just an inference machine. It
is a cycle-consistent entropy gradient resolver, aligning structure and
specificity via path-dependent, content-seeded simulation.

</details>


### [56] [Diffusion Tree Sampling: Scalable inference-time alignment of diffusion models](https://arxiv.org/abs/2506.20701)
*Vineet Jain,Kusha Sareen,Mohammad Pedramfar,Siamak Ravanbakhsh*

Main category: cs.LG

TL;DR: 该论文提出了一种基于树搜索的方法（DTS和DTS*），用于在推理时调整预训练的扩散模型，以更高效地生成符合目标分布的样本。


<details>
  <summary>Details</summary>
Motivation: 现有的扩散模型在推理时调整目标时存在价值估计不准确和计算资源利用效率低的问题，尤其是在高噪声水平下。

Method: 论文提出了一种基于蒙特卡洛树搜索的方法，通过将推理时对齐问题转化为搜索问题，并利用过去的计算信息来改进样本质量。具体包括扩散树采样（DTS）和其贪婪变体扩散树搜索（DTS*）。

Result: 在MNIST和CIFAR-10的类条件生成任务中，DTS在计算量减少10倍的情况下，达到了与最佳基线相当的FID分数。在文本到图像生成和语言完成任务中，DTS*能够高效搜索到高奖励样本，计算量减少5倍。

Conclusion: 通过复用过去生成的信息，该方法提供了一个可扩展的推理时对齐扩散模型的解决方案，能够将额外的计算资源转化为持续改进的样本质量。

Abstract: Adapting a pretrained diffusion model to new objectives at inference time
remains an open problem in generative modeling. Existing steering methods
suffer from inaccurate value estimation, especially at high noise levels, which
biases guidance. Moreover, information from past runs is not reused to improve
sample quality, resulting in inefficient use of compute. Inspired by the
success of Monte Carlo Tree Search, we address these limitations by casting
inference-time alignment as a search problem that reuses past computations. We
introduce a tree-based approach that samples from the reward-aligned target
density by propagating terminal rewards back through the diffusion chain and
iteratively refining value estimates with each additional generation. Our
proposed method, Diffusion Tree Sampling (DTS), produces asymptotically exact
samples from the target distribution in the limit of infinite rollouts, and its
greedy variant, Diffusion Tree Search (DTS$^\star$), performs a global search
for high reward samples. On MNIST and CIFAR-10 class-conditional generation,
DTS matches the FID of the best-performing baseline with up to $10\times$ less
compute. In text-to-image generation and language completion tasks, DTS$^\star$
effectively searches for high reward samples that match best-of-N with up to
$5\times$ less compute. By reusing information from previous generations, we
get an anytime algorithm that turns additional compute into steadily better
samples, providing a scalable approach for inference-time alignment of
diffusion models.

</details>


### [57] [On Convolutions, Intrinsic Dimension, and Diffusion Models](https://arxiv.org/abs/2506.20705)
*Kin Kwan Leung,Rasa Hosseinzadeh,Gabriel Loaiza-Ganem*

Main category: cs.LG

TL;DR: 该论文证明了FLIPD方法在现实假设下的正确性，填补了理论空白，并探讨了均匀卷积的类似结果。


<details>
  <summary>Details</summary>
Motivation: 扩散模型（DMs）能够学习低维支撑的分布，但FLIPD方法的理论基础仅在非现实的仿射子流形假设下被证明。本文旨在填补这一理论空白。

Method: 通过理论分析，证明了FLIPD方法在现实假设下的正确性，并探讨了高斯卷积替换为均匀卷积时的类似结果。

Result: FLIPD方法在现实假设下具有理论正确性，且均匀卷积也能得到类似结果，进一步扩展了其应用范围。

Conclusion: 本文完善了FLIPD方法的理论基础，为其在异常检测、对抗样本识别等领域的应用提供了更坚实的支持。

Abstract: The manifold hypothesis asserts that data of interest in high-dimensional
ambient spaces, such as image data, lies on unknown low-dimensional
submanifolds. Diffusion models (DMs) -- which operate by convolving data with
progressively larger amounts of Gaussian noise and then learning to revert this
process -- have risen to prominence as the most performant generative models,
and are known to be able to learn distributions with low-dimensional support.
For a given datum in one of these submanifolds, we should thus intuitively
expect DMs to have implicitly learned its corresponding local intrinsic
dimension (LID), i.e. the dimension of the submanifold it belongs to. Kamkari
et al. (2024b) recently showed that this is indeed the case by linking this LID
to the rate of change of the log marginal densities of the DM with respect to
the amount of added noise, resulting in an LID estimator known as FLIPD. LID
estimators such as FLIPD have a plethora of uses, among others they quantify
the complexity of a given datum, and can be used to detect outliers,
adversarial examples and AI-generated text. FLIPD achieves state-of-the-art
performance at LID estimation, yet its theoretical underpinnings are incomplete
since Kamkari et al. (2024b) only proved its correctness under the highly
unrealistic assumption of affine submanifolds. In this work we bridge this gap
by formally proving the correctness of FLIPD under realistic assumptions.
Additionally, we show that an analogous result holds when Gaussian convolutions
are replaced with uniform ones, and discuss the relevance of this result.

</details>


### [58] [Test-time Scaling Techniques in Theoretical Physics -- A Comparison of Methods on the TPBench Dataset](https://arxiv.org/abs/2506.20729)
*Zhiqi Gao,Tianyi Li,Yurii Kvasiuk,Sai Chaitanya Tadepalli,Maja Rudolph,Daniel J. H. Chung,Frederic Sala,Moritz Münchmeyer*

Main category: cs.LG

TL;DR: 论文研究了测试时扩展技术在高级理论物理领域的适用性，提出了一种新的符号弱验证框架，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 研究测试时扩展技术在数学推理基准（如AIME）上的成功是否可推广到高级理论物理领域。

Method: 开发了一种符号弱验证框架，以更好地利用物理问题的结构，提升并行扩展效果。

Result: 新方法在TPBench物理数据集上显著优于现有测试时扩展技术，并在AIME上验证了其解决高级数学问题的有效性。

Conclusion: 逐步符号验证在解决复杂科学问题中具有强大潜力。

Abstract: Large language models (LLMs) have shown strong capabilities in complex
reasoning, and test-time scaling techniques can enhance their performance with
comparably low cost. Many of these methods have been developed and evaluated on
mathematical reasoning benchmarks such as AIME. This paper investigates whether
the lessons learned from these benchmarks generalize to the domain of advanced
theoretical physics. We evaluate a range of common test-time scaling methods on
the TPBench physics dataset and compare their effectiveness with results on
AIME. To better leverage the structure of physics problems, we develop a novel,
symbolic weak-verifier framework to improve parallel scaling results. Our
empirical results demonstrate that this method significantly outperforms
existing test-time scaling approaches on TPBench. We also evaluate our method
on AIME, confirming its effectiveness in solving advanced mathematical
problems. Our findings highlight the power of step-wise symbolic verification
for tackling complex scientific problems.

</details>


### [59] [A Survey of AI for Materials Science: Foundation Models, LLM Agents, Datasets, and Tools](https://arxiv.org/abs/2506.20743)
*Minh-Hao Van,Prateek Verma,Chen Zhao,Xintao Wu*

Main category: cs.LG

TL;DR: 本文综述了基础模型在材料科学中的应用，包括其跨领域泛化能力、多模态特性及面临的挑战，并提出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 材料科学研究涉及多样化的数据类型和尺度，传统机器学习模型因其局限性难以应对。基础模型因其跨领域泛化和多模态能力，为材料科学提供了新的研究工具和方法。

Method: 通过任务驱动的分类法，将应用领域分为六大类，并综述了单模态和多模态基础模型、大型语言模型代理、标准化数据集及开源工具的最新进展。

Result: 基础模型在材料科学中已取得初步成功，但仍存在泛化性、可解释性、数据不平衡、安全问题和多模态融合不足等挑战。

Conclusion: 未来研究应集中在可扩展的预训练、持续学习、数据治理和可信赖性等方面，以进一步推动基础模型在材料科学中的应用。

Abstract: Foundation models (FMs) are catalyzing a transformative shift in materials
science (MatSci) by enabling scalable, general-purpose, and multimodal AI
systems for scientific discovery. Unlike traditional machine learning models,
which are typically narrow in scope and require task-specific engineering, FMs
offer cross-domain generalization and exhibit emergent capabilities. Their
versatility is especially well-suited to materials science, where research
challenges span diverse data types and scales. This survey provides a
comprehensive overview of foundation models, agentic systems, datasets, and
computational tools supporting this growing field. We introduce a task-driven
taxonomy encompassing six broad application areas: data extraction,
interpretation and Q\&A; atomistic simulation; property prediction; materials
structure, design and discovery; process planning, discovery, and optimization;
and multiscale modeling. We discuss recent advances in both unimodal and
multimodal FMs, as well as emerging large language model (LLM) agents.
Furthermore, we review standardized datasets, open-source tools, and autonomous
experimental platforms that collectively fuel the development and integration
of FMs into research workflows. We assess the early successes of foundation
models and identify persistent limitations, including challenges in
generalizability, interpretability, data imbalance, safety concerns, and
limited multimodal fusion. Finally, we articulate future research directions
centered on scalable pretraining, continual learning, data governance, and
trustworthiness.

</details>


### [60] [Multiple Streams of Relation Extraction: Enriching and Recalling in Transformers](https://arxiv.org/abs/2506.20746)
*Todd Nief,David Reber,Sean Richardson,Ari Holtzman*

Main category: cs.LG

TL;DR: 论文探讨了微调后LLM中关系信息的存储与提取机制，提出动态权重嫁接方法揭示信息提取与回忆的双路径机制。


<details>
  <summary>Details</summary>
Motivation: 现有定位方法（如激活修补）不适合分析微调后LLM中关系信息的存储位置与提取方式，因其可能破坏信息完整性。

Method: 采用动态权重嫁接技术，对比分析微调与预训练语言模型，研究关系信息的提取与回忆路径。

Result: 发现微调模型通过处理实体时提取信息，并在预测前通过特定注意力机制和关系提取步骤回忆信息。

Conclusion: 微调模型通过双路径机制处理关系信息，部分情况下单一路径即可满足需求，揭示了模型内部信息处理的具体层与组件。

Abstract: When an LLM learns a relation during finetuning (e.g., new movie releases,
corporate mergers, etc.), where does this information go? Is it extracted when
the model processes an entity, recalled just-in-time before a prediction, or
are there multiple separate heuristics? Existing localization approaches (e.g.
activation patching) are ill-suited for this analysis because they tend to
replace parts of the residual stream, potentially deleting information. To fill
this gap, we propose dynamic weight-grafting between fine-tuned and pre-trained
language models to show that fine-tuned language models both (1) extract
relation information learned during finetuning while processing entities and
(2) ``recall" this information in later layers while generating predictions. In
some cases, models need both of these pathways to correctly generate finetuned
information while, in other cases, a single ``enrichment" or ``recall" pathway
alone is sufficient. We examine the necessity and sufficiency of these
information pathways, examining what layers they occur at, how much redundancy
they exhibit, and which model components are involved -- finding that the
``recall" pathway occurs via both task-specific attention mechanisms and a
relation extraction step in the output of the attention and the feedforward
networks at the final layers before next token prediction.

</details>


### [61] [Characterization and Mitigation of Training Instabilities in Microscaling Formats](https://arxiv.org/abs/2506.20752)
*Huangyuan Su,Mujin Kwun,Stephanie Gil,Sham Kakade,Nikhil Anand*

Main category: cs.LG

TL;DR: 论文研究了在大型语言模型训练中使用低精度算术格式（如MX格式）的挑战和可行性，发现训练过程中会出现不稳定性，并提出了一些稳定化策略。


<details>
  <summary>Details</summary>
Motivation: 训练大型语言模型是一个计算密集且昂贵的过程，随着模型规模扩大、算法改进和新数据的收集，这一过程需要不断重复。为了应对这一问题，新一代硬件加速器开始支持低精度算术格式，如NVIDIA Blackwell架构中的MX格式。这些格式通过共享参数块内的比例来扩展可表示范围，并在降低精度的GEMM操作中提高效率。

Method: 论文通过训练近千个语言模型，覆盖不同的计算预算和权重-激活精度组合，研究了块缩放精度格式在模型训练中的挑战和可行性。此外，还通过控制实验和消融研究，分析了架构设置、超参数和精度格式对训练稳定性的影响。

Result: 研究发现，使用MX格式训练时，损失函数会出现尖锐的随机不稳定性，尤其是在较大计算规模下。通过实验，论文提出了一个简单模型，解释了量化层规范参数和少量激活引入的乘法梯度偏差如何引发失控发散。通过干预实验，论文展示了通过修改精度方案可以避免或延迟不稳定性。

Conclusion: 论文提出了一些稳定化策略，并在大型语言模型设置中评估了这些策略，结果表明某些混合配置可以恢复与全精度训练相竞争的性能。

Abstract: Training large language models is an expensive, compute-bound process that
must be repeated as models scale, algorithms improve, and new data is
collected. To address this, next-generation hardware accelerators increasingly
support lower-precision arithmetic formats, such as the Microscaling (MX)
formats introduced in NVIDIA's Blackwell architecture. These formats use a
shared scale within blocks of parameters to extend representable range and
perform forward/backward GEMM operations in reduced precision for efficiency
gains. In this work, we investigate the challenges and viability of
block-scaled precision formats during model training. Across nearly one
thousand language models trained from scratch -- spanning compute budgets from
$2 \times 10^{17}$ to $4.8 \times 10^{19}$ FLOPs and sweeping over a broad
range of weight-activation precision combinations -- we consistently observe
that training in MX formats exhibits sharp, stochastic instabilities in the
loss, particularly at larger compute scales. To explain this phenomenon, we
conduct controlled experiments and ablations on a smaller proxy model that
exhibits similar behavior as the language model, sweeping across architectural
settings, hyperparameters, and precision formats. These experiments motivate a
simple model in which multiplicative gradient bias introduced by the
quantization of layer-norm affine parameters and a small fraction of
activations can trigger runaway divergence. Through \emph{in situ} intervention
experiments on our proxy model, we demonstrate that instabilities can be
averted or delayed by modifying precision schemes mid-training. Guided by these
findings, we evaluate stabilization strategies in the LLM setting and show that
certain hybrid configurations recover performance competitive with
full-precision training. We release our code at
https://github.com/Hither1/systems-scaling.

</details>


### [62] [Stochastic and Non-local Closure Modeling for Nonlinear Dynamical Systems via Latent Score-based Generative Models](https://arxiv.org/abs/2506.20771)
*Xinghao Dong,Huchen Yang,Jin-Long Wu*

Main category: cs.LG

TL;DR: 提出了一种基于潜在分数的生成AI框架，用于学习计算力学中非线性动力系统的随机非局部闭合模型，通过联合训练卷积自编码器和条件扩散模型降低计算成本并保持预测精度。


<details>
  <summary>Details</summary>
Motivation: 针对复杂多尺度动力系统建模中缺乏明确尺度分离的挑战，传统闭合模型在确定性及局部假设上受限，而现有扩散模型计算成本过高，难以实际应用。

Method: 联合训练卷积自编码器与潜在空间中的条件扩散模型，通过降维保留物理特性，显著减少采样过程的计算负担。

Result: 数值实验表明，联合训练方法发现的潜在空间不仅重构误差小，且扩散模型性能良好；集成到数值模拟中时，在保持精度的同时显著加速计算。

Conclusion: 基于潜在条件扩散模型的随机建模框架实现了计算效率与预测精度的平衡，为工程湍流等实际应用提供了可行解决方案。

Abstract: We propose a latent score-based generative AI framework for learning
stochastic, non-local closure models and constitutive laws in nonlinear
dynamical systems of computational mechanics. This work addresses a key
challenge of modeling complex multiscale dynamical systems without a clear
scale separation, for which numerically resolving all scales is prohibitively
expensive, e.g., for engineering turbulent flows. While classical closure
modeling methods leverage domain knowledge to approximate subgrid-scale
phenomena, their deterministic and local assumptions can be too restrictive in
regimes lacking a clear scale separation. Recent developments of
diffusion-based stochastic models have shown promise in the context of closure
modeling, but their prohibitive computational inference cost limits practical
applications for many real-world applications. This work addresses this
limitation by jointly training convolutional autoencoders with conditional
diffusion models in the latent spaces, significantly reducing the
dimensionality of the sampling process while preserving essential physical
characteristics. Numerical results demonstrate that the joint training approach
helps discover a proper latent space that not only guarantees small
reconstruction errors but also ensures good performance of the diffusion model
in the latent space. When integrated into numerical simulations, the proposed
stochastic modeling framework via latent conditional diffusion models achieves
significant computational acceleration while maintaining comparable predictive
accuracy to standard diffusion models in physical spaces.

</details>


### [63] [Stochastic Parameter Decomposition](https://arxiv.org/abs/2506.20790)
*Lucius Bushnaq,Dan Braun,Lee Sharkey*

Main category: cs.LG

TL;DR: 本文提出了一种名为随机参数分解（SPD）的新方法，用于更高效、更稳健地分解神经网络参数，解决了现有方法（如APD）的计算成本高和超参数敏感性问题。


<details>
  <summary>Details</summary>
Motivation: 当前神经网络分解方法中的线性参数分解框架存在计算成本高和对超参数敏感的问题，限制了其在更大、更复杂模型上的应用。

Method: 引入了随机参数分解（SPD）方法，该方法比现有的基于归因的参数分解（APD）更具可扩展性和超参数鲁棒性。

Result: SPD能够分解比APD更大、更复杂的模型，避免了参数收缩等问题，并在玩具模型中更好地识别了真实机制。

Conclusion: SPD通过结合因果中介分析和网络分解方法，为机制可解释性研究开辟了新的可能性，并提供了可扩展的线性参数分解方法。

Abstract: A key step in reverse engineering neural networks is to decompose them into
simpler parts that can be studied in relative isolation. Linear parameter
decomposition -- a framework that has been proposed to resolve several issues
with current decomposition methods -- decomposes neural network parameters into
a sum of sparsely used vectors in parameter space. However, the current main
method in this framework, Attribution-based Parameter Decomposition (APD), is
impractical on account of its computational cost and sensitivity to
hyperparameters. In this work, we introduce \textit{Stochastic Parameter
Decomposition} (SPD), a method that is more scalable and robust to
hyperparameters than APD, which we demonstrate by decomposing models that are
slightly larger and more complex than was possible to decompose with APD. We
also show that SPD avoids other issues, such as shrinkage of the learned
parameters, and better identifies ground truth mechanisms in toy models. By
bridging causal mediation analysis and network decomposition methods, this
demonstration opens up new research possibilities in mechanistic
interpretability by removing barriers to scaling linear parameter decomposition
methods to larger models. We release a library for running SPD and reproducing
our experiments at https://github.com/goodfire-ai/spd.

</details>


### [64] [GPU Kernel Scientist: An LLM-Driven Framework for Iterative Kernel Optimization](https://arxiv.org/abs/2506.20807)
*Martin Andrews,Sam Witteveen*

Main category: cs.LG

TL;DR: 该论文提出了一种基于LLM的自动化方法，用于优化GPU内核，特别是在资源受限或硬件快速变化的环境中。


<details>
  <summary>Details</summary>
Motivation: 优化GPU内核是一个复杂且耗时的过程，尤其是在针对较新或文档较少的GPU架构时，传统开发辅助工具稀缺。

Method: 采用多阶段进化过程，利用LLM选择代码版本、生成优化假设并自主实现实验，仅通过观察计时数据作为性能反馈。

Result: 由于性能竞赛的定量结果在提交时被禁止公开，论文主要展示了架构设计、工作流程和定性见解。

Conclusion: LLM驱动的代理有潜力在资源受限或快速演进的硬件环境中民主化和加速GPU内核优化。

Abstract: Optimizing GPU kernels for high performance is a complex task, often
demanding deep architectural knowledge, extensive profiling, and iterative
experimentation. This challenge is amplified when targeting newer or
less-documented GPU architectures where traditional development aids are
scarce. This paper introduces an LLM-powered "GPU Kernel Scientist," an
automated methodology for iteratively refining accelerator kernels.
  Our methodology employs LLMs in a multi-stage, evolutionary process: (a)
strategically selecting promising prior code versions as a basis for new
iterations; (b) generating hypotheses for optimization experiments, based on
existing code and assimilated knowledge from general GPU literature; and (c)
autonomously implementing these experiments through code modification and
subsequent submission to an external evaluation system, using only observed
timing data as performance feedback. We detail how this approach navigates the
challenges of the AMD MI300 target architecture and leverages LLMs to
compensate for limited domain-specific human expertise.
  Since quantitative results from an ongoing performance competition were
embargoed on paper submission date, we present the architectural design,
operational workflow, and qualitative insights, highlighting the potential of
LLM-driven agents to democratise and accelerate GPU kernel optimization,
especially in resource-constrained or rapidly evolving hardware environments.

</details>


### [65] [FINN-GL: Generalized Mixed-Precision Extensions for FPGA-Accelerated LSTMs](https://arxiv.org/abs/2506.20810)
*Shashwat Khandelwal,Jakoba Petri-Koenig,Thomas B. Preußer,Michaela Blott,Shreejith Shanker*

Main category: cs.LG

TL;DR: 该论文提出了一种利用FINN框架在FPGA上高效部署LSTM的方法，通过量化技术和硬件加速优化，在保持精度的同时提升了性能和资源效率。


<details>
  <summary>Details</summary>
Motivation: 尽管RNN（尤其是LSTM）在时间序列任务中表现优异，但其计算复杂度在资源受限环境中的实时部署面临挑战。FPGA虽为高效AI加速提供了可能，但现有工具主要针对前馈网络，LSTM加速通常需要完全定制实现。

Method: 论文利用开源的FINN框架，通过ONNX的Scan操作符建模LSTM的循环计算，支持混合量化和功能验证，并在FINN编译器中引入自定义转换，将量化后的计算图映射到硬件块。

Result: 通过量化ConvLSTM模型在股价预测任务上的验证，生成的硬件IP在性能和资源消耗之间取得了平衡，同时保持了与先进模型相当的推理精度。

Conclusion: 论文提出的通用流程为FPGA上资源高效的RNN加速器设计铺平了道路，展示了在资源受限环境中高效部署LSTM的潜力。

Abstract: Recurrent neural networks (RNNs), particularly LSTMs, are effective for
time-series tasks like sentiment analysis and short-term stock prediction.
However, their computational complexity poses challenges for real-time
deployment in resource constrained environments. While FPGAs offer a promising
platform for energy-efficient AI acceleration, existing tools mainly target
feed-forward networks, and LSTM acceleration typically requires full custom
implementation. In this paper, we address this gap by leveraging the
open-source and extensible FINN framework to enable the generalized deployment
of LSTMs on FPGAs. Specifically, we leverage the Scan operator from the Open
Neural Network Exchange (ONNX) specification to model the recurrent nature of
LSTM computations, enabling support for mixed quantisation within them and
functional verification of LSTM-based models. Furthermore, we introduce custom
transformations within the FINN compiler to map the quantised ONNX computation
graph to hardware blocks from the HLS kernel library of the FINN compiler and
Vitis HLS. We validate the proposed tool-flow by training a quantised ConvLSTM
model for a mid-price stock prediction task using the widely used dataset and
generating a corresponding hardware IP of the model using our flow, targeting
the XCZU7EV device. We show that the generated quantised ConvLSTM accelerator
through our flow achieves a balance between performance (latency) and resource
consumption, while matching (or bettering) inference accuracy of
state-of-the-art models with reduced precision. We believe that the
generalisable nature of the proposed flow will pave the way for
resource-efficient RNN accelerator designs on FPGAs.

</details>


### [66] [Divide, Specialize, and Route: A New Approach to Efficient Ensemble Learning](https://arxiv.org/abs/2506.20814)
*Jakub Piwko,Jędrzej Ruciński,Dawid Płudowski,Antoni Zajko,Patryzja Żak,Mateusz Zacharecki,Anna Kozak,Katarzyna Woźnica*

Main category: cs.LG

TL;DR: Hellsemble是一种新颖且可解释的集成学习框架，通过根据数据复杂度分层训练模型，提升分类性能并保持计算效率。


<details>
  <summary>Details</summary>
Motivation: 传统集成学习方法（如bagging、boosting和DES）计算成本高且难以适应异构数据分布，需要更高效且自适应的解决方案。

Method: Hellsemble通过迭代将误分类实例传递给后续模型，逐步划分数据复杂度，并训练专用基学习器，同时使用路由模型分配新实例。

Result: 在OpenML-CC18和Tabzilla基准测试中，Hellsemble通常优于传统集成方法，展现了更高的分类准确性和效率。

Conclusion: 基于实例难度构建集成系统是提升效率和鲁棒性的有效途径，Hellsemble为此提供了可行方案。

Abstract: Ensemble learning has proven effective in boosting predictive performance,
but traditional methods such as bagging, boosting, and dynamic ensemble
selection (DES) suffer from high computational cost and limited adaptability to
heterogeneous data distributions. To address these limitations, we propose
Hellsemble, a novel and interpretable ensemble framework for binary
classification that leverages dataset complexity during both training and
inference. Hellsemble incrementally partitions the dataset into circles of
difficulty by iteratively passing misclassified instances from simpler models
to subsequent ones, forming a committee of specialised base learners. Each
model is trained on increasingly challenging subsets, while a separate router
model learns to assign new instances to the most suitable base model based on
inferred difficulty. Hellsemble achieves strong classification accuracy while
maintaining computational efficiency and interpretability. Experimental results
on OpenML-CC18 and Tabzilla benchmarks demonstrate that Hellsemble often
outperforms classical ensemble methods. Our findings suggest that embracing
instance-level difficulty offers a promising direction for constructing
efficient and robust ensemble systems.

</details>


### [67] [Universal and Efficient Detection of Adversarial Data through Nonuniform Impact on Network Layers](https://arxiv.org/abs/2506.20816)
*Furkan Mumcu,Yasin Yilmaz*

Main category: cs.LG

TL;DR: 该论文提出了一种新颖的通用高效方法来检测对抗样本，通过分析攻击对不同DNN层的影响程度，训练轻量级回归模型预测深层特征，并利用预测误差检测对抗样本。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络（DNNs）对对抗性输入设计非常脆弱，现有的防御方法要么通过抵消扰动效果来提高DNN鲁棒性，要么使用辅助模型检测对抗数据。然而，这些方法要么对最新攻击技术无效，要么计算效率低下，无法实时处理。因此，论文提出了一种更实用的攻击检测方法。

Method: 论文提出了一种新颖的通用高效方法，通过分析攻击对不同DNN层的影响程度，训练一个轻量级回归模型来从早期层特征预测深层特征，并利用预测误差来检测对抗样本。

Result: 通过理论论证和大量实验，论文证明该检测方法高度有效，计算效率高，适用于实时处理，兼容任何DNN架构，并适用于不同领域（如图像、视频和音频）。

Conclusion: 该论文提出的对抗样本检测方法在效果、效率和通用性方面均表现出色，为DNN防御对抗攻击提供了一种实用的解决方案。

Abstract: Deep Neural Networks (DNNs) are notoriously vulnerable to adversarial input
designs with limited noise budgets. While numerous successful attacks with
subtle modifications to original input have been proposed, defense techniques
against these attacks are relatively understudied. Existing defense approaches
either focus on improving DNN robustness by negating the effects of
perturbations or use a secondary model to detect adversarial data. Although
equally important, the attack detection approach, which is studied in this
work, provides a more practical defense compared to the robustness approach. We
show that the existing detection methods are either ineffective against the
state-of-the-art attack techniques or computationally inefficient for real-time
processing. We propose a novel universal and efficient method to detect
adversarial examples by analyzing the varying degrees of impact of attacks on
different DNN layers. {Our method trains a lightweight regression model that
predicts deeper-layer features from early-layer features, and uses the
prediction error to detect adversarial samples.} Through theoretical arguments
and extensive experiments, we demonstrate that our detection method is highly
effective, computationally efficient for real-time processing, compatible with
any DNN architecture, and applicable across different domains, such as image,
video, and audio.

</details>


### [68] [Demystifying Distributed Training of Graph Neural Networks for Link Prediction](https://arxiv.org/abs/2506.20818)
*Xin Huang,Chul-Ho Lee*

Main category: cs.LG

TL;DR: 该论文研究了分布式图神经网络（GNN）在链接预测任务中的性能下降问题，并提出了一种名为SpLPG的方法，通过图稀疏化减少通信开销，同时保持预测准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的分布式GNN框架主要针对节点分类任务优化，而在链接预测任务中的性能表现尚未充分探索。研究发现，由于图分区导致的信息丢失和负采样方式不当，分布式训练会导致性能下降。

Method: 论文提出SpLPG方法，通过图稀疏化技术减少通信开销，同时保留关键信息以维持链接预测的准确性。

Result: 实验结果表明，SpLPG在多个真实数据集上有效减少了高达80%的通信开销，同时基本保持了链接预测的准确性。

Conclusion: SpLPG通过图稀疏化解决了分布式GNN在链接预测中的性能下降问题，显著降低了通信成本，为大规模图数据的链接预测提供了高效解决方案。

Abstract: Graph neural networks (GNNs) are powerful tools for solving graph-related
problems. Distributed GNN frameworks and systems enhance the scalability of
GNNs and accelerate model training, yet most are optimized for node
classification. Their performance on link prediction remains underexplored.
This paper demystifies distributed training of GNNs for link prediction by
investigating the issue of performance degradation when each worker trains a
GNN on its assigned partitioned subgraph without having access to the entire
graph. We discover that the main sources of the issue come from not only the
information loss caused by graph partitioning but also the ways of drawing
negative samples during model training. While sharing the complete graph
information with each worker resolves the issue and preserves link prediction
accuracy, it incurs a high communication cost. We propose SpLPG, which
effectively leverages graph sparsification to mitigate the issue of performance
degradation at a reduced communication cost. Experiment results on several
public real-world datasets demonstrate the effectiveness of SpLPG, which
reduces the communication overhead by up to about 80% while mostly preserving
link prediction accuracy.

</details>


### [69] [Learning-Based Resource Management in Integrated Sensing and Communication Systems](https://arxiv.org/abs/2506.20849)
*Ziyang Lu,M. Cenk Gursoy,Chilukuri K. Mohan,Pramod K. Varshney*

Main category: cs.LG

TL;DR: 提出了一种基于约束深度强化学习的方法，用于优化雷达通信系统中的时间资源分配，以提升目标通信质量。


<details>
  <summary>Details</summary>
Motivation: 在集成感知与通信系统中，如何高效分配时间资源以同时满足多目标跟踪和数据传输的需求是一个关键挑战。

Method: 采用约束深度强化学习（CDRL）方法，优化跟踪与通信之间的时间分配，确保在时间预算限制下最大化通信质量。

Result: 数值实验表明，所提出的CDRL框架在动态环境中能有效提升通信质量，同时满足时间约束。

Conclusion: 该研究为雷达通信系统中的自适应时间分配提供了一种高效解决方案，显著提升了系统性能。

Abstract: In this paper, we tackle the task of adaptive time allocation in integrated
sensing and communication systems equipped with radar and communication units.
The dual-functional radar-communication system's task involves allocating dwell
times for tracking multiple targets and utilizing the remaining time for data
transmission towards estimated target locations. We introduce a novel
constrained deep reinforcement learning (CDRL) approach, designed to optimize
resource allocation between tracking and communication under time budget
constraints, thereby enhancing target communication quality. Our numerical
results demonstrate the efficiency of our proposed CDRL framework, confirming
its ability to maximize communication quality in highly dynamic environments
while adhering to time constraints.

</details>


### [70] [Multi-Objective Reinforcement Learning for Cognitive Radar Resource Management](https://arxiv.org/abs/2506.20853)
*Ziyang Lu,Subodh Kalia,M. Cenk Gursoy,Chilukuri K. Mohan,Pramod K. Varshney*

Main category: cs.LG

TL;DR: 该论文研究了多功能认知雷达系统中的时间分配问题，通过深度强化学习算法寻找帕累托最优解，并比较了DDPG和SAC算法的性能。


<details>
  <summary>Details</summary>
Motivation: 解决多功能认知雷达系统在动态环境中平衡新目标扫描和已检测目标跟踪的时间分配问题。

Method: 采用多目标优化问题建模，使用深度强化学习（DDPG和SAC算法）寻找帕累托最优解，并利用NSGA-II算法估计帕累托前沿上界。

Result: 实验结果表明，DDPG和SAC算法均能有效适应不同场景，其中SAC在稳定性和样本效率上表现更优。

Conclusion: 该研究为开发更高效、自适应的认知雷达系统提供了方法，能够在动态环境中平衡多个竞争目标。

Abstract: The time allocation problem in multi-function cognitive radar systems focuses
on the trade-off between scanning for newly emerging targets and tracking the
previously detected targets. We formulate this as a multi-objective
optimization problem and employ deep reinforcement learning to find
Pareto-optimal solutions and compare deep deterministic policy gradient (DDPG)
and soft actor-critic (SAC) algorithms. Our results demonstrate the
effectiveness of both algorithms in adapting to various scenarios, with SAC
showing improved stability and sample efficiency compared to DDPG. We further
employ the NSGA-II algorithm to estimate an upper bound on the Pareto front of
the considered problem. This work contributes to the development of more
efficient and adaptive cognitive radar systems capable of balancing multiple
competing objectives in dynamic environments.

</details>


### [71] [Leaner Training, Lower Leakage: Revisiting Memorization in LLM Fine-Tuning with LoRA](https://arxiv.org/abs/2506.20856)
*Fei Wang,Baochun Li*

Main category: cs.LG

TL;DR: LoRA微调相比全参数微调显著降低记忆风险，同时保持良好任务性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）的记忆特性使其易受数据提取攻击。虽然预训练阶段的记忆问题已被广泛研究，但微调阶段（尤其是广泛采用的参数高效方法LoRA）的影响尚未深入探索。

Method: 使用基于相似性的记忆度量标准，重新评估不同微调策略下的记忆现象，重点关注模型规模和数据重复等因素在LoRA微调中的表现。

Result: 发现LoRA微调与全参数微调在记忆模式上存在显著差异：模型规模和数据重复等传统影响因素在LoRA中呈现不同趋势，且LoRA能有效降低记忆风险。

Conclusion: LoRA作为一种参数高效的微调方法，在保持模型性能的同时，显著降低了记忆敏感数据的风险，为安全部署提供了新思路。

Abstract: Memorization in large language models (LLMs) makes them vulnerable to data
extraction attacks. While pre-training memorization has been extensively
studied, fewer works have explored its impact in fine-tuning, particularly for
LoRA fine-tuning, a widely adopted parameter-efficient method.
  In this work, we re-examine memorization in fine-tuning and uncover a
surprising divergence from prior findings across different fine-tuning
strategies. Factors such as model scale and data duplication, which strongly
influence memorization in pre-training and full fine-tuning, do not follow the
same trend in LoRA fine-tuning. Using a more relaxed similarity-based
memorization metric, we demonstrate that LoRA significantly reduces
memorization risks compared to full fine-tuning, while still maintaining strong
task performance.

</details>


### [72] [Omniwise: Predicting GPU Kernels Performance with LLMs](https://arxiv.org/abs/2506.20886)
*Zixian Wang,Cole Ramos,Muhammad A. Awad,Keith Lowery*

Main category: cs.LG

TL;DR: Omniwise是一个端到端、自监督的微调管道，首次将大型语言模型应用于GPU内核性能预测，无需代码执行或分析工具即可预测关键性能指标。


<details>
  <summary>Details</summary>
Motivation: 近年来，深度神经网络的快速发展使得人工智能模型在理解、生成和处理复杂数据方面具有前所未有的能力。然而，如何高效预测GPU内核性能仍是一个挑战。

Method: Omniwise是一个模型无关且轻量级的管道，使用自监督微调方法，直接从内核代码预测性能指标，如内存带宽、缓存命中率等。

Result: 在AMD MI250和MI300X架构上，Omniwise的预测结果相对误差在10%以内的比例超过90%。

Conclusion: Omniwise不仅提供了一个高效的性能预测管道，还通过在线推理服务器和Visual Studio Code插件将其集成到开发者的工作流程中。

Abstract: In recent years, the rapid advancement of deep neural networks (DNNs) has
revolutionized artificial intelligence, enabling models with unprecedented
capabilities in understanding, generating, and processing complex data. These
powerful architectures have transformed a wide range of downstream
applications, tackling tasks beyond human reach. In this paper, we introduce
Omniwise, the first end-to-end, self-supervised fine-tuning pipeline that
applies large language models (LLMs) to GPU kernel performance prediction--a
novel use case in performance profiling. Omniwise is model-agnostic and
lightweight, achieving strong results even with a small 3B-parameter model. It
can predict key performance metrics, including memory bandwidth, cache hit
rates, GFLOPs, and arithmetic intensity, directly from kernel code without the
need for code execution or profiling tools. Our approach achieves over 90% of
predictions within 10% relative error on GPU kernels executed on AMD MI250 and
MI300X architectures. In addition to the pipeline, we develop an online
inference server and a Visual Studio Code plugin that seamlessly integrate
LLM-based performance prediction into developers' workflows.

</details>


### [73] [On the Necessity of Output Distribution Reweighting for Effective Class Unlearning](https://arxiv.org/abs/2506.20893)
*Yian Wang,Ali Ebrahimpour-Boroojeny,Hari Sundaram*

Main category: cs.LG

TL;DR: 本文提出了一种轻量级的输出重加权遗忘方法RWFT，无需完全重新训练即可从分类器中擦除整个类别，并通过新攻击MIA-NN和TV距离指标验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有遗忘方法无法完全模拟重新训练模型的行为，且完全重新训练成本高昂。为保障用户删除权和减少有害/偏见预测，需要更高效的类别遗忘技术。

Method: 通过概率质量重新分配处理被遗忘类别的样本预测，设计MIA-NN攻击验证漏洞，并提出基于TV距离的新指标量化信息残留。

Result: RWFT在传统指标上优于现有方法2.79%，在TV指标上提升111.45%，且完全匹配重新训练模型的表现。

Conclusion: RWFT首次实现无需重新训练的类别遗忘，其防御能力通过新攻击验证，TV指标为未来方法提供了鲁棒性评估标准。

Abstract: In this work, we introduce an output-reweighting unlearning method, RWFT, a
lightweight technique that erases an entire class from a trained classifier
without full retraining. Forgetting specific classes from trained models is
essential for enforcing user deletion rights and mitigating harmful or biased
predictions. The full retraining is costly and existing unlearning methods fail
to replicate the behavior of the retrained models when predicting samples from
the unlearned class. We prove this failure by designing a variant of membership
inference attacks, MIA-NN that successfully reveals the unlearned class for any
of these methods. We propose a simple redistribution of the probability mass
for the prediction on the samples in the forgotten class which is robust to
MIA-NN. We also introduce a new metric based on the total variation (TV)
distance of the prediction probabilities to quantify residual leakage to
prevent future methods from susceptibility to the new attack. Through extensive
experiments with state of the art baselines in machine unlearning, we show that
our approach matches the results of full retraining in both metrics used for
evaluation by prior work and the new metric we propose in this work. Compare to
state-of-the-art methods, we gain 2.79% in previously used metrics and 111.45%
in our new TV-based metric over the best existing method.

</details>


### [74] [Graph-Structured Feedback Multimodel Ensemble Online Conformal Prediction](https://arxiv.org/abs/2506.20898)
*Erfan Hajihashemi,Yanning Shen*

Main category: cs.LG

TL;DR: 提出了一种新的多模型在线共形预测算法，通过二分图反馈动态选择有效模型子集，降低计算复杂度并缩小预测集大小，同时保证覆盖率和次线性遗憾。


<details>
  <summary>Details</summary>
Motivation: 多模型在线共形预测虽能应对分布漂移，但预选模型集的选择带来挑战：过多模型增加计算负担，无关模型可能降低性能并导致预测集过大。

Method: 设计动态二分图反馈机制，每步筛选有效模型子集，从中选择模型构建预测集；结合预测集大小和模型损失反馈提升效率。

Result: 算法在真实和合成数据集上验证，能构建更小预测集且优于现有方法，同时确保有效覆盖和次线性遗憾。

Conclusion: 所提方法通过动态模型选择和双重反馈机制，显著提升了多模型在线共形预测的效率和预测集质量。

Abstract: Online conformal prediction has demonstrated its capability to construct a
prediction set for each incoming data point that covers the true label with a
predetermined probability. To cope with potential distribution shift,
multi-model online conformal prediction has been introduced to select and
leverage different models from a preselected candidate set. Along with the
improved flexibility, the choice of the preselected set also brings challenges.
A candidate set that includes a large number of models may increase the
computational complexity. In addition, the inclusion of irrelevant models with
poor performance may negatively impact the performance and lead to
unnecessarily large prediction sets. To address these challenges, we propose a
novel multi-model online conformal prediction algorithm that identifies a
subset of effective models at each time step by collecting feedback from a
bipartite graph, which is refined upon receiving new data. A model is then
selected from this subset to construct the prediction set, resulting in reduced
computational complexity and smaller prediction sets. Additionally, we
demonstrate that using prediction set size as feedback, alongside model loss,
can significantly improve efficiency by constructing smaller prediction sets
while still satisfying the required coverage guarantee. The proposed algorithms
are proven to ensure valid coverage and achieve sublinear regret. Experiments
on real and synthetic datasets validate that the proposed methods construct
smaller prediction sets and outperform existing multi-model online conformal
prediction approaches.

</details>


### [75] [Optimal Single-Policy Sample Complexity and Transient Coverage for Average-Reward Offline RL](https://arxiv.org/abs/2506.20904)
*Matthew Zurek,Guy Zamir,Yudong Chen*

Main category: cs.LG

TL;DR: 本文研究了平均奖励MDP中的离线强化学习，提出了仅依赖目标策略的复杂度度量方法，并开发了一种基于悲观折扣值迭代的新算法。


<details>
  <summary>Details</summary>
Motivation: 现有工作在单一策略数据覆盖假设下获得性能保证，但这些保证利用了所有策略统一的复杂度度量，如均匀混合时间。本文旨在开发仅依赖目标策略的复杂度度量方法。

Method: 引入了一种基于悲观折扣值迭代的算法，并结合了一种新颖的分位数裁剪技术，以使用更尖锐的基于经验跨度的惩罚函数。

Result: 首次实现了完全单一策略样本复杂度边界，并首次处理了一般弱通信MDP，无需任何先验参数知识。

Conclusion: 本文通过硬示例表明，学习条件需要超越目标策略的稳态分布的覆盖假设，区分了单一策略复杂度度量与先前研究的情况。

Abstract: We study offline reinforcement learning in average-reward MDPs, which
presents increased challenges from the perspectives of distribution shift and
non-uniform coverage, and has been relatively underexamined from a theoretical
perspective. While previous work obtains performance guarantees under
single-policy data coverage assumptions, such guarantees utilize additional
complexity measures which are uniform over all policies, such as the uniform
mixing time. We develop sharp guarantees depending only on the target policy,
specifically the bias span and a novel policy hitting radius, yielding the
first fully single-policy sample complexity bound for average-reward offline
RL. We are also the first to handle general weakly communicating MDPs,
contrasting restrictive structural assumptions made in prior work. To achieve
this, we introduce an algorithm based on pessimistic discounted value iteration
enhanced by a novel quantile clipping technique, which enables the use of a
sharper empirical-span-based penalty function. Our algorithm also does not
require any prior parameter knowledge for its implementation. Remarkably, we
show via hard examples that learning under our conditions requires coverage
assumptions beyond the stationary distribution of the target policy,
distinguishing single-policy complexity measures from previously examined
cases. We also develop lower bounds nearly matching our main result.

</details>


### [76] [Explainable AI for Radar Resource Management: Modified LIME in Deep Reinforcement Learning](https://arxiv.org/abs/2506.20916)
*Ziyang Lu,M. Cenk Gursoy,Chilukuri K. Mohan,Pramod K. Varshney*

Main category: cs.LG

TL;DR: 论文提出了一种改进的LIME方法DL-LIME，通过将深度学习融入采样过程，提高了在雷达资源管理中深度强化学习的解释性和任务性能。


<details>
  <summary>Details</summary>
Motivation: 神经网络的黑箱特性限制了其在决策过程中的可解释性，特别是在雷达资源管理（RRM）等关键领域。现有的LIME方法在采样过程中忽略了特征间的相关性，因此需要一种更有效的解释性AI方法。

Method: 提出了一种改进的LIME方法DL-LIME，将深度学习（DL）融入采样过程，以更好地捕捉特征间的相关性，并将其应用于深度强化学习的雷达资源管理中。

Result: 数值结果表明，DL-LIME在保真度和任务性能上均优于传统LIME方法，同时揭示了雷达资源管理决策中的关键因素。

Conclusion: DL-LIME不仅提高了深度强化学习的解释性，还显著提升了任务性能，为雷达资源管理等领域的决策提供了更透明的支持。

Abstract: Deep reinforcement learning has been extensively studied in decision-making
processes and has demonstrated superior performance over conventional
approaches in various fields, including radar resource management (RRM).
However, a notable limitation of neural networks is their ``black box" nature
and recent research work has increasingly focused on explainable AI (XAI)
techniques to describe the rationale behind neural network decisions. One
promising XAI method is local interpretable model-agnostic explanations (LIME).
However, the sampling process in LIME ignores the correlations between
features. In this paper, we propose a modified LIME approach that integrates
deep learning (DL) into the sampling process, which we refer to as DL-LIME. We
employ DL-LIME within deep reinforcement learning for radar resource
management. Numerical results show that DL-LIME outperforms conventional LIME
in terms of both fidelity and task performance, demonstrating superior
performance with both metrics. DL-LIME also provides insights on which factors
are more important in decision making for radar resource management.

</details>


### [77] [LLM-guided Chemical Process Optimization with a Multi-Agent Approach](https://arxiv.org/abs/2506.20921)
*Tong Zeng,Srivathsan Badrinarayanan,Janghoon Ock,Cheng-Kai Lai,Amir Barati Farimani*

Main category: cs.LG

TL;DR: 论文提出了一种基于多智能体大语言模型（LLM）的框架，用于在操作约束不明确或不可用时自主推断约束并优化化学过程，显著提高了计算效率和性能。


<details>
  <summary>Details</summary>
Motivation: 传统化学过程优化方法在操作约束不明确或不可用时效率低下，需要依赖主观启发式方法估计可行参数范围，这成为优化过程的瓶颈。

Method: 采用基于AutoGen的多智能体框架，利用OpenAI的o3模型，通过专门设计的智能体进行约束生成、参数验证、模拟执行和优化指导，分两阶段实现自主约束生成和迭代多智能体优化。

Result: 在氢化脱烷基过程中验证，该框架在成本、收率和收率-成本比等指标上与传统优化方法性能相当，但计算效率更高，收敛所需迭代次数更少，速度比网格搜索快31倍。

Conclusion: 该框架在操作约束不明确或不可用的优化场景中展现出巨大潜力，特别是在新兴过程和改造应用中，能够通过推理引导的搜索实现高效优化。

Abstract: Chemical process optimization is crucial to maximize production efficiency
and economic performance. Traditional methods, including gradient-based
solvers, evolutionary algorithms, and parameter grid searches, become
impractical when operating constraints are ill-defined or unavailable,
requiring engineers to rely on subjective heuristics to estimate feasible
parameter ranges. To address this constraint definition bottleneck, we present
a multi-agent framework of large language model (LLM) agents that autonomously
infer operating constraints from minimal process descriptions, then
collaboratively guide optimization using the inferred constraints. Our
AutoGen-based agentic framework employs OpenAI's o3 model, with specialized
agents for constraint generation, parameter validation, simulation execution,
and optimization guidance. Through two phases - autonomous constraint
generation using embedded domain knowledge, followed by iterative multi-agent
optimization - the framework eliminates the need for predefined operational
bounds. Validated on the hydrodealkylation process across cost, yield, and
yield-to-cost ratio metrics, the framework demonstrated competitive performance
with conventional optimization methods while achieving better computational
efficiency, requiring fewer iterations to converge. Our approach converged in
under 20 minutes, achieving a 31-fold speedup over grid search. Beyond
computational efficiency, the framework's reasoning-guided search demonstrates
sophisticated process understanding, correctly identifying utility trade-offs,
and applying domain-informed heuristics. This approach shows significant
potential for optimization scenarios where operational constraints are poorly
characterized or unavailable, particularly for emerging processes and retrofit
applications.

</details>


### [78] [Interpretable Representation Learning for Additive Rule Ensembles](https://arxiv.org/abs/2506.20927)
*Shahrzad Behzadimanesh,Pierre Le Bodic,Geoffrey I. Webb,Mario Boley*

Main category: cs.LG

TL;DR: 论文提出了一种改进的符号规则集成方法，通过引入可学习的稀疏线性变换，提升模型表达能力同时保持可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统符号规则集成方法依赖轴平行决策区域，需要精心设计的特征才能保证准确性和可解释性。当特征不理想时，需增加规则复杂度，导致可解释性下降。

Method: 扩展经典规则集成，引入可学习稀疏权重向量的逻辑命题，形成斜决策面。采用基于迭代重加权逻辑回归的序列贪婪优化方法进行学习。

Result: 实验表明，该方法在保持与先进方法相同测试风险的同时，显著降低了模型复杂度。

Conclusion: 所提方法能高效构建规则集成，在保证准确性的同时提升可解释性，适用于多种基准数据集。

Abstract: Small additive ensembles of symbolic rules offer interpretable prediction
models. Traditionally, these ensembles use rule conditions based on
conjunctions of simple threshold propositions $x \geq t$ on a single input
variable $x$ and threshold $t$, resulting geometrically in axis-parallel
polytopes as decision regions. While this form ensures a high degree of
interpretability for individual rules and can be learned efficiently using the
gradient boosting approach, it relies on having access to a curated set of
expressive and ideally independent input features so that a small ensemble of
axis-parallel regions can describe the target variable well. Absent such
features, reaching sufficient accuracy requires increasing the number and
complexity of individual rules, which diminishes the interpretability of the
model. Here, we extend classical rule ensembles by introducing logical
propositions with learnable sparse linear transformations of input variables,
i.e., propositions of the form $\mathbf{x}^\mathrm{T}\mathbf{w} \geq t$, where
$\mathbf{w}$ is a learnable sparse weight vector, enabling decision regions as
general polytopes with oblique faces. We propose a learning method using
sequential greedy optimization based on an iteratively reweighted formulation
of logistic regression. Experimental results demonstrate that the proposed
method efficiently constructs rule ensembles with the same test risk as
state-of-the-art methods while significantly reducing model complexity across
ten benchmark datasets.

</details>


### [79] [Model State Arithmetic for Machine Unlearning](https://arxiv.org/abs/2506.20941)
*Keivan Rezaei,Mehrdad Saberi,Abhilasha Ravichander,Soheil Feizi*

Main category: cs.LG

TL;DR: 提出了一种名为MSA的新算法，通过利用模型检查点来估计和消除数据点的影响，优于现有的机器遗忘算法。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型训练数据可能包含隐私、版权或低质量内容，完全重新训练成本高昂，需要开发高效的遗忘算法。

Method: 利用模型检查点（训练过程中保存的状态）来估计和消除特定数据点的影响。

Result: MSA在多个基准测试、模型和评估指标上优于现有机器遗忘算法。

Conclusion: MSA是实现大型语言模型数据擦除的有效方法，可提升模型灵活性。

Abstract: Large language models are trained on massive corpora of web data, which may
include private data, copyrighted material, factually inaccurate data, or data
that degrades model performance. Eliminating the influence of such problematic
datapoints through complete retraining -- by repeatedly pretraining the model
on datasets that exclude these specific instances -- is computationally
prohibitive. For this reason, unlearning algorithms have emerged that aim to
eliminate the influence of particular datapoints, while otherwise preserving
the model -- at a low computational cost. However, precisely estimating and
undoing the influence of individual datapoints has proved to be challenging. In
this work, we propose a new algorithm, MSA, for estimating and undoing the
influence of datapoints -- by leveraging model checkpoints i.e. artifacts
capturing model states at different stages of pretraining. Our experimental
results demonstrate that MSA consistently outperforms existing machine
unlearning algorithms across multiple benchmarks, models, and evaluation
metrics, suggesting that MSA could be an effective approach towards more
flexible large language models that are capable of data erasure.

</details>


### [80] [Antibody Design and Optimization with Multi-scale Equivariant Graph Diffusion Models for Accurate Complex Antigen Binding](https://arxiv.org/abs/2506.20957)
*Jiameng Chen,Xiantao Cai,Jia Wu,Wenbin Hu*

Main category: cs.LG

TL;DR: AbMEGD是一个端到端的抗体序列与结构协同设计框架，通过多尺度等变图扩散方法提升抗体设计的几何精度和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前抗体设计方法在捕捉几何特征和泛化新抗原界面方面存在不足，难以准确模拟分子相互作用和保持结构完整性。

Method: AbMEGD结合多尺度等变图扩散，利用几何深度学习整合原子级几何特征和残基级嵌入，确保几何精度和计算效率。

Result: 实验显示，AbMEGD在SAbDab数据库中氨基酸恢复率提高10.13%，关键CDR-H3区域的RMSD降低0.062Å，性能优于DiffAb模型。

Conclusion: AbMEGD在保持结构完整性的同时提升了功能表现，为抗体序列-结构协同设计和亲和力优化设立了新标准。

Abstract: Antibody design remains a critical challenge in therapeutic and diagnostic
development, particularly for complex antigens with diverse binding interfaces.
Current computational methods face two main limitations: (1) capturing
geometric features while preserving symmetries, and (2) generalizing novel
antigen interfaces. Despite recent advancements, these methods often fail to
accurately capture molecular interactions and maintain structural integrity. To
address these challenges, we propose \textbf{AbMEGD}, an end-to-end framework
integrating \textbf{M}ulti-scale \textbf{E}quivariant \textbf{G}raph
\textbf{D}iffusion for antibody sequence and structure co-design. Leveraging
advanced geometric deep learning, AbMEGD combines atomic-level geometric
features with residue-level embeddings, capturing local atomic details and
global sequence-structure interactions. Its E(3)-equivariant diffusion method
ensures geometric precision, computational efficiency, and robust
generalizability for complex antigens. Furthermore, experiments using the
SAbDab database demonstrate a 10.13\% increase in amino acid recovery, 3.32\%
rise in improvement percentage, and a 0.062~\AA\ reduction in root mean square
deviation within the critical CDR-H3 region compared to DiffAb, a leading
antibody design model. These results highlight AbMEGD's ability to balance
structural integrity with improved functionality, establishing a new benchmark
for sequence-structure co-design and affinity optimization. The code is
available at: https://github.com/Patrick221215/AbMEGD.

</details>


### [81] [SharpZO: Hybrid Sharpness-Aware Vision Language Model Prompt Tuning via Forward-Only Passes](https://arxiv.org/abs/2506.20990)
*Yifan Yang,Zhen Zhang,Rupak Vignesh Swaminathan,Jing Liu,Nathan Susanj,Zheng Zhang*

Main category: cs.LG

TL;DR: 提出SharpZO方法，通过两阶段优化提升零阶视觉语言模型微调性能，仅需前向传播即可实现高效训练。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型微调依赖反向传播，无法在内存受限的边缘设备上运行，而现有无需反向传播的方法性能不足。

Method: 采用两阶段优化：先通过锐度感知进化策略全局探索和平滑损失函数，再用稀疏零阶优化进行局部精细搜索。

Result: 在CLIP模型上实验表明，SharpZO比现有前向传播方法平均提升7%准确率，并加快收敛速度。

Conclusion: SharpZO为资源受限设备提供了一种高性能的视觉语言模型微调解决方案。

Abstract: Fine-tuning vision language models (VLMs) has achieved remarkable performance
across various downstream tasks; yet, it requires access to model gradients
through backpropagation (BP), making them unsuitable for memory-constrained,
inference-only edge devices. To address this limitation, previous work has
explored various BP-free fine-tuning methods. However, these approaches often
rely on high-variance evolutionary strategies (ES) or zeroth-order (ZO)
optimization, and often fail to achieve satisfactory performance. In this
paper, we propose a hybrid Sharpness-aware Zeroth-order optimization (SharpZO)
approach, specifically designed to enhance the performance of ZO VLM
fine-tuning via a sharpness-aware warm-up training. SharpZO features a
two-stage optimization process: a sharpness-aware ES stage that globally
explores and smooths the loss landscape to construct a strong initialization,
followed by a fine-grained local search via sparse ZO optimization. The entire
optimization relies solely on forward passes. Detailed theoretical analysis and
extensive experiments on CLIP models demonstrate that SharpZO significantly
improves accuracy and convergence speed, achieving up to 7% average gain over
state-of-the-art forward-only methods.

</details>


### [82] [Distilling Normalizing Flows](https://arxiv.org/abs/2506.21003)
*Steven Walton,Valeriy Klyukin,Maksim Artemev,Denis Derkach,Nikita Orlov,Humphrey Shi*

Main category: cs.LG

TL;DR: 本文提出了一种新颖的知识蒸馏技术，用于提升小型学生归一化流的采样质量和密度估计能力，通过中间层知识转移实现模型压缩与性能提升。


<details>
  <summary>Details</summary>
Motivation: 归一化流作为显式密度模型，虽在概率分布建模上优于生成对抗网络，但存在训练难度大、采样质量低的问题。研究旨在通过知识蒸馏技术优化小型归一化流的性能。

Method: 采用非传统的知识转移形式，在归一化流的中间层进行知识蒸馏，利用可组合的双射函数简化概率分布建模。

Result: 实验表明，通过蒸馏技术可显著减小学生模型规模，同时在性能上大幅超越未蒸馏的模型，且模型越小吞吐量提升越明显。

Conclusion: 知识蒸馏在归一化流架构中展现出独特优势，能有效平衡模型大小与性能，为生成模型的高效部署提供了新思路。

Abstract: Explicit density learners are becoming an increasingly popular technique for
generative models because of their ability to better model probability
distributions. They have advantages over Generative Adversarial Networks due to
their ability to perform density estimation and having exact latent-variable
inference. This has many advantages, including: being able to simply
interpolate, calculate sample likelihood, and analyze the probability
distribution. The downside of these models is that they are often more
difficult to train and have lower sampling quality.
  Normalizing flows are explicit density models, that use composable bijective
functions to turn an intractable probability function into a tractable one. In
this work, we present novel knowledge distillation techniques to increase
sampling quality and density estimation of smaller student normalizing flows.
We seek to study the capacity of knowledge distillation in Compositional
Normalizing Flows to understand the benefits and weaknesses provided by these
architectures. Normalizing flows have unique properties that allow for a
non-traditional forms of knowledge transfer, where we can transfer that
knowledge within intermediate layers. We find that through this distillation,
we can make students significantly smaller while making substantial performance
gains over a non-distilled student. With smaller models there is a
proportionally increased throughput as this is dependent upon the number of
bijectors, and thus parameters, in the network.

</details>


### [83] [TRIDENT: Tri-Modal Molecular Representation Learning with Taxonomic Annotations and Local Correspondence](https://arxiv.org/abs/2506.21028)
*Feng Jiang,Mangal Prakash,Hehuan Ma,Jianyuan Deng,Yuzhi Guo,Amina Mollaysa,Tommaso Mansi,Rui Liao,Junzhou Huang*

Main category: cs.LG

TL;DR: TRIDENT框架通过整合分子SMILES、文本描述和分类功能注释，采用全局和局部对齐目标，实现了分子性质预测的最新性能。


<details>
  <summary>Details</summary>
Motivation: 现有分子表示学习方法大多忽视了分子的文本和分类信息，TRIDENT旨在通过多模态学习整合这些信息以提升分子性质预测。

Method: TRIDENT使用体积对齐目标和局部对齐目标，结合动量机制动态平衡全局和局部对齐，学习分子表示。

Result: TRIDENT在11个下游任务中取得了最先进的性能，证明了其方法的有效性。

Conclusion: 结合SMILES、文本和分类功能注释能显著提升分子性质预测的准确性。

Abstract: Molecular property prediction aims to learn representations that map chemical
structures to functional properties. While multimodal learning has emerged as a
powerful paradigm to learn molecular representations, prior works have largely
overlooked textual and taxonomic information of molecules for representation
learning. We introduce TRIDENT, a novel framework that integrates molecular
SMILES, textual descriptions, and taxonomic functional annotations to learn
rich molecular representations. To achieve this, we curate a comprehensive
dataset of molecule-text pairs with structured, multi-level functional
annotations. Instead of relying on conventional contrastive loss, TRIDENT
employs a volume-based alignment objective to jointly align tri-modal features
at the global level, enabling soft, geometry-aware alignment across modalities.
Additionally, TRIDENT introduces a novel local alignment objective that
captures detailed relationships between molecular substructures and their
corresponding sub-textual descriptions. A momentum-based mechanism dynamically
balances global and local alignment, enabling the model to learn both broad
functional semantics and fine-grained structure-function mappings. TRIDENT
achieves state-of-the-art performance on 11 downstream tasks, demonstrating the
value of combining SMILES, textual, and taxonomic functional annotations for
molecular property prediction.

</details>


### [84] [Little By Little: Continual Learning via Self-Activated Sparse Mixture-of-Rank Adaptive Learning](https://arxiv.org/abs/2506.21035)
*Haodong Lu,Chongyang Zhao,Jason Xue,Lina Yao,Kristen Moore,Dong Gong*

Main category: cs.LG

TL;DR: 论文提出MoRA方法，通过细粒度混合低秩专家和自适应稀疏激活，解决持续学习中的灾难性遗忘和任务干扰问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于LoRA的混合专家方法在持续学习中面临任务干扰、冗余和路由模糊的问题，导致模型性能下降和遗忘加速。

Method: MoRA将每个低秩更新分解为多个独立的秩-1专家，通过自激活和稀疏激活机制，实现细粒度专家选择和自适应混合。

Result: MoRA在CLIP和大型语言模型的持续学习任务中表现出色，有效提升模型泛化能力并减少遗忘。

Conclusion: MoRA通过细粒度专家混合和自适应稀疏激活，显著提升了持续学习的性能，同时缓解了灾难性遗忘问题。

Abstract: Continual learning (CL) with large pre-trained models is challenged by
catastrophic forgetting and task interference. Existing LoRA-based
Mixture-of-Experts (MoE) approaches mitigate forgetting by assigning and
freezing task-specific adapters, but suffer from interference, redundancy, and
ambiguous routing due to coarse adapter-level selection. However, this design
introduces three key challenges: 1) Interference: Activating full LoRA experts
per input leads to subspace interference and prevents selective reuse of useful
components across tasks. 2) Redundancy: Newly added experts often duplicate or
contradict existing knowledge due to unnecessary activation of unrelated ranks
and insufficient reuse of relevant ones. 3) Ambiguity: Overlapping features
across tasks confuse the router, resulting in unstable expert assignments. As
more experts accumulate, earlier task routing degrades, accelerating
forgetting. We propose MoRA, a Mixture-of-Rank Adaptive learning approach with
self-activated and sparse rank activation for CL. Unlike mixing multiple
low-rank matrices, MoRA decomposes each rank-r update into r rank-1 components,
each treated as an independent expert, enabling fine-grained mixture of rank-1
expert utilization while mitigating interference and redundancy. To avoid
ambiguous routing, we propose that each rank-1 expert can infer its own
relevance via intermediate activations. Coupled with our proposed rank pruning
and activation budgets, MoRA adaptively selects a sparse mixture of ranks per
input. We validate MoRA on continual learning tasks with CLIP and large
language models (LLMs), analyzing both in-domain learning and out-of-domain
forgetting/generalization during fine-tuning. MoRA shows significant
effectiveness on enhancing CL with PTMs, and improving generalization while
mitigating forgetting.

</details>


### [85] [An Information-Theoretic Analysis for Federated Learning under Concept Drift](https://arxiv.org/abs/2506.21036)
*Fu Peng,Meng Zhang,Ming Tang*

Main category: cs.LG

TL;DR: 该论文分析了联邦学习在概念漂移下的性能，提出了一种基于信息理论的算法来缓解性能下降，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的数据通常以流的形式出现，且分布会随时间变化（概念漂移），导致联邦学习模型性能下降。现有研究多基于静态数据集，缺乏对动态数据流的适应性研究。

Method: 论文将概念漂移建模为马尔可夫链，提出用KL散度和互信息推导的'稳态泛化误差'评估模型对未来数据的适应能力，并设计了一种结合KL散度和互信息的正则化算法。

Result: 实验表明，三种漂移模式（周期性/渐进性/随机性）对性能影响显著，所提算法在Raspberry Pi4测试平台上均优于现有方法，且通过帕累托前沿实现了性能-成本权衡。

Conclusion: 该研究为联邦学习中的概念漂移问题提供了理论分析和实用解决方案，通过信息理论工具和正则化方法有效提升了模型在动态数据环境下的长期性能。

Abstract: Recent studies in federated learning (FL) commonly train models on static
datasets. However, real-world data often arrives as streams with shifting
distributions, causing performance degradation known as concept drift. This
paper analyzes FL performance under concept drift using information theory and
proposes an algorithm to mitigate the performance degradation. We model concept
drift as a Markov chain and introduce the \emph{Stationary Generalization
Error} to assess a model's capability to capture characteristics of future
unseen data. Its upper bound is derived using KL divergence and mutual
information. We study three drift patterns (periodic, gradual, and random) and
their impact on FL performance. Inspired by this, we propose an algorithm that
regularizes the empirical risk minimization approach with KL divergence and
mutual information, thereby enhancing long-term performance. We also explore
the performance-cost tradeoff by identifying a Pareto front. To validate our
approach, we build an FL testbed using Raspberry Pi4 devices. Experimental
results corroborate with theoretical findings, confirming that drift patterns
significantly affect performance. Our method consistently outperforms existing
approaches for these three patterns, demonstrating its effectiveness in
adapting concept drift in FL.

</details>


### [86] [RL-Selector: Reinforcement Learning-Guided Data Selection via Redundancy Assessment](https://arxiv.org/abs/2506.21037)
*Suorong Yang,Peijia Li,Furao Shen,Jian Zhao*

Main category: cs.LG

TL;DR: 论文提出RL-Selector方法，通过强化学习优化数据选择策略，减少冗余样本，提升训练效率和模型性能。


<details>
  <summary>Details</summary>
Motivation: 现实数据集通常存在大量冗余，传统数据选择方法依赖静态评分或预训练模型，忽略了样本间的动态关系。

Method: 引入ε-sample cover量化样本冗余，将数据选择建模为强化学习问题，通过轻量级RL代理优化选择策略。

Result: 在多个基准数据集和架构上的实验表明，该方法显著优于现有基线，训练效率更高且模型泛化性能更好。

Conclusion: RL-Selector通过动态样本选择有效减少冗余，为高效深度学习训练提供了新范式。

Abstract: Modern deep architectures often rely on large-scale datasets, but training on
these datasets incurs high computational and storage overhead. Real-world
datasets often contain substantial redundancies, prompting the need for more
data-efficient training paradigms. Data selection has shown promise to mitigate
redundancy by identifying the most representative samples, thereby reducing
training costs without compromising performance. Existing methods typically
rely on static scoring metrics or pretrained models, overlooking the combined
effect of selected samples and their evolving dynamics during training. We
introduce the concept of epsilon-sample cover, which quantifies sample
redundancy based on inter-sample relationships, capturing the intrinsic
structure of the dataset. Based on this, we reformulate data selection as a
reinforcement learning (RL) process and propose RL-Selector, where a
lightweight RL agent optimizes the selection policy by leveraging
epsilon-sample cover derived from evolving dataset distribution as a reward
signal. Extensive experiments across benchmark datasets and diverse
architectures demonstrate that our method consistently outperforms existing
state-of-the-art baselines. Models trained with our selected datasets show
enhanced generalization performance with improved training efficiency.

</details>


### [87] [Strict Subgoal Execution: Reliable Long-Horizon Planning in Hierarchical Reinforcement Learning](https://arxiv.org/abs/2506.21039)
*Jaebak Hwang,Sanghyeon Lee,Jeongmo Kim,Seungyul Han*

Main category: cs.LG

TL;DR: 论文提出了一种名为SSE的分层强化学习框架，通过严格子目标可达性和解耦探索策略，有效解决了长时程目标导向任务中的子目标不可行和规划效率低下的问题。


<details>
  <summary>Details</summary>
Motivation: 长时程目标导向任务在强化学习中面临目标遥远和奖励稀疏的挑战，现有的分层和图基方法存在子目标不可行和规划效率低下的问题。

Method: SSE框架通过结构约束高层决策确保单步子目标可达性，采用解耦探索策略系统探索目标空间未探索区域，并通过失败感知路径细化动态调整边成本以提高子目标可靠性。

Result: 在多种长时程基准测试中，SSE在效率和成功率上均优于现有的目标导向强化学习和分层强化学习方法。

Conclusion: SSE框架通过严格子目标执行和动态路径优化，显著提升了长时程目标导向任务的性能，为相关研究提供了新的解决方案。

Abstract: Long-horizon goal-conditioned tasks pose fundamental challenges for
reinforcement learning (RL), particularly when goals are distant and rewards
are sparse. While hierarchical and graph-based methods offer partial solutions,
they often suffer from subgoal infeasibility and inefficient planning. We
introduce Strict Subgoal Execution (SSE), a graph-based hierarchical RL
framework that enforces single-step subgoal reachability by structurally
constraining high-level decision-making. To enhance exploration, SSE employs a
decoupled exploration policy that systematically traverses underexplored
regions of the goal space. Furthermore, a failure-aware path refinement, which
refines graph-based planning by dynamically adjusting edge costs according to
observed low-level success rates, thereby improving subgoal reliability.
Experimental results across diverse long-horizon benchmarks demonstrate that
SSE consistently outperforms existing goal-conditioned RL and hierarchical RL
approaches in both efficiency and success rate.

</details>


### [88] [Efficient Skill Discovery via Regret-Aware Optimization](https://arxiv.org/abs/2506.21044)
*He Zhang,Ming Zhou,Shaopeng Zhai,Ying Sun,Hui Xiong*

Main category: cs.LG

TL;DR: 该论文提出了一种基于遗憾感知的无监督技能发现方法，通过将技能发现构建为技能生成与策略学习的极小极大博弈，提升了高维环境下的效率和多样性。


<details>
  <summary>Details</summary>
Motivation: 现有无监督技能发现方法虽在探索性上表现良好，但在高维情境下效率不足。论文旨在通过对抗性框架优化技能发现过程，提升策略学习的效率与多样性。

Method: 将技能发现建模为技能生成与策略学习的极小极大博弈，利用遗憾评分衡量技能强度收敛程度，并通过可学习的技能生成器引导探索方向，避免退化。

Result: 实验表明，该方法在多样性和效率上均优于基线方法，并在高维环境中实现了15%的零样本性能提升。

Conclusion: 通过对抗性框架和遗憾感知机制，该方法显著提升了技能发现的效率与适应性，尤其适用于高维复杂环境。

Abstract: Unsupervised skill discovery aims to learn diverse and distinguishable
behaviors in open-ended reinforcement learning. For existing methods, they
focus on improving diversity through pure exploration, mutual information
optimization, and learning temporal representation. Despite that they perform
well on exploration, they remain limited in terms of efficiency, especially for
the high-dimensional situations. In this work, we frame skill discovery as a
min-max game of skill generation and policy learning, proposing a regret-aware
method on top of temporal representation learning that expands the discovered
skill space along the direction of upgradable policy strength. The key insight
behind the proposed method is that the skill discovery is adversarial to the
policy learning, i.e., skills with weak strength should be further explored
while less exploration for the skills with converged strength. As an
implementation, we score the degree of strength convergence with regret, and
guide the skill discovery with a learnable skill generator. To avoid
degeneration, skill generation comes from an up-gradable population of skill
generators. We conduct experiments on environments with varying complexities
and dimension sizes. Empirical results show that our method outperforms
baselines in both efficiency and diversity. Moreover, our method achieves a 15%
zero shot improvement in high-dimensional environments, compared to existing
methods.

</details>


### [89] [FedDAA: Dynamic Client Clustering for Concept Drift Adaptation in Federated Learning](https://arxiv.org/abs/2506.21054)
*Fu Peng,Ming Tang*

Main category: cs.LG

TL;DR: 该论文提出FedDAA框架，通过动态聚类和三种模块解决联邦学习中多源概念漂移问题，显著提升模型精度。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习方法主要关注真实漂移（P(y|x)变化），但忽略了虚拟漂移（P(x)变化）和标签漂移（P(y)变化），导致历史知识遗忘和泛化性能下降。需要区分不同漂移来源并采取针对性策略。

Method: 提出FedDAA框架，包含三个核心模块：1) 聚类数量确定模块；2) 真实漂移检测模块；3) 概念漂移适应模块。通过动态聚类和针对性策略保留有用历史知识。

Result: 在Fashion-MNIST、CIFAR-10和CIFAR-100数据集上，FedDAA比现有方法精度提升7.84%至8.52%，并提供理论收敛保证。

Conclusion: FedDAA能有效区分并适应多源概念漂移，在保留历史知识的同时提升模型性能，为联邦学习中的动态数据分布问题提供了解决方案。

Abstract: In federated learning (FL), the data distribution of each client may change
over time, introducing both temporal and spatial data heterogeneity, known as
concept drift. Data heterogeneity arises from three drift sources: real drift
(a shift in the conditional distribution P(y|x)), virtual drift (a shift in the
input distribution P(x)), and label drift (a shift in the label distribution
P(y)). However, most existing FL methods addressing concept drift primarily
focus on real drift. When clients experience virtual or label drift, these
methods often fail to selectively retain useful historical knowledge, leading
to catastrophic forgetting. A key challenge lies in distinguishing different
sources of drift, as they require distinct adaptation strategies: real drift
calls for discarding outdated data, while virtual or label drift benefits from
retaining historical data. Without explicitly identifying the drift sources, a
general adaptation strategy is suboptimal and may harm generalization. To
address this challenge, we propose FedDAA, a dynamic clustered FL framework
designed to adapt to multi-source concept drift while preserving valuable
historical knowledge. Specifically, FedDAA integrates three modules: a cluster
number determination module to find the optimal number of clusters; a real
drift detection module to distinguish real drift from virtual/label drift; and
a concept drift adaptation module to adapt to new data while retaining useful
historical information. We provide theoretical convergence guarantees, and
experiments show that FedDAA achieves 7.84% to 8.52% accuracy improvements over
state-of-the-art methods on Fashion-MNIST, CIFAR-10, and CIFAR-100.

</details>


### [90] [Enhancing LLM Tool Use with High-quality Instruction Data from Knowledge Graph](https://arxiv.org/abs/2506.21071)
*Jingwei Wang,Zai Zhang,Hao Qian,Chunjing Gan,Binbin Hu,Ziqi Liu,Zhiqiang Zhang,Jun Zhou,Bin Shi,Bo Dong*

Main category: cs.LG

TL;DR: 提出一种利用知识图谱生成高质量指令数据的新方法，显著提升大语言模型的工具使用能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖大语言模型生成指令数据，但质量不足，难以有效提升模型对工具功能及用户意图的理解。

Method: 从知识图谱提取查询路径并转化为用户查询，将实体关系映射为可操作工具，解析查询路径为详细解决步骤，生成高质量指令数据。

Result: 实验表明，仅需少量合成数据微调即可显著提升大语言模型的工具利用率和整体能力。

Conclusion: 知识图谱生成的指令数据能高效解决大语言模型工具使用难题，为模型能力扩展提供新思路。

Abstract: Teaching large language models (LLMs) to use tools is crucial for improving
their problem-solving abilities and expanding their applications. However,
effectively using tools is challenging because it requires a deep understanding
of tool functionalities and user intentions. Previous methods relied mainly on
LLMs to generate instruction data, but the quality of these data was often
insufficient. In this paper, we propose a new method that uses knowledge graphs
to generate high-quality instruction data for LLMs. Knowledge graphs are
manually curated datasets rich in semantic information. We begin by extracting
various query pathways from a given knowledge graph, which are transformed into
a broad spectrum of user queries. We then translate the relationships between
entities into actionable tools and parse the pathways of each query into
detailed solution steps, thereby creating high-quality instruction data. Our
experiments show that fine-tuning on just a small sample of this synthetic data
can significantly improve the tool utilization and overall capabilities of
LLMs.

</details>


### [91] [Chain-of-Thought Enhanced Shallow Transformers for Wireless Symbol Detection](https://arxiv.org/abs/2506.21093)
*Li Fan,Peng Wang,Jing Yang,Cong Shen*

Main category: cs.LG

TL;DR: 论文提出了一种名为CHOOSE的浅层Transformer框架，通过引入自回归潜在推理步骤，在不增加模型深度的情况下提升推理能力，适用于资源受限的移动设备。


<details>
  <summary>Details</summary>
Motivation: 现有的基于ICL的Transformer模型依赖深层架构，导致存储和计算成本高昂，难以在资源受限的设备上部署。

Method: 提出CHOOSE框架，通过在隐藏空间中引入自回归潜在推理步骤，增强浅层Transformer的推理能力。

Result: 实验表明，CHOOSE在保持存储和计算效率的同时，性能优于传统浅层Transformer，并与深层Transformer相当。

Conclusion: CHOOSE为在计算资源有限的无线接收器中实现基于Transformer的算法提供了有前景的方向。

Abstract: Transformers have shown potential in solving wireless communication problems,
particularly via in-context learning (ICL), where models adapt to new tasks
through prompts without requiring model updates. However, prior ICL-based
Transformer models rely on deep architectures with many layers to achieve
satisfactory performance, resulting in substantial storage and computational
costs. In this work, we propose CHain Of thOught Symbol dEtection (CHOOSE), a
CoT-enhanced shallow Transformer framework for wireless symbol detection. By
introducing autoregressive latent reasoning steps within the hidden space,
CHOOSE significantly improves the reasoning capacity of shallow models (1-2
layers) without increasing model depth. This design enables lightweight
Transformers to achieve detection performance comparable to much deeper models,
making them well-suited for deployment on resource-constrained mobile devices.
Experimental results demonstrate that our approach outperforms conventional
shallow Transformers and achieves performance comparable to that of deep
Transformers, while maintaining storage and computational efficiency. This
represents a promising direction for implementing Transformer-based algorithms
in wireless receivers with limited computational resources.

</details>


### [92] [FeDa4Fair: Client-Level Federated Datasets for Fairness Evaluation](https://arxiv.org/abs/2506.21095)
*Xenia Heilmann,Luca Corbucci,Mattia Cerrato,Anna Monreale*

Main category: cs.LG

TL;DR: 该论文提出了FeDa4Fair库，用于在异构客户端偏差下评估公平联邦学习方法，并发布了四个偏差异构数据集及基准测试。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中的公平性问题是一个关键挑战，尤其是当不同客户端的数据分布不均时，可能导致模型对某些客户端更公平。现有研究多关注单一敏感属性的公平性，忽略了不同客户端的多样化需求。

Method: 论文通过引入FeDa4Fair库生成定制化的表格数据集，用于评估公平联邦学习方法，并提供了四个偏差异构数据集及相应的基准测试。

Result: 论文贡献包括：1) FeDa4Fair库；2) 四个偏差异构数据集及基准测试；3) 用于评估公平性的现成函数。

Conclusion: 该研究为联邦学习中的公平性研究提供了更稳健和可复现的基准测试工具，支持在全局和客户端级别进行公平性评估。

Abstract: Federated Learning (FL) enables collaborative model training across multiple
clients without sharing clients' private data. However, fairness remains a key
concern, as biases in local clients' datasets can impact the entire federated
system. Heterogeneous data distributions across clients may lead to models that
are fairer for some clients than others. Although several fairness-enhancing
solutions are present in the literature, most focus on mitigating bias for a
single sensitive attribute, typically binary, overlooking the diverse and
sometimes conflicting fairness needs of different clients. This limited
perspective can limit the effectiveness of fairness interventions for the
different clients. To support more robust and reproducible fairness research in
FL, we aim to enable a consistent benchmarking of fairness-aware FL methods at
both the global and client levels. In this paper, we contribute in three ways:
(1) We introduce FeDa4Fair, a library to generate tabular datasets tailored to
evaluating fair FL methods under heterogeneous client bias; (2) we release four
bias-heterogeneous datasets and corresponding benchmarks to compare fairness
mitigation methods in a controlled environment; (3) we provide ready-to-use
functions for evaluating fairness outcomes for these datasets.

</details>


### [93] [Learning to Skip the Middle Layers of Transformers](https://arxiv.org/abs/2506.21103)
*Tim Lawson,Laurence Aitchison*

Main category: cs.LG

TL;DR: 本文提出了一种动态跳过Transformer中间层的新架构，旨在提高效率，但实验结果显示在验证交叉熵和FLOPs之间的权衡上未优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 受可解释性研究的启发，发现Transformer中间层存在较大冗余，早期层将信息聚合到token位置，因此提出动态跳过中间层以减少计算需求。

Method: 提出了一种新架构，通过学习的门控机制动态跳过中间层，并使用门控注意力机制防止后续token关注被跳过的位置，同时采用残差范数控制和自适应正则化损失。

Result: 在研究的规模下，该方法在验证交叉熵和估计FLOPs之间的权衡上未能优于层数较少的密集基线模型。

Conclusion: 虽然方法旨在提高效率并促进多级表示层次的出现，但在当前规模下未达到预期效果，代码已开源供进一步研究。

Abstract: Conditional computation is a popular strategy to make Transformers more
efficient. Existing methods often target individual modules (e.g.,
mixture-of-experts layers) or skip layers independently of one another.
However, interpretability research has demonstrated that the middle layers of
Transformers exhibit greater redundancy, and that early layers aggregate
information into token positions. Guided by these insights, we propose a novel
architecture that dynamically skips a variable number of layers from the middle
outward. In particular, a learned gating mechanism determines whether to bypass
a symmetric span of central blocks based on the input, and a gated attention
mechanism prevents subsequent tokens from attending to skipped token positions.
Residual norms are controlled with a 'sandwich' or 'perilayernorm' scheme and
gate sparsity with an adaptive regularization loss. We had aimed to reduce
compute requirements for 'simpler' tokens and potentially foster an emergent
multi-level representational hierarchy but, at the scales investigated, our
approach does not achieve improvements in the trade-off between validation
cross-entropy and estimated FLOPs compared to dense baselines with fewer
layers. We release our code at https://github.com/tim-lawson/skip-middle.

</details>


### [94] [Interpretable Hierarchical Concept Reasoning through Attention-Guided Graph Learning](https://arxiv.org/abs/2506.21102)
*David Debot,Pietro Barbiero,Gabriele Dominici,Giuseppe Marra*

Main category: cs.LG

TL;DR: H-CMR是一种新型概念模型，通过层次化概念推理同时提升任务和概念预测的可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有概念模型仅对最终任务预测提供解释，而概念预测本身仍为黑箱。H-CMR旨在解决这一局限性。

Method: 使用有向无环图建模概念间关系，通过神经注意力机制选择逻辑规则进行层次化推理。

Result: H-CMR在保持SOTA性能的同时，支持通过概念干预和模型干预增强人机交互。

Conclusion: 该模型实现了双层次可解释性，并在推理时提升精度、训练时提高数据效率。

Abstract: Concept-Based Models (CBMs) are a class of deep learning models that provide
interpretability by explaining predictions through high-level concepts. These
models first predict concepts and then use them to perform a downstream task.
However, current CBMs offer interpretability only for the final task
prediction, while the concept predictions themselves are typically made via
black-box neural networks. To address this limitation, we propose Hierarchical
Concept Memory Reasoner (H-CMR), a new CBM that provides interpretability for
both concept and task predictions. H-CMR models relationships between concepts
using a learned directed acyclic graph, where edges represent logic rules that
define concepts in terms of other concepts. During inference, H-CMR employs a
neural attention mechanism to select a subset of these rules, which are then
applied hierarchically to predict all concepts and the final task. Experimental
results demonstrate that H-CMR matches state-of-the-art performance while
enabling strong human interaction through concept and model interventions. The
former can significantly improve accuracy at inference time, while the latter
can enhance data efficiency during training when background knowledge is
available.

</details>


### [95] [Complexity-aware fine-tuning](https://arxiv.org/abs/2506.21220)
*Andrey Goncharov,Daniil Vyazhev,Petr Sychev,Edvard Khalafyan,Alexey Zaytsev*

Main category: cs.LG

TL;DR: 该论文提出了一种基于熵值分类数据的高效微调方法，通过区分数据复杂度并针对性应用监督微调和蒸馏技术，显著提升了小规模开源模型的性能，同时大幅减少数据需求。


<details>
  <summary>Details</summary>
Motivation: 当前通用大语言模型（LLMs）通过监督微调（SFT）在特定领域提升性能时，依赖大量数据和昂贵计算资源。论文旨在开发一种更高效的微调方法，减少资源消耗。

Method: 1. 基于单标记答案熵对训练数据进行复杂度分类（ROC AUC 0.73）
2. 对复杂数据应用思维链蒸馏技术
3. 对简单数据采用标准监督微调
4. 在两个30亿参数开源模型上验证方法

Result: 1. 平均准确率0.55，显著优于标准SFT的0.43
2. 与蒸馏方法性能相当（均为0.55）
3. 数据使用量减少62%

Conclusion: 通过熵值驱动的数据复杂度分类和差异化微调策略，可在保持性能的同时显著降低计算资源需求，为高效LLM微调提供了新思路。

Abstract: General-purpose Large Language Models (LLMs) are frequently fine-tuned
through supervised fine-tuning (SFT) to enhance performance in specific
domains. Better results can be achieved by distilling the chain-of-thought of a
larger model at the cost of numerous expensive calls and a much greater amount
of data. We propose a novel blueprint for efficient fine-tuning that uses
reasoning only for complex data identified by entropy. Specifically, across two
small open models ($\approx 3B$) we split the training data into complexity
categories by a single token answer entropy (ROC AUC $0.73$), fine-tune large
language models (LLMs) via SFT and distillation, and show that our pipeline
significantly outperforms the standard SFT approach ($0.55$ vs $0.43$ average
accuracy) and provides comparable with distillation performance while using
$62\%$ less data ($0.55$ average accuracy for both). We publish our code and
data to facilitate further research in this direction.

</details>


### [96] [Unlasting: Unpaired Single-Cell Multi-Perturbation Estimation by Dual Conditional Diffusion Implicit Bridges](https://arxiv.org/abs/2506.21107)
*Changxi Chi,Jun Xia,Yufei Huang,Jingbo Zhou,Siyuan Li,Yunfan Liu,Chang Yu,Stan Z. Li*

Main category: cs.LG

TL;DR: 该论文提出了一种基于双扩散隐式桥接（DDIB）的框架，用于解决单细胞扰动数据不成对的问题，并通过整合基因调控网络（GRN）和掩码机制提升生成质量。


<details>
  <summary>Details</summary>
Motivation: 单细胞测序是一种破坏性过程，无法捕捉同一细胞在扰动前后的表型，导致扰动和非扰动条件下的数据本质上不成对。现有方法要么强行配对数据，要么忽略未扰动和扰动细胞之间的内在关系。

Method: 提出基于双扩散隐式桥接（DDIB）的框架，整合GRN信息以生物有意义的方式传播扰动信号，并引入掩码机制预测沉默基因。此外，还提出了更适合的评估指标来捕捉单细胞响应的内在异质性。

Result: 该方法有效解决了单细胞扰动数据不成对的挑战，提升了生成质量，并通过新的评估指标更好地反映了单细胞响应的异质性。

Conclusion: 该框架不仅解决了单细胞扰动数据不成对的问题，还通过GRN指导和掩码机制提升了模型的生成质量和生物学意义。

Abstract: Estimating single-cell responses across various perturbations facilitates the
identification of key genes and enhances drug screening, significantly boosting
experimental efficiency. However, single-cell sequencing is a destructive
process, making it impossible to capture the same cell's phenotype before and
after perturbation. Consequently, data collected under perturbed and
unperturbed conditions are inherently unpaired. Existing methods either attempt
to forcibly pair unpaired data using random sampling, or neglect the inherent
relationship between unperturbed and perturbed cells during the modeling. In
this work, we propose a framework based on Dual Diffusion Implicit Bridges
(DDIB) to learn the mapping between different data distributions, effectively
addressing the challenge of unpaired data. We further interpret this framework
as a form of data augmentation. We integrate gene regulatory network (GRN)
information to propagate perturbation signals in a biologically meaningful way,
and further incorporate a masking mechanism to predict silent genes, improving
the quality of generated profiles. Moreover, gene expression under the same
perturbation often varies significantly across cells, frequently exhibiting a
bimodal distribution that reflects intrinsic heterogeneity. To capture this, we
introduce a more suitable evaluation metric. We propose Unlasting, dual
conditional diffusion models that overcome the problem of unpaired single-cell
perturbation data and strengthen the model's insight into perturbations under
the guidance of the GRN, with a dedicated mask model designed to improve
generation quality by predicting silent genes. In addition, we introduce a
biologically grounded evaluation metric that better reflects the inherent
heterogeneity in single-cell responses.

</details>


### [97] [DiLoCoX: A Low-Communication Large-Scale Training Framework for Decentralized Cluster](https://arxiv.org/abs/2506.21263)
*Ji Qi,WenPeng Zhu,Li Li,Ming Wu,YingJun Wu,Wu He,Xun Gao,Jason Zeng,Michael Heinrich*

Main category: cs.LG

TL;DR: DiLoCoX框架通过低通信分布式训练技术，成功在慢速网络上预训练超1000亿参数的大模型，速度提升357倍。


<details>
  <summary>Details</summary>
Motivation: 集中式集群的高通信需求限制了大规模模型训练的灵活性，作者探索在慢速网络上实现去中心化训练的可能性。

Method: 结合流水线并行、双优化器策略、通信与本地训练的一步延迟重叠及自适应梯度压缩方案。

Result: 在1Gbps网络上预训练107B参数模型，相比AllReduce提速357倍且收敛性几乎无损。

Conclusion: DiLoCoX是首个支持超千亿参数模型的去中心化训练框架，突破了传统集群通信瓶颈。

Abstract: The distributed training of foundation models, particularly large language
models (LLMs), demands a high level of communication. Consequently, it is
highly dependent on a centralized cluster with fast and reliable interconnects.
Can we conduct training on slow networks and thereby unleash the power of
decentralized clusters when dealing with models exceeding 100 billion
parameters? In this paper, we propose DiLoCoX, a low-communication large-scale
decentralized cluster training framework. It combines Pipeline Parallelism with
Dual Optimizer Policy, One-Step-Delay Overlap of Communication and Local
Training, and an Adaptive Gradient Compression Scheme. This combination
significantly improves the scale of parameters and the speed of model
pre-training. We justify the benefits of one-step-delay overlap of
communication and local training, as well as the adaptive gradient compression
scheme, through a theoretical analysis of convergence. Empirically, we
demonstrate that DiLoCoX is capable of pre-training a 107B foundation model
over a 1Gbps network. Compared to vanilla AllReduce, DiLoCoX can achieve a 357x
speedup in distributed training while maintaining negligible degradation in
model convergence. To the best of our knowledge, this is the first
decentralized training framework successfully applied to models with over 100
billion parameters.

</details>


### [98] [Robust Policy Switching for Antifragile Reinforcement Learning for UAV Deconfliction in Adversarial Environments](https://arxiv.org/abs/2506.21127)
*Deepak Kumar Panda,Weisi Guo*

Main category: cs.LG

TL;DR: 该论文提出了一种抗脆弱强化学习框架，通过动态切换策略来应对无人机导航中的对抗攻击，相比传统方法表现更优。


<details>
  <summary>Details</summary>
Motivation: 现有鲁棒强化学习方法对固定扰动的处理能力有限，难以应对超出最优值分布范围的对抗攻击。

Method: 采用基于折扣汤普森采样（DTS）的切换机制，动态选择多个鲁棒策略，以最小化对抗性引起的状态-动作-值分布偏移。

Result: 在复杂导航环境中，该方法相比传统鲁棒强化学习方法，实现了更短的导航路径长度和更高的无冲突导航轨迹率。

Conclusion: 抗脆弱强化学习框架能有效适应未知对抗攻击，提升无人机导航的安全性和适应性。

Abstract: The increasing automation of navigation for unmanned aerial vehicles (UAVs)
has exposed them to adversarial attacks that exploit vulnerabilities in
reinforcement learning (RL) through sensor manipulation. Although existing
robust RL methods aim to mitigate such threats, their effectiveness has limited
generalization to out-of-distribution shifts from the optimal value
distribution, as they are primarily designed to handle fixed perturbation. To
address this limitation, this paper introduces an antifragile RL framework that
enhances adaptability to broader distributional shifts by incorporating a
switching mechanism based on discounted Thompson sampling (DTS). This mechanism
dynamically selects among multiple robust policies to minimize adversarially
induced state-action-value distribution shifts. The proposed approach first
derives a diverse ensemble of action robust policies by accounting for a range
of perturbations in the policy space. These policies are then modeled as a
multiarmed bandit (MAB) problem, where DTS optimally selects policies in
response to nonstationary Bernoulli rewards, effectively adapting to evolving
adversarial strategies. Theoretical framework has also been provided where by
optimizing the DTS to minimize the overall regrets due to distributional shift,
results in effective adaptation against unseen adversarial attacks thus
inducing antifragility. Extensive numerical simulations validate the
effectiveness of the proposed framework in complex navigation environments with
multiple dynamic three-dimensional obstacles and with stronger projected
gradient descent (PGD) and spoofing attacks. Compared to conventional robust,
non-adaptive RL methods, the antifragile approach achieves superior
performance, demonstrating shorter navigation path lengths and a higher rate of
conflict-free navigation trajectories compared to existing robust RL techniques

</details>


### [99] [Curriculum-Guided Antifragile Reinforcement Learning for Secure UAV Deconfliction under Observation-Space Attacks](https://arxiv.org/abs/2506.21129)
*Deepak Kumar Panda,Adolfo Perrusquia,Weisi Guo*

Main category: cs.LG

TL;DR: 该论文提出了一种抗脆弱强化学习框架，通过逐步增加的对抗性扰动训练RL策略，使其能适应分布外观测攻击，在无人机避障场景中显著提升了安全性和性能。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习策略在安全关键系统（如无人机导航）中易受观测空间分布外对抗攻击的影响，导致价值估计退化、决策不安全。需要开发能主动适应威胁演化的抗脆弱RL方法。

Method: 提出抗脆弱RL框架：1) 理论定义脆弱性为价值函数随扰动强度单调发散；2) 通过Wasserstein距离最小化实现增量扰动下的专家指导评论家对齐；3) 采用课程学习逐步增强对抗扰动强度。

Result: 在含动态3D障碍物的无人机避碰场景中，抗脆弱策略相比基准方法：1) 累计奖励提高15%；2) 冲突事件减少30%；3) 对PGD和GPS欺骗攻击均表现出更强鲁棒性。

Conclusion: 该研究从理论和实践层面验证了抗脆弱RL在动态威胁环境中的有效性，为安全关键系统的弹性决策提供了新范式。

Abstract: Reinforcement learning (RL) policies deployed in safety-critical systems,
such as unmanned aerial vehicle (UAV) navigation in dynamic airspace, are
vulnerable to out-ofdistribution (OOD) adversarial attacks in the observation
space. These attacks induce distributional shifts that significantly degrade
value estimation, leading to unsafe or suboptimal decision making rendering the
existing policy fragile. To address this vulnerability, we propose an
antifragile RL framework designed to adapt against curriculum of incremental
adversarial perturbations. The framework introduces a simulated attacker which
incrementally increases the strength of observation-space perturbations which
enables the RL agent to adapt and generalize across a wider range of OOD
observations and anticipate previously unseen attacks. We begin with a
theoretical characterization of fragility, formally defining catastrophic
forgetting as a monotonic divergence in value function distributions with
increasing perturbation strength. Building on this, we define antifragility as
the boundedness of such value shifts and derive adaptation conditions under
which forgetting is stabilized. Our method enforces these bounds through
iterative expert-guided critic alignment using Wasserstein distance
minimization across incrementally perturbed observations. We empirically
evaluate the approach in a UAV deconfliction scenario involving dynamic 3D
obstacles. Results show that the antifragile policy consistently outperforms
standard and robust RL baselines when subjected to both projected gradient
descent (PGD) and GPS spoofing attacks, achieving up to 15% higher cumulative
reward and over 30% fewer conflict events. These findings demonstrate the
practical and theoretical viability of antifragile reinforcement learning for
secure and resilient decision-making in environments with evolving threat
scenarios.

</details>


### [100] [NaLaFormer: Norm-Aware Linear Attention for Transformer Models](https://arxiv.org/abs/2506.21137)
*Weikang Meng,Yadan Luo,Liangyu Huo,Yaowei Wang,Xin Li,Zheng Zhang*

Main category: cs.LG

TL;DR: 本文提出了一种新的Norm-Aware Linear Attention机制，通过解耦查询和键矩阵为范数和方向两部分，解决了线性注意力中查询范数被忽视和负值抑制的问题，提升了表达能力和效率。


<details>
  <summary>Details</summary>
Motivation: 当前线性注意力机制通过使用线性可分离的核函数和L1归一化来替代softmax操作，但忽视了查询范数，导致熵差距增大，同时抑制了查询和键向量的负值，影响了内积交互。

Method: 提出Norm-Aware Linear Attention机制，解耦查询和键矩阵为范数和方向两部分，实现范数感知的动态稀疏性控制和范数一致性，并使用余弦相似度抑制相反方向的维度。

Result: 实验表明，NaLaFormer在视觉和语言任务上性能提升高达4.2%，同时增强了表达能力和效率。

Conclusion: 通过范数感知的线性注意力机制，有效解决了现有方法的局限性，提升了模型性能。

Abstract: Linear attention has emerged as a viable alternative to softmax attention by
reducing complexity from quadratic to linear in sequence length. To preserve
two fundamental properties of softmax, non-negativity and entropy reduction,
current works employ various linearly separatable kernel functions with $L1$
normalization instead of softmax operator. However, query norms are neglected
by the normalization operation in linear attention, such degradation heavily
leads to an entropy gap. Meanwhile, existing works inhibit negative values of
query and key vectors resulting in a missing inner-product interactions after
being mapped. To address these dual challenges, we propose a novel Norm-Aware
Linear Attention mechanism serving to restore norm-guided dynamic spikiness and
recover kernel-perturbed norm distributions. Specifically, we first decouple
query and key matrices into two components: norm and direction, to achieve
norm-aware spikiness control and norm consistency, respectively. We
mathematically reveal that the extent of entropy reduction varies with the
query norm in softmax normalization, motivating a query-norm aware kernel
function for dynamic control over entropy reduction. Furthermore, to ensure
norm consistency and enforce non-negativity constraints, we employ a
norm-preserving mapping to project all elements of the angular matrix into
positive values, leveraging cosine similarity to inhibit dimensions with
opposite directions. We conduct extensive experiments demonstrating that the
NaLaFormer improves performance on vision and language tasks, enhancing both
expressiveness and efficiency by up to 4.2\%.

</details>


### [101] [Latent Prototype Routing: Achieving Near-Perfect Load Balancing in Mixture-of-Experts](https://arxiv.org/abs/2506.21328)
*Jiajie Yang*

Main category: cs.LG

TL;DR: 该论文提出了一种名为Latent Prototype Routing（LPR）的新型路由框架，通过聚类视角重新审视专家路由，解决了当前MoE架构中专家负载不平衡的问题，显著提高了专家利用率和计算资源效率。


<details>
  <summary>Details</summary>
Motivation: 当前Mixture-of-Experts（MoE）架构在训练和推理过程中存在严重的负载不平衡问题，只有少数专家被持续激活，导致模型容量和计算资源的严重浪费。

Method: 论文提出了Latent Prototype Routing（LPR）框架，通过聚类视角重新设计专家路由机制，在保持下游性能的同时促进专家负载的平衡利用。

Result: 实验表明，LPR在多个开源MoE模型（如DeepSeek-V3、Qwen3-MoE和Mixtral）上显著改善了专家负载平衡，Gini系数从0.70降至0.035，最小-最大专家负载比从1e-6提升至0.70。

Conclusion: LPR框架有效解决了MoE架构中的负载不平衡问题，实现了近乎完美的负载平衡，同时不损害模型性能。

Abstract: Mixture-of-Experts (MoE) architectures have emerged as a key strategy for
scaling large language models (LLMs) efficiently. However, current MoE systems
suffer from severe load imbalance, where only a small subset of experts is
consistently activated during training and inference, leading to significant
underutilization of model capacity and computational resources. In this work,
we revisit expert routing through a clustering perspective and propose Latent
Prototype Routing (LPR), a novel routing framework that generalizes existing
approaches while promoting balanced expert utilization without compromising
downstream performance. Extensive experiments across multiple open-source MoE
models -- including DeepSeek-V3, Qwen3-MoE, and Mixtral -- demonstrate that LPR
reduces the Gini coefficient of expert load from 0.70 to 0.035 on average,
improves the min-max expert load ratio from 1e-6 to 0.70, achieving
near-perfect load balancing.

</details>


### [102] [DBConformer: Dual-Branch Convolutional Transformer for EEG Decoding](https://arxiv.org/abs/2506.21140)
*Ziwei Wang,Hongbin Wang,Tianwang Jia,Xingyi He,Siyang Li,Dongrui Wu*

Main category: cs.LG

TL;DR: 论文提出DBConformer，一种双分支卷积Transformer网络，用于EEG解码，能同时捕捉时间动态和空间模式，性能优于现有模型且参数更少。


<details>
  <summary>Details</summary>
Motivation: 现有基于CNN的EEG解码方法难以捕捉长程时间依赖和全局通道关系，CNN-Transformer混合模型通常采用串行设计，未能有效整合局部和全局特征，且缺乏显式的通道建模。

Method: 提出DBConformer，结合时间Conformer建模长程时间依赖和空间Conformer提取通道间交互，并引入轻量级通道注意力模块优化空间表征。

Result: 在五个运动想象数据集和两个癫痫检测数据集上的实验表明，DBConformer性能优于10个基线模型，参数量仅为EEG Conformer的八分之一，且特征具有生理可解释性。

Conclusion: DBConformer在EEG解码中表现出优越性能和可解释性，适用于鲁棒且可解释的脑机接口应用。

Abstract: Electroencephalography (EEG)-based brain-computer interfaces (BCIs) transform
spontaneous/evoked neural activity into control commands for external
communication. While convolutional neural networks (CNNs) remain the mainstream
backbone for EEG decoding, their inherently short receptive field makes it
difficult to capture long-range temporal dependencies and global inter-channel
relationships. Recent CNN-Transformer (Conformers) hybrids partially address
this issue, but most adopt a serial design, resulting in suboptimal integration
of local and global features, and often overlook explicit channel-wise
modeling. To address these limitations, we propose DBConformer, a dual-branch
convolutional Transformer network tailored for EEG decoding. It integrates a
temporal Conformer to model long-range temporal dependencies and a spatial
Conformer to extract inter-channel interactions, capturing both temporal
dynamics and spatial patterns in EEG signals. A lightweight channel attention
module further refines spatial representations by assigning data-driven
importance to EEG channels. Extensive experiments on five motor imagery (MI)
datasets and two seizure detection datasets under three evaluation settings
demonstrate that DBConformer consistently outperforms 10 competitive baseline
models, with over eight times fewer parameters than the high-capacity EEG
Conformer baseline. Further, the visualization results confirm that the
features extracted by DBConformer are physiologically interpretable and aligned
with sensorimotor priors in MI. The superior performance and interpretability
of DBConformer make it reliable for robust and explainable EEG decoding. Code
is publicized at https://github.com/wzwvv/DBConformer.

</details>


### [103] [Generative Adversarial Evasion and Out-of-Distribution Detection for UAV Cyber-Attacks](https://arxiv.org/abs/2506.21142)
*Deepak Kumar Panda,Weisi Guo*

Main category: cs.LG

TL;DR: 该论文提出了一种基于条件生成对抗网络（cGAN）的框架，用于生成能够躲避入侵检测系统（IDS）的隐蔽对抗攻击，并通过条件变分自编码器（CVAE）有效检测此类攻击。


<details>
  <summary>Details</summary>
Motivation: 随着无人机在民用空域的广泛应用，传统异常检测方法难以识别新型威胁，且现有方法无法有效区分隐蔽对抗攻击与真实异常事件，因此需要更智能的入侵检测系统。

Method: 论文首先训练了一个多类IDS分类器，然后使用cGAN生成对抗样本，这些样本能误导分类器将其分类为良性数据，同时保持统计特性。最后，采用CVAE通过负对数似然来区分对抗样本与真实异常。

Result: 实验表明，基于CVAE的检测方法在识别隐蔽对抗攻击方面显著优于传统的马氏距离检测器。

Conclusion: 论文强调了高级概率建模在提升入侵检测系统对抗生成模型攻击能力中的重要性。

Abstract: The growing integration of UAVs into civilian airspace underscores the need
for resilient and intelligent intrusion detection systems (IDS), as traditional
anomaly detection methods often fail to identify novel threats. A common
approach treats unfamiliar attacks as out-of-distribution (OOD) samples;
however, this leaves systems vulnerable when mitigation is inadequate.
Moreover, conventional OOD detectors struggle to distinguish stealthy
adversarial attacks from genuine OOD events. This paper introduces a
conditional generative adversarial network (cGAN)-based framework for crafting
stealthy adversarial attacks that evade IDS mechanisms. We first design a
robust multi-class IDS classifier trained on benign UAV telemetry and known
cyber-attacks, including Denial of Service (DoS), false data injection (FDI),
man-in-the-middle (MiTM), and replay attacks. Using this classifier, our cGAN
perturbs known attacks to generate adversarial samples that misclassify as
benign while retaining statistical resemblance to OOD distributions. These
adversarial samples are iteratively refined to achieve high stealth and success
rates. To detect such perturbations, we implement a conditional variational
autoencoder (CVAE), leveraging negative log-likelihood to separate adversarial
inputs from authentic OOD samples. Comparative evaluation shows that CVAE-based
regret scores significantly outperform traditional Mahalanobis distance-based
detectors in identifying stealthy adversarial threats. Our findings emphasize
the importance of advanced probabilistic modeling to strengthen IDS
capabilities against adaptive, generative-model-based cyber intrusions.

</details>


### [104] [Scalable Bayesian Low-Rank Adaptation of Large Language Models via Stochastic Variational Subspace Inference](https://arxiv.org/abs/2506.21408)
*Colin Samplawski,Adam D. Cobb,Manoj Acharya,Ramneet Kaur,Susmit Jha*

Main category: cs.LG

TL;DR: 该论文提出了一种名为ScalaBL的可扩展贝叶斯低秩适配方法，通过随机变分子空间推理在低维子空间进行贝叶斯推断，显著减少了参数数量，同时保持了与现有先进方法竞争的性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在广泛应用中存在幻觉问题和校准不足的问题，尤其在自主系统和医疗等高风险领域，不确定性量化变得至关重要。现有基于贝叶斯深度学习的方法虽然有效，但由于需要额外参数，难以扩展到更大的LLMs。

Method: 论文提出了ScalaBL方法，通过在低秩适配（LoRA）的r维子空间中进行贝叶斯推断，并重新利用LoRA参数作为投影矩阵，将子空间样本映射到LLM的完整权重空间。所有参数均通过随机变分推断学习。

Result: 尽管子空间维度较低，ScalaBL仅需约1000个额外参数，就能与现有先进方法竞争，并且成功扩展到迄今为止最大的贝叶斯LLM，其基础参数数量是先前工作的四倍。

Conclusion: ScalaBL方法在保持高性能的同时，显著减少了参数需求，为大型语言模型的不确定性量化提供了一种可扩展的解决方案。

Abstract: Despite their widespread use, large language models (LLMs) are known to
hallucinate incorrect information and be poorly calibrated. This makes the
uncertainty quantification of these models of critical importance, especially
in high-stakes domains, such as autonomy and healthcare. Prior work has made
Bayesian deep learning-based approaches to this problem more tractable by
performing inference over the low-rank adaptation (LoRA) parameters of a
fine-tuned model. While effective, these approaches struggle to scale to larger
LLMs due to requiring further additional parameters compared to LoRA. In this
work we present $\textbf{Scala}$ble $\textbf{B}$ayesian $\textbf{L}$ow-Rank
Adaptation via Stochastic Variational Subspace Inference (ScalaBL). We perform
Bayesian inference in an $r$-dimensional subspace, for LoRA rank $r$. By
repurposing the LoRA parameters as projection matrices, we are able to map
samples from this subspace into the full weight space of the LLM. This allows
us to learn all the parameters of our approach using stochastic variational
inference. Despite the low dimensionality of our subspace, we are able to
achieve competitive performance with state-of-the-art approaches while only
requiring ${\sim}1000$ additional parameters. Furthermore, it allows us to
scale up to the largest Bayesian LLM to date, with four times as a many base
parameters as prior work.

</details>


### [105] [Personalized Federated Learning via Dual-Prompt Optimization and Cross Fusion](https://arxiv.org/abs/2506.21144)
*Yuguang Zhang,Kuangpu Guo,Zhihe Lu,Yunbo Wang,Jian Liang*

Main category: cs.LG

TL;DR: 本文提出了一种基于双提示学习和交叉融合的个性化联邦学习框架pFedDC，通过全局和局部提示捕获共享知识和特定客户端语义，解决了数据、计算和通信异质性挑战。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在分散客户端间协作训练模型时面临数据、计算和通信异质性的挑战。预训练的视觉语言模型因其强大的泛化能力和轻量级提示调优提供了解决方案，但现有方法仅依赖文本提示且忽视了联合标签-域分布偏移。

Method: 提出pFedDC框架，每个客户端在视觉和语言模态上维护全局和局部提示：全局提示捕获联邦共享的通用知识，局部提示编码客户端特定语义和域特征。设计交叉融合模块自适应整合不同层次的提示，生成与客户端独特数据分布对齐的个性化表示。

Result: 在九种具有不同异质性的数据集上的广泛实验表明，pFedDC始终优于现有最先进方法。

Conclusion: pFedDC通过双提示学习和交叉融合有效解决了联邦学习中的异质性问题，实现了优于现有方法的性能。

Abstract: Federated learning (FL) enables collaborative model training across
decentralized clients without sharing local data, but is challenged by
heterogeneity in data, computation, and communication. Pretrained
vision-language models (VLMs), with their strong generalization and lightweight
tuning via prompts, offer a promising solution. However, existing federated
prompt-learning methods rely only on text prompts and overlook joint
label-domain distribution shifts. In this paper, we propose a personalized FL
framework based on dual-prompt learning and cross fusion, termed pFedDC.
Specifically, each client maintains both global and local prompts across vision
and language modalities: global prompts capture common knowledge shared across
the federation, while local prompts encode client-specific semantics and domain
characteristics. Meanwhile, a cross-fusion module is designed to adaptively
integrate prompts from different levels, enabling the model to generate
personalized representations aligned with each client's unique data
distribution. Extensive experiments across nine datasets with various types of
heterogeneity show that pFedDC consistently outperforms state-of-the-art
methods.

</details>


### [106] [Linearity-based neural network compression](https://arxiv.org/abs/2506.21146)
*Silas Dobler,Florian Lemmerich*

Main category: cs.LG

TL;DR: 该论文提出了一种基于线性度的神经网络压缩新方法，通过合并行为接近线性的神经元层，实现了模型大小的显著减小且无损性能。


<details>
  <summary>Details</summary>
Motivation: 当前神经网络压缩方法主要通过衡量参数重要性和冗余性来减少参数，但已有优化方案的潜力尚未充分挖掘。论文提出利用ReLU类激活函数中近似线性行为的神经元特性，探索新的压缩维度。

Method: 提出基于线性度的压缩理论：当神经元因持续激活而呈现线性行为时，可合并相邻线性层。该方法可与现有剪枝方法协同使用。

Result: 在多数测试模型上实现无损压缩至原模型1/4大小，与基于重要性的剪枝方法结合时干扰极小，证明技术组合的可行性。

Conclusion: 该工作开创了新型压缩范式，通过利用神经网络中的线性特性，为构建更小、更高效的模型奠定基础，且能与现有技术互补使用。

Abstract: In neural network compression, most current methods reduce unnecessary
parameters by measuring importance and redundancy. To augment already highly
optimized existing solutions, we propose linearity-based compression as a novel
way to reduce weights in a neural network. It is based on the intuition that
with ReLU-like activation functions, neurons that are almost always activated
behave linearly, allowing for merging of subsequent layers. We introduce the
theory underlying this compression and evaluate our approach experimentally.
Our novel method achieves a lossless compression down to 1/4 of the original
model size in over the majority of tested models. Applying our method on
already importance-based pruned models shows very little interference between
different types of compression, demonstrating the option of successful
combination of techniques. Overall, our work lays the foundation for a new type
of compression method that enables smaller and ultimately more efficient neural
network models.

</details>


### [107] [Diverse Mini-Batch Selection in Reinforcement Learning for Efficient Chemical Exploration in de novo Drug Design](https://arxiv.org/abs/2506.21158)
*Hampus Gummesson Svensson,Ola Engkvist,Jon Paul Janet,Christian Tyrchan,Morteza Haghir Chehreghani*

Main category: cs.LG

TL;DR: 论文提出了一种用于强化学习的多样化小批量选择方法，通过使用行列式点过程来提高解决方案的多样性，应用于药物发现领域，实验证明能显著提升化学探索的有效性。


<details>
  <summary>Details</summary>
Motivation: 在许多实际应用中，评估实例的质量（如人类反馈和物理模拟）通常成本高昂且耗时，尤其是在强化学习中，需要评估与环境的交互以提供奖励信号。多样化的小批量学习对充分探索至关重要，有助于避免模式崩溃。

Method: 论文引入了多样化小批量选择方法，并提出了使用行列式点过程（DPP）来完成这一任务，特别应用于药物发现中的化学探索。

Result: 实验结果表明，提出的多样化小批量选择框架能显著提高解决方案的多样性，同时保持高质量。在药物发现中，这种结果可能更快地满足未满足的医疗需求。

Conclusion: 通过多样化小批量选择框架，可以在药物发现等需要多样性和高质量解决方案的领域中，有效提升探索效果。

Abstract: In many real-world applications, evaluating the goodness of instances is
often costly and time-consuming, e.g., human feedback and physics simulations,
in contrast to proposing new instances. In particular, this is even more
critical in reinforcement learning, as new interactions with the environment
(i.e., new instances) need to be evaluated to provide a reward signal to learn
from. As sufficient exploration is crucial, learning from a diverse mini-batch
can have a large impact and help mitigate mode collapse. In this paper, we
introduce diverse mini-batch selection for reinforcement learning and propose
to use determinantal point processes for this task. We study this framework in
the context of a real-world problem, namely drug discovery. We experimentally
study how our proposed framework can improve the effectiveness of chemical
exploration in de novo drug design, where finding diverse and high-quality
solutions is essential. We conduct a comprehensive evaluation with three
well-established molecular generation oracles over numerous generative steps.
Our experiments conclude that our diverse mini-batch selection framework can
substantially improve the diversity of the solutions, while still obtaining
solutions of high quality. In drug discovery, such outcome can potentially lead
to fulfilling unmet medication needs faster.

</details>


### [108] [Artificial Delegates Resolve Fairness Issues in Perpetual Voting with Partial Turnout](https://arxiv.org/abs/2506.21186)
*Apurva Shah,Axel Abels,Ann Nowé,Tom Lenaerts*

Main category: cs.LG

TL;DR: 该论文研究了在连续集体决策中，通过引入人工智能代表来弥补缺席投票对公平性和代表性的影响。


<details>
  <summary>Details</summary>
Motivation: 现有的永久投票规则假设完全参与和完整的批准信息，但在实践中，部分参与是常态。缺席投票会影响公平性和代表性，因此需要一种方法来补偿这种缺失。

Method: 论文提出将人工智能代表（通过学习选民偏好的代理）整合到永久投票系统中，评估其在各种投票方法下对公平性和代表性的影响。

Result: 研究发现，虽然缺席投票显著影响公平性，但人工智能代表能够有效缓解这些影响，并在多种场景中增强系统的稳健性。

Conclusion: 人工智能代表可以可靠地弥补缺席投票带来的负面影响，提升永久投票系统的公平性和代表性。

Abstract: Perpetual voting addresses fairness in sequential collective decision-making
by evaluating representational equity over time. However, existing perpetual
voting rules rely on full participation and complete approval information,
assumptions that rarely hold in practice, where partial turnout is the norm. In
this work, we study the integration of Artificial Delegates,
preference-learning agents trained to represent absent voters, into perpetual
voting systems. We examine how absenteeism affects fairness and
representativeness under various voting methods and evaluate the extent to
which Artificial Delegates can compensate for missing participation. Our
findings indicate that while absenteeism significantly affects fairness,
Artificial Delegates reliably mitigate these effects and enhance robustness
across diverse scenarios.

</details>


### [109] [Zero-Shot Learning for Obsolescence Risk Forecasting](https://arxiv.org/abs/2506.21240)
*Elie Saad,Aya Mrabah,Mariem Besbes,Marc Zolghadri,Victor Czmil,Claude Baron,Vincent Bourgeois*

Main category: cs.LG

TL;DR: 该论文提出了一种利用零样本学习和大语言模型预测电子元件淘汰风险的新方法，解决了数据不足的问题，并在实际数据集中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 电子元件淘汰对依赖这些元件的行业带来了高成本和系统可用性及安全性问题，但由于缺乏可靠数据，准确预测淘汰风险面临挑战。

Method: 采用零样本学习（ZSL）结合大语言模型（LLMs），利用表格数据集中的领域特定知识来预测元件淘汰风险。

Result: 在两个实际数据集上的应用表明，该方法能有效预测淘汰风险，同时比较四种大语言模型强调了选择合适模型的重要性。

Conclusion: 通过零样本学习和大语言模型的结合，可以有效解决电子元件淘汰风险预测中的数据限制问题，为行业提供了实用的预测工具。

Abstract: Component obsolescence poses significant challenges in industries reliant on
electronic components, causing increased costs and disruptions in the security
and availability of systems. Accurate obsolescence risk prediction is essential
but hindered by a lack of reliable data. This paper proposes a novel approach
to forecasting obsolescence risk using zero-shot learning (ZSL) with large
language models (LLMs) to address data limitations by leveraging
domain-specific knowledge from tabular datasets. Applied to two real-world
datasets, the method demonstrates effective risk prediction. A comparative
evaluation of four LLMs underscores the importance of selecting the right model
for specific forecasting tasks.

</details>


### [110] [Improved seeding strategies for k-means and k-GMM](https://arxiv.org/abs/2506.21291)
*Guillaume Carrière,Frédéric Cazals*

Main category: cs.LG

TL;DR: 本文重新审视了k-means聚类和k-GMM的随机种子技术，提出了新的初始化方法，通过前瞻原则和多轮策略改进最终指标，实验显示优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 研究旨在改进k-means和k-GMM的初始化方法，通过分析种子技术的三个关键要素，提出更优的种子选择策略。

Method: 提出基于前瞻原则和多轮策略的新型初始化方法，优化种子选择与最终评估指标的匹配度。

Result: 实验结果表明，新方法在最终指标（SSE和log-likelihood）上优于现有技术，且计算开销适中。

Conclusion: 新方法在实践和理论上均有重要意义，有望成为标准技术，并为后续分析开辟新方向。

Abstract: We revisit the randomized seeding techniques for k-means clustering and k-GMM
(Gaussian Mixture model fitting with Expectation-Maximization), formalizing
their three key ingredients: the metric used for seed sampling, the number of
candidate seeds, and the metric used for seed selection. This analysis yields
novel families of initialization methods exploiting a lookahead
principle--conditioning the seed selection to an enhanced coherence with the
final metric used to assess the algorithm, and a multipass strategy to tame
down the effect of randomization.
  Experiments show a consistent constant factor improvement over classical
contenders in terms of the final metric (SSE for k-means, log-likelihood for
k-GMM), at a modest overhead. In particular, for k-means, our methods improve
on the recently designed multi-swap strategy, which was the first one to
outperform the greedy k-means++ seeding.
  Our experimental analysis also shed light on subtle properties of k-means
often overlooked, including the (lack of) correlations between the SSE upon
seeding and the final SSE, the variance reduction phenomena observed in
iterative seeding methods, and the sensitivity of the final SSE to the pool
size for greedy methods.
  Practically, our most effective seeding methods are strong candidates to
become one of the--if not the--standard techniques. From a theoretical
perspective, our formalization of seeding opens the door to a new line of
analytical approaches.

</details>


### [111] [AGTCNet: A Graph-Temporal Approach for Principled Motor Imagery EEG Classification](https://arxiv.org/abs/2506.21338)
*Galvin Brice S. Lim,Brian Godwin S. Lim,Argel A. Bandala,John Anthony C. Jose,Timothy Scott C. Chu,Edwin Sybingco*

Main category: cs.LG

TL;DR: 该论文提出了一种名为AGTCNet的新型图-时序卷积网络，用于改进脑机接口（BCI）中的运动想象脑电图（MI-EEG）分类，显著提升了分类准确率并降低了模型复杂度和推理时间。


<details>
  <summary>Details</summary>
Motivation: 当前基于脑电图（EEG）的脑机接口技术在开发跨受试者和跨会话的稳定系统时面临挑战，主要由于神经活动的复杂性和变异性，以及现有方法在捕捉多通道EEG信号的时空依赖性方面的不足。

Method: 研究引入了注意力图-时序卷积网络（AGTCNet），利用EEG电极的地形配置作为归纳偏置，并结合图卷积注意力网络（GCAT）来共同学习表达性的时空EEG表征。

Result: AGTCNet在BCI Competition IV Dataset 2a上实现了66.82%的受试者独立分类准确率（微调后达82.88%），在EEG Motor Movement/Imagery Dataset上4类和2类分类的准确率分别为64.14%和85.22%（微调后提升至72.13%和90.54%），同时模型大小减少49.87%，推理时间加快64.65%。

Conclusion: AGTCNet通过其紧凑架构和高效性能，为脑机接口的实际部署提供了有效且实用的解决方案，显著优于现有的MI-EEG分类器。

Abstract: Brain-computer interface (BCI) technology utilizing electroencephalography
(EEG) marks a transformative innovation, empowering motor-impaired individuals
to engage with their environment on equal footing. Despite its promising
potential, developing subject-invariant and session-invariant BCI systems
remains a significant challenge due to the inherent complexity and variability
of neural activity across individuals and over time, compounded by EEG hardware
constraints. While prior studies have sought to develop robust BCI systems,
existing approaches remain ineffective in capturing the intricate
spatiotemporal dependencies within multichannel EEG signals. This study
addresses this gap by introducing the attentive graph-temporal convolutional
network (AGTCNet), a novel graph-temporal model for motor imagery EEG (MI-EEG)
classification. Specifically, AGTCNet leverages the topographic configuration
of EEG electrodes as an inductive bias and integrates graph convolutional
attention network (GCAT) to jointly learn expressive spatiotemporal EEG
representations. The proposed model significantly outperformed existing MI-EEG
classifiers, achieving state-of-the-art performance while utilizing a compact
architecture, underscoring its effectiveness and practicality for BCI
deployment. With a 49.87% reduction in model size, 64.65% faster inference
time, and shorter input EEG signal, AGTCNet achieved a moving average accuracy
of 66.82% for subject-independent classification on the BCI Competition IV
Dataset 2a, which further improved to 82.88% when fine-tuned for
subject-specific classification. On the EEG Motor Movement/Imagery Dataset,
AGTCNet achieved moving average accuracies of 64.14% and 85.22% for 4-class and
2-class subject-independent classifications, respectively, with further
improvements to 72.13% and 90.54% for subject-specific classifications.

</details>


### [112] [DynamicBench: Evaluating Real-Time Report Generation in Large Language Models](https://arxiv.org/abs/2506.21343)
*Jingyao Li,Hao Sun,Zile Qiao,Yong Jiang,Pengjun Xie,Fei Huang,Hong Xu,Jiaya Jia*

Main category: cs.LG

TL;DR: 论文提出了DynamicBench，一个评估大语言模型实时信息处理能力的新基准，结合了网络搜索和本地报告数据库，并在无文档和有文档场景下超越了GPT4o。


<details>
  <summary>Details</summary>
Motivation: 传统的大语言模型评估基准多基于静态的故事讲述或观点表达，无法满足实时信息处理的动态需求。

Method: DynamicBench采用双路径检索管道，结合网络搜索和本地报告数据库，要求模型具备领域知识以生成准确报告。

Result: 实验结果显示，该方法在无文档和有文档场景下分别超越GPT4o 7.0%和5.8%。

Conclusion: DynamicBench有效评估了大语言模型处理实时信息的能力，其方法在性能上达到了当前最优水平。

Abstract: Traditional benchmarks for large language models (LLMs) typically rely on
static evaluations through storytelling or opinion expression, which fail to
capture the dynamic requirements of real-time information processing in
contemporary applications. To address this limitation, we present DynamicBench,
a benchmark designed to evaluate the proficiency of LLMs in storing and
processing up-to-the-minute data. DynamicBench utilizes a dual-path retrieval
pipeline, integrating web searches with local report databases. It necessitates
domain-specific knowledge, ensuring accurate responses report generation within
specialized fields. By evaluating models in scenarios that either provide or
withhold external documents, DynamicBench effectively measures their capability
to independently process recent information or leverage contextual
enhancements. Additionally, we introduce an advanced report generation system
adept at managing dynamic information synthesis. Our experimental results
confirm the efficacy of our approach, with our method achieving
state-of-the-art performance, surpassing GPT4o in document-free and
document-assisted scenarios by 7.0% and 5.8%, respectively. The code and data
will be made publicly available.

</details>


### [113] [Lipschitz Bounds for Persistent Laplacian Eigenvalues under One-Simplex Insertions](https://arxiv.org/abs/2506.21352)
*Le Vu Anh,Mehmet Dik,Nguyen Viet Anh*

Main category: cs.LG

TL;DR: 该论文证明了持久拉普拉斯算子的特征值在添加单形时的稳定性，为拓扑数据分析提供了首个特征值级别的鲁棒性保证。


<details>
  <summary>Details</summary>
Motivation: 持久拉普拉斯算子在生物学、物理学和机器学习中广泛应用，但其特征值在添加单形时的变化尚未明确，这对依赖这些特征值的下游工具至关重要。

Method: 通过证明一个统一的Lipschitz界，研究了添加单形时持久拉普拉斯算子特征值的变化。

Result: 添加一个单形后，每个上持久拉普拉斯算子的特征值最多变化为该单形边界欧几里得范数的两倍，不受过滤尺度和复合体大小的影响。

Conclusion: 该结果为动态数据环境中的谱拓扑数据分析提供了稳定的特征值保证，确保了局部更新下的可靠性和误差控制。

Abstract: Persistent Laplacians are matrix operators that track how the shape and
structure of data transform across scales and are popularly adopted in biology,
physics, and machine learning. Their eigenvalues are concise descriptors of
geometric and topological features in a filtration. Although earlier work
established global algebraic stability for these operators, the precise change
in a single eigenvalue when one simplex, such as a vertex, edge, or triangle,
is added has remained unknown. This is important because downstream tools,
including heat-kernel signatures and spectral neural networks, depend directly
on these eigenvalues. We close this gap by proving a uniform Lipschitz bound:
after inserting one simplex, every up-persistent Laplacian eigenvalue can vary
by at most twice the Euclidean norm of that simplex's boundary, independent of
filtration scale and complex size. This result delivers the first
eigenvalue-level robustness guarantee for spectral topological data analysis.
It guarantees that spectral features remain stable under local updates and
enables reliable error control in dynamic data settings.

</details>


### [114] [SMMILE: An Expert-Driven Benchmark for Multimodal Medical In-Context Learning](https://arxiv.org/abs/2506.21355)
*Melanie Rieff,Maya Varma,Ossian Rabow,Subathra Adithan,Julie Kim,Ken Chang,Hannah Lee,Nidhi Rohatgi,Christian Bluethgen,Mohamed S. Muneer,Jean-Benoit Delbrouck,Michael Moor*

Main category: cs.LG

TL;DR: 论文提出了首个医学多模态上下文学习基准SMMILE及增强版SMMILE++，评估了15个多模态大语言模型在医学任务中的表现，发现其上下文学习能力有限且易受干扰。


<details>
  <summary>Details</summary>
Motivation: 医学领域存在大量需要从有限示例中学习的专业任务，而当前多模态大语言模型在医学视觉问答中的上下文学习能力尚未明确。

Method: 通过11位医学专家构建包含111个问题（517个问答图像三元组）的SMMILE基准及1038个问题的增强版SMMILE++，评估15个模型的医学多模态上下文学习能力。

Result: 多数模型表现中等或较差，上下文学习仅带来8%-9.4%的性能提升；模型易受无关示例干扰（单噪声示例可降低9.5%性能），且存在近因偏差（相关示例置末可提升71%性能）。

Conclusion: 当前多模态大语言模型在医学任务中的上下文学习存在显著局限性和偏差，需进一步改进。

Abstract: Multimodal in-context learning (ICL) remains underexplored despite
significant potential for domains such as medicine. Clinicians routinely
encounter diverse, specialized tasks requiring adaptation from limited
examples, such as drawing insights from a few relevant prior cases or
considering a constrained set of differential diagnoses. While multimodal large
language models (MLLMs) have shown advances in medical visual question
answering (VQA), their ability to learn multimodal tasks from context is
largely unknown. We introduce SMMILE, the first expert-driven multimodal ICL
benchmark for medical tasks. Eleven medical experts curated problems, each
including a multimodal query and multimodal in-context examples as task
demonstrations. SMMILE encompasses 111 problems (517 question-image-answer
triplets) covering 6 medical specialties and 13 imaging modalities. We further
introduce SMMILE++, an augmented variant with 1038 permuted problems. A
comprehensive evaluation of 15 MLLMs demonstrates that most models exhibit
moderate to poor multimodal ICL ability in medical tasks. In open-ended
evaluations, ICL contributes only 8% average improvement over zero-shot on
SMMILE and 9.4% on SMMILE++. We observe a susceptibility for irrelevant
in-context examples: even a single noisy or irrelevant example can degrade
performance by up to 9.5%. Moreover, example ordering exhibits a recency bias,
i.e., placing the most relevant example last can lead to substantial
performance improvements by up to 71%. Our findings highlight critical
limitations and biases in current MLLMs when learning multimodal medical tasks
from context.

</details>


### [115] [rQdia: Regularizing Q-Value Distributions With Image Augmentation](https://arxiv.org/abs/2506.21367)
*Sam Lerman,Jing Bi*

Main category: cs.LG

TL;DR: rQdia通过增强图像正则化Q值分布，在基于像素的深度强化学习中提升性能。


<details>
  <summary>Details</summary>
Motivation: 在基于像素的深度强化学习中，如何提升样本效率和长期训练性能是一个关键问题。rQdia旨在通过正则化Q值分布来解决这一问题。

Method: rQdia使用简单的辅助损失函数，通过均方误差（MSE）均衡增强图像的Q值分布，从而正则化Q值分布。

Result: rQdia在MuJoCo连续控制任务中显著提升了DrQ和SAC的性能（分别在9/12和10/12任务中表现更好），并在Atari Arcade环境中提升了Data-Efficient Rainbow的性能（18/26任务）。此外，rQdia还首次使基于像素的无模型连续控制性能超过了基于状态编码的基线。

Conclusion: rQdia通过正则化Q值分布，有效提升了基于像素的深度强化学习的性能，尤其在样本效率和长期训练方面表现突出。

Abstract: rQdia regularizes Q-value distributions with augmented images in pixel-based
deep reinforcement learning. With a simple auxiliary loss, that equalizes these
distributions via MSE, rQdia boosts DrQ and SAC on 9/12 and 10/12 tasks
respectively in the MuJoCo Continuous Control Suite from pixels, and
Data-Efficient Rainbow on 18/26 Atari Arcade environments. Gains are measured
in both sample efficiency and longer-term training. Moreover, the addition of
rQdia finally propels model-free continuous control from pixels over the state
encoding baseline.

</details>


### [116] [MAx-DNN: Multi-Level Arithmetic Approximation for Energy-Efficient DNN Hardware Accelerators](https://arxiv.org/abs/2506.21371)
*Vasileios Leon,Georgios Makris,Sotirios Xydis,Kiamal Pekmestzi,Dimitrios Soudris*

Main category: cs.LG

TL;DR: 该论文探讨了在低功耗DNN计算中，通过细粒度错误恢复与硬件近似技术的结合，实现更高能效的方法。


<details>
  <summary>Details</summary>
Motivation: 随着深度神经网络（DNN）架构的快速发展，其在提供高精度机器学习任务方面已成为主流。然而，高能效的需求促使研究者探索如何在保持精度的同时降低能耗。

Method: 论文采用ROUP近似乘法器，通过层、滤波器和内核级别的细粒度分布策略，系统地评估了其对网络精度和能耗的影响。实验基于ResNet-8模型和CIFAR-10数据集。

Result: 与基线量化模型相比，所提方案在牺牲最多4%精度的情况下，实现了高达54%的能耗节省；与现有DNN近似技术相比，能耗降低2倍且精度更高。

Conclusion: 研究表明，通过细粒度分布近似乘法器，可以在精度损失可控的情况下显著提升DNN的能效，为低功耗计算提供了有效解决方案。

Abstract: Nowadays, the rapid growth of Deep Neural Network (DNN) architectures has
established them as the defacto approach for providing advanced Machine
Learning tasks with excellent accuracy. Targeting low-power DNN computing, this
paper examines the interplay of fine-grained error resilience of DNN workloads
in collaboration with hardware approximation techniques, to achieve higher
levels of energy efficiency. Utilizing the state-of-the-art ROUP approximate
multipliers, we systematically explore their fine-grained distribution across
the network according to our layer-, filter-, and kernel-level approaches, and
examine their impact on accuracy and energy. We use the ResNet-8 model on the
CIFAR-10 dataset to evaluate our approximations. The proposed solution delivers
up to 54% energy gains in exchange for up to 4% accuracy loss, compared to the
baseline quantized model, while it provides 2x energy gains with better
accuracy versus the state-of-the-art DNN approximations.

</details>


### [117] [Pay Attention to Small Weights](https://arxiv.org/abs/2506.21374)
*Chao Zhou,Tom Jacobs,Advait Gadhikar,Rebekka Burkholz*

Main category: cs.LG

TL;DR: 论文提出NANOADAM方法，通过动态更新小幅度权重来优化微调过程，减少资源消耗并提升性能。


<details>
  <summary>Details</summary>
Motivation: 微调大型预训练神经网络在内存和计算成本上资源密集，现有方法通常限制训练参数子集以减少消耗。作者观察到梯度与权重间的特定关系，即大梯度常与小幅度权重相关，这启发了新方法的提出。

Method: 提出NANOADAM方法，动态更新小幅度权重，无需梯度计算确定参数子集，保留大幅度权重以减少灾难性遗忘，并允许使用更大学习率。

Result: 在NLP和视觉任务实验中，NANOADAM表现出更好的泛化性能，同时减少了资源消耗。

Conclusion: NANOADAM通过优化微调过程中的权重更新策略，有效平衡了资源消耗与模型性能，为微调大型神经网络提供了实用解决方案。

Abstract: Finetuning large pretrained neural networks is known to be
resource-intensive, both in terms of memory and computational cost. To mitigate
this, a common approach is to restrict training to a subset of the model
parameters. By analyzing the relationship between gradients and weights during
finetuning, we observe a notable pattern: large gradients are often associated
with small-magnitude weights. This correlation is more pronounced in finetuning
settings than in training from scratch. Motivated by this observation, we
propose NANOADAM, which dynamically updates only the small-magnitude weights
during finetuning and offers several practical advantages: first, this
criterion is gradient-free -- the parameter subset can be determined without
gradient computation; second, it preserves large-magnitude weights, which are
likely to encode critical features learned during pretraining, thereby reducing
the risk of catastrophic forgetting; thirdly, it permits the use of larger
learning rates and consistently leads to better generalization performance in
experiments. We demonstrate this for both NLP and vision tasks.

</details>


### [118] [Temporal-Aware Graph Attention Network for Cryptocurrency Transaction Fraud Detection](https://arxiv.org/abs/2506.21382)
*Zhi Zheng,Bochuan Zhou,Yuping Song*

Main category: cs.LG

TL;DR: 提出了一种增强时序感知的图注意力网络（ATGAT），通过多尺度时间差特征融合和时序感知三重注意力机制，显著提升了加密货币交易欺诈检测的性能。


<details>
  <summary>Details</summary>
Motivation: 加密货币交易欺诈检测面临交易模式日益复杂和类别严重不平衡的双重挑战，传统方法依赖人工特征工程且难以捕捉交易网络的时序和结构依赖关系。

Method: 设计了三个模块：(1) 多尺度时间差特征与周期性位置编码融合的时序嵌入模块；(2) 联合优化结构、时序和全局上下文注意力的时序感知三重注意力机制；(3) 使用加权BCE损失解决类别不平衡问题。

Result: 在Elliptic++数据集上，ATGAT的AUC达到0.9130，相比XGBoost、GCN和标准GAT分别提升9.2%、12.0%和10.0%。

Conclusion: 该方法验证了时序感知和多重注意力机制对图神经网络的增强效果，为金融机构提供了更可靠的欺诈检测工具，其设计原则可推广至其他时序图异常检测任务。

Abstract: Cryptocurrency transaction fraud detection faces the dual challenges of
increasingly complex transaction patterns and severe class imbalance.
Traditional methods rely on manual feature engineering and struggle to capture
temporal and structural dependencies in transaction networks. This paper
proposes an Augmented Temporal-aware Graph Attention Network (ATGAT) that
enhances detection performance through three modules: (1) designing an advanced
temporal embedding module that fuses multi-scale time difference features with
periodic position encoding; (2) constructing a temporal-aware triple attention
mechanism that jointly optimizes structural, temporal, and global context
attention; (3) employing weighted BCE loss to address class imbalance.
Experiments on the Elliptic++ cryptocurrency dataset demonstrate that ATGAT
achieves an AUC of 0.9130, representing a 9.2% improvement over the best
traditional method XGBoost, 12.0% over GCN, and 10.0% over standard GAT. This
method not only validates the enhancement effect of temporal awareness and
triple attention mechanisms on graph neural networks, but also provides
financial institutions with more reliable fraud detection tools, with its
design principles generalizable to other temporal graph anomaly detection
tasks.

</details>


### [119] [Early Stopping Tabular In-Context Learning](https://arxiv.org/abs/2506.21387)
*Jaris Küken,Lennart Purucker,Frank Hutter*

Main category: cs.LG

TL;DR: 提出了一种通过动态评估提前停止上下文学习的方法，显著提升了表格基础模型的推理效率，同时保持预测性能。


<details>
  <summary>Details</summary>
Motivation: 表格基础模型在上下文学习中表现出色，但推理成本较高，尤其是在处理较大数据集时。为了降低计算开销，作者探索了提前停止上下文学习的可能性。

Method: 通过动态评估决定是否在每一层Transformer编码器后停止上下文学习，并使用预训练的分层解码器解码嵌入。

Result: 在34个小规模分类任务中，推理速度提升至1.3倍，预测性能几乎无下降；在5个大规模任务中，速度提升至2.2倍。

Conclusion: 提前退出策略是一种有效且实用的方法，可显著提升表格上下文学习的效率。

Abstract: Tabular foundation models have shown strong performance across various
tabular learning tasks via in-context learning, offering robust generalization
without any downstream finetuning. However, their inference-time costs remain
high, particularly for larger datasets. To address this, we propose
early-stopping the in-context learning process. We achieve this by dynamically
evaluating whether to stop in-context learning after each Transformer encoder
layer. Once stopped, we decode the embedding using a pre-trained layer-wise
decoder. Experiments across 34 small classification tasks size show that early
stopping in-context learning accelerates inference by up to x1.3 with
negligible degradation in predictive performance. To assess scalability, we
further evaluate our method on five larger classification tasks, achieving
speedups of up to x2.2. Our results demonstrate the potential of early exiting
as an effective and practical strategy for improving the efficiency of tabular
in-context learning.

</details>


### [120] [Distributed Cross-Channel Hierarchical Aggregation for Foundation Models](https://arxiv.org/abs/2506.21411)
*Aristeidis Tsaris,Isaac Lyngaas,John Lagregren,Mohamed Wahib,Larry York,Prasanna Balaprakash,Dan Lu,Feiyi Wang,Xiao Wang*

Main category: cs.LG

TL;DR: 论文提出了一种名为D-CHAG的分布式跨通道分层聚合方法，旨在解决基于视觉的科学基础模型在处理多通道图像数据时的计算效率问题。


<details>
  <summary>Details</summary>
Motivation: 当前基于视觉的科学基础模型在处理多源图像数据时，由于图像标记化和聚合的计算密集性，面临效率挑战。现有分布式方法未能完全解决这一问题。

Method: 提出了分布式跨通道分层聚合（D-CHAG）方法，适用于任何模型并行策略和视觉Transformer架构，显著提高了计算效率。

Result: 在超光谱成像和天气预报任务中，结合张量并行和模型分片，D-CHAG实现了内存使用减少75%，并在1024个AMD GPU上使持续吞吐量提高了一倍以上。

Conclusion: D-CHAG方法有效提升了多通道图像数据处理的计算效率，为科学发现和创新提供了有力支持。

Abstract: Vision-based scientific foundation models hold significant promise for
advancing scientific discovery and innovation. This potential stems from their
ability to aggregate images from diverse sources such as varying physical
groundings or data acquisition systems and to learn spatio-temporal
correlations using transformer architectures. However, tokenizing and
aggregating images can be compute-intensive, a challenge not fully addressed by
current distributed methods. In this work, we introduce the Distributed
Cross-Channel Hierarchical Aggregation (D-CHAG) approach designed for datasets
with a large number of channels across image modalities. Our method is
compatible with any model-parallel strategy and any type of vision transformer
architecture, significantly improving computational efficiency. We evaluated
D-CHAG on hyperspectral imaging and weather forecasting tasks. When integrated
with tensor parallelism and model sharding, our approach achieved up to a 75%
reduction in memory usage and more than doubled sustained throughput on up to
1,024 AMD GPUs on the Frontier Supercomputer.

</details>


### [121] [Flow-Based Single-Step Completion for Efficient and Expressive Policy Learning](https://arxiv.org/abs/2506.21427)
*Prajwal Koirala,Cody Fleming*

Main category: cs.LG

TL;DR: 提出了一种名为SSCP的单步生成策略，结合了生成模型的表达能力和单峰策略的效率，适用于多种强化学习场景。


<details>
  <summary>Details</summary>
Motivation: 现有的生成模型（如扩散和流匹配）在离线强化学习中提供了丰富的多模态动作分布，但其迭代采样导致高推理成本和训练不稳定性。

Method: SSCP通过增强的流匹配目标训练，直接从中间流样本预测完成向量，实现高效的单步动作生成，无需长反向传播链。

Result: SSCP在标准离线强化学习和行为克隆基准测试中表现优异，显著提升了速度和适应性。

Conclusion: SSCP是一种高效、表达力强的生成策略框架，适用于深度强化学习和序列决策任务。

Abstract: Generative models such as diffusion and flow-matching offer expressive
policies for offline reinforcement learning (RL) by capturing rich, multimodal
action distributions, but their iterative sampling introduces high inference
costs and training instability due to gradient propagation across sampling
steps. We propose the \textit{Single-Step Completion Policy} (SSCP), a
generative policy trained with an augmented flow-matching objective to predict
direct completion vectors from intermediate flow samples, enabling accurate,
one-shot action generation. In an off-policy actor-critic framework, SSCP
combines the expressiveness of generative models with the training and
inference efficiency of unimodal policies, without requiring long
backpropagation chains. Our method scales effectively to offline,
offline-to-online, and online RL settings, offering substantial gains in speed
and adaptability over diffusion-based baselines. We further extend SSCP to
goal-conditioned RL, enabling flat policies to exploit subgoal structures
without explicit hierarchical inference. SSCP achieves strong results across
standard offline RL and behavior cloning benchmarks, positioning it as a
versatile, expressive, and efficient framework for deep RL and sequential
decision-making.

</details>


### [122] [Deception Detection in Dyadic Exchanges Using Multimodal Machine Learning: A Study on a Swedish Cohort](https://arxiv.org/abs/2506.21429)
*Franco Rugolon,Thomas Jack Samuels,Stephan Hau,Lennart Högman*

Main category: cs.LG

TL;DR: 该研究探讨了多模态机器学习在检测双向互动中欺骗行为的有效性，结合语音和面部信息显著提高了检测准确率，最佳效果达71%。


<details>
  <summary>Details</summary>
Motivation: 研究旨在验证多模态数据（语音和面部信息）在欺骗检测中的效果，并探索双向互动中欺骗行为的心理学基础。

Method: 研究采用早期和晚期融合策略，结合音频和视频数据（动作单元和凝视信息），分析瑞典母语者在真实或谎言情境下的互动数据。

Result: 结合语音和面部信息的多模态方法优于单模态方法，且包含双方数据显著提高准确率，晚期融合策略最佳效果达71%。

Conclusion: 研究支持心理学理论，即面部和声音表达在初始互动中存在差异控制，为未来双向互动研究（如心理治疗）奠定了基础。

Abstract: This study investigates the efficacy of using multimodal machine learning
techniques to detect deception in dyadic interactions, focusing on the
integration of data from both the deceiver and the deceived. We compare early
and late fusion approaches, utilizing audio and video data - specifically,
Action Units and gaze information - across all possible combinations of
modalities and participants. Our dataset, newly collected from Swedish native
speakers engaged in truth or lie scenarios on emotionally relevant topics,
serves as the basis for our analysis. The results demonstrate that
incorporating both speech and facial information yields superior performance
compared to single-modality approaches. Moreover, including data from both
participants significantly enhances deception detection accuracy, with the best
performance (71%) achieved using a late fusion strategy applied to both
modalities and participants. These findings align with psychological theories
suggesting differential control of facial and vocal expressions during initial
interactions. As the first study of its kind on a Scandinavian cohort, this
research lays the groundwork for future investigations into dyadic
interactions, particularly within psychotherapy settings.

</details>


### [123] [Towards an Optimal Control Perspective of ResNet Training](https://arxiv.org/abs/2506.21453)
*Jens Püttschneider,Simon Heilig,Asja Fischer,Timm Faulwasser*

Main category: cs.LG

TL;DR: 该论文提出了一种基于最优控制理论的ResNet训练方法，通过惩罚隐藏层中间输出来实现网络剪枝。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于将最优控制理论应用于标准ResNet架构，以优化训练过程并减少不必要的深层网络结构。

Method: 方法是通过惩罚隐藏层的中间输出（类似于最优控制中的阶段成本项），并利用跳跃连接和输出层传播状态。

Result: 实验结果表明，该方法能够使不必要的深层残差层权重趋近于零，从而为理论驱动的网络剪枝提供了可能。

Conclusion: 结论是这种训练方法不仅适用于标准架构和通用损失函数，还为网络剪枝提供了理论依据。

Abstract: We propose a training formulation for ResNets reflecting an optimal control
problem that is applicable for standard architectures and general loss
functions. We suggest bridging both worlds via penalizing intermediate outputs
of hidden states corresponding to stage cost terms in optimal control. For
standard ResNets, we obtain intermediate outputs by propagating the state
through the subsequent skip connections and the output layer. We demonstrate
that our training dynamic biases the weights of the unnecessary deeper residual
layers to vanish. This indicates the potential for a theory-grounded layer
pruning strategy.

</details>


### [124] [A Keyword-Based Technique to Evaluate Broad Question Answer Script](https://arxiv.org/abs/2506.21461)
*Tamim Al Mahmud,Md Gulzar Hussain,Sumaiya Kabir,Hasnain Ahmad,Mahmudus Sobhan*

Main category: cs.LG

TL;DR: 该论文提出了一种电子化评估主观答题纸的高效方法，通过关键词提取和语法检查实现自动评分。


<details>
  <summary>Details</summary>
Motivation: 传统主观题评分效率低下且易受主观影响，需要一种自动化的评估方法来提高效率和准确性。

Method: 系统从答题纸中提取关键词，并与开放和封闭域解析的关键词对比，同时检查语法和拼写错误。

Result: 在100份学生答题纸上测试，系统精确度达到0.91。

Conclusion: 提出的系统能有效评估主观答题纸，具有较高的精确度和实用性。

Abstract: Evaluation is the method of assessing and determining the educational system
through various techniques such as verbal or viva-voice test, subjective or
objective written test. This paper presents an efficient solution to evaluate
the subjective answer script electronically. In this paper, we proposed and
implemented an integrated system that examines and evaluates the written answer
script. This article focuses on finding the keywords from the answer script and
then compares them with the keywords that have been parsed from both open and
closed domain. The system also checks the grammatical and spelling errors in
the answer script. Our proposed system tested with answer scripts of 100
students and gives precision score 0.91.

</details>


### [125] [Optimising 4th-Order Runge-Kutta Methods: A Dynamic Heuristic Approach for Efficiency and Low Storage](https://arxiv.org/abs/2506.21465)
*Gavin Lee Goodship,Luis Miralles-Pechuan,Stephen O'Sullivan*

Main category: cs.LG

TL;DR: 该研究提出了一种结合遗传算法和强化学习的混合方法，用于自动发现启发式规则，优化低存储扩展稳定性Runge-Kutta方法，显著提高了计算效率，同时保持了数值稳定性和精度。


<details>
  <summary>Details</summary>
Motivation: 扩展稳定性Runge-Kutta方法在解决大规模科学和工程计算问题中至关重要，但平衡其精度、稳定性和计算效率仍具挑战性，尤其是对于高阶低存储方案。

Method: 采用遗传算法驱动的变异进行搜索空间探索，并结合强化学习启发的状态转移机制动态优化启发式选择，实现系统参数减少，同时保持四阶精度。

Result: 在基准测试中，最优启发式规则相比传统优化过程减少了25%的IPOPT运行时间，同时保持了数值稳定性和精度。

Conclusion: 该研究为数值方法的启发式优化建立了新范式，展示了自适应启发式发现在提高高保真模拟资源效率和拓宽低存储Runge-Kutta方法应用范围方面的潜力。

Abstract: Extended Stability Runge-Kutta (ESRK) methods are crucial for solving
large-scale computational problems in science and engineering, including
weather forecasting, aerodynamic analysis, and complex biological modelling.
However, balancing accuracy, stability, and computational efficiency remains
challenging, particularly for high-order, low-storage schemes. This study
introduces a hybrid Genetic Algorithm (GA) and Reinforcement Learning (RL)
approach for automated heuristic discovery, optimising low-storage ESRK
methods. Unlike traditional approaches that rely on manually designed
heuristics or exhaustive numerical searches, our method leverages GA-driven
mutations for search-space exploration and an RL-inspired state transition
mechanism to refine heuristic selection dynamically. This enables systematic
parameter reduction, preserving fourth-order accuracy while significantly
improving computational efficiency.The proposed GA-RL heuristic optimisation
framework is validated through rigorous testing on benchmark problems,
including the 1D and 2D Brusselator systems and the steady-state Navier-Stokes
equations. The best-performing heuristic achieves a 25\% reduction in IPOPT
runtime compared to traditional ESRK optimisation processes while maintaining
numerical stability and accuracy. These findings demonstrate the potential of
adaptive heuristic discovery to improve resource efficiency in high-fidelity
simulations and broaden the applicability of low-storage Runge-Kutta methods in
real-world computational fluid dynamics, physics simulations, and other
demanding fields. This work establishes a new paradigm in heuristic
optimisation for numerical methods, opening pathways for further exploration
using Deep RL and AutoML-based heuristic search

</details>


### [126] [Devising a solution to the problems of Cancer awareness in Telangana](https://arxiv.org/abs/2506.21500)
*Priyanka Avhad,Vedanti Kshirsagar,Urvi Ranjan,Mahek Nakhua*

Main category: cs.LG

TL;DR: 该研究针对印度特伦甘纳地区女性癌症筛查率低的问题，开发了基于机器学习的分类模型预测乳腺癌和宫颈癌风险，并设计了就近医疗推荐系统。


<details>
  <summary>Details</summary>
Motivation: 特伦甘纳地区2020年数据显示，女性宫颈癌、乳腺癌和口腔癌筛查率分别仅为3.3%、0.3%和2.3%。由于早期检测是降低发病率和死亡率的唯一途径，但民众对癌症症状和筛查认知不足，因此需要提升癌症意识和筛查率。

Method: 采用决策树分类和支持向量机分类算法，分别构建宫颈癌和乳腺癌易感性预测模型，并开发基于地理位置的就医推荐系统，整合电子健康卡以管理个人医疗记录。

Result: 开发出可预测癌症风险的机器学习模型及配套医疗推荐系统，为提升地区癌症防治能力提供技术解决方案。

Conclusion: 该研究通过技术手段促进癌症早筛早治，有助于降低特伦甘纳地区癌症死亡率并提升公众健康素养。

Abstract: According to the data, the percent of women who underwent screening for
cervical cancer, breast and oral cancer in Telangana in the year 2020 was 3.3
percent, 0.3 percent and 2.3 percent respectively. Although early detection is
the only way to reduce morbidity and mortality, people have very low awareness
about cervical and breast cancer signs and symptoms and screening practices. We
developed an ML classification model to predict if a person is susceptible to
breast or cervical cancer based on demographic factors. We devised a system to
provide suggestions for the nearest hospital or Cancer treatment centres based
on the users location or address. In addition to this, we can integrate the
health card to maintain medical records of all individuals and conduct
awareness drives and campaigns. For ML classification models, we used decision
tree classification and support vector classification algorithms for cervical
cancer susceptibility and breast cancer susceptibility respectively. Thus, by
devising this solution we come one step closer to our goal which is spreading
cancer awareness, thereby, decreasing the cancer mortality and increasing
cancer literacy among the people of Telangana.

</details>


### [127] [Process mining-driven modeling and simulation to enhance fault diagnosis in cyber-physical systems](https://arxiv.org/abs/2506.21502)
*Francesco Vitale,Nicola Dall'Ora,Sebastiano Gaiardelli,Enrico Fraccaroli,Nicola Mazzocca,Franco Fummi*

Main category: cs.LG

TL;DR: 提出了一种结合多元时间序列分析、过程挖掘和随机模拟的无监督故障诊断方法，用于自动建模和分类CPS中的故障行为。


<details>
  <summary>Details</summary>
Motivation: 传统手动建模故障行为需要大量领域知识，且模型复杂、易错、难以解释。本文旨在解决这一问题，提出自动化方法以提升故障诊断效率和准确性。

Method: 通过多元时间序列分析检测集体异常，将其转化为结构化事件日志，利用过程挖掘生成可解释的过程模型，并通过随机模拟增强根因分析。

Result: 在智能制造的Robotic Arm Dataset上验证了方法的有效性，能够成功建模、模拟和分类故障行为，支持预测性维护和数字孪生开发。

Conclusion: 该方法为CPS故障诊断提供了一种高效、自动化的解决方案，有助于构建全面的故障字典并支持工业环境中的数字孪生应用。

Abstract: Fault diagnosis in Cyber-Physical Systems (CPSs) is essential for ensuring
system dependability and operational efficiency by accurately detecting
anomalies and identifying their root causes. However, the manual modeling of
faulty behaviors often demands extensive domain expertise and produces models
that are complex, error-prone, and difficult to interpret. To address this
challenge, we present a novel unsupervised fault diagnosis methodology that
integrates collective anomaly detection in multivariate time series, process
mining, and stochastic simulation. Initially, collective anomalies are detected
from low-level sensor data using multivariate time-series analysis. These
anomalies are then transformed into structured event logs, enabling the
discovery of interpretable process models through process mining. By
incorporating timing distributions into the extracted Petri nets, the approach
supports stochastic simulation of faulty behaviors, thereby enhancing root
cause analysis and behavioral understanding. The methodology is validated using
the Robotic Arm Dataset (RoAD), a widely recognized benchmark in smart
manufacturing. Experimental results demonstrate its effectiveness in modeling,
simulating, and classifying faulty behaviors in CPSs. This enables the creation
of comprehensive fault dictionaries that support predictive maintenance and the
development of digital twins for industrial environments.

</details>


### [128] [mTSBench: Benchmarking Multivariate Time Series Anomaly Detection and Model Selection at Scale](https://arxiv.org/abs/2506.21550)
*Xiaona Zhou,Constantin Brif,Ismini Lourentzou*

Main category: cs.LG

TL;DR: 论文介绍了mTSBench，一个用于多元时间序列异常检测（MTS-AD）和无监督模型选择的最大基准，评估了24种异常检测方法，并揭示了当前模型选择方法的不足。


<details>
  <summary>Details</summary>
Motivation: 多元时间序列异常检测在医疗、网络安全和工业监控等领域至关重要，但由于变量间复杂的依赖关系、时间动态性和稀疏的异常标签，该任务仍具挑战性。

Method: 作者构建了mTSBench基准，涵盖19个数据集中的344个标记时间序列，评估了24种异常检测方法，包括基于大型语言模型（LLM）的检测器，并系统性地在标准化条件下对无监督模型选择技术进行了基准测试。

Result: 研究结果证实，没有单一检测器在所有数据集上表现优异，强调了模型选择的重要性。然而，即使是最先进的选择方法也远未达到最优，揭示了关键差距。

Conclusion: mTSBench提供了一个统一的评估套件，支持严格、可重复的比较，并推动自适应异常检测和鲁棒模型选择的未来发展。

Abstract: Multivariate time series anomaly detection (MTS-AD) is critical in domains
like healthcare, cybersecurity, and industrial monitoring, yet remains
challenging due to complex inter-variable dependencies, temporal dynamics, and
sparse anomaly labels. We introduce mTSBench, the largest benchmark to date for
MTS-AD and unsupervised model selection, spanning 344 labeled time series
across 19 datasets and 12 diverse application domains. mTSBench evaluates 24
anomaly detection methods, including large language model (LLM)-based detectors
for multivariate time series, and systematically benchmarks unsupervised model
selection techniques under standardized conditions. Consistent with prior
findings, our results confirm that no single detector excels across datasets,
underscoring the importance of model selection. However, even state-of-the-art
selection methods remain far from optimal, revealing critical gaps. mTSBench
provides a unified evaluation suite to enable rigorous, reproducible
comparisons and catalyze future advances in adaptive anomaly detection and
robust model selection.

</details>


### [129] [Where to find Grokking in LLM Pretraining? Monitor Memorization-to-Generalization without Test](https://arxiv.org/abs/2506.21551)
*Ziyue Li,Chenrui Fan,Tianyi Zhou*

Main category: cs.LG

TL;DR: 研究发现，在大规模基础模型预训练中仍存在‘顿悟’现象，即测试性能在训练损失收敛后持续提升，揭示了从记忆到泛化的内部动态转换机制。


<details>
  <summary>Details</summary>
Motivation: 探索神经网络训练中‘顿悟’现象的机制，特别是在大规模语言模型预训练中的表现，以理解泛化能力和其他新兴能力（如推理）的形成过程。

Method: 通过分析7B参数大语言模型OLMoE的单次预训练检查点，计算训练损失并评估在数学推理、代码生成和常识/领域知识检索等多样化基准任务上的泛化能力。开发两种新指标量化路径距离和单个路径的复杂性。

Result: 验证了大规模基础模型预训练中‘顿悟’现象的存在，发现训练样本的路径从随机、实例特定演变为更结构化且样本间可共享，路径复杂性降低，表明从记忆到泛化的转换。新指标能有效预测下游任务的泛化提升。

Conclusion: 研究不仅证实了‘顿悟’在大规模预训练中的存在，还通过内部动态分析揭示了其机制，提出的新指标具有实际应用价值，可用于无需微调和测试的泛化性能监控。理论分析表明，更结构化的路径降低了模型复杂性并改善了泛化界限。

Abstract: Grokking, i.e., test performance keeps improving long after training loss
converged, has been recently witnessed in neural network training, making the
mechanism of generalization and other emerging capabilities such as reasoning
mysterious. While prior studies usually train small models on a few toy or
highly-specific tasks for thousands of epochs, we conduct the first study of
grokking on checkpoints during one-pass pretraining of a 7B large language
model (LLM), i.e., OLMoE. We compute the training loss and evaluate
generalization on diverse benchmark tasks, including math reasoning, code
generation, and commonsense/domain-specific knowledge retrieval tasks.
  Our study, for the first time, verifies that grokking still happens in the
pretraining of large-scale foundation models, though different data may enter
grokking stages asynchronously. We further demystify grokking's "emergence of
generalization" by investigating LLM internal dynamics. Specifically, we find
that training samples' pathways (i.e., expert choices across layers) evolve
from random, instance-specific to more structured and shareable between samples
during grokking. Also, the complexity of a sample's pathway reduces despite the
converged loss. These indicate a memorization-to-generalization conversion,
providing a mechanistic explanation of delayed generalization. In the study, we
develop two novel metrics to quantify pathway distance and the complexity of a
single pathway. We show their ability to predict the generalization improvement
on diverse downstream tasks. They are efficient, simple to compute and solely
dependent on training data. Hence, they have practical value for pretraining,
enabling us to monitor the generalization performance without finetuning and
test. Theoretically, we show that more structured pathways reduce model
complexity and improve the generalization bound.

</details>


<div id='q-fin.PM'></div>

# q-fin.PM [[Back]](#toc)

### [130] [From On-chain to Macro: Assessing the Importance of Data Source Diversity in Cryptocurrency Market Forecasting](https://arxiv.org/abs/2506.21246)
*Giorgos Demosthenous,Chryssis Georgiou,Eliada Polydorou*

Main category: q-fin.PM

TL;DR: 该研究探讨了数据源多样性对加密货币预测模型性能的影响，通过整合多种数据类型并提出新特征降维算法，发现多样性显著提升预测准确性，尤其是链上指标对短期和长期预测的重要性。


<details>
  <summary>Details</summary>
Motivation: 研究旨在揭示不同数据源对加密货币市场预测模型的影响，特别是如何通过整合多样化的数据提升预测的准确性和稳健性，以更好地理解加密货币市场的驱动因素。

Method: 研究引入了Crypto100指数，代表市值前100的加密货币，并提出了一种新的特征降维算法，从多种数据源中识别最具影响力和稳健性的特征。

Result: 实验表明，数据源多样性显著提升了预测模型在不同时间范围内的性能，链上指标对短期和长期预测至关重要，传统市场指数和宏观经济指标对长期预测的贡献逐渐增加。

Conclusion: 研究结果为加密货币市场的短期和长期驱动因素提供了新的见解，为开发更准确和稳健的预测模型奠定了基础。

Abstract: This study investigates the impact of data source diversity on the
performance of cryptocurrency forecasting models by integrating various data
categories, including technical indicators, on-chain metrics, sentiment and
interest metrics, traditional market indices, and macroeconomic indicators. We
introduce the Crypto100 index, representing the top 100 cryptocurrencies by
market capitalization, and propose a novel feature reduction algorithm to
identify the most impactful and resilient features from diverse data sources.
Our comprehensive experiments demonstrate that data source diversity
significantly enhances the predictive performance of forecasting models across
different time horizons. Key findings include the paramount importance of
on-chain metrics for both short-term and long-term predictions, the growing
relevance of traditional market indices and macroeconomic indicators for
longer-term forecasts, and substantial improvements in model accuracy when
diverse data sources are utilized. These insights help demystify the short-term
and long-term driving factors of the cryptocurrency market and lay the
groundwork for developing more accurate and resilient forecasting models.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [131] [Spiking Neural Networks for SAR Interferometric Phase Unwrapping: A Theoretical Framework for Energy-Efficient Processing](https://arxiv.org/abs/2506.20782)
*Marc Bara*

Main category: cs.NE

TL;DR: 首次提出将脉冲神经网络（SNN）应用于合成孔径雷达（SAR）干涉相位解缠的理论框架，填补了该领域的方法空白，并展示了SNN在能效和处理大规模地球观测数据方面的潜力。


<details>
  <summary>Details</summary>
Motivation: 随着地球观测数据量的指数级增长（如NISAR任务预计在两年内生成100PB数据），能效处理成为数据中心可持续运行的关键。SNN因其事件驱动的计算模型，相比传统方法可节省30-100倍能源，同时保持相当的精度。

Method: 开发了专门针对缠绕相位数据的脉冲编码方案，提出了利用相位解缠空间传播特性的SNN架构，并对计算复杂度和收敛性进行了理论分析。

Result: 提出的框架展示了SNN的时间动态特性如何自然地建模相位解缠的空间连续性约束，为大规模InSAR处理提供了可持续的互补方法。

Conclusion: 这项工作在神经形态计算和SAR干涉测量的交叉领域开辟了新的研究方向，为现有算法提供了互补方案，有望实现更可持续的大规模InSAR处理。

Abstract: We present the first theoretical framework for applying spiking neural
networks (SNNs) to synthetic aperture radar (SAR) interferometric phase
unwrapping. Despite extensive research in both domains, our comprehensive
literature review confirms that SNNs have never been applied to phase
unwrapping, representing a significant gap in current methodologies. As Earth
observation data volumes continue to grow exponentially (with missions like
NISAR expected to generate 100PB in two years) energy-efficient processing
becomes critical for sustainable data center operations. SNNs, with their
event-driven computation model, offer potential energy savings of 30-100x
compared to conventional approaches while maintaining comparable accuracy. We
develop spike encoding schemes specifically designed for wrapped phase data,
propose SNN architectures that leverage the spatial propagation nature of phase
unwrapping, and provide theoretical analysis of computational complexity and
convergence properties. Our framework demonstrates how the temporal dynamics
inherent in SNNs can naturally model the spatial continuity constraints
fundamental to phase unwrapping. This work opens a new research direction at
the intersection of neuromorphic computing and SAR interferometry, offering a
complementary approach to existing algorithms that could enable more
sustainable large-scale InSAR processing.

</details>


### [132] [Stochastic Quantum Spiking Neural Networks with Quantum Memory and Local Learning](https://arxiv.org/abs/2506.21324)
*Jiechen Chen,Bipin Rajendran,Osvaldo Simeone*

Main category: cs.NE

TL;DR: 本文提出了一种新型随机量子脉冲（SQS）神经元模型，结合神经形态计算的时间序列处理效率和量子计算的指数级状态空间优势，解决了现有量子脉冲模型的局限性。


<details>
  <summary>Details</summary>
Motivation: 神经形态计算和量子计算在人工智能领域展现出互补优势，但现有量子脉冲神经元模型依赖经典内存机制和重复测量，且训练过程需传统反向传播。本文旨在解决这些限制。

Method: 提出SQS神经元模型，利用多量子比特电路实现具有内部量子记忆的脉冲单元，支持单次事件驱动的概率脉冲生成，并通过硬件友好的局部学习规则训练SQS神经网络（SQSNN）。

Result: SQS模型实现了量子硬件上的模块化、可扩展和可训练性，融合了神经形态计算的高效时间序列处理与量子计算的指数级状态空间优势。

Conclusion: SQSNN为量子脉冲神经网络的发展开辟了新路径，解决了现有模型的局限性，展现了在量子硬件上实现高效计算的潜力。

Abstract: Neuromorphic and quantum computing have recently emerged as promising
paradigms for advancing artificial intelligence, each offering complementary
strengths. Neuromorphic systems built on spiking neurons excel at processing
time-series data efficiently through sparse, event-driven computation,
consuming energy only upon input events. Quantum computing, on the other hand,
leverages superposition and entanglement to explore feature spaces that are
exponentially large in the number of qubits. Hybrid approaches combining these
paradigms have begun to show potential, but existing quantum spiking models
have important limitations. Notably, prior quantum spiking neuron
implementations rely on classical memory mechanisms on single qubits, requiring
repeated measurements to estimate firing probabilities, and they use
conventional backpropagation on classical simulators for training. Here we
propose a stochastic quantum spiking (SQS) neuron model that addresses these
challenges. The SQS neuron uses multi-qubit quantum circuits to realize a
spiking unit with internal quantum memory, enabling event-driven probabilistic
spike generation in a single shot. Furthermore, we outline how networks of SQS
neurons -- dubbed SQS neural networks (SQSNNs) -- can be trained via a
hardware-friendly local learning rule, eliminating the need for global
classical backpropagation. The proposed SQSNN model fuses the time-series
efficiency of neuromorphic computing with the exponentially large inner state
space of quantum computing, paving the way for quantum spiking neural networks
that are modular, scalable, and trainable on quantum hardware.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [133] [Real-time and personalized product recommendations for large e-commerce platforms](https://arxiv.org/abs/2506.21368)
*Matteo Tolloso,Davide Bacciu,Shahab Mokarizadeh,Marco Varesi*

Main category: cs.IR

TL;DR: 提出了一种基于图神经网络和简约学习方法的实时个性化时尚商品推荐方法，适用于大型电商平台。


<details>
  <summary>Details</summary>
Motivation: 为了在大型电商平台上实现实时、准确且可扩展的个性化产品推荐，特别是在时尚零售领域，提升用户满意度。

Method: 利用图神经网络（Graph Neural Networks）和简约学习方法（parsimonious learning methodologies），处理多交互场景并预测购买序列。

Result: 在大型电商平台数据集上的广泛实验表明，该方法能够高效地提供个性化推荐，并在实际约束下表现优异。

Conclusion: 该方法能够有效实现实时、个性化的时尚商品推荐，满足用户需求并在实际应用中表现出色。

Abstract: We present a methodology to provide real-time and personalized product
recommendations for large e-commerce platforms, specifically focusing on
fashion retail. Our approach aims to achieve accurate and scalable
recommendations with minimal response times, ensuring user satisfaction,
leveraging Graph Neural Networks and parsimonious learning methodologies.
Extensive experimentation with datasets from one of the largest e-commerce
platforms demonstrates the effectiveness of our approach in forecasting
purchase sequences and handling multi-interaction scenarios, achieving
efficient personalized recommendations under real-world constraints.

</details>


### [134] [EraRAG: Efficient and Incremental Retrieval Augmented Generation for Growing Corpora](https://arxiv.org/abs/2506.20963)
*Fangyuan Zhang,Zhengjun Huang,Yingli Zhou,Qintian Guo,Zhixun Li,Wensheng Luo,Di Jiang,Yixiang Fang,Xiaofang Zhou*

Main category: cs.IR

TL;DR: EraRAG提出了一种新型多层Graph-RAG框架，通过超平面局部敏感哈希实现动态更新，显著提高了处理动态语料库的效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的Graph-RAG方法通常假设语料库是静态的，当新文档加入时需要昂贵的全图重建，限制了其在动态环境中的可扩展性。

Method: EraRAG利用基于超平面的局部敏感哈希（LSH）将原始语料库分层组织成图结构，支持高效、局部的数据插入而不破坏现有拓扑。

Result: 实验表明，EraRAG在更新时间和令牌消耗上比现有Graph-RAG系统减少了一个数量级，同时保持了高检索准确性和低延迟。

Conclusion: EraRAG为持续增长的语料库提供了高效的检索增强生成解决方案，弥合了检索效率和适应性之间的差距。

Abstract: Graph-based Retrieval-Augmented Generation (Graph-RAG) enhances large
language models (LLMs) by structuring retrieval over an external corpus.
However, existing approaches typically assume a static corpus, requiring
expensive full-graph reconstruction whenever new documents arrive, limiting
their scalability in dynamic, evolving environments. To address these
limitations, we introduce EraRAG, a novel multi-layered Graph-RAG framework
that supports efficient and scalable dynamic updates. Our method leverages
hyperplane-based Locality-Sensitive Hashing (LSH) to partition and organize the
original corpus into hierarchical graph structures, enabling efficient and
localized insertions of new data without disrupting the existing topology. The
design eliminates the need for retraining or costly recomputation while
preserving high retrieval accuracy and low latency. Experiments on large-scale
benchmarks demonstrate that EraRag achieves up to an order of magnitude
reduction in update time and token consumption compared to existing Graph-RAG
systems, while providing superior accuracy performance. This work offers a
practical path forward for RAG systems that must operate over continually
growing corpora, bridging the gap between retrieval efficiency and
adaptability. Our code and data are available at
https://github.com/EverM0re/EraRAG-Official.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [135] [Control and optimization for Neural Partial Differential Equations in Supervised Learning](https://arxiv.org/abs/2506.20764)
*Alain Bensoussan,Minh-Binh Tran,Bangjie Wang*

Main category: math.OC

TL;DR: 该论文提出了一种新颖的视角，将神经网络视为偏微分方程（PDEs），并探讨了在PDEs框架下优化和控制抛物型及双曲型算子系数的问题。


<details>
  <summary>Details</summary>
Motivation: 尽管抛物型和双曲型系统的控制和优化问题已有大量研究，但针对这些系统中算子系数的控制和优化问题尚未深入探索。这一问题的研究在神经网络和监督学习的背景下具有自然的应用价值。

Method: 论文提出了一种双系统公式化方法，用于处理与抛物型PDEs相关的控制和优化问题，并为未来开发高效数值方案奠定了基础。同时，论文还研究了双曲型PDEs的控制问题。

Result: 论文证明了抛物型PDEs的控制和优化问题存在极小解，并证明了双曲型PDEs的近似控制问题解的存在性。

Conclusion: 该论文为PDEs控制理论中算子系数的优化和控制问题提供了新的研究方向和理论基础，未来可进一步开发高效的数值计算方法。

Abstract: Although there is a substantial body of literature on control and
optimization problems for parabolic and hyperbolic systems, the specific
problem of controlling and optimizing the coefficients of the associated
operators within such systems has not yet been thoroughly explored. In this
work, we aim to initiate a line of research in control theory focused on
optimizing and controlling the coefficients of these operators-a problem that
naturally arises in the context of neural networks and supervised learning.
  In supervised learning, the primary objective is to transport initial data
toward target data through the layers of a neural network. We propose a novel
perspective: neural networks can be interpreted as partial differential
equations (PDEs). From this viewpoint, the control problem traditionally
studied in the context of ordinary differential equations (ODEs) is
reformulated as a control problem for PDEs, specifically targeting the
optimization and control of coefficients in parabolic and hyperbolic operators.
To the best of our knowledge, this specific problem has not yet been
systematically addressed in the control theory of PDEs.
  To this end, we propose a dual system formulation for the control and
optimization problem associated with parabolic PDEs, laying the groundwork for
the development of efficient numerical schemes in future research. We also
provide a theoretical proof showing that the control and optimization problem
for parabolic PDEs admits minimizers. Finally, we investigate the control
problem associated with hyperbolic PDEs and prove the existence of solutions
for a corresponding approximated control problem.

</details>


### [136] [Faster Fixed-Point Methods for Multichain MDPs](https://arxiv.org/abs/2506.20910)
*Matthew Zurek,Yudong Chen*

Main category: math.OC

TL;DR: 本文研究了多链马尔可夫决策过程（MDPs）在平均奖励准则下的值迭代（VI）算法，提出了改进的算法以解决导航子问题，从而获得更快的收敛速度和更精确的复杂度度量。


<details>
  <summary>Details</summary>
Motivation: 多链MDPs在平均奖励准则下的求解面临理论挑战，包括缺乏收缩性和贝尔曼算子解的非唯一性。此外，最优策略还需解决导航子问题，即在每个连通组件内优化长期性能的同时，引导向最佳连通组件。

Method: 开发了改进的值迭代算法，通过更好地解决导航子问题来加速多链MDPs的收敛。方法包括建立平均奖励与折扣问题的新联系、扩展至一般Banach空间的最优固定点方法、以及改进的子最优性分解。

Result: 获得了比先前工作更快的收敛速度和更精确的复杂度度量。关键结果包括折扣值误差的次线性收敛速率和多链MDPs的精细化子最优性分解。

Conclusion: 本文的结果为折扣和平均奖励问题提供了更快的收敛速度，并扩展了值迭代方法的理论基础。

Abstract: We study value-iteration (VI) algorithms for solving general (a.k.a.
multichain) Markov decision processes (MDPs) under the average-reward
criterion, a fundamental but theoretically challenging setting. Beyond the
difficulties inherent to all average-reward problems posed by the lack of
contractivity and non-uniqueness of solutions to the Bellman operator, in the
multichain setting an optimal policy must solve the navigation subproblem of
steering towards the best connected component, in addition to optimizing
long-run performance within each component. We develop algorithms which better
solve this navigational subproblem in order to achieve faster convergence for
multichain MDPs, obtaining improved rates of convergence and sharper measures
of complexity relative to prior work. Many key components of our results are of
potential independent interest, including novel connections between
average-reward and discounted problems, optimal fixed-point methods for
discounted VI which extend to general Banach spaces, new sublinear convergence
rates for the discounted value error, and refined suboptimality decompositions
for multichain MDPs. Overall our results yield faster convergence rates for
discounted and average-reward problems and expand the theoretical foundations
of VI approaches.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [137] [Quantum Reinforcement Learning Trading Agent for Sector Rotation in the Taiwan Stock Market](https://arxiv.org/abs/2506.20930)
*Chi-Sheng Chen,Xinyu Zhang,Ya-Chuan Chen*

Main category: quant-ph

TL;DR: 该论文提出了一种混合量子-经典强化学习框架用于台湾股市板块轮动策略，发现量子模型虽在训练奖励上表现更优，但在实际投资指标上却逊于经典模型。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探索量子强化学习在金融领域的实际应用潜力，特别是针对板块轮动策略的优化，并揭示当前奖励设计与实际投资目标之间的不匹配问题。

Method: 采用PPO算法作为主干，结合经典模型（LSTM、Transformer）和量子增强模型（QNN、QRWKV、QASA）构建策略与价值网络，并通过自动化特征工程处理资本份额数据。

Result: 量子模型在训练奖励上优于经典模型，但在累计收益和夏普比率等实际投资指标上表现较差，突显了代理奖励信号与真实投资目标之间的不匹配问题。

Conclusion: 当前奖励设计可能导致模型过度拟合短期波动而非优化风险调整后收益，未来需改进奖励设计、模型正则化和基于验证的早停策略，以提升量子强化学习在金融领域的实用性。

Abstract: We propose a hybrid quantum-classical reinforcement learning framework for
sector rotation in the Taiwan stock market. Our system employs Proximal Policy
Optimization (PPO) as the backbone algorithm and integrates both classical
architectures (LSTM, Transformer) and quantum-enhanced models (QNN, QRWKV,
QASA) as policy and value networks. An automated feature engineering pipeline
extracts financial indicators from capital share data to ensure consistent
model input across all configurations. Empirical backtesting reveals a key
finding: although quantum-enhanced models consistently achieve higher training
rewards, they underperform classical models in real-world investment metrics
such as cumulative return and Sharpe ratio. This discrepancy highlights a core
challenge in applying reinforcement learning to financial domains -- namely,
the mismatch between proxy reward signals and true investment objectives. Our
analysis suggests that current reward designs may incentivize overfitting to
short-term volatility rather than optimizing risk-adjusted returns. This issue
is compounded by the inherent expressiveness and optimization instability of
quantum circuits under Noisy Intermediate-Scale Quantum (NISQ) constraints. We
discuss the implications of this reward-performance gap and propose directions
for future improvement, including reward shaping, model regularization, and
validation-based early stopping. Our work offers a reproducible benchmark and
critical insights into the practical challenges of deploying quantum
reinforcement learning in real-world finance.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [138] [The final solution of the Hitchhiker's problem #5](https://arxiv.org/abs/2506.20672)
*Matjaž Omladič,Martin Vuk,Aljaž Zalar*

Main category: stat.ML

TL;DR: 本文通过解析方法解决了关于多元拟连接函数质量分布极值的开放性问题，并反驳了相关猜想。


<details>
  <summary>Details</summary>
Motivation: 尽管拟连接函数在统计解释上存在不足，但《Hitchhiker's Guide》调查提升了其在依赖建模领域的关注度。本文旨在解决该调查中的开放性问题。

Method: 采用解析方法进行研究，而非之前使用的线性规划方法。

Result: 完全解答了原始问题，并在维度d=17内解决了开放性问题，同时反驳了最近的一个猜想。

Conclusion: 通过解析方法成功解决了拟连接函数质量分布极值的开放性问题，为相关领域提供了完整的答案。

Abstract: A recent survey, nicknamed "Hitchhiker's Guide", J.J. Arias-Garc{\i}a, R.
Mesiar, and B. De Baets, A hitchhiker's guide to quasi-copulas, Fuzzy Sets and
Systems 393 (2020) 1-28, has raised the rating of quasi-copula problems in the
dependence modeling community in spite of the lack of statistical
interpretation of quasi-copulas. In our previous work (arXiv:2410.19339,
accepted in Fuzzy Sets and Systems), we addressed the question of extreme
values of the mass distribution associated with multivariate quasi-copulas.
Using a linear programming approach, we were able to solve Open Problem 5 of
the "Guide" up to dimension d = 17 and disprove a recent conjecture on the
solution to that problem. In this paper, we use an analytical approach to
provide a complete answer to the original question.

</details>


### [139] [Stable Minima of ReLU Neural Networks Suffer from the Curse of Dimensionality: The Neural Shattering Phenomenon](https://arxiv.org/abs/2506.20779)
*Tongtong Liang,Dan Qiao,Yu-Xiang Wang,Rahul Parhi*

Main category: stat.ML

TL;DR: 论文研究了平坦性/低曲率在过参数化ReLU网络中的隐式偏差及其对泛化的影响，发现平坦解在高维输入下泛化性能会指数级恶化。


<details>
  <summary>Details</summary>
Motivation: 研究梯度下降训练中最小值稳定性和边缘稳定性现象，探讨平坦解在多元输入下的泛化性能。

Method: 通过理论分析和数值模拟，研究平坦解在泛化差距和非参数函数估计中的表现。

Result: 平坦解在高维输入下泛化性能指数级恶化，与低范数解形成鲜明对比。

Conclusion: 平坦解在高维输入下可能无法有效泛化，揭示了平坦性与维度灾难的关系。

Abstract: We study the implicit bias of flatness / low (loss) curvature and its effects
on generalization in two-layer overparameterized ReLU networks with
multivariate inputs -- a problem well motivated by the minima stability and
edge-of-stability phenomena in gradient-descent training. Existing work either
requires interpolation or focuses only on univariate inputs. This paper
presents new and somewhat surprising theoretical results for multivariate
inputs. On two natural settings (1) generalization gap for flat solutions, and
(2) mean-squared error (MSE) in nonparametric function estimation by stable
minima, we prove upper and lower bounds, which establish that while flatness
does imply generalization, the resulting rates of convergence necessarily
deteriorate exponentially as the input dimension grows. This gives an
exponential separation between the flat solutions vis-\`a-vis low-norm
solutions (i.e., weight decay), which knowingly do not suffer from the curse of
dimensionality. In particular, our minimax lower bound construction, based on a
novel packing argument with boundary-localized ReLU neurons, reveals how flat
solutions can exploit a kind of ''neural shattering'' where neurons rarely
activate, but with high weight magnitudes. This leads to poor performance in
high dimensions. We corroborate these theoretical findings with extensive
numerical simulations. To the best of our knowledge, our analysis provides the
first systematic explanation for why flat minima may fail to generalize in high
dimensions.

</details>


### [140] [Hyperspherical Variational Autoencoders Using Efficient Spherical Cauchy Distribution](https://arxiv.org/abs/2506.21278)
*Lukas Sablica,Kurt Hornik*

Main category: stat.ML

TL;DR: 提出了一种新型变分自编码器（VAE）架构，采用球形柯西（spCauchy）潜在分布，相比传统高斯或von Mises-Fisher分布，能更自然地表示方向性数据，避免过正则化，且计算更稳定高效。


<details>
  <summary>Details</summary>
Motivation: 传统VAE使用高斯潜在分布或von Mises-Fisher（vMF）分布存在局限性，如高斯分布不适合方向性数据，vMF存在数值不稳定性和计算复杂度高的问题。spCauchy分布能更好地捕捉方向性数据，同时避免这些问题。

Method: 采用球形柯西（spCauchy）作为潜在分布，利用其重尾特性防止过正则化，并通过Möbius变换实现高效可微分的重参数化技巧，KL散度通过快速收敛的幂级数计算。

Result: spCauchy分布在高维生成模型中表现出色，提供了更灵活和表达力强的潜在表示，同时避免了vMF的数值不稳定性和计算瓶颈。

Conclusion: spCauchy分布是VAE中一种有理论优势且实际高效的潜在分布选择，特别适合高维生成建模任务。

Abstract: We propose a novel variational autoencoder (VAE) architecture that employs a
spherical Cauchy (spCauchy) latent distribution. Unlike traditional Gaussian
latent spaces or the widely used von Mises-Fisher (vMF) distribution, spCauchy
provides a more natural hyperspherical representation of latent variables,
better capturing directional data while maintaining flexibility. Its
heavy-tailed nature prevents over-regularization, ensuring efficient latent
space utilization while offering a more expressive representation.
Additionally, spCauchy circumvents the numerical instabilities inherent to vMF,
which arise from computing normalization constants involving Bessel functions.
Instead, it enables a fully differentiable and efficient reparameterization
trick via M\"obius transformations, allowing for stable and scalable training.
The KL divergence can be computed through a rapidly converging power series,
eliminating concerns of underflow or overflow associated with evaluation of
ratios of hypergeometric functions. These properties make spCauchy a compelling
alternative for VAEs, offering both theoretical advantages and practical
efficiency in high-dimensional generative modeling.

</details>


### [141] [Active Learning for Manifold Gaussian Process Regression](https://arxiv.org/abs/2506.20928)
*Yuanxing Cheng,Lulu Kang,Yiwei Wang,Chun Liu*

Main category: stat.ML

TL;DR: 该论文提出了一种结合流形学习与主动学习的高斯过程回归框架，通过联合优化降维神经网络和潜在空间高斯过程回归器，提升高维空间预测精度。


<details>
  <summary>Details</summary>
Motivation: 高维空间中的复杂、不连续函数回归问题传统方法效果有限，需结合流形学习与主动学习以提高预测准确性和计算效率。

Method: 联合训练降维神经网络与潜在空间高斯过程回归器，采用主动学习准则最小化全局预测误差，实现数据高效选择。

Result: 合成数据实验表明，该方法在精度上显著优于随机顺序学习，且能保持计算可行性。

Conclusion: 该框架为科学和工程应用提供了实用价值，未来将聚焦可扩展性和不确定性感知的流形学习改进。

Abstract: This paper introduces an active learning framework for manifold Gaussian
Process (GP) regression, combining manifold learning with strategic data
selection to improve accuracy in high-dimensional spaces. Our method jointly
optimizes a neural network for dimensionality reduction and a Gaussian process
regressor in the latent space, supervised by an active learning criterion that
minimizes global prediction error. Experiments on synthetic data demonstrate
superior performance over randomly sequential learning. The framework
efficiently handles complex, discontinuous functions while preserving
computational tractability, offering practical value for scientific and
engineering applications. Future work will focus on scalability and
uncertainty-aware manifold learning.

</details>


### [142] [Lower Bounds on the Size of Markov Equivalence Classes](https://arxiv.org/abs/2506.20933)
*Erik Jahn,Frederick Eberhardt,Leonard J. Schulman*

Main category: stat.ML

TL;DR: 论文探讨了因果发现算法在放宽假设条件下马尔可夫等价类的规模问题，证明在稀疏随机有向无环图、均匀随机混合图和循环图中，等价类规模呈指数级增长。


<details>
  <summary>Details</summary>
Motivation: 传统因果发现算法在特定假设下（如无环性、因果充分性等）能有效识别因果图，但放宽这些假设后等价类的规模变化尚未明确。本文旨在填补这一研究空白。

Method: 通过理论分析，研究三种放宽假设的场景：稀疏随机有向无环图、均匀随机混合图和循环图，推导等价类规模的数学下界。

Result: 证明在上述三种场景中，马尔可夫等价类的期望规模呈指数级增长，显著高于传统假设下的结果。

Conclusion: 放宽因果发现的基本假设会导致马尔可夫等价类规模急剧扩大，凸显了观测数据因果推断的固有局限性。

Abstract: Causal discovery algorithms typically recover causal graphs only up to their
Markov equivalence classes unless additional parametric assumptions are made.
The sizes of these equivalence classes reflect the limits of what can be
learned about the underlying causal graph from purely observational data. Under
the assumptions of acyclicity, causal sufficiency, and a uniform model prior,
Markov equivalence classes are known to be small on average. In this paper, we
show that this is no longer the case when any of these assumptions is relaxed.
Specifically, we prove exponentially large lower bounds for the expected size
of Markov equivalence classes in three settings: sparse random directed acyclic
graphs, uniformly random acyclic directed mixed graphs, and uniformly random
directed cyclic graphs.

</details>


### [143] [Forecasting Geopolitical Events with a Sparse Temporal Fusion Transformer and Gaussian Process Hybrid: A Case Study in Middle Eastern and U.S. Conflict Dynamics](https://arxiv.org/abs/2506.20935)
*Hsin-Hsiung Huang,Hayden Hampton*

Main category: stat.ML

TL;DR: STFT-VNNGP模型通过结合TFT和VNNGP，解决了地缘政治冲突预测中的数据稀疏性和不确定性问题，显著提升了长期预测的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习模型在处理稀疏、突发和过度分散的地缘政治事件数据时，长期预测不可靠，需要一种更稳健的方法来提升预测准确性。

Method: 采用两阶段混合架构：TFT捕捉复杂时间动态生成多分位数预测，VNNGP进行时空平滑和不确定性量化。

Result: 在中东和美国的冲突动态预测中，STFT-VNNGP显著优于单独TFT，尤其在长期预测突发事件的时机和规模上表现更优。

Conclusion: STFT-VNNGP为从复杂事件数据中生成可靠且可操作的情报提供了稳健框架，所有代码和流程公开以确保可重复性。

Abstract: Forecasting geopolitical conflict from data sources like the Global Database
of Events, Language, and Tone (GDELT) is a critical challenge for national
security. The inherent sparsity, burstiness, and overdispersion of such data
cause standard deep learning models, including the Temporal Fusion Transformer
(TFT), to produce unreliable long-horizon predictions. We introduce STFT-VNNGP,
a hybrid architecture that won the 2023 Algorithms for Threat Detection (ATD)
competition by overcoming these limitations. Designed to bridge this gap, our
model employs a two-stage process: first, a TFT captures complex temporal
dynamics to generate multi-quantile forecasts. These quantiles then serve as
informed inputs for a Variational Nearest Neighbor Gaussian Process (VNNGP),
which performs principled spatiotemporal smoothing and uncertainty
quantification. In a case study forecasting conflict dynamics in the Middle
East and the U.S., STFT-VNNGP consistently outperforms a standalone TFT,
showing a superior ability to predict the timing and magnitude of bursty event
periods, particularly at long-range horizons. This work offers a robust
framework for generating more reliable and actionable intelligence from
challenging event data, with all code and workflows made publicly available to
ensure reproducibility.

</details>


### [144] [Homogenization of Multi-agent Learning Dynamics in Finite-state Markov Games](https://arxiv.org/abs/2506.21079)
*Yann Kerzreho*

Main category: stat.ML

TL;DR: 该论文提出了一种新方法，通过调整学习率和更新频率来近似多智能体在有限状态马尔可夫博弈中的学习动态，并将其收敛到一个常微分方程（ODE）。


<details>
  <summary>Details</summary>
Motivation: 研究多智能体强化学习在复杂环境中的动态学习过程，旨在提供一个可处理的确定性近似方法。

Method: 通过同时降低学习率和增加更新频率，将智能体参数视为慢变变量，受快速混合的游戏状态影响，并在一定假设下证明其收敛到ODE。

Result: 证明了调整后的学习过程可以收敛到一个ODE，该ODE能够有效近似智能体的学习动态。

Conclusion: 该方法为多智能体强化学习提供了一种有效的确定性近似框架，有助于理解和预测学习动态。

Abstract: This paper introduces a new approach for approximating the learning dynamics
of multiple reinforcement learning (RL) agents interacting in a finite-state
Markov game. The idea is to rescale the learning process by simultaneously
reducing the learning rate and increasing the update frequency, effectively
treating the agent's parameters as a slow-evolving variable influenced by the
fast-mixing game state. Under mild assumptions-ergodicity of the state process
and continuity of the updates-we prove the convergence of this rescaled process
to an ordinary differential equation (ODE). This ODE provides a tractable,
deterministic approximation of the agent's learning dynamics. An implementation
of the framework is available at\,:
https://github.com/yannKerzreho/MarkovGameApproximation

</details>


### [145] [Wild refitting for black box prediction](https://arxiv.org/abs/2506.21460)
*Martin J. Wainwright*

Main category: stat.ML

TL;DR: 本文提出了一种名为wild refitting的高效计算方法，用于估计非参数惩罚最小二乘预测的实例均方误差上界，适用于噪声异质性和多种应用场景。


<details>
  <summary>Details</summary>
Motivation: 在非参数估计中，准确评估预测误差的上界对于模型可靠性和应用至关重要。现有方法通常需要多个数据集或对预测方法的深入了解，限制了其实际应用。本文旨在提出一种仅需单数据集和黑盒预测方法的高效误差上界估计方法。

Method: wild refitting方法包括三个步骤：计算合适的残差、用预因子ρ对称化和缩放残差、以及基于当前估计重新定义和求解修正的预测问题。该方法利用Rademacher残差对称化，类似于wild bootstrap变体。

Result: 在允许噪声异质性的相对温和条件下，本文证明了wild refitting方法的高概率性能保证，表明选择合适的wild噪声尺度ρ可以提供预测误差的上界。理论分析为方法设计提供了指导，包括残差形成、噪声缩放量以及黑盒过程的局部稳定性。

Conclusion: wild refitting方法在多种问题中表现出良好的适用性，包括非刚性结构恢复、深度神经网络先验的图像恢复以及核方法的随机草图。该方法为预测误差上界提供了一种高效且通用的计算框架。

Abstract: We describe and analyze a computionally efficient refitting procedure for
computing high-probability upper bounds on the instance-wise mean-squared
prediction error of penalized nonparametric estimates based on least-squares
minimization. Requiring only a single dataset and black box access to the
prediction method, it consists of three steps: computing suitable residuals,
symmetrizing and scaling them with a pre-factor $\rho$, and using them to
define and solve a modified prediction problem recentered at the current
estimate. We refer to it as wild refitting, since it uses Rademacher residual
symmetrization as in a wild bootstrap variant. Under relatively mild conditions
allowing for noise heterogeneity, we establish a high probability guarantee on
its performance, showing that the wild refit with a suitably chosen wild noise
scale $\rho$ gives an upper bound on prediction error. This theoretical
analysis provides guidance into the design of such procedures, including how
the residuals should be formed, the amount of noise rescaling in the wild
sub-problem needed for upper bounds, and the local stability properties of the
block-box procedure. We illustrate the applicability of this procedure to
various problems, including non-rigid structure-from-motion recovery with
structured matrix penalties; plug-and-play image restoration with deep neural
network priors; and randomized sketching with kernel methods.

</details>


### [146] [Gaussian Invariant Markov Chain Monte Carlo](https://arxiv.org/abs/2506.21511)
*Michalis K. Titsias,Angelos Alexopoulos,Siran Liu,Petros Dellaportas*

Main category: stat.ML

TL;DR: 该论文提出了高斯不变采样方法，改进了传统RWM和MALA算法的统计效率，并通过控制变量降低估计方差，在高维潜在高斯模型中取得了先进成果。


<details>
  <summary>Details</summary>
Motivation: 传统随机游走Metropolis（RWM）和Metropolis调整Langevin算法（MALA）在采样效率上存在局限，作者希望通过高斯不变性提升采样效率和估计精度。

Method: 开发了高斯不变版本的RWM、MALA和二阶Hessian或流形MALA，利用泊松方程的解析解构建控制变量以减少方差。

Result: 新方法在高维潜在高斯模型中表现优异，优于多种先进方法，并提供了关于几何遍历性和最优接受率的理论分析。

Conclusion: 高斯不变采样方法显著提升了采样效率和估计精度，适用于复杂目标分布，尤其在高质量高斯模型中表现突出。

Abstract: We develop sampling methods, which consist of Gaussian invariant versions of
random walk Metropolis (RWM), Metropolis adjusted Langevin algorithm (MALA) and
second order Hessian or Manifold MALA. Unlike standard RWM and MALA we show
that Gaussian invariant sampling can lead to ergodic estimators with improved
statistical efficiency. This is due to a remarkable property of Gaussian
invariance that allows us to obtain exact analytical solutions to the Poisson
equation for Gaussian targets. These solutions can be used to construct
efficient and easy to use control variates for variance reduction of estimators
under any intractable target. We demonstrate the new samplers and estimators in
several examples, including high dimensional targets in latent Gaussian models
where we compare against several advanced methods and obtain state-of-the-art
results. We also provide theoretical results regarding geometric ergodicity,
and an optimal scaling analysis that shows the dependence of the optimal
acceptance rate on the Gaussianity of the target.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [147] [Enhancing Homophily-Heterophily Separation: Relation-Aware Learning in Heterogeneous Graphs](https://arxiv.org/abs/2506.20980)
*Ziyu Zheng,Yaming Yang,Ziyu Guan,Wei Zhao,Weigang Lu*

Main category: cs.SI

TL;DR: 该论文提出了一种名为RASH的新型对比学习框架，用于在异质图中捕捉节点异质性，通过动态构建同质和异质图来解决异质图中的异质性问题。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的网络通常具有节点异质性，即相连的节点具有不同的特征或标签。这一问题在同质图中已有广泛研究，但在异质图中仍未被充分探索。现有方法通常将异质图转换为同质图来学习节点异质性，但这样会丢失异质关系所传达的潜在异质性。

Method: RASH框架引入了双重异质超图来编码多关系二分子图，并基于关系重要性动态构建同质图和异质图。通过设计多关系对比损失，最大化异质视图与同质/异质视图之间的互信息。

Result: 在多个基准数据集上的广泛实验表明，RASH在各种下游任务中均表现出色。

Conclusion: RASH框架成功解决了异质图中的异质性和异质性问题，为相关研究提供了新的思路和方法。

Abstract: Real-world networks usually have a property of node heterophily, that is, the
connected nodes usually have different features or different labels. This
heterophily issue has been extensively studied in homogeneous graphs but
remains under-explored in heterogeneous graphs, where there are multiple types
of nodes and edges. Capturing node heterophily in heterogeneous graphs is very
challenging since both node/edge heterogeneity and node heterophily should be
carefully taken into consideration. Existing methods typically convert
heterogeneous graphs into homogeneous ones to learn node heterophily, which
will inevitably lose the potential heterophily conveyed by heterogeneous
relations. To bridge this gap, we propose Relation-Aware Separation of
Homophily and Heterophily (RASH), a novel contrastive learning framework that
explicitly models high-order semantics of heterogeneous interactions and
adaptively separates homophilic and heterophilic patterns. Particularly, RASH
introduces dual heterogeneous hypergraphs to encode multi-relational bipartite
subgraphs and dynamically constructs homophilic graphs and heterophilic graphs
based on relation importance. A multi-relation contrastive loss is designed to
align heterogeneous and homophilic/heterophilic views by maximizing mutual
information. In this way, RASH simultaneously resolves the challenges of
heterogeneity and heterophily in heterogeneous graphs. Extensive experiments on
benchmark datasets demonstrate the effectiveness of RASH across various
downstream tasks. The code is available at:
https://github.com/zhengziyu77/RASH.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [148] [Leveraging Vision-Language Models to Select Trustworthy Super-Resolution Samples Generated by Diffusion Models](https://arxiv.org/abs/2506.20832)
*Cansu Korkmaz,Ahmet Murat Tekalp,Zafer Dogan*

Main category: cs.CV

TL;DR: 该论文提出了一种利用视觉语言模型（VLMs）从扩散模型生成的多幅超分辨率图像中选择最可信样本的自动化框架，并通过新的可信度评分（TWS）验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 超分辨率（SR）是一个病态逆问题，存在多个可行解。回归模型在平衡保真度和感知质量时可能引入伪影，而扩散模型生成的多样化解中难以选择最可信的样本。

Method: 利用BLIP-2、GPT-4o等VLMs进行结构化查询评估语义正确性、视觉质量和伪影存在，并通过TWS（结合CLIP嵌入、SSIM边缘图和小波分解）量化SR可靠性。

Result: TWS与人类偏好高度相关，VLM引导的选择始终获得高TWS值，优于传统指标如PSNR和LPIPS。

Conclusion: 该工作通过语义对齐和可信度评估，为生成式SR设定了新的可信度基准，提供了可扩展的解决方案。

Abstract: Super-resolution (SR) is an ill-posed inverse problem with many feasible
solutions consistent with a given low-resolution image. On one hand, regressive
SR models aim to balance fidelity and perceptual quality to yield a single
solution, but this trade-off often introduces artifacts that create ambiguity
in information-critical applications such as recognizing digits or letters. On
the other hand, diffusion models generate a diverse set of SR images, but
selecting the most trustworthy solution from this set remains a challenge. This
paper introduces a robust, automated framework for identifying the most
trustworthy SR sample from a diffusion-generated set by leveraging the semantic
reasoning capabilities of vision-language models (VLMs). Specifically, VLMs
such as BLIP-2, GPT-4o, and their variants are prompted with structured queries
to assess semantic correctness, visual quality, and artifact presence. The
top-ranked SR candidates are then ensembled to yield a single trustworthy
output in a cost-effective manner. To rigorously assess the validity of
VLM-selected samples, we propose a novel Trustworthiness Score (TWS) a hybrid
metric that quantifies SR reliability based on three complementary components:
semantic similarity via CLIP embeddings, structural integrity using SSIM on
edge maps, and artifact sensitivity through multi-level wavelet decomposition.
We empirically show that TWS correlates strongly with human preference in both
ambiguous and natural images, and that VLM-guided selections consistently yield
high TWS values. Compared to conventional metrics like PSNR, LPIPS, which fail
to reflect information fidelity, our approach offers a principled, scalable,
and generalizable solution for navigating the uncertainty of the diffusion SR
space. By aligning outputs with human expectations and semantic correctness,
this work sets a new benchmark for trustworthiness in generative SR.

</details>


### [149] [FixCLR: Negative-Class Contrastive Learning for Semi-Supervised Domain Generalization](https://arxiv.org/abs/2506.20841)
*Ha Min Son,Shahbaz Rezaei,Xin Liu*

Main category: cs.CV

TL;DR: FixCLR是一种新的半监督域泛化方法，通过对比学习实现显式域不变性正则化，提升模型在标签稀缺情况下的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有半监督域泛化方法未显式学习跨所有域的域不变表示，导致在标签稀缺时泛化性能不足。

Method: 提出FixCLR，利用伪标签的类别信息和排斥项，调整对比学习的关键组件以实现显式域不变性正则化。

Result: FixCLR显著提升性能，尤其与其他半监督方法结合时，并在多领域数据集和预训练模型测试中表现优异。

Conclusion: FixCLR是有效的半监督域泛化方法，通过对比学习显式正则化域不变性，为现有方法提供互补性改进。

Abstract: Semi-supervised domain generalization (SSDG) aims to solve the problem of
generalizing to out-of-distribution data when only a few labels are available.
Due to label scarcity, applying domain generalization methods often
underperform. Consequently, existing SSDG methods combine semi-supervised
learning methods with various regularization terms. However, these methods do
not explicitly regularize to learn domains invariant representations across all
domains, which is a key goal for domain generalization. To address this, we
introduce FixCLR. Inspired by success in self-supervised learning, we change
two crucial components to adapt contrastive learning for explicit domain
invariance regularization: utilization of class information from pseudo-labels
and using only a repelling term. FixCLR can also be added on top of most
existing SSDG and semi-supervised methods for complementary performance
improvements. Our research includes extensive experiments that have not been
previously explored in SSDG studies. These experiments include benchmarking
different improvements to semi-supervised methods, evaluating the performance
of pretrained versus non-pretrained models, and testing on datasets with many
domains. Overall, FixCLR proves to be an effective SSDG method, especially when
combined with other semi-supervised methods.

</details>


### [150] [THIRDEYE: Cue-Aware Monocular Depth Estimation via Brain-Inspired Multi-Stage Fusion](https://arxiv.org/abs/2506.20877)
*Calin Teodor Ioan*

Main category: cs.CV

TL;DR: ThirdEye提出了一种新的单目深度估计方法，通过显式引入人类视觉依赖的线索（如遮挡边界、阴影和透视）并利用预训练网络融合这些线索，提高了深度估计的准确性和分辨率。


<details>
  <summary>Details</summary>
Motivation: 传统的单目深度估计方法通常通过深度模型直接从RGB像素中推断深度，忽视了人类视觉系统依赖的显式线索（如遮挡边界、阴影和透视）。ThirdEye旨在通过显式引入这些线索，提升深度估计的性能。

Method: ThirdEye采用了一种线索感知的流程，通过专门的预训练和冻结网络提供每种线索，并在一个三阶段皮层层次结构（V1->V2->V3）中融合这些线索，使用键值工作记忆模块根据可靠性加权。最后，自适应分箱变换器头生成高分辨率视差图。

Result: 由于线索专家网络被冻结，ThirdEye继承了大量的外部监督，仅需少量微调。该方法在深度估计任务中表现出色，具体定量结果将在未来版本中公布。

Conclusion: ThirdEye通过显式引入和融合人类视觉线索，显著提升了单目深度估计的性能，同时减少了训练开销。

Abstract: Monocular depth estimation methods traditionally train deep models to infer
depth directly from RGB pixels. This implicit learning often overlooks explicit
monocular cues that the human visual system relies on, such as occlusion
boundaries, shading, and perspective. Rather than expecting a network to
discover these cues unaided, we present ThirdEye, a cue-aware pipeline that
deliberately supplies each cue through specialised, pre-trained, and frozen
networks. These cues are fused in a three-stage cortical hierarchy (V1->V2->V3)
equipped with a key-value working-memory module that weights them by
reliability. An adaptive-bins transformer head then produces a high-resolution
disparity map. Because the cue experts are frozen, ThirdEye inherits large
amounts of external supervision while requiring only modest fine-tuning. This
extended version provides additional architectural detail, neuroscientific
motivation, and an expanded experimental protocol; quantitative results will
appear in a future revision.

</details>


### [151] [HumanOmniV2: From Understanding to Omni-Modal Reasoning with Context](https://arxiv.org/abs/2506.21277)
*Qize Yang,Shimin Yao,Weixuan Chen,Shenghao Fu,Detao Bai,Jiaxing Zhao,Boyuan Sun,Bowen Yin,Xihan Wei,Jingren Zhou*

Main category: cs.CV

TL;DR: 该论文针对多模态大型语言模型在理解人类意图时存在的全局上下文理解不足和捷径问题，提出了一种结合强化学习的方法，通过引入上下文奖励、格式奖励、准确性奖励和逻辑奖励来提升模型的推理能力，并在多模态基准测试中展示了优越性能。


<details>
  <summary>Details</summary>
Motivation: 随着多模态大型语言模型的快速发展，深入理解和解释人类意图的能力变得至关重要。然而，现有模型在适应多模态数据和格式时仍面临全局上下文理解不足和捷径问题，导致错误答案或忽略关键信息。

Method: 论文提出了一种方法，通过强化学习增强多模态推理能力，具体包括：1) 使用大型语言模型评估上下文奖励；2) 引入格式和准确性奖励；3) 通过逻辑奖励确保推理过程整合多模态信息与逻辑方法。此外，还提出了一个新的多模态基准测试IntentBench。

Result: 与其他开源多模态模型相比，该方法在多个全模态基准测试中表现出先进的性能，能够更准确地理解复杂的人类意图和情感。

Conclusion: 论文通过强化学习和多模态奖励机制，有效解决了多模态推理中的上下文理解不足和捷径问题，提升了模型的全局推理能力，并在基准测试中验证了其优越性。

Abstract: With the rapid evolution of multimodal large language models, the capacity to
deeply understand and interpret human intentions has emerged as a critical
capability, which demands detailed and thoughtful reasoning. In recent studies,
Reinforcement Learning (RL) has demonstrated potential in enhancing the
reasoning capabilities of Large Language Models (LLMs). Nonetheless, the
challenges associated with adapting RL to multimodal data and formats remain
largely unaddressed. In this paper, we identify two issues in existing
multimodal reasoning models: insufficient global context understanding and
shortcut problems. Insufficient context understanding can happen when a model
misinterprets multimodal context, resulting in incorrect answers. The shortcut
problem occurs when the model overlooks crucial clues in multimodal inputs,
directly addressing the query without considering the multimodal information.
To tackle these issues, we emphasize the necessity for the model to reason with
a clear understanding of the global context within multimodal inputs. This
global context understanding can effectively prevent the model from overlooking
key multimodal cues and ensure a thorough reasoning process. To ensure the
accurate interpretation of multimodal context information, we implement a
context reward judged by a large language model, alongside format and accuracy
rewards. Additionally, to improve complex reasoning capability, we employ the
LLM to assess the logical reward, determining whether the reasoning process
successfully integrates multimodal information with logical methods. We also
introduce a reasoning omni-modal benchmark, IntentBench, aimed at evaluating
models in understanding complex human intentions and emotions. Our proposed
method demonstrates advanced performance across multiple omni-modal benchmarks
compared to other open-source omni-modal models.

</details>


### [152] [OmniEval: A Benchmark for Evaluating Omni-modal Models with Visual, Auditory, and Textual Inputs](https://arxiv.org/abs/2506.20960)
*Yiman Zhang,Ziheng Luo,Qiangyu Yan,Wei He,Borui Jiang,Xinghao Chen,Kai Han*

Main category: cs.CV

TL;DR: OmniEval是一个用于评估全模态模型（如MiniCPM-O 2.6）的基准测试，涵盖视觉、听觉和文本输入，具有全模态协作、多样化的视频和任务特点。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试无法全面评估全模态模型的协作感知能力，因此需要一个新的基准测试来填补这一空白。

Method: 设计了包含810个音视频同步视频和2617个问答对的OmniEval基准测试，涵盖3大任务类型和12个子任务类型，特别引入了更细粒度的视频定位任务Grounding。

Result: 在OmniEval上对多个全模态模型进行了实验，验证了基准测试的有效性。

Conclusion: OmniEval为评估全模态模型的上下文构建和理解能力提供了一个全面的平台，代码和数据已开源。

Abstract: In this paper, we introduce OmniEval, a benchmark for evaluating
omni-modality models like MiniCPM-O 2.6, which encompasses visual, auditory,
and textual inputs. Compared with existing benchmarks, our OmniEval has several
distinctive features: (i) Full-modal collaboration: We design evaluation tasks
that highlight the strong coupling between audio and video, requiring models to
effectively leverage the collaborative perception of all modalities; (ii)
Diversity of videos: OmniEval includes 810 audio-visual synchronized videos,
285 Chinese videos and 525 English videos; (iii) Diversity and granularity of
tasks: OmniEval contains 2617 question-answer pairs, comprising 1412 open-ended
questions and 1205 multiple-choice questions. These questions are divided into
3 major task types and 12 sub-task types to achieve comprehensive evaluation.
Among them, we introduce a more granular video localization task named
Grounding. Then we conduct experiments on OmniEval with several omni-modality
models. We hope that our OmniEval can provide a platform for evaluating the
ability to construct and understand coherence from the context of all
modalities. Codes and data could be found at https://omnieval.github.io/.

</details>


### [153] [Evidence-based diagnostic reasoning with multi-agent copilot for human pathology](https://arxiv.org/abs/2506.20964)
*Chengkuan Chen,Luca L. Weishaupt,Drew F. K. Williamson,Richard J. Chen,Tong Ding,Bowen Chen,Anurag Vaidya,Long Phi Le,Guillaume Jaume,Ming Y. Lu,Faisal Mahmood*

Main category: cs.CV

TL;DR: PathChat+是一种新型多模态大语言模型，专为病理学设计，通过大量病理学特定指令样本训练，显著优于现有模型，并结合SlideSeek系统实现高精度自主诊断。


<details>
  <summary>Details</summary>
Motivation: 当前计算病理学中的多模态大语言模型存在训练数据不足、多图像理解支持不足及缺乏自主诊断推理能力等问题，需要开发更先进的模型来解决这些限制。

Method: 提出PathChat+模型，基于超过100万病理学特定指令样本和550万问答对训练，并结合SlideSeek系统进行迭代式分层诊断推理。

Result: PathChat+在多个病理学基准测试中显著优于现有模型，SlideSeek系统在DDxBench上达到高精度，并能生成易于理解的总结报告。

Conclusion: PathChat+和SlideSeek系统为计算病理学提供了强大的多模态理解和自主诊断能力，推动了病理学数字化进程。

Abstract: Pathology is experiencing rapid digital transformation driven by whole-slide
imaging and artificial intelligence (AI). While deep learning-based
computational pathology has achieved notable success, traditional models
primarily focus on image analysis without integrating natural language
instruction or rich, text-based context. Current multimodal large language
models (MLLMs) in computational pathology face limitations, including
insufficient training data, inadequate support and evaluation for multi-image
understanding, and a lack of autonomous, diagnostic reasoning capabilities. To
address these limitations, we introduce PathChat+, a new MLLM specifically
designed for human pathology, trained on over 1 million diverse,
pathology-specific instruction samples and nearly 5.5 million question answer
turns. Extensive evaluations across diverse pathology benchmarks demonstrated
that PathChat+ substantially outperforms the prior PathChat copilot, as well as
both state-of-the-art (SOTA) general-purpose and other pathology-specific
models. Furthermore, we present SlideSeek, a reasoning-enabled multi-agent AI
system leveraging PathChat+ to autonomously evaluate gigapixel whole-slide
images (WSIs) through iterative, hierarchical diagnostic reasoning, reaching
high accuracy on DDxBench, a challenging open-ended differential diagnosis
benchmark, while also capable of generating visually grounded,
humanly-interpretable summary reports.

</details>


### [154] [Logios : An open source Greek Polytonic Optical Character Recognition system](https://arxiv.org/abs/2506.21474)
*Perifanos Konstantinos,Goutsos Dionisis*

Main category: cs.CV

TL;DR: 提出了一种针对希腊多调文字的光学字符识别系统，结合卷积和循环神经网络提升准确率。


<details>
  <summary>Details</summary>
Motivation: 传统OCR方法在处理希腊多调文字时存在局限性，需要更高效的解决方案。

Method: 结合卷积层进行特征提取和循环层进行序列学习。

Result: 系统在准确率和效率上有显著提升，并开源模型供学术使用。

Conclusion: 该系统有效解决了希腊多调文字的识别问题，具有实际应用价值。

Abstract: In this paper, we present an Optical Character Recognition (OCR) system
specifically designed for the accurate recognition and digitization of Greek
polytonic texts. By leveraging the combined strengths of convolutional layers
for feature extraction and recurrent layers for sequence learning, our system
addresses the unique challenges posed by Greek polytonic scripts. This approach
aims to overcome the limitations of traditional OCR methods, offering
significant improvements in accuracy and efficiency. We release the underlying
model as an open-source library and make our OCR platform available for
academic use.

</details>


### [155] [DFVEdit: Conditional Delta Flow Vector for Zero-shot Video Editing](https://arxiv.org/abs/2506.20967)
*Lingling Cai,Kang Zhao,Hangjie Yuan,Xiang Wang,Yingya Zhang,Kejie Huang*

Main category: cs.CV

TL;DR: DFVEdit是一种针对视频扩散变换器（Video DiTs）的高效零样本视频编辑方法，通过流变换直接在干净潜在空间操作，避免了计算开销大的注意力修改或微调，显著提升了推理速度和内存效率。


<details>
  <summary>Details</summary>
Motivation: 现有视频编辑方法直接应用于Video DiTs时，由于资源密集的注意力修改或微调，会产生大量计算开销。DFVEdit旨在解决这一问题，提供高效的视频编辑方案。

Method: DFVEdit通过流变换直接在干净潜在空间操作，提出条件增量流向量（CDFV）作为理论无偏估计，并结合隐式交叉注意力（ICA）指导和嵌入增强（ER）来提升编辑质量。

Result: DFVEdit在Video DiTs上实现了至少20倍的推理加速和85%的内存减少，并在结构保真度、时空一致性和编辑质量上达到最先进水平。

Conclusion: DFVEdit是一种高效、高质量的零样本视频编辑方法，适用于主流Video DiTs，显著提升了计算效率和编辑性能。

Abstract: The advent of Video Diffusion Transformers (Video DiTs) marks a milestone in
video generation. However, directly applying existing video editing methods to
Video DiTs often incurs substantial computational overhead, due to
resource-intensive attention modification or finetuning. To alleviate this
problem, we present DFVEdit, an efficient zero-shot video editing method
tailored for Video DiTs. DFVEdit eliminates the need for both attention
modification and fine-tuning by directly operating on clean latents via flow
transformation. To be more specific, we observe that editing and sampling can
be unified under the continuous flow perspective. Building upon this
foundation, we propose the Conditional Delta Flow Vector (CDFV) -- a
theoretically unbiased estimation of DFV -- and integrate Implicit Cross
Attention (ICA) guidance as well as Embedding Reinforcement (ER) to further
enhance editing quality. DFVEdit excels in practical efficiency, offering at
least 20x inference speed-up and 85\% memory reduction on Video DiTs compared
to attention-engineering-based editing methods. Extensive quantitative and
qualitative experiments demonstrate that DFVEdit can be seamlessly applied to
popular Video DiTs (e.g., CogVideoX and Wan2.1), attaining state-of-the-art
performance on structural fidelity, spatial-temporal consistency, and editing
quality.

</details>


### [156] [From Cradle to Cane: A Two-Pass Framework for High-Fidelity Lifespan Face Aging](https://arxiv.org/abs/2506.20977)
*Tao Liu,Dafeng Zhang,Gengchen Li,Shizhuo Liu,Yongqi Song,Senmao Li,Shiqi Yang,Boqian Li,Kai Wang,Yaxing Wang*

Main category: cs.CV

TL;DR: 该论文提出了一种名为Cradle2Cane的两阶段人脸老化框架，通过自适应噪声注入和身份感知嵌入技术，解决了年龄准确性和身份保持之间的平衡问题。


<details>
  <summary>Details</summary>
Motivation: 现有的人脸老化方法在实现真实且无缝的年龄转换时，难以平衡年龄准确性和身份一致性，尤其是在处理大年龄跨度或极端头部姿势时。

Method: 论文提出了一种两阶段框架：第一阶段通过自适应噪声注入（AdaNI）机制实现年龄准确性；第二阶段通过两种身份感知嵌入（IDEmb）增强身份保持。

Result: 在CelebA-HQ测试数据集上的实验表明，Cradle2Cane在年龄准确性和身份一致性方面优于现有方法。

Conclusion: Cradle2Cane通过两阶段方法有效解决了人脸老化中的Age-ID权衡问题，实现了更高的年龄准确性和身份一致性。

Abstract: Face aging has become a crucial task in computer vision, with applications
ranging from entertainment to healthcare. However, existing methods struggle
with achieving a realistic and seamless transformation across the entire
lifespan, especially when handling large age gaps or extreme head poses. The
core challenge lies in balancing age accuracy and identity preservation--what
we refer to as the Age-ID trade-off. Most prior methods either prioritize age
transformation at the expense of identity consistency or vice versa. In this
work, we address this issue by proposing a two-pass face aging framework, named
Cradle2Cane, based on few-step text-to-image (T2I) diffusion models. The first
pass focuses on solving age accuracy by introducing an adaptive noise injection
(AdaNI) mechanism. This mechanism is guided by including prompt descriptions of
age and gender for the given person as the textual condition. Also, by
adjusting the noise level, we can control the strength of aging while allowing
more flexibility in transforming the face. However, identity preservation is
weakly ensured here to facilitate stronger age transformations. In the second
pass, we enhance identity preservation while maintaining age-specific features
by conditioning the model on two identity-aware embeddings (IDEmb): SVR-ArcFace
and Rotate-CLIP. This pass allows for denoising the transformed image from the
first pass, ensuring stronger identity preservation without compromising the
aging accuracy. Both passes are jointly trained in an end-to-end way. Extensive
experiments on the CelebA-HQ test dataset, evaluated through Face++ and Qwen-VL
protocols, show that our Cradle2Cane outperforms existing face aging methods in
age accuracy and identity consistency.

</details>


### [157] [HalluSegBench: Counterfactual Visual Reasoning for Segmentation Hallucination Evaluation](https://arxiv.org/abs/2506.21546)
*Xinzhuo Li,Adheesh Juvekar,Xingyou Liu,Muntasir Wahed,Kiet A. Nguyen,Ismini Lourentzou*

Main category: cs.CV

TL;DR: 论文提出了首个针对视觉语言分割模型幻觉问题的基准测试HalluSegBench，通过反事实视觉推理评估模型在视觉基础中的幻觉现象。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉语言分割模型常产生与图像内容无关的分割掩码或错误标记区域，但现有评估协议主要关注标签或文本幻觉，缺乏对视觉上下文的操控，难以诊断关键失败。

Method: 引入HalluSegBench基准，包含1340个反事实实例对和281个独特对象类别的新数据集，并提出新指标量化视觉连贯场景编辑下的幻觉敏感性。

Result: 实验表明，视觉驱动的幻觉比标签驱动的更普遍，模型常坚持错误分割，凸显反事实推理对诊断基础保真度的重要性。

Conclusion: HalluSegBench揭示了视觉语言分割模型中的幻觉问题，强调需要通过反事实推理提升模型的视觉基础能力。

Abstract: Recent progress in vision-language segmentation has significantly advanced
grounded visual understanding. However, these models often exhibit
hallucinations by producing segmentation masks for objects not grounded in the
image content or by incorrectly labeling irrelevant regions. Existing
evaluation protocols for segmentation hallucination primarily focus on label or
textual hallucinations without manipulating the visual context, limiting their
capacity to diagnose critical failures. In response, we introduce
HalluSegBench, the first benchmark specifically designed to evaluate
hallucinations in visual grounding through the lens of counterfactual visual
reasoning. Our benchmark consists of a novel dataset of 1340 counterfactual
instance pairs spanning 281 unique object classes, and a set of newly
introduced metrics that quantify hallucination sensitivity under visually
coherent scene edits. Experiments on HalluSegBench with state-of-the-art
vision-language segmentation models reveal that vision-driven hallucinations
are significantly more prevalent than label-driven ones, with models often
persisting in false segmentation, highlighting the need for counterfactual
reasoning to diagnose grounding fidelity.

</details>


### [158] [Segment Anything in Pathology Images with Natural Language](https://arxiv.org/abs/2506.20988)
*Zhixuan Chen,Junlin Hou,Liqi Lin,Yihui Wang,Yequan Bie,Xi Wang,Yanning Zhou,Ronald Cheong Kin Chan,Hao Chen*

Main category: cs.CV

TL;DR: 提出首个基于文本提示的病理图像分割基础模型PathSegmentor，并构建最大病理分割数据集PathSeg，显著提升分割精度和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有病理图像分割方法因标注数据有限和类别定义狭窄难以满足临床需求，需开发更灵活、通用的解决方案。

Method: 构建包含275k样本的PathSeg数据集，设计支持自然语言提示的PathSegmentor模型，无需空间标注输入即可实现语义分割。

Result: 模型Dice分数超越现有空间/文本提示方法0.145和0.429，在复杂结构分割和外部数据集泛化中展现强鲁棒性，并能提升诊断模型可解释性。

Conclusion: PathSegmentor推动了精准肿瘤学可解释AI的发展，为临床决策提供基于证据的病理分析支持。

Abstract: Pathology image segmentation is crucial in computational pathology for
analyzing histological features relevant to cancer diagnosis and prognosis.
However, current methods face major challenges in clinical applications due to
limited annotated data and restricted category definitions. To address these
limitations, we propose PathSegmentor, the first text-prompted segmentation
foundation model designed specifically for pathology images. We also introduce
PathSeg , the largest and most comprehensive dataset for pathology
segmentation, built from 17 public sources and containing 275k image-mask-label
triples across 160 diverse categories. With PathSegmentor, users can perform
semantic segmentation using natural language prompts, eliminating the need for
laborious spatial inputs such as points or boxes. Extensive experiments
demonstrate that PathSegmentor outperforms specialized models with higher
accuracy and broader applicability, while maintaining a compact architecture.
It significantly surpasses existing spatial- and text-prompted models by 0.145
and 0.429 in overall Dice scores, respectively, showing strong robustness in
segmenting complex structures and generalizing to external datasets. Moreover,
PathSegmentor's outputs enhance the interpretability of diagnostic models
through feature importance estimation and imaging biomarker discovery, offering
pathologists evidence-based support for clinical decision-making. This work
advances the development of explainable AI in precision oncology.

</details>


### [159] [Multimodal Prompt Alignment for Facial Expression Recognition](https://arxiv.org/abs/2506.21017)
*Fuyan Ma,Yiran He,Bin Sun,Shutao Li*

Main category: cs.CV

TL;DR: 提出MPA-FER框架，通过多模态提示对齐提升视觉语言模型在面部表情识别中的细粒度语义理解能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于视觉语言模型的面部表情识别方法难以捕捉文本与视觉间的细粒度关系，影响对细微表情差异的区分。

Method: 结合LLM生成细粒度硬提示，通过特征差异最小化注入软提示；采用原型引导的视觉特征对齐和跨模态全局-局部对齐模块。

Result: 在三个基准数据集上超越现有方法，同时保留预训练模型优势并降低计算成本。

Conclusion: MPA-FER框架通过多粒度提示对齐实现了更精准、可解释的面部表情表征。

Abstract: Prompt learning has been widely adopted to efficiently adapt vision-language
models (VLMs) like CLIP for various downstream tasks. Despite their success,
current VLM-based facial expression recognition (FER) methods struggle to
capture fine-grained textual-visual relationships, which are essential for
distinguishing subtle differences between facial expressions. To address this
challenge, we propose a multimodal prompt alignment framework for FER, called
MPA-FER, that provides fine-grained semantic guidance to the learning process
of prompted visual features, resulting in more precise and interpretable
representations. Specifically, we introduce a multi-granularity hard prompt
generation strategy that utilizes a large language model (LLM) like ChatGPT to
generate detailed descriptions for each facial expression. The LLM-based
external knowledge is injected into the soft prompts by minimizing the feature
discrepancy between the soft prompts and the hard prompts. To preserve the
generalization abilities of the pretrained CLIP model, our approach
incorporates prototype-guided visual feature alignment, ensuring that the
prompted visual features from the frozen image encoder align closely with
class-specific prototypes. Additionally, we propose a cross-modal global-local
alignment module that focuses on expression-relevant facial features, further
improving the alignment between textual and visual features. Extensive
experiments demonstrate our framework outperforms state-of-the-art methods on
three FER benchmark datasets, while retaining the benefits of the pretrained
model and minimizing computational costs.

</details>


### [160] [Improving Diffusion-Based Image Editing Faithfulness via Guidance and Scheduling](https://arxiv.org/abs/2506.21045)
*Hansam Cho,Seoung Bum Kim*

Main category: cs.CV

TL;DR: 提出FGS方法，通过忠实性引导和调度策略，在保持图像编辑灵活性的同时提升忠实性。


<details>
  <summary>Details</summary>
Motivation: 文本引导扩散模型在图像编辑中存在编辑灵活性与忠实性之间的权衡问题，需要一种方法在两者间取得平衡。

Method: 提出FGS（Faithfulness Guidance and Scheduling），包含忠实性引导以增强输入图像信息的保留，并引入调度策略解决编辑灵活性与忠实性之间的不对齐问题。

Result: 实验表明，FGS在保持编辑灵活性的同时实现了更高的忠实性，且兼容多种编辑方法，适用于多样化的图像编辑任务。

Conclusion: FGS是一种有效的图像编辑方法，能够在编辑灵活性和忠实性之间取得良好平衡，适用于多种编辑场景。

Abstract: Text-guided diffusion models have become essential for high-quality image
synthesis, enabling dynamic image editing. In image editing, two crucial
aspects are editability, which determines the extent of modification, and
faithfulness, which reflects how well unaltered elements are preserved.
However, achieving optimal results is challenging because of the inherent
trade-off between editability and faithfulness. To address this, we propose
Faithfulness Guidance and Scheduling (FGS), which enhances faithfulness with
minimal impact on editability. FGS incorporates faithfulness guidance to
strengthen the preservation of input image information and introduces a
scheduling strategy to resolve misalignment between editability and
faithfulness. Experimental results demonstrate that FGS achieves superior
faithfulness while maintaining editability. Moreover, its compatibility with
various editing methods enables precise, high-quality image edits across
diverse tasks.

</details>


### [161] [EgoAdapt: Adaptive Multisensory Distillation and Policy Learning for Efficient Egocentric Perception](https://arxiv.org/abs/2506.21080)
*Sanjoy Chowdhury,Subrata Biswas,Sayan Nag,Tushar Nagarajan,Calvin Murdock,Ishwarya Ananthabhotla,Yijun Qian,Vamsi Krishna Ithapu,Dinesh Manocha,Ruohan Gao*

Main category: cs.CV

TL;DR: 提出EgoAdapt框架，通过跨模态蒸馏和策略学习实现高效的第一人称感知任务推理，显著降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 当前多模态第一人称感知模型计算成本高，难以在资源受限环境中部署，需提升效率。

Method: 采用自适应跨模态蒸馏和策略学习框架，适配不同任务的动作空间。

Result: 在三个数据集上，计算量降低89.09%，参数量减少82.02%，能效提升9.6倍，性能持平或超越SOTA。

Conclusion: EgoAdapt在保持性能的同时大幅提升效率，适用于资源受限场景的第一人称感知任务。

Abstract: Modern perception models, particularly those designed for multisensory
egocentric tasks, have achieved remarkable performance but often come with
substantial computational costs. These high demands pose challenges for
real-world deployment, especially in resource-constrained environments. In this
paper, we introduce EgoAdapt, a framework that adaptively performs cross-modal
distillation and policy learning to enable efficient inference across different
egocentric perception tasks, including egocentric action recognition, active
speaker localization, and behavior anticipation. Our proposed policy module is
adaptable to task-specific action spaces, making it broadly applicable.
Experimental results on three challenging egocentric datasets EPIC-Kitchens,
EasyCom, and Aria Everyday Activities demonstrate that our method significantly
enhances efficiency, reducing GMACs by up to 89.09%, parameters up to 82.02%,
and energy up to 9.6x, while still on-par and in many cases outperforming, the
performance of corresponding state-of-the-art models.

</details>


### [162] [IPFormer-VideoLLM: Enhancing Multi-modal Video Understanding for Multi-shot Scenes](https://arxiv.org/abs/2506.21116)
*Yujia Liang,Jile Jiao,Zhicheng Wang,Xuetao Feng,Zixuan Ye,Yuan Wang,Hao Lu*

Main category: cs.CV

TL;DR: 视频大语言模型在多镜头场景中存在性能瓶颈，本文提出新数据集MultiClip-Bench和模型IPFormer-VideoLLM以提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有视频大语言模型在多镜头场景（如不同摄像机角度或场景变化）中表现不佳，主要由于缺乏多镜头标注数据和实例特征编码不充分。

Method: 引入多镜头标注数据集MultiClip-Bench，并提出IPFormer-VideoLLM模型，通过基于注意力的连接器注入实例级特征作为提示。

Result: 实验表明，新数据集和模型显著提升了多场景视频理解能力，并在多个视频基准测试中表现优异。

Conclusion: 本文的数据集和模型有效解决了多镜头场景中的视频理解问题，为未来研究提供了可靠基准。

Abstract: Video Large Language Models (VideoLLMs) have demonstrated remarkable
understanding capabilities, but are found struggling to tackle multi-shot
scenarios,e.g., video clips with varying camera angles or scene changes. This
challenge can render failures such as instance identity forgetting and key
frame negligence. In this work, we first attribute the challenge to the lack of
multi-shot annotations among existing datasets and therefore we introduce a new
dataset termed MultiClip-Bench, featuring dense descriptions and
instruction-based question-answering pairs tailored for multi-shot scenarios.
We empirically find that the training set significantly boosts the multi-shot
performance, while the testing benchmark provides a reliable measure of the
model capability in multi-shot scenarios. By further analyzing and discovering
that current models only encode instance features in a discrete or lossy
manner, at the risk of missing identity information, we then contribute a new
model IPFormer-VideoLLM. Its key idea is the injection of instance-level
features as instance prompts through an efficient attention-based connector.
This allows for the aggregation of instance-specific information across scenes.
Experiments demonstrate that our proposed dataset and model not only enhance
the multi-scene video understanding significantly, but also offer distinct
advantages across various video benchmarks.

</details>


### [163] [Robust Deep Learning for Myocardial Scar Segmentation in Cardiac MRI with Noisy Labels](https://arxiv.org/abs/2506.21151)
*Aida Moafi,Danial Moafi,Evgeny M. Mirkes,Gerry P. McCann,Abbas S. Alatrany,Jayanth R. Arnold,Mostafa Mehdipour Ghazi*

Main category: cs.CV

TL;DR: 该研究提出了一种基于深度学习的自动化心肌瘢痕分割方法，通过优化模型和数据处理技术，显著提升了分割准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 心肌瘢痕的准确分割对临床评估和治疗计划至关重要，但现有方法面临标签噪声、数据异构性和类别不平衡等挑战。

Method: 研究通过微调先进模型，结合Kullback-Leibler损失函数和大量数据增强技术，解决了标签噪声和数据异质性问题。

Result: 该方法在急性和慢性病例中均表现出色，优于当前最先进的nnU-Net模型，并在分布外测试集上展示了强大的泛化能力。

Conclusion: 该研究为自动化心肌瘢痕量化提供了可靠基础，支持深度学习在心脏影像中的更广泛应用。

Abstract: The accurate segmentation of myocardial scars from cardiac MRI is essential
for clinical assessment and treatment planning. In this study, we propose a
robust deep-learning pipeline for fully automated myocardial scar detection and
segmentation by fine-tuning state-of-the-art models. The method explicitly
addresses challenges of label noise from semi-automatic annotations, data
heterogeneity, and class imbalance through the use of Kullback-Leibler loss and
extensive data augmentation. We evaluate the model's performance on both acute
and chronic cases and demonstrate its ability to produce accurate and smooth
segmentations despite noisy labels. In particular, our approach outperforms
state-of-the-art models like nnU-Net and shows strong generalizability in an
out-of-distribution test set, highlighting its robustness across various
imaging conditions and clinical tasks. These results establish a reliable
foundation for automated myocardial scar quantification and support the broader
clinical adoption of deep learning in cardiac imaging.

</details>


### [164] [Transferring disentangled representations: bridging the gap between synthetic and real images](https://arxiv.org/abs/2409.18017)
*Jacopo Dapueto,Nicoletta Noceti,Francesca Odone*

Main category: cs.CV

TL;DR: 该论文研究了如何利用合成数据学习可迁移到真实数据的解耦表示，并提出了新的评估指标。


<details>
  <summary>Details</summary>
Motivation: 解耦表示学习在真实图像上尚未充分发挥潜力，主要由于生成因素的相关性、分辨率限制及缺乏真实标签。

Method: 通过合成数据学习通用解耦表示，分析微调效果及解耦特性在迁移中的保留情况，并提出基于干预的可解释评估指标。

Result: 研究表明，从合成数据到真实数据的表示迁移具有一定效果，并能保持部分解耦特性。

Conclusion: 合成数据可用于学习适用于真实数据的解耦表示，且迁移过程有效。

Abstract: Developing meaningful and efficient representations that separate the
fundamental structure of the data generation mechanism is crucial in
representation learning. However, Disentangled Representation Learning has not
fully shown its potential on real images, because of correlated generative
factors, their resolution and limited access to ground truth labels.
Specifically on the latter, we investigate the possibility of leveraging
synthetic data to learn general-purpose disentangled representations applicable
to real data, discussing the effect of fine-tuning and what properties of
disentanglement are preserved after the transfer. We provide an extensive
empirical study to address these issues. In addition, we propose a new
interpretable intervention-based metric, to measure the quality of factors
encoding in the representation. Our results indicate that some level of
disentanglement, transferring a representation from synthetic to real data, is
possible and effective.

</details>


### [165] [Task-Aware KV Compression For Cost-Effective Long Video Understanding](https://arxiv.org/abs/2506.21184)
*Minghao Qin,Yan Shu,Peitian Zhang,Kun Lun,Huaying Yuan,Juenjie Zhou,Shitao Xiao,Bo Zhao,Zheng Liu*

Main category: cs.CV

TL;DR: Video-X^2L通过双层KV压缩和选择性KV重加载技术，有效解决了长视频理解中的计算成本问题，同时减少了信息损失。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态大语言模型（MLLMs）在处理长视频理解（LVU）任务时，由于高昂的计算成本而面临挑战。现有的KV压缩方法在高压缩比下往往导致显著的信息损失。

Method: Video-X^2L采用双层KV压缩（生成低压缩和高压缩的KV）和选择性KV重加载（在解码阶段选择性加载关键视频块的KV），无需额外训练且兼容现有MLLMs。

Result: 在多个LVU基准测试（如VideoMME、MLVU等）中，Video-X^2L显著优于现有KV压缩方法，同时大幅节省计算成本。

Conclusion: Video-X^2L是一种简单有效的方法，能够在保持视频信息完整性的同时，显著降低计算成本，适用于长视频理解任务。

Abstract: Long-video understanding (LVU) remains a severe challenge for existing
multimodal large language models (MLLMs), primarily due to the prohibitive
computational cost. Recent approaches have explored KV compression to mitigate
this issue, but they often suffer from significant information loss at high
compression ratios. In this paper, we introduce Video-X^2L, which flexibly
preserves critical video information for each LVU task. Video-X^2L involves two
key operations. The first one is called bi-level KV compression. During the
MLLM's pre-filling stage, Video-X^2L generates two types of compressed KVs:
low-compression KVs (L-KVs) to capture fine-grained video details and
high-compression KVs (H-KVs) to offer compact video representations. The second
one is called selective KV re-loading. During the MLLM's decoding stage,
Video-X^2L selectively re-loads L-KVs for the most critical video chunks while
using H-KVs for other less important ones. This allows the MLLM to fully
utilize task-specific information while maintaining the overall compactness.
Video-X^2L is simple yet effective: it is free from additional training and
directly compatible with existing KV-compressible MLLMs. We evaluate Video-X^2L
with a variety of popular LVU benchmarks, including VideoMME, MLVU,
LongVideoBench, and VNBench. Our experiment result shows that Video-X^2L
outperforms existing KV-compression methods by a huge advantage while
substantially saving the computation cost.

</details>


### [166] [BitMark for Infinity: Watermarking Bitwise Autoregressive Image Generative Models](https://arxiv.org/abs/2506.21209)
*Louis Kerner,Michel Meintz,Bihe Zhao,Franziska Boenisch,Adam Dziedzic*

Main category: cs.CV

TL;DR: 本文提出BitMark，一种针对Infinity文本到图像模型的鲁棒位级水印框架，旨在防止模型因使用生成内容训练而导致的性能退化问题。


<details>
  <summary>Details</summary>
Motivation: 随着文本到图像模型生成的图像在互联网上大量传播，这些图像可能被重新用作训练数据，导致模型性能逐渐退化（模型崩溃）。水印技术可以识别生成内容，从而缓解这一问题。

Method: BitMark通过在Infinity图像生成过程中，在多个尺度（分辨率）的令牌流中嵌入位级水印，既保持视觉保真度和生成速度，又对多种去除技术具有鲁棒性。

Result: BitMark不仅能够有效嵌入水印，还具有高放射性，即使用带水印的图像训练其他生成模型时，新模型的输出也会携带水印。即使对扩散模型或自回归模型进行微调，水印痕迹仍可检测。

Conclusion: BitMark为图像生成模型提供了一种防止模型崩溃的可行方案，通过可靠检测生成内容，为模型可持续发展迈出了重要一步。

Abstract: State-of-the-art text-to-image models like Infinity generate photorealistic
images at an unprecedented speed. These models operate in a bitwise
autoregressive manner over a discrete set of tokens that is practically
infinite in size. However, their impressive generative power comes with a
growing risk: as their outputs increasingly populate the Internet, they are
likely to be scraped and reused as training data-potentially by the very same
models. This phenomenon has been shown to lead to model collapse, where
repeated training on generated content, especially from the models' own
previous versions, causes a gradual degradation in performance. A promising
mitigation strategy is watermarking, which embeds human-imperceptible yet
detectable signals into generated images-enabling the identification of
generated content. In this work, we introduce BitMark, a robust bitwise
watermarking framework for Infinity. Our method embeds a watermark directly at
the bit level of the token stream across multiple scales (also referred to as
resolutions) during Infinity's image generation process. Our bitwise watermark
subtly influences the bits to preserve visual fidelity and generation speed
while remaining robust against a spectrum of removal techniques. Furthermore,
it exhibits high radioactivity, i.e., when watermarked generated images are
used to train another image generative model, this second model's outputs will
also carry the watermark. The radioactive traces remain detectable even when
only fine-tuning diffusion or image autoregressive models on images watermarked
with our BitMark. Overall, our approach provides a principled step toward
preventing model collapse in image generative models by enabling reliable
detection of generated outputs.

</details>


### [167] [Holistic Surgical Phase Recognition with Hierarchical Input Dependent State Space Models](https://arxiv.org/abs/2506.21330)
*Haoyang Wu,Tsun-Hsuan Wang,Mathias Lechner,Ramin Hasani,Jennifer A. Eckhoff,Paul Pak,Ozanan R. Meireles,Guy Rosman,Yutong Ban,Daniela Rus*

Main category: cs.CV

TL;DR: 提出了一种新型分层输入依赖状态空间模型，用于高效处理长时间手术视频，显著提升手术工作流分析性能。


<details>
  <summary>Details</summary>
Motivation: 机器人辅助手术视频分析因时长问题面临挑战，现有Transformer模型因二次注意力机制难以高效处理长视频。

Method: 结合局部聚合和全局关系状态空间模块，采用混合离散-连续监督策略训练模型，捕捉局部动态和全局时序依赖。

Result: 在三个数据集上大幅超越现有方法（Cholec80 +2.8%，MICCAI2016 +4.3%，Heichole +12.9%）。

Conclusion: 该模型通过线性扩展特性实现了全视频分析，为手术工作流分析提供了高效解决方案。

Abstract: Surgical workflow analysis is essential in robot-assisted surgeries, yet the
long duration of such procedures poses significant challenges for comprehensive
video analysis. Recent approaches have predominantly relied on transformer
models; however, their quadratic attention mechanism restricts efficient
processing of lengthy surgical videos. In this paper, we propose a novel
hierarchical input-dependent state space model that leverages the linear
scaling property of state space models to enable decision making on full-length
videos while capturing both local and global dynamics. Our framework
incorporates a temporally consistent visual feature extractor, which appends a
state space model head to a visual feature extractor to propagate temporal
information. The proposed model consists of two key modules: a
local-aggregation state space model block that effectively captures intricate
local dynamics, and a global-relation state space model block that models
temporal dependencies across the entire video. The model is trained using a
hybrid discrete-continuous supervision strategy, where both signals of discrete
phase labels and continuous phase progresses are propagated through the
network. Experiments have shown that our method outperforms the current
state-of-the-art methods by a large margin (+2.8% on Cholec80, +4.3% on
MICCAI2016, and +12.9% on Heichole datasets). Code will be publicly available
after paper acceptance.

</details>


### [168] [CA-I2P: Channel-Adaptive Registration Network with Global Optimal Selection](https://arxiv.org/abs/2506.21364)
*Zhixin Cheng,Jiacheng Deng,Xinjun Li,Xiaotian Yin,Bohao Liao,Baoqun Yin,Wenfei Yang,Tianzhu Zhang*

Main category: cs.CV

TL;DR: 该论文提出了一种改进图像与点云配准的方法，通过通道自适应调整模块和全局最优选择模块提升匹配精度。


<details>
  <summary>Details</summary>
Motivation: 现有无检测方法在图像与点云特征匹配时，因通道注意力差异和场景结构相似性导致匹配结果下降和冗余对应问题。

Method: 提出通道自适应调整模块（CAA）增强模态内特征并抑制跨模态敏感性，全局最优选择模块（GOS）替代局部选择进行全局优化。

Result: 在RGB-D Scenes V2和7-Scenes数据集上实现了最先进的图像到点云配准性能。

Conclusion: 所提方法有效解决了跨模态匹配中的特征差异和冗余问题，显著提升了配准精度。

Abstract: Detection-free methods typically follow a coarse-to-fine pipeline, extracting
image and point cloud features for patch-level matching and refining dense
pixel-to-point correspondences. However, differences in feature channel
attention between images and point clouds may lead to degraded matching
results, ultimately impairing registration accuracy. Furthermore, similar
structures in the scene could lead to redundant correspondences in cross-modal
matching. To address these issues, we propose Channel Adaptive Adjustment
Module (CAA) and Global Optimal Selection Module (GOS). CAA enhances
intra-modal features and suppresses cross-modal sensitivity, while GOS replaces
local selection with global optimization. Experiments on RGB-D Scenes V2 and
7-Scenes demonstrate the superiority of our method, achieving state-of-the-art
performance in image-to-point cloud registration.

</details>


### [169] [Step-by-Step Video-to-Audio Synthesis via Negative Audio Guidance](https://arxiv.org/abs/2506.20995)
*Akio Hayakawa,Masato Ishii,Takashi Shibuya,Yuki Mitsufuji*

Main category: cs.CV

TL;DR: 提出了一种逐步生成视频对应多音轨的方法，通过文本引导和先前音轨条件化，实现高质量复合音频合成。


<details>
  <summary>Details</summary>
Motivation: 受传统Foley工作流程启发，旨在全面捕捉视频中所有声音事件，通过分步生成提升音频合成的语义多样性和质量。

Method: 采用基于预训练视频-音频模型的训练框架，无需专用配对数据，通过文本提示和已有音轨条件化逐步生成各音轨。

Result: 实验表明，该方法能为单一视频生成多个语义不同的音轨，其复合音频质量优于现有基线。

Conclusion: 分步条件化生成策略有效提升了视频到音频合成的灵活性和质量，为多音轨合成提供了新思路。

Abstract: We propose a novel step-by-step video-to-audio generation method that
sequentially produces individual audio tracks, each corresponding to a specific
sound event in the video. Our approach mirrors traditional Foley workflows,
aiming to capture all sound events induced by a given video comprehensively.
Each generation step is formulated as a guided video-to-audio synthesis task,
conditioned on a target text prompt and previously generated audio tracks. This
design is inspired by the idea of concept negation from prior compositional
generation frameworks. To enable this guided generation, we introduce a
training framework that leverages pre-trained video-to-audio models and
eliminates the need for specialized paired datasets, allowing training on more
accessible data. Experimental results demonstrate that our method generates
multiple semantically distinct audio tracks for a single input video, leading
to higher-quality composite audio synthesis than existing baselines.

</details>


### [170] [HybridQ: Hybrid Classical-Quantum Generative Adversarial Network for Skin Disease Image Generation](https://arxiv.org/abs/2506.21015)
*Qingyue Jiao,Kangyu Zheng,Yiyu Shi,Zhiding Liang*

Main category: cs.CV

TL;DR: 该论文提出了一种新型的经典-量子潜在空间融合技术，首次实现了能够生成彩色医学图像的经典-量子生成对抗网络（GAN），在图像生成质量和分类性能提升上均优于现有方法，且参数和训练周期大幅减少。


<details>
  <summary>Details</summary>
Motivation: 皮肤疾病检测中，机器学习辅助诊断需要大量高质量数据，但现有数据集常面临类别不平衡、隐私问题和对象偏差等问题。传统生成模型计算资源消耗大，而现有量子图像生成方法只能生成低质量灰度图像。

Method: 通过一种新颖的经典-量子潜在空间融合技术，构建了首个能够生成彩色医学图像的经典-量子生成对抗网络（GAN）。

Result: 该模型在图像生成质量和分类性能提升上均优于经典深度卷积GAN和现有混合经典-量子GAN，且参数减少25倍、训练周期减少10倍，性能提升与最先进的经典生成模型相当。

Conclusion: 随着量子硬件的进步，量子图像生成技术前景广阔。该模型在真实IBM量子机器上表现出鲁棒性能。

Abstract: Machine learning-assisted diagnosis is gaining traction in skin disease
detection, but training effective models requires large amounts of high-quality
data. Skin disease datasets often suffer from class imbalance, privacy
concerns, and object bias, making data augmentation essential. While classical
generative models are widely used, they demand extensive computational
resources and lengthy training time. Quantum computing offers a promising
alternative, but existing quantum-based image generation methods can only yield
grayscale low-quality images. Through a novel classical-quantum latent space
fusion technique, our work overcomes this limitation and introduces the first
classical-quantum generative adversarial network (GAN) capable of generating
color medical images. Our model outperforms classical deep convolutional GANs
and existing hybrid classical-quantum GANs in both image generation quality and
classification performance boost when used as data augmentation. Moreover, the
performance boost is comparable with that achieved using state-of-the-art
classical generative models, yet with over 25 times fewer parameters and 10
times fewer training epochs. Such results suggest a promising future for
quantum image generation as quantum hardware advances. Finally, we demonstrate
the robust performance of our model on real IBM quantum machine with hardware
noise.

</details>


### [171] [TITAN: Query-Token based Domain Adaptive Adversarial Learning](https://arxiv.org/abs/2506.21484)
*Tajamul Ashraf,Janibul Bashir*

Main category: cs.CV

TL;DR: 论文提出TITAN方法，通过将目标域图像分为易/难样本并引入查询令牌对抗模块，解决无源域自适应目标检测中伪标签噪声导致的模型退化问题，在多个数据集上显著超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有无源域自适应目标检测方法采用师生框架生成伪标签，但域偏移和噪声会导致教师模型崩溃。需要开发可靠伪标签生成机制以提升模型适应性。

Method: 1) 基于检测方差将目标域划分为易/难样本子集 2) 在师生框架中嵌入查询令牌对抗模块，缩小特征表示的域间差异

Result: 在4个自然图像和2个医学数据集上验证：相比SOTA方法，在C2F等基准上mAP提升最高达22.7个百分点

Conclusion: TITAN通过域划分和对抗学习有效缓解伪标签噪声问题，为无源域自适应目标检测提供了新解决方案

Abstract: We focus on the source-free domain adaptive object detection (SF-DAOD)
problem when source data is unavailable during adaptation and the model must
adapt to an unlabeled target domain. The majority of approaches for the problem
employ a self-supervised approach using a student-teacher (ST) framework where
pseudo-labels are generated via a source-pretrained model for further
fine-tuning. We observe that the performance of a student model often degrades
drastically, due to the collapse of the teacher model, primarily caused by high
noise in pseudo-labels, resulting from domain bias, discrepancies, and a
significant domain shift across domains. To obtain reliable pseudo-labels, we
propose a Target-based Iterative Query-Token Adversarial Network (TITAN), which
separates the target images into two subsets: those similar to the source
(easy) and those dissimilar (hard). We propose a strategy to estimate variance
to partition the target domain. This approach leverages the insight that higher
detection variances correspond to higher recall and greater similarity to the
source domain. Also, we incorporate query-token-based adversarial modules into
a student-teacher baseline framework to reduce the domain gaps between two
feature representations. Experiments conducted on four natural imaging datasets
and two challenging medical datasets have substantiated the superior
performance of TITAN compared to existing state-of-the-art (SOTA)
methodologies. We report an mAP improvement of +22.7, +22.2, +21.1, and +3.7
percent over the current SOTA on C2F, C2B, S2C, and K2C benchmarks,
respectively.

</details>


### [172] [Pushing Trade-Off Boundaries: Compact yet Effective Remote Sensing Change Detection](https://arxiv.org/abs/2506.21109)
*Luosheng Xu,Dalin Zhang,Zhaohui Song*

Main category: cs.CV

TL;DR: 该论文提出了一种轻量级遥感变化检测模型FlickCD，通过增强差异模块和多尺度特征融合，在降低计算资源消耗的同时保持高精度。


<details>
  <summary>Details</summary>
Motivation: 当前深度学习模型在遥感变化检测中计算复杂度高，但精度提升有限。研究旨在开发轻量级模型，满足卫星上处理的需求。

Method: FlickCD采用增强差异模块(EDM)突出关键特征差异，并利用局部-全局融合块(Local-Global Fusion Blocks)结合SWSA和EGSA捕捉多尺度语义信息。

Result: 在四个基准数据集上，FlickCD显著降低了计算和存储开销(超过一个数量级)，同时达到或接近最优性能(F1分数损失<1%)。

Conclusion: FlickCD在性能与资源消耗之间取得了良好平衡，为轻量级遥感变化检测提供了有效解决方案。

Abstract: Remote sensing change detection is essential for monitoring urban expansion,
disaster assessment, and resource management, offering timely, accurate, and
large-scale insights into dynamic landscape transformations. While deep
learning has revolutionized change detection, the increasing complexity and
computational demands of modern models have not necessarily translated into
significant accuracy gains. Instead of following this trend, this study
explores a more efficient approach, focusing on lightweight models that
maintain high accuracy while minimizing resource consumption, which is an
essential requirement for on-satellite processing. To this end, we propose
FlickCD, which means quick flick then get great results, pushing the boundaries
of the performance-resource trade-off. FlickCD introduces an Enhanced
Difference Module (EDM) to amplify critical feature differences between
temporal phases while suppressing irrelevant variations such as lighting and
weather changes, thereby reducing computational costs in the subsequent change
decoder. Additionally, the FlickCD decoder incorporates Local-Global Fusion
Blocks, leveraging Shifted Window Self-Attention (SWSA) and Enhanced Global
Self-Attention (EGSA) to efficiently capture semantic information at multiple
scales, preserving both coarse- and fine-grained changes. Extensive experiments
on four benchmark datasets demonstrate that FlickCD reduces computational and
storage overheads by more than an order of magnitude while achieving
state-of-the-art (SOTA) performance or incurring only a minor (<1\% F1)
accuracy trade-off. The implementation code is publicly available at
https://github.com/xulsh8/FlickCD.

</details>


### [173] [Whole-Body Conditioned Egocentric Video Prediction](https://arxiv.org/abs/2506.21552)
*Yutong Bai,Danny Tran,Amir Bar,Yann LeCun,Trevor Darrell,Jitendra Malik*

Main category: cs.CV

TL;DR: 训练模型通过人体动作预测第一人称视角视频，结合3D姿态和自回归扩散变换器，在大型数据集上验证预测能力。


<details>
  <summary>Details</summary>
Motivation: 研究如何从第一人称视角模拟人类动作对环境的影响，解决复杂现实环境和具身行为的建模挑战。

Method: 使用基于3D身体姿态的自回归条件扩散变换器，结合层次化评估协议，在Nymeria数据集上训练。

Result: 模型能够从人体动作预测第一人称视频，并通过层次化任务验证其预测和控制能力。

Conclusion: 该研究为复杂现实环境和具身行为的视频预测提供了初步探索。

Abstract: We train models to Predict Ego-centric Video from human Actions (PEVA), given
the past video and an action represented by the relative 3D body pose. By
conditioning on kinematic pose trajectories, structured by the joint hierarchy
of the body, our model learns to simulate how physical human actions shape the
environment from a first-person point of view. We train an auto-regressive
conditional diffusion transformer on Nymeria, a large-scale dataset of
real-world egocentric video and body pose capture. We further design a
hierarchical evaluation protocol with increasingly challenging tasks, enabling
a comprehensive analysis of the model's embodied prediction and control
abilities. Our work represents an initial attempt to tackle the challenges of
modeling complex real-world environments and embodied agent behaviors with
video prediction from the perspective of a human.

</details>


### [174] [A Comprehensive Dataset for Underground Miner Detection in Diverse Scenario](https://arxiv.org/abs/2506.21451)
*Cyrus Addy,Ajay Kumar Gurumadaiah,Yixiang Gao,Kwame Awuah-Offei*

Main category: cs.CV

TL;DR: 该论文提出了一种专门设计的热成像数据集，用于开发和验证矿工检测系统，以应对地下采矿环境中的紧急情况。


<details>
  <summary>Details</summary>
Motivation: 地下采矿作业面临重大安全挑战，机器人辅助搜救的潜力依赖于可靠的矿工检测能力，但目前缺乏针对地下采矿环境的全面训练数据集。

Method: 论文通过系统性地捕捉各种采矿活动和场景的热成像图像，创建了一个用于检测算法的数据集，并评估了包括YOLOv8、YOLOv10、YOLO11和RT-DETR在内的多种先进目标检测算法。

Result: 研究证明了使用热成像进行矿工检测的可行性，并为此类关键安全应用的未来研究奠定了基础。

Conclusion: 该数据集是开发可靠热成像矿工检测系统的重要第一步，未来有望应用于实际紧急情况中。

Abstract: Underground mining operations face significant safety challenges that make
emergency response capabilities crucial. While robots have shown promise in
assisting with search and rescue operations, their effectiveness depends on
reliable miner detection capabilities. Deep learning algorithms offer potential
solutions for automated miner detection, but require comprehensive training
datasets, which are currently lacking for underground mining environments. This
paper presents a novel thermal imaging dataset specifically designed to enable
the development and validation of miner detection systems for potential
emergency applications. We systematically captured thermal imagery of various
mining activities and scenarios to create a robust foundation for detection
algorithms. To establish baseline performance metrics, we evaluated several
state-of-the-art object detection algorithms including YOLOv8, YOLOv10, YOLO11,
and RT-DETR on our dataset. While not exhaustive of all possible emergency
situations, this dataset serves as a crucial first step toward developing
reliable thermal-based miner detection systems that could eventually be
deployed in real emergency scenarios. This work demonstrates the feasibility of
using thermal imaging for miner detection and establishes a foundation for
future research in this critical safety application.

</details>


### [175] [Evaluation of Traffic Signals for Daily Traffic Pattern](https://arxiv.org/abs/2506.21469)
*Mohammad Shokrolah Shirazi,Hung-Fu Chang*

Main category: cs.CV

TL;DR: 该论文提出动态、静态和混合三种基于转向流量计数（TMC）的交通信号配置方法，通过视觉追踪系统估算路口数据，并在仿真中验证90秒和120秒信号周期效果最佳。混合方法能根据交通峰谷动态切换配置，提升流量管理效率。


<details>
  <summary>Details</summary>
Motivation: 转向流量计数数据对交通信号设计、路口几何规划及拥堵分析至关重要。现有方法需适应不同交通模式（如双峰型日流量），因此需要开发更灵活的信号配置方案。

Method: 开发视觉追踪系统采集6个路口的TMC数据，结合SUMO仿真平台，设计静态、动态及混合信号配置方法。混合方法在高峰/平峰期自动切换动态和静态模式。

Result: 实验表明：90秒和120秒信号周期效果最优；动态配置在4个路口表现更好，其余2个因车道-车流比低而性能较差；混合方法在东西/南北向车流集中的路口表现最佳。

Conclusion: 区域交通分布影响信号设计选择：静态适用于均衡分布，混合方法更适应定向车流集中的路口。动态调整信号策略可有效提升交通管理效率。

Abstract: The turning movement count data is crucial for traffic signal design,
intersection geometry planning, traffic flow, and congestion analysis. This
work proposes three methods called dynamic, static, and hybrid configuration
for TMC-based traffic signals. A vision-based tracking system is developed to
estimate the TMC of six intersections in Las Vegas using traffic cameras. The
intersection design, route (e.g. vehicle movement directions), and signal
configuration files with compatible formats are synthesized and imported into
Simulation of Urban MObility for signal evaluation with realistic data. The
initial experimental results based on estimated waiting times indicate that the
cycle time of 90 and 120 seconds works best for all intersections. In addition,
four intersections show better performance for dynamic signal timing
configuration, and the other two with lower performance have a lower ratio of
total vehicle count to total lanes of the intersection leg. Since daily traffic
flow often exhibits a bimodal pattern, we propose a hybrid signal method that
switches between dynamic and static methods, adapting to peak and off-peak
traffic conditions for improved flow management. So, a built-in traffic
generator module creates vehicle routes for 4 hours, including peak hours, and
a signal design module produces signal schedule cycles according to static,
dynamic, and hybrid methods. Vehicle count distributions are weighted
differently for each zone (i.e., West, North, East, South) to generate diverse
traffic patterns. The extended experimental results for 6 intersections with 4
hours of simulation time imply that zone-based traffic pattern distributions
affect signal design selection. Although the static method works great for
evenly zone-based traffic distribution, the hybrid method works well for highly
weighted traffic at intersection pairs of the West-East and North-South zones.

</details>


### [176] [Towards Reliable Detection of Empty Space: Conditional Marked Point Processes for Object Detection](https://arxiv.org/abs/2506.21486)
*Tobias J. Riedlinger,Kira Maag,Hanno Gottschalk*

Main category: cs.CV

TL;DR: 提出基于空间统计的目标检测模型，解决现有方法在未检测区域不确定性量化不足的问题。


<details>
  <summary>Details</summary>
Motivation: 现有目标检测模型的置信度估计存在校准问题，且无法量化未检测区域的不确定性，这在自动驾驶等安全关键应用中存在风险。

Method: 采用标记点过程的空间统计框架，通过似然训练提供区域可行驶性的明确定义置信度估计。

Result: 通过校准评估和性能验证，证明了该方法的有效性。

Conclusion: 基于空间统计的模型能更可靠地量化检测和未检测区域的不确定性，提升安全性。

Abstract: Deep neural networks have set the state-of-the-art in computer vision tasks
such as bounding box detection and semantic segmentation. Object detectors and
segmentation models assign confidence scores to predictions, reflecting the
model's uncertainty in object detection or pixel-wise classification. However,
these confidence estimates are often miscalibrated, as their architectures and
loss functions are tailored to task performance rather than probabilistic
foundation. Even with well calibrated predictions, object detectors fail to
quantify uncertainty outside detected bounding boxes, i.e., the model does not
make a probability assessment of whether an area without detected objects is
truly free of obstacles. This poses a safety risk in applications such as
automated driving, where uncertainty in empty areas remains unexplored. In this
work, we propose an object detection model grounded in spatial statistics.
Bounding box data matches realizations of a marked point process, commonly used
to describe the probabilistic occurrence of spatial point events identified as
bounding box centers, where marks are used to describe the spatial extension of
bounding boxes and classes. Our statistical framework enables a
likelihood-based training and provides well-defined confidence estimates for
whether a region is drivable, i.e., free of objects. We demonstrate the
effectiveness of our method through calibration assessments and evaluation of
performance.

</details>


### [177] [Maximal Matching Matters: Preventing Representation Collapse for Robust Cross-Modal Retrieval](https://arxiv.org/abs/2506.21538)
*Hani Alomari,Anushka Sivakumar,Andrew Zhang,Chris Thomas*

Main category: cs.CV

TL;DR: 该论文提出了一种新的跨模态图像-文本检索方法，通过最大化嵌入集间的一对一匹配来保持语义多样性，并结合两种损失函数提升表示效果，在MS-COCO和Flickr30k上取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 传统方法使用单向量嵌入难以捕捉跨模态间多样且细微的关联，而基于集合的方法虽能表示更丰富的关系，但仍面临稀疏监督和集合坍缩的问题。

Method: 提出了最大配对分配相似度优化嵌入集间的一对一匹配，并引入全局判别损失和集合内差异损失来增强表示效果。

Result: 该方法在MS-COCO和Flickr30k数据集上无需外部数据即达到了最先进的性能。

Conclusion: 通过优化嵌入集间的匹配和引入新的损失函数，该方法有效提升了跨模态检索的性能和语义多样性。

Abstract: Cross-modal image-text retrieval is challenging because of the diverse
possible associations between content from different modalities. Traditional
methods learn a single-vector embedding to represent semantics of each sample,
but struggle to capture nuanced and diverse relationships that can exist across
modalities. Set-based approaches, which represent each sample with multiple
embeddings, offer a promising alternative, as they can capture richer and more
diverse relationships. In this paper, we show that, despite their promise,
these set-based representations continue to face issues including sparse
supervision and set collapse, which limits their effectiveness. To address
these challenges, we propose Maximal Pair Assignment Similarity to optimize
one-to-one matching between embedding sets which preserve semantic diversity
within the set. We also introduce two loss functions to further enhance the
representations: Global Discriminative Loss to enhance distinction among
embeddings, and Intra-Set Divergence Loss to prevent collapse within each set.
Our method achieves state-of-the-art performance on MS-COCO and Flickr30k
without relying on external data.

</details>


<div id='physics.geo-ph'></div>

# physics.geo-ph [[Back]](#toc)

### [178] [Efficacy of Temporal Fusion Transformers for Runoff Simulation](https://arxiv.org/abs/2506.20831)
*Sinan Rasiya Koya,Tirthankar Roy*

Main category: physics.geo-ph

TL;DR: 该研究比较了Temporal Fusion Transformers (TFT)和Long Short-Term Memory (LSTM)在降雨径流建模中的表现，发现TFT略优于LSTM，尤其在处理长序列和大流域时表现更佳。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探索结合注意力机制和循环结构在序列建模中的优势，特别是在水文预测中的应用，比较TFT和LSTM在降雨径流建模中的性能差异。

Method: 研究训练了10个随机初始化的TFT和LSTM模型，应用于美国的531个CAMELS流域，并在代表不同国家的Caravan数据集子集上重复实验，评估模型性能及其与流域属性和数据集的关系。

Result: TFT在模拟水文过程线的中段和峰值时略优于LSTM，且能更好地处理长序列和大流域数据。TFT还能识别关键动态和静态变量，提供科学见解。但两种模型在Caravan数据集上性能均显著下降，可能反映数据质量问题。

Conclusion: 研究表明TFT在水文建模和理解中具有潜力，尤其在处理复杂流域时表现更优，但其性能受数据质量影响。

Abstract: Combining attention with recurrence has shown to be valuable in sequence
modeling, including hydrological predictions. Here, we explore the strength of
Temporal Fusion Transformers (TFTs) over Long Short-Term Memory (LSTM) networks
in rainfall-runoff modeling. We train ten randomly initialized models, TFT and
LSTM, for 531 CAMELS catchments in the US. We repeat the experiment with five
subsets of the Caravan dataset, each representing catchments in the US,
Australia, Brazil, Great Britain, and Chile. Then, the performance of the
models, their variability regarding the catchment attributes, and the
difference according to the datasets are assessed. Our findings show that TFT
slightly outperforms LSTM, especially in simulating the midsection and peak of
hydrographs. Furthermore, we show the ability of TFT to handle longer sequences
and why it can be a better candidate for higher or larger catchments. Being an
explainable AI technique, TFT identifies the key dynamic and static variables,
providing valuable scientific insights. However, both TFT and LSTM exhibit a
considerable drop in performance with the Caravan dataset, indicating possible
data quality issues. Overall, the study highlights the potential of TFT in
improving hydrological modeling and understanding.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [179] [Uncertainty-Aware Machine-Learning Framework for Predicting Dislocation Plasticity and Stress-Strain Response in FCC Alloys](https://arxiv.org/abs/2506.20839)
*Jing Luo,Yejun Gu,Yanfei Wang,Xiaolong Ma,Jaafar. A El-Awady*

Main category: cond-mat.mtrl-sci

TL;DR: 该研究利用混合密度网络模型预测位错密度分布和应力分布，通过量化不确定性提升材料力学性能预测的准确性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 机器学习在结构材料领域的应用日益重要，但现有数据整合和预测模型中的不确定性量化仍需改进。本研究旨在通过新方法优化合金设计并推动新材料开发。

Method: 采用混合密度网络（MDN）模型，基于大量文献实验数据训练，预测位错密度分布（作为潜变量）和晶粒级应力分布，并结合统计参数到位错介导的塑性模型中。

Result: 该方法不仅提高了应力-应变预测的准确性，还实现了显式的不确定性量化，为材料力学性能预测提供了更高可靠性。

Conclusion: 该策略显著提升了材料性能预测的精度和可靠性，对快速发展的材料工业中优化合金设计和开发新材料具有重要价值。

Abstract: Machine learning has significantly advanced the understanding and application
of structural materials, with an increasing emphasis on integrating existing
data and quantifying uncertainties in predictive modeling. This study presents
a comprehensive methodology utilizing a mixed density network (MDN) model,
trained on extensive experimental data from literature. This approach uniquely
predicts the distribution of dislocation density, inferred as a latent
variable, and the resulting stress distribution at the grain level. The
incorporation of statistical parameters of those predicted distributions into a
dislocation-mediated plasticity model allows for accurate stress-strain
predictions with explicit uncertainty quantification. This strategy not only
improves the accuracy and reliability of mechanical property predictions but
also plays a vital role in optimizing alloy design, thereby facilitating the
development of new materials in a rapidly evolving industry.

</details>


<div id='math.DS'></div>

# math.DS [[Back]](#toc)

### [180] [Structural System Identification via Validation and Adaptation](https://arxiv.org/abs/2506.20799)
*Cristian López,Keegan J. Moore*

Main category: math.DS

TL;DR: 提出了一种基于生成对抗网络的结构系统识别方法，用于参数估计和模型验证。


<details>
  <summary>Details</summary>
Motivation: 为了整合实验数据与科学理论，理解、验证和预测复杂系统的动态行为，需要准确估计控制方程的参数值。

Method: 使用神经网络将随机噪声映射到物理参数，通过已知运动方程生成假加速度，并与真实数据比较；利用独立验证数据集和判别器网络验证参数。

Result: 分析和真实实验表明，该方法在不同非线性结构系统中实现了高精度的参数估计和模型验证。

Conclusion: 该方法有效结合了生成对抗网络和物理方程，实现了高精度的系统识别和验证。

Abstract: Estimating the governing equation parameter values is essential for
integrating experimental data with scientific theory to understand, validate,
and predict the dynamics of complex systems. In this work, we propose a new
method for structural system identification (SI), uncertainty quantification,
and validation directly from data. Inspired by generative modeling frameworks,
a neural network maps random noise to physically meaningful parameters. These
parameters are then used in the known equation of motion to obtain fake
accelerations, which are compared to real training data via a mean square error
loss. To simultaneously validate the learned parameters, we use independent
validation datasets. The generated accelerations from these datasets are
evaluated by a discriminator network, which determines whether the output is
real or fake, and guides the parameter-generator network. Analytical and real
experiments show the parameter estimation accuracy and model validation for
different nonlinear structural systems.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [181] [Hybrid Deep Learning and Signal Processing for Arabic Dialect Recognition in Low-Resource Settings](https://arxiv.org/abs/2506.21386)
*Ghazal Al-Shwayyat,Omer Nezih Gerek*

Main category: eess.AS

TL;DR: 该研究探讨了在低资源环境下，结合传统信号处理技术和深度学习架构的混合模型用于阿拉伯方言识别的有效性。MFCC + CNN模型表现最佳，准确率达91.2%，显著优于Wavelet + RNN模型。


<details>
  <summary>Details</summary>
Motivation: 阿拉伯方言识别面临语言多样性和标注数据稀缺的挑战，尤其是在资源有限的情况下。研究旨在通过混合建模策略解决这一问题。

Method: 研究开发并评估了两种混合模型：MFCC结合CNN，以及DWT特征结合RNN。使用Common Voice阿拉伯数据集的方言过滤子集进行训练。

Result: MFCC + CNN模型表现最佳，准确率达91.2%，而Wavelet + RNN模型准确率为66.5%。结果表明，在有限标注数据下，频谱特征与卷积模型结合效果显著。

Conclusion: 研究为资源受限环境下的阿拉伯方言识别建立了强基线，并建议未来采用更大标注语料库、自监督学习技术和先进神经网络架构（如Transformer）以进一步提升性能。

Abstract: Arabic dialect recognition presents a significant challenge in speech
technology due to the linguistic diversity of Arabic and the scarcity of large
annotated datasets, particularly for underrepresented dialects. This research
investigates hybrid modeling strategies that integrate classical signal
processing techniques with deep learning architectures to address this problem
in low-resource scenarios. Two hybrid models were developed and evaluated: (1)
Mel-Frequency Cepstral Coefficients (MFCC) combined with a Convolutional Neural
Network (CNN), and (2) Discrete Wavelet Transform (DWT) features combined with
a Recurrent Neural Network (RNN). The models were trained on a dialect-filtered
subset of the Common Voice Arabic dataset, with dialect labels assigned based
on speaker metadata. Experimental results demonstrate that the MFCC + CNN
architecture achieved superior performance, with an accuracy of 91.2% and
strong precision, recall, and F1-scores, significantly outperforming the
Wavelet + RNN configuration, which achieved an accuracy of 66.5%. These
findings highlight the effectiveness of leveraging spectral features with
convolutional models for Arabic dialect recognition, especially when working
with limited labeled data. The study also identifies limitations related to
dataset size, potential regional overlaps in labeling, and model optimization,
providing a roadmap for future research. Recommendations for further
improvement include the adoption of larger annotated corpora, integration of
self-supervised learning techniques, and exploration of advanced neural
architectures such as Transformers. Overall, this research establishes a strong
baseline for future developments in Arabic dialect recognition within
resource-constrained environments.

</details>


### [182] [Performance improvement of spatial semantic segmentation with enriched audio features and agent-based error correction for DCASE 2025 Challenge Task 4](https://arxiv.org/abs/2506.21174)
*Jongyeon Park,Joonhee Lee,Do-Hyeon Lim,Hong Kook Kim,Hyeongcheol Geum,Jeong Eun Lim*

Main category: eess.AS

TL;DR: 该技术报告介绍了DCASE 2025挑战赛任务4的提交系统，通过融合额外音频特征、标签校正系统和数据集优化，提升了音频分类性能。


<details>
  <summary>Details</summary>
Motivation: 混合音频中的细微线索难以仅通过梅尔频谱图捕捉，因此需要引入额外音频特征以提供更多视角。

Method: 结合频谱滚降和色度特征增强嵌入特征，应用基于代理的标签校正系统减少误报，并通过优化训练数据集提升低性能类别的分类准确率。

Result: 实验表明，所提系统相较于基线在CA-SDRi指标上最高相对提升了14.7%。

Conclusion: 通过多特征融合和数据集优化，显著提升了音频分类模型的性能。

Abstract: This technical report presents submission systems for Task 4 of the DCASE
2025 Challenge. This model incorporates additional audio features (spectral
roll-off and chroma features) into the embedding feature extracted from the
mel-spectral feature to im-prove the classification capabilities of an
audio-tagging model in the spatial semantic segmentation of sound scenes (S5)
system. This approach is motivated by the fact that mixed audio often contains
subtle cues that are difficult to capture with mel-spectrograms alone. Thus,
these additional features offer alterna-tive perspectives for the model.
Second, an agent-based label correction system is applied to the outputs
processed by the S5 system. This system reduces false positives, improving the
final class-aware signal-to-distortion ratio improvement (CA-SDRi) metric.
Finally, we refine the training dataset to enhance the classi-fication accuracy
of low-performing classes by removing irrele-vant samples and incorporating
external data. That is, audio mix-tures are generated from a limited number of
data points; thus, even a small number of out-of-class data points could
degrade model performance. The experiments demonstrate that the submit-ted
systems employing these approaches relatively improve CA-SDRi by up to 14.7%
compared to the baseline of DCASE 2025 Challenge Task 4.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [183] [On Uniform Weighted Deep Polynomial approximation](https://arxiv.org/abs/2506.21306)
*Kingsley Yeon,Steven B. Damelin*

Main category: math.NA

TL;DR: 该论文提出了一种加权深度多项式逼近方法，用于处理具有不对称行为的函数，通过结合可学习的深度多项式和单侧权重，有效捕捉局部非光滑性和全局增长特性。


<details>
  <summary>Details</summary>
Motivation: 传统有理函数逼近在非光滑或奇异函数（如|x|和x^{1/p}）上表现优异，而多项式逼近仅能实现代数收敛。近期研究表明复合多项式结构可以在无光滑性条件下恢复指数逼近速率。本文旨在解决具有不对称行为函数的逼近问题。

Method: 提出了一种加权深度多项式逼近框架，通过将可学习的深度多项式与单侧权重相乘，以同时捕捉局部非光滑性和全局增长特性，并采用基于图的稳定参数化策略进行优化。

Result: 数值实验表明，该方法在相同参数数量下优于泰勒、切比雪夫和标准深度多项式逼近方法。

Conclusion: 加权深度多项式逼近方法在处理具有不对称行为的函数时表现出色，为高效逼近非光滑函数提供了新的解决方案。

Abstract: It is a classical result in rational approximation theory that certain
non-smooth or singular functions, such as $|x|$ and $x^{1/p}$, can be
efficiently approximated using rational functions with root-exponential
convergence in terms of degrees of freedom \cite{Sta, GN}. In contrast,
polynomial approximations admit only algebraic convergence by Jackson's theorem
\cite{Lub2}. Recent work shows that composite polynomial architectures can
recover exponential approximation rates even without smoothness \cite{KY}. In
this work, we introduce and analyze a class of weighted deep polynomial
approximants tailored for functions with asymmetric behavior-growing unbounded
on one side and decaying on the other. By multiplying a learnable deep
polynomial with a one-sided weight, we capture both local non-smoothness and
global growth. We show numerically that this framework outperforms Taylor,
Chebyshev, and standard deep polynomial approximants, even when all use the
same number of parameters. To optimize these approximants in practice, we
propose a stable graph-based parameterization strategy building on \cite{Jar}.

</details>


<div id='q-bio.CB'></div>

# q-bio.CB [[Back]](#toc)

### [184] [scMamba: A Scalable Foundation Model for Single-Cell Multi-Omics Integration Beyond Highly Variable Feature Selection](https://arxiv.org/abs/2506.20697)
*Zhen Yuan,Shaoqing Jiao,Yihang Xiao,Jiajie Peng*

Main category: q-bio.CB

TL;DR: scMamba是一种无需特征选择即可整合单细胞多组学数据的基础模型，通过创新的标记化策略和对比学习方法显著提升了数据对齐和下游任务表现。


<details>
  <summary>Details</summary>
Motivation: 当前单细胞多组学数据整合方法依赖高变基因筛选，可能丢失关键生物信息。需要一种能保留基因组位置信息且无需预选特征的整合方法。

Method: 提出基于补丁的细胞标记化策略，将基因组区域视为词汇（标记），细胞视为句子。采用状态空间对偶概念和带余弦相似度正则化的对比学习框架。

Result: 在多数据集测试中，scMamba在保留生物变异、组学层对齐及聚类/细胞注释/轨迹推断等下游任务上显著优于现有方法。

Conclusion: scMamba成为处理大规模单细胞多组学图谱的有力工具，可推动生物医学发现。

Abstract: The advent of single-cell multi-omics technologies has enabled the
simultaneous profiling of diverse omics layers within individual cells.
Integrating such multimodal data provides unprecedented insights into cellular
identity, regulatory processes, and disease mechanisms. However, it remains
challenging, as current methods often rely on selecting highly variable genes
or peaks during preprocessing, which may inadvertently discard crucial
biological information. Here, we present scMamba, a foundation model designed
to integrate single-cell multi-omics data without the need for prior feature
selection while preserving genomic positional information. scMamba introduces a
patch-based cell tokenization strategy that treats genomics regions as words
(tokens) and cells as sentences. Building upon the concept of state space
duality, scMamba distills rich biological insights from high-dimensional,
sparse single-cell multi-omics data. Additionally, our novel contrastive
learning approach, enhanced with cosine similarity regularization, enables
superior alignment across omics layers compared to traditional methods.
Systematic benchmarking across multiple datasets demonstrates that scMamba
significantly outperforms state-of-the-art methods in preserving biological
variation, aligning omics layers, and enhancing key downstream tasks such as
clustering, cell type annotation, and trajectory inference. Our findings
position scMamba as a powerful tool for large-scale single-cell multi-omics
integration, capable of handling large-scale atlases and advancing biological
discovery.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [185] [Poster: Enhancing GNN Robustness for Network Intrusion Detection via Agent-based Analysis](https://arxiv.org/abs/2506.20806)
*Zhonghao Zhan,Huichi Zhou,Hamed Haddadi*

Main category: cs.CR

TL;DR: 该论文提出了一种利用大型语言模型（LLMs）作为模拟网络安全专家代理，以增强图神经网络（GNNs）在网络入侵检测系统（NIDS）中的鲁棒性和泛化能力的新方法。


<details>
  <summary>Details</summary>
Motivation: 当前图神经网络在网络入侵检测系统中面临分布漂移和对抗攻击的挑战，且现有的鲁棒性评估方法依赖于不现实的合成扰动，缺乏对不同类型对抗攻击的系统分析。

Method: 通过将大型语言模型（LLMs）作为模拟网络安全专家代理，分析网络流量数据生成的图结构，识别并减轻可疑或对抗性扰动元素，从而提升GNN的鲁棒性。

Result: 实验表明，集成LLM分析能显著提高基于GNN的NIDS对各种对抗攻击的抵御能力，展示了LLM代理在入侵检测架构中的潜在补充作用。

Conclusion: 该研究展示了LLM代理在增强GNN鲁棒性和泛化能力方面的潜力，为网络入侵检测系统提供了一种新的补充方法。

Abstract: Graph Neural Networks (GNNs) show great promise for Network Intrusion
Detection Systems (NIDS), particularly in IoT environments, but suffer
performance degradation due to distribution drift and lack robustness against
realistic adversarial attacks. Current robustness evaluations often rely on
unrealistic synthetic perturbations and lack demonstrations on systematic
analysis of different kinds of adversarial attack, which encompass both
black-box and white-box scenarios. This work proposes a novel approach to
enhance GNN robustness and generalization by employing Large Language Models
(LLMs) in an agentic pipeline as simulated cybersecurity expert agents. These
agents scrutinize graph structures derived from network flow data, identifying
and potentially mitigating suspicious or adversarially perturbed elements
before GNN processing. Our experiments, using a framework designed for
realistic evaluation and testing with a variety of adversarial attacks
including a dataset collected from physical testbed experiments, demonstrate
that integrating LLM analysis can significantly improve the resilience of
GNN-based NIDS against challenges, showcasing the potential of LLM agent as a
complementary layer in intrusion detection architectures.

</details>


### [186] [ZKPROV: A Zero-Knowledge Approach to Dataset Provenance for Large Language Models](https://arxiv.org/abs/2506.20915)
*Mina Namazi,Alexander Nemecek,Erman Ayday*

Main category: cs.CR

TL;DR: ZKPROV是一种新型加密框架，通过零知识证明验证大语言模型的数据来源，确保模型训练数据的可靠性而不泄露敏感信息。


<details>
  <summary>Details</summary>
Motivation: 在敏感领域（如医疗）部署大语言模型时，确保其计算来源的完整性至关重要，尤其是在数据集使用受到严格监管的情况下。

Method: ZKPROV利用零知识证明和数据集签名元数据，将训练模型与其授权数据集加密绑定，避免验证每一步训练过程的高计算成本。

Result: 实验证明ZKPROV在生成和验证证明时高效且可扩展，为实际部署提供了实用解决方案，并提供了形式化的安全保障。

Conclusion: ZKPROV在保护数据集机密性的同时，确保了可信的数据来源，为大语言模型在敏感领域的应用提供了安全可靠的解决方案。

Abstract: As the deployment of large language models (LLMs) grows in sensitive domains,
ensuring the integrity of their computational provenance becomes a critical
challenge, particularly in regulated sectors such as healthcare, where strict
requirements are applied in dataset usage. We introduce ZKPROV, a novel
cryptographic framework that enables zero-knowledge proofs of LLM provenance.
It allows users to verify that a model is trained on a reliable dataset without
revealing sensitive information about it or its parameters. Unlike prior
approaches that focus on complete verification of the training process
(incurring significant computational cost) or depend on trusted execution
environments, ZKPROV offers a distinct balance. Our method cryptographically
binds a trained model to its authorized training dataset(s) through
zero-knowledge proofs while avoiding proof of every training step. By
leveraging dataset-signed metadata and compact model parameter commitments,
ZKPROV provides sound and privacy-preserving assurances that the result of the
LLM is derived from a model trained on the claimed authorized and relevant
dataset. Experimental results demonstrate the efficiency and scalability of the
ZKPROV in generating this proof and verifying it, achieving a practical
solution for real-world deployments. We also provide formal security
guarantees, proving that our approach preserves dataset confidentiality while
ensuring trustworthy dataset provenance.

</details>


### [187] [PhishKey: A Novel Centroid-Based Approach for Enhanced Phishing Detection Using Adaptive HTML Component Extraction](https://arxiv.org/abs/2506.21106)
*Felipe Castaño,Eduardo Fidalgo,Enrique Alegre,Rocio Alaiz-Rodríguez,Raul Orduna,Francesco Zola*

Main category: cs.CR

TL;DR: PhishKey是一种新型钓鱼检测方法，结合字符级CNN和基于质心的关键词提取器CAPE，通过软投票集成提高分类准确性，实验显示其F1分数高达98.70%且抗对抗攻击能力强。


<details>
  <summary>Details</summary>
Motivation: 钓鱼攻击快速演变，绕过检测机制并利用人类弱点，亟需一种适应性强、鲁棒且高效的检测方法。

Method: PhishKey采用混合特征自动提取：字符级CNN处理URL分类，CAPE处理HTML内容以减少噪声并避免数据裁剪，最后通过软投票集成预测结果。

Result: 在四个前沿数据集上测试，PhishKey达到98.70%的F1分数，对抗注入攻击等对抗操作时性能下降极小。

Conclusion: PhishKey通过多模态特征融合和集成策略，显著提升了钓鱼检测的准确性和鲁棒性，为网络安全提供了有效解决方案。

Abstract: Phishing attacks pose a significant cybersecurity threat, evolving rapidly to
bypass detection mechanisms and exploit human vulnerabilities. This paper
introduces PhishKey to address the challenges of adaptability, robustness, and
efficiency. PhishKey is a novel phishing detection method using automatic
feature extraction from hybrid sources. PhishKey combines character-level
processing with Convolutional Neural Networks (CNN) for URL classification, and
a Centroid-Based Key Component Phishing Extractor (CAPE) for HTML content at
the word level. CAPE reduces noise and ensures complete sample processing
avoiding crop operations on the input data. The predictions from both modules
are integrated using a soft-voting ensemble to achieve more accurate and
reliable classifications. Experimental evaluations on four state-of-the-art
datasets demonstrate the effectiveness of PhishKey. It achieves up to 98.70% F1
Score and shows strong resistance to adversarial manipulations such as
injection attacks with minimal performance degradation.

</details>


### [188] [Empowering Digital Agriculture: A Privacy-Preserving Framework for Data Sharing and Collaborative Research](https://arxiv.org/abs/2506.20872)
*Osama Zafar,Rosemarie Santa González,Mina Namazi,Alfonso Morales,Erman Ayday*

Main category: cs.CR

TL;DR: 本文提出了一种保护隐私的框架，结合降维技术和差分隐私，以安全共享农业数据并促进农民与研究者的协作，同时保护敏感信息。


<details>
  <summary>Details</summary>
Motivation: 数据驱动的农业可以提高作物产量和土壤健康，但农民因隐私问题（如歧视和资源操纵）不愿共享数据。本文旨在解决这一障碍。

Method: 框架结合了主成分分析（PCA）和拉普拉斯噪声的差分隐私技术，支持联邦学习和隐私保护数据聚合，帮助农民和研究者安全协作。

Result: 在真实数据集上的验证表明，该框架能有效抵御对抗攻击，保护隐私，同时保持与集中式系统相当的实用性。

Conclusion: 该框架通过解决隐私挑战，支持安全数据整合，推动农业系统的创新和可持续发展，为数据驱动农业的进步铺平道路。

Abstract: Data-driven agriculture, which integrates technology and data into
agricultural practices, has the potential to improve crop yield, disease
resilience, and long-term soil health. However, privacy concerns, such as
adverse pricing, discrimination, and resource manipulation, deter farmers from
sharing data, as it can be used against them. To address this barrier, we
propose a privacy-preserving framework that enables secure data sharing and
collaboration for research and development while mitigating privacy risks. The
framework combines dimensionality reduction techniques (like Principal
Component Analysis (PCA)) and differential privacy by introducing Laplacian
noise to protect sensitive information. The proposed framework allows
researchers to identify potential collaborators for a target farmer and train
personalized machine learning models either on the data of identified
collaborators via federated learning or directly on the aggregated
privacy-protected data. It also allows farmers to identify potential
collaborators based on similarities. We have validated this on real-life
datasets, demonstrating robust privacy protection against adversarial attacks
and utility performance comparable to a centralized system. We demonstrate how
this framework can facilitate collaboration among farmers and help researchers
pursue broader research objectives. The adoption of the framework can empower
researchers and policymakers to leverage agricultural data responsibly, paving
the way for transformative advances in data-driven agriculture. By addressing
critical privacy challenges, this work supports secure data integration,
fostering innovation and sustainability in agricultural systems.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [189] [Exploring the Effects of Chatbot Anthropomorphism and Human Empathy on Human Prosocial Behavior Toward Chatbots](https://arxiv.org/abs/2506.20748)
*Jingshu Li,Zicheng Zhu,Renwen Zhang,Yi-Chieh Lee*

Main category: cs.HC

TL;DR: 研究发现，聊天机器人的人形化特征（如人类身份和情感表达）能增强人类对其的共情，从而促进亲社会行为和意图。


<details>
  <summary>Details</summary>
Motivation: 尽管聊天机器人被广泛使用，但人类帮助聊天机器人的动机因素尚未充分研究。本文旨在探讨聊天机器人的人形化如何影响人类的共情和亲社会行为。

Method: 通过在线实验（N=244），让聊天机器人在协作图像标注任务中犯错并向参与者解释原因，随后测量参与者的亲社会行为和意图。

Result: 人类身份和情感表达显著增加了参与者的亲社会行为和意图，共情在其中起中介作用。定性分析还揭示了两种动机：对聊天机器人的共情和将其视为类人存在。

Conclusion: 聊天机器人的人形化设计能有效激发人类的共情和亲社会行为，这对理解和促进人机协作具有重要意义。

Abstract: Chatbots are increasingly integrated into people's lives and are widely used
to help people. Recently, there has also been growing interest in the reverse
direction-humans help chatbots-due to a wide range of benefits including better
chatbot performance, human well-being, and collaborative outcomes. However,
little research has explored the factors that motivate people to help chatbots.
To address this gap, we draw on the Computers Are Social Actors (CASA)
framework to examine how chatbot anthropomorphism-including human-like
identity, emotional expression, and non-verbal expression-influences human
empathy toward chatbots and their subsequent prosocial behaviors and
intentions. We also explore people's own interpretations of their prosocial
behaviors toward chatbots. We conducted an online experiment (N = 244) in which
chatbots made mistakes in a collaborative image labeling task and explained the
reasons to participants. We then measured participants' prosocial behaviors and
intentions toward the chatbots. Our findings revealed that human identity and
emotional expression of chatbots increased participants' prosocial behavior and
intention toward chatbots, with empathy mediating these effects. Qualitative
analysis further identified two motivations for participants' prosocial
behaviors: empathy for the chatbot and perceiving the chatbot as human-like. We
discuss the implications of these results for understanding and promoting human
prosocial behaviors toward chatbots.

</details>


### [190] [A Systematic Review of Human-AI Co-Creativity](https://arxiv.org/abs/2506.21333)
*Saloni Singh,Koen Hndriks,Drik Heylen,Kim Baraka*

Main category: cs.HC

TL;DR: 本文通过系统文献综述分析了62篇关于共创系统的论文，总结了影响系统设计的关键维度和设计考虑，强调高用户控制和自适应主动系统对提升创造体验的重要性。


<details>
  <summary>Details</summary>
Motivation: 共创社区在开发更复杂、定制化的系统以支持和增强人类创造力方面取得了显著进展。为了支持这一努力，本文旨在通过系统文献综述，总结现有研究中的设计考虑，为未来系统提供高效的基础。

Method: 本文对62篇关于共创系统的论文进行了系统文献综述，涵盖了视觉艺术、设计和写作等多个应用领域，分析了AI作为主动协作者在创造过程中的作用。

Result: 研究发现，高用户控制的系统能带来更高的满意度、信任感和对创作成果的拥有感；自适应的主动系统能增强协作效果。此外，本文提取了24条设计考虑，强调了用户思维外化和系统社交存在与透明度的重要性。

Conclusion: 尽管共创系统取得了进展，但仍存在重要空白，如对早期创造阶段（如问题澄清）的支持不足，以及用户适应AI系统的挑战。未来的研究需要进一步解决这些问题。

Abstract: The co creativity community is making significant progress in developing more
sophisticated and tailored systems to support and enhance human creativity.
Design considerations from prior work can serve as a valuable and efficient
foundation for future systems. To support this effort, we conducted a
systematic literature review of 62 papers on co-creative systems. These papers
cover a diverse range of applications, including visual arts, design, and
writing, where the AI acts not just as a tool but as an active collaborator in
the creative process. From this review, we identified several key dimensions
relevant to system design: phase of the creative process, creative task,
proactive behavior of the system, user control, system embodiment, and AI model
type. Our findings suggest that systems offering high user control lead to
greater satisfaction, trust, and a stronger sense of ownership over creative
outcomes. Furthermore, proactive systems, when adaptive and context sensitive,
can enhance collaboration. We also extracted 24 design considerations,
highlighting the value of encouraging users to externalize their thoughts and
of increasing the system's social presence and transparency to foster trust.
Despite recent advancements, important gaps remain, such as limited support for
early creative phases like problem clarification, and challenges related to
user adaptation to AI systems.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [191] [Agile Management for Machine Learning: A Systematic Mapping Study](https://arxiv.org/abs/2506.20759)
*Lucas Romao,Hugo Villamizar,Romeu Oliveira,Silvio Alonso,Marcos Kalinowski*

Main category: cs.SE

TL;DR: 该研究通过系统映射研究分析了机器学习（ML）支持系统中敏捷管理的现状，识别了8个关键主题框架，并指出ML任务工作量估计是主要挑战。


<details>
  <summary>Details</summary>
Motivation: 机器学习（ML）支持系统的动态特性对传统项目管理提出了挑战，敏捷方法因其灵活性被认为适合应对这种动态性，但如何有效应用尚不明确。

Method: 采用混合搜索策略进行系统映射研究，结合数据库检索和前后向雪球迭代法。

Result: 研究识别了27篇相关论文，归纳出8个框架和关键主题，如迭代灵活性和最小可行模型，并发现ML任务工作量估计是主要挑战。

Conclusion: 研究总结了ML支持系统中敏捷管理的现状和未解决问题，指出需要更多实证评估来验证现有成果。

Abstract: [Context] Machine learning (ML)-enabled systems are present in our society,
driving significant digital transformations. The dynamic nature of ML
development, characterized by experimental cycles and rapid changes in data,
poses challenges to traditional project management. Agile methods, with their
flexibility and incremental delivery, seem well-suited to address this
dynamism. However, it is unclear how to effectively apply these methods in the
context of ML-enabled systems, where challenges require tailored approaches.
[Goal] Our goal is to outline the state of the art in agile management for
ML-enabled systems. [Method] We conducted a systematic mapping study using a
hybrid search strategy that combines database searches with backward and
forward snowballing iterations. [Results] Our study identified 27 papers
published between 2008 and 2024. From these, we identified eight frameworks and
categorized recommendations and practices into eight key themes, such as
Iteration Flexibility, Innovative ML-specific Artifacts, and the Minimal Viable
Model. The main challenge identified across studies was accurate effort
estimation for ML-related tasks. [Conclusion] This study contributes by mapping
the state of the art and identifying open gaps in the field. While relevant
work exists, more robust empirical evaluation is still needed to validate these
contributions.

</details>


### [192] [Generating Reliable Adverse event Profiles for Health through Automated Integrated Data (GRAPH-AID): A Semi-Automated Ontology Building Approach](https://arxiv.org/abs/2506.20851)
*Srikar Reddy Gadusu,Larry Callahan,Samir Lababidi,Arunasri Nishtala,Sophia Healey,Hande McGinty*

Main category: cs.SE

TL;DR: 本文提出了一种利用Python和rdflib库简化Neo4j数据库与OWL本体集成的用户友好方法，应用于FDA不良事件报告系统数据，以支持药物安全监测。


<details>
  <summary>Details</summary>
Motivation: 随着数据和知识的迅速增长，采用系统化的本体生成方法变得至关重要。现有方法在Neo4j与OWL集成时需理解描述逻辑语法，对许多用户不友好，因此需要更易用的解决方案。

Method: 使用Python及其rdflib库开发脚本，自动生成类及其公理，实现Neo4j数据到本体的无缝集成，并以FDA不良事件数据为例进行展示。

Result: 成功开发了一种无需深入描述逻辑知识的本体生成方法，有效支持了药物不良事件数据的本体构建。

Conclusion: 该方法为快速增长的不良药物事件数据集的本体生成提供了实用解决方案，有助于提升药物安全监测和公共卫生决策。

Abstract: As data and knowledge expand rapidly, adopting systematic methodologies for
ontology generation has become crucial. With the daily increases in data
volumes and frequent content changes, the demand for databases to store and
retrieve information for the creation of knowledge graphs has become
increasingly urgent. The previously established Knowledge Acquisition and
Representation Methodology (KNARM) outlines a systematic approach to address
these challenges and create knowledge graphs. However, following this
methodology highlights the existing challenge of seamlessly integrating Neo4j
databases with the Web Ontology Language (OWL). Previous attempts to integrate
data from Neo4j into an ontology have been discussed, but these approaches
often require an understanding of description logics (DL) syntax, which may not
be familiar to many users. Thus, a more accessible method is necessary to
bridge this gap. This paper presents a user-friendly approach that utilizes
Python and its rdflib library to support ontology development. We showcase our
novel approach through a Neo4j database we created by integrating data from the
Food and Drug Administration (FDA) Adverse Event Reporting System (FAERS)
database. Using this dataset, we developed a Python script that automatically
generates the required classes and their axioms, facilitating a smoother
integration process. This approach offers a practical solution to the
challenges of ontology generation in the context of rapidly growing adverse
drug event datasets, supporting improved drug safety monitoring and public
health decision-making.

</details>


### [193] [Engineering RAG Systems for Real-World Applications: Design, Development, and Evaluation](https://arxiv.org/abs/2506.20869)
*Md Toufique Hasan,Muhammad Waseem,Kai-Kristian Kemell,Ayman Asad Khan,Mika Saari,Pekka Abrahamsson*

Main category: cs.SE

TL;DR: 本文介绍了五个基于检索增强生成（RAG）的领域专用应用，通过用户评估总结了12条关键经验教训。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏基于真实用例的RAG系统实证研究，本文旨在填补这一空白，探讨RAG在实际应用中的表现和挑战。

Method: 开发了五个领域专用RAG应用，结合多语言OCR、语义检索和领域适配LLM，通过100名用户的网络评估从六个维度进行测试。

Result: 用户评估显示RAG系统在易用性、相关性等方面表现良好，同时总结了影响可靠性和可用性的技术、操作及伦理挑战。

Conclusion: RAG系统在实际应用中具有潜力，但需解决技术、操作和伦理问题以提高可靠性和用户体验。

Abstract: Retrieval-Augmented Generation (RAG) systems are emerging as a key approach
for grounding Large Language Models (LLMs) in external knowledge, addressing
limitations in factual accuracy and contextual relevance. However, there is a
lack of empirical studies that report on the development of RAG-based
implementations grounded in real-world use cases, evaluated through general
user involvement, and accompanied by systematic documentation of lessons
learned. This paper presents five domain-specific RAG applications developed
for real-world scenarios across governance, cybersecurity, agriculture,
industrial research, and medical diagnostics. Each system incorporates
multilingual OCR, semantic retrieval via vector embeddings, and domain-adapted
LLMs, deployed through local servers or cloud APIs to meet distinct user needs.
A web-based evaluation involving a total of 100 participants assessed the
systems across six dimensions: (i) Ease of Use, (ii) Relevance, (iii)
Transparency, (iv) Responsiveness, (v) Accuracy, and (vi) Likelihood of
Recommendation. Based on user feedback and our development experience, we
documented twelve key lessons learned, highlighting technical, operational, and
ethical challenges affecting the reliability and usability of RAG systems in
practice.

</details>


### [194] [Complex Model Transformations by Reinforcement Learning with Uncertain Human Guidance](https://arxiv.org/abs/2506.20883)
*Kyanna Dagenais,Istvan David*

Main category: cs.SE

TL;DR: 论文提出了一种结合人类指导和强化学习的方法，用于开发复杂的模型转换序列，即使人类建议存在不确定性，也能显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 模型驱动工程中，复杂模型转换的开发既容易出错又难以手动实现，而强化学习在复杂问题中表现不佳，需要人类指导来提升效率。

Method: 通过将用户定义的模型转换映射到强化学习原语，并作为强化学习程序执行，以寻找最优模型转换序列。

Result: 评估表明，即使人类建议存在不确定性，也能显著提高强化学习性能，并更高效地开发复杂模型转换。

Conclusion: 该方法在人类建议的确定性和及时性之间取得平衡，为强化学习驱动的人机协同工程方法迈出了一步。

Abstract: Model-driven engineering problems often require complex model transformations
(MTs), i.e., MTs that are chained in extensive sequences. Pertinent examples of
such problems include model synchronization, automated model repair, and design
space exploration. Manually developing complex MTs is an error-prone and often
infeasible process. Reinforcement learning (RL) is an apt way to alleviate
these issues. In RL, an autonomous agent explores the state space through trial
and error to identify beneficial sequences of actions, such as MTs. However, RL
methods exhibit performance issues in complex problems. In these situations,
human guidance can be of high utility. In this paper, we present an approach
and technical framework for developing complex MT sequences through RL, guided
by potentially uncertain human advice. Our framework allows user-defined MTs to
be mapped onto RL primitives, and executes them as RL programs to find optimal
MT sequences. Our evaluation shows that human guidance, even if uncertain,
substantially improves RL performance, and results in more efficient
development of complex MTs. Through a trade-off between the certainty and
timeliness of human advice, our method takes a step towards RL-driven
human-in-the-loop engineering methods.

</details>


### [195] [How Good Are Synthetic Requirements ? Evaluating LLM-Generated Datasets for AI4RE](https://arxiv.org/abs/2506.21138)
*Abdelkarim El-Hajjami,Camille Salinesi*

Main category: cs.SE

TL;DR: 论文提出Synthline v1方法，通过改进的生成策略和优化技术生成高质量需求数据，证明合成数据在特定任务上可超越人工数据。


<details>
  <summary>Details</summary>
Motivation: 公开标记的需求数据集稀缺阻碍了AI在需求工程中的应用，现有合成数据生成方法缺乏系统性的质量控制。

Method: 采用增强的产品线方法Synthline v1，结合多样本提示、自动提示优化(PACE)和生成后筛选技术，评估四种分类任务下的数据质量。

Result: 多样本提示使F1值提升6-44分；PACE优化使功能分类提升32.5分但降低其他任务性能；合成数据在安全(+7.8)和缺陷分类(+15.4)任务上超越人工数据。

Conclusion: 系统性合成生成可缓解数据集短缺问题，为AI4RE提供实用路径，部分场景下合成数据质量优于人工数据。

Abstract: The shortage of publicly available, labeled requirements datasets remains a
major barrier to advancing Artificial Intelligence for Requirements Engineering
(AI4RE). While Large Language Models offer promising capabilities for synthetic
data generation, systematic approaches to control and optimize the quality of
generated requirements remain underexplored. This paper presents Synthline v1,
an enhanced Product Line approach for generating synthetic requirements data
that extends our earlier v0 version with advanced generation strategies and
curation techniques. We investigate four research questions assessing how
prompting strategies, automated prompt optimization, and post-generation
curation affect data quality across four classification tasks: defect
detection, functional vs. non-functional, quality vs. non-quality, and security
vs. non-security. Our evaluation shows that multi-sample prompting
significantly boosts both utility and diversity over single-sample generation,
with F1-score gains from 6 to 44 points. The use of PACE (Prompt Actor-Critic
Editing) for automated prompt optimization yields task-dependent results,
greatly improving functional classification (+32.5 points) but reducing
performance on others. Interestingly, similarity-based curation improves
diversity but often harms classification performance, indicating that some
redundancy may help ML models. Most importantly, our results show that
synthetic requirements can match or outperform human-authored ones for specific
tasks, with synthetic data surpassing human data for security (+7.8 points) and
defect classification (+15.4 points). These findings offer practical insights
for AI4RE and chart a viable path to mitigating dataset scarcity through
systematic synthetic generation.

</details>


### [196] [$T^3$: Multi-level Tree-based Automatic Program Repair with Large Language Models](https://arxiv.org/abs/2506.21211)
*Quanming Liu,Xupeng Bu,Zhichao Yan,Ru Li*

Main category: cs.SE

TL;DR: 该论文提出了一种名为$T^3$的创新框架，结合大语言模型的强大推理能力和树搜索技术，有效提升了自动程序修复（APR）任务中候选修复方案的生成精度。


<details>
  <summary>Details</summary>
Motivation: 近年来，大语言模型（LLMs）和思维链（CoT）技术的显著进步极大增强了模型的推理能力。然而，由于自动程序修复（APR）领域需要复杂的逻辑和多步推理能力，CoT技术在该领域的应用仍显不足。

Method: 本研究系统评估了几种常见CoT技术在APR任务中的表现，并提出了一种创新框架$T^3$，该框架将LLMs的强大推理能力与树搜索相结合。

Result: $T^3$有效提高了生成候选修复方案的精度，并为APR任务中的样本选择和修复策略优化提供了有价值的指导。

Conclusion: $T^3$为高效实现自动化调试建立了稳健的框架，推动了自动程序修复技术的发展。

Abstract: Automatic Program Repair (APR) is a core technology in software development
and maintenance, with aims to enable automated defect repair with minimal human
intervention. In recent years, the substantial advancements in Large Language
Models (LLMs) and the Chain-of-Thought (CoT) techniques have significantly
enhanced the reasoning capabilities of these models. However, due to the
complex logic and multi-step reasoning ability needed, the application of CoT
techniques in the APR domain remains insufficient. This study systematically
evaluates the performance of several common CoT techniques in APR tasks and
proposes an innovative framework $T^3$, which integrates the powerful reasoning
capabilities of LLMs with tree search, effectively improving the precision of
generating candidate repair solutions. Furthermore, $T^3$ provides valuable
guidance for optimizing sample selection and repair strategies in APR tasks,
establishing a robust framework for achieving efficient automated debugging.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [197] [Parallels Between VLA Model Post-Training and Human Motor Learning: Progress, Challenges, and Trends](https://arxiv.org/abs/2506.20966)
*Tian-Yu Xiang,Ao-Qun Jin,Xiao-Hu Zhou,Mei-Jiang Gui,Xiao-Liang Xie,Shi-Qi Liu,Shuang-Yi Wang,Sheng-Bin Duan,Fu-Chao Xie,Wen-Kai Wang,Si-Cheng Wang,Ling-Yun Li,Tian Tu,Zeng-Guang Hou*

Main category: cs.RO

TL;DR: 该论文从人类运动学习的角度，综述了视觉-语言-动作（VLA）模型的后训练策略，提出了一个结构化分类法，并指出了未来研究的关键挑战和趋势。


<details>
  <summary>Details</summary>
Motivation: VLA模型在机器人操作任务中展现出良好的泛化能力，但在高精度和高准确性要求的应用中表现不足，需要通过后训练来提升模型性能。

Method: 论文通过人类运动学习的视角，从环境、体现和任务三个维度，提出了一个结构化分类法，包括增强环境感知、提升体现意识、深化任务理解和多组件集成四个方面。

Result: 论文提出了一个概念框架，为VLA模型的后训练方法提供了全面的综述，并为未来的研究提供了实用的见解。

Conclusion: 该工作不仅从人类运动学习的角度综述了当前VLA模型的后训练方法，还为VLA模型的开发提供了实践指导，并建立了未来研究的概念框架。

Abstract: Vision-language-action (VLA) models extend vision-language models (VLM) by
integrating action generation modules for robotic manipulation. Leveraging
strengths of VLM in vision perception and instruction understanding, VLA models
exhibit promising generalization across diverse manipulation tasks. However,
applications demanding high precision and accuracy reveal performance gaps
without further adaptation. Evidence from multiple domains highlights the
critical role of post-training to align foundational models with downstream
applications, spurring extensive research on post-training VLA models. VLA
model post-training aims to address the challenge of improving an embodiment's
ability to interact with the environment for the given tasks, analogous to the
process of humans motor skills acquisition. Accordingly, this paper reviews
post-training strategies for VLA models through the lens of human motor
learning, focusing on three dimensions: environments, embodiments, and tasks. A
structured taxonomy is introduced aligned with human learning mechanisms: (1)
enhancing environmental perception, (2) improving embodiment awareness, (3)
deepening task comprehension, and (4) multi-component integration. Finally, key
challenges and trends in post-training VLA models are identified, establishing
a conceptual framework to guide future research. This work delivers both a
comprehensive overview of current VLA model post-training methods from a human
motor learning perspective and practical insights for VLA model development.
(Project website: https://github.com/AoqunJin/Awesome-VLA-Post-Training)

</details>


### [198] [V2X-REALM: Vision-Language Model-Based Robust End-to-End Cooperative Autonomous Driving with Adaptive Long-Tail Modeling](https://arxiv.org/abs/2506.21041)
*Junwei You,Pei Li,Zhuoyu Jiang,Zilin Huang,Rui Gan,Haotian Shi,Bin Ran*

Main category: cs.RO

TL;DR: 提出V2X-REALM框架，通过视觉语言模型和自适应多模态学习解决自动驾驶在长尾场景下的鲁棒性问题。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶在罕见、多样且视觉退化的长尾场景中面临规划和决策的挑战，特别是在协同感知和推理的复杂环境中。

Method: V2X-REALM框架包含三个核心创新：长尾场景生成与评估管道、门控多场景自适应注意力模块和多任务场景感知对比学习目标。

Result: 实验表明，V2X-REALM在复杂驾驶条件下的鲁棒性、语义推理、安全性和规划准确性上显著优于现有基线。

Conclusion: V2X-REALM提升了端到端协同自动驾驶的可扩展性，为长尾场景下的鲁棒驾驶提供了有效解决方案。

Abstract: Ensuring robust planning and decision-making under rare, diverse, and
visually degraded long-tail scenarios remains a fundamental challenge for
autonomous driving in urban environments. This issue becomes more critical in
cooperative settings, where vehicles and infrastructure jointly perceive and
reason across complex environments. To address this challenge, we propose
V2X-REALM, a vision-language model (VLM)-based framework with adaptive
multimodal learning for robust cooperative autonomous driving under long-tail
scenarios. V2X-REALM introduces three core innovations: (i) a prompt-driven
long-tail scenario generation and evaluation pipeline that leverages foundation
models to synthesize realistic long-tail conditions such as snow and fog across
vehicle- and infrastructure-side views, enriching training diversity
efficiently; (ii) a gated multi-scenario adaptive attention module that
modulates the visual stream using scenario priors to recalibrate ambiguous or
corrupted features; and (iii) a multi-task scenario-aware contrastive learning
objective that improves multimodal alignment and promotes cross-scenario
feature separability. Extensive experiments demonstrate that V2X-REALM
significantly outperforms existing baselines in robustness, semantic reasoning,
safety, and planning accuracy under complex, challenging driving conditions,
advancing the scalability of end-to-end cooperative autonomous driving.

</details>


### [199] [WorldVLA: Towards Autoregressive Action World Model](https://arxiv.org/abs/2506.21539)
*Jun Cen,Chaohui Yu,Hangjie Yuan,Yuming Jiang,Siteng Huang,Jiayan Guo,Xin Li,Yibing Song,Hao Luo,Fan Wang,Deli Zhao,Hao Chen*

Main category: cs.RO

TL;DR: WorldVLA是一个结合视觉-语言-动作模型与世界模型的统一框架，通过自回归方式预测未来图像和生成动作，提出注意力掩码策略以改善动作序列生成的性能。


<details>
  <summary>Details</summary>
Motivation: 论文旨在通过整合视觉-语言-动作模型与世界模型，提升对环境的物理理解，从而改进动作生成和图像理解。

Method: WorldVLA框架结合了动作和图像理解，通过自回归方式预测未来图像并生成动作，同时提出注意力掩码策略以减少动作序列生成中的误差传播。

Result: 实验表明，WorldVLA在动作和世界模型任务上表现优于独立模型，注意力掩码策略显著提升了动作块生成任务的性能。

Conclusion: WorldVLA展示了世界模型与动作模型之间的相互增强效应，提出的注意力掩码策略有效缓解了自回归动作生成中的性能下降问题。

Abstract: We present WorldVLA, an autoregressive action world model that unifies action
and image understanding and generation. Our WorldVLA intergrates
Vision-Language-Action (VLA) model and world model in one single framework. The
world model predicts future images by leveraging both action and image
understanding, with the purpose of learning the underlying physics of the
environment to improve action generation. Meanwhile, the action model generates
the subsequent actions based on image observations, aiding in visual
understanding and in turn helps visual generation of the world model. We
demonstrate that WorldVLA outperforms standalone action and world models,
highlighting the mutual enhancement between the world model and the action
model. In addition, we find that the performance of the action model
deteriorates when generating sequences of actions in an autoregressive manner.
This phenomenon can be attributed to the model's limited generalization
capability for action prediction, leading to the propagation of errors from
earlier actions to subsequent ones. To address this issue, we propose an
attention mask strategy that selectively masks prior actions during the
generation of the current action, which shows significant performance
improvement in the action chunk generation task.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [200] [Transformer-Based Spatial-Temporal Counterfactual Outcomes Estimation](https://arxiv.org/abs/2506.21154)
*He Li,Haoang Chi,Mingyu Liu,Wanrong Huang,Liyang Xu,Wenjing Yang*

Main category: stat.ME

TL;DR: 该论文提出了一种基于Transformer的新框架，用于估计具有时空属性的反事实结果，相比传统统计方法具有更强的估计能力和泛化性。


<details>
  <summary>Details</summary>
Motivation: 现实世界天然具有时间和空间维度，而现有基于经典统计模型的反事实结果估计方法在性能和泛化性上存在局限。

Method: 论文提出了一种基于Transformer的新框架，用于估计具有时空属性的反事实结果，并在温和假设下证明了估计量的一致性和渐近正态性。

Result: 仿真实验表明，该估计器比基线方法具有更强的估计能力；真实数据实验得出了关于哥伦比亚冲突对森林流失因果效应的有价值结论。

Conclusion: 该论文提出的基于Transformer的框架在反事实结果估计中表现出优越性能，并通过实验验证了其有效性。

Abstract: The real world naturally has dimensions of time and space. Therefore,
estimating the counterfactual outcomes with spatial-temporal attributes is a
crucial problem. However, previous methods are based on classical statistical
models, which still have limitations in performance and generalization. This
paper proposes a novel framework for estimating counterfactual outcomes with
spatial-temporal attributes using the Transformer, exhibiting stronger
estimation ability. Under mild assumptions, the proposed estimator within this
framework is consistent and asymptotically normal. To validate the
effectiveness of our approach, we conduct simulation experiments and real data
experiments. Simulation experiments show that our estimator has a stronger
estimation capability than baseline methods. Real data experiments provide a
valuable conclusion to the causal effect of conflicts on forest loss in
Colombia. The source code is available at
https://github.com/lihe-maxsize/DeppSTCI_Release_Version-master.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [201] [ClusterRCA: Network Failure Diagnosis in HPC Systems Using Multimodal Data](https://arxiv.org/abs/2506.20673)
*Yongqian Sun,Xijie Pan,Xiao Xiong,Lei Tao,Jiaju Wang,Shenglin Zhang,Yuan Yuan,Yuqi Li,Kunlin Jian*

Main category: cs.DC

TL;DR: ClusterRCA：一种利用多模态数据定位HPC系统网络故障节点和类型的新框架。


<details>
  <summary>Details</summary>
Motivation: 高性能计算（HPC）系统中的网络故障诊断具有挑战性且至关重要。现有方法由于数据异构性和准确性不足，无法直接应用于HPC场景。

Method: ClusterRCA通过从拓扑连接的网络接口控制器（NIC）对中提取特征，分析HPC系统中的多模态数据，结合基于分类器和基于图的方法，构建故障图并通过定制随机游走定位根本原因。

Result: 在顶级HPC设备供应商收集的数据集上，ClusterRCA在诊断HPC系统网络故障方面表现出高准确性，并在不同应用场景中保持稳健性能。

Conclusion: ClusterRCA能有效定位故障节点并确定故障类型，适用于HPC系统的网络故障诊断。

Abstract: Network failure diagnosis is challenging yet critical for high-performance
computing (HPC) systems. Existing methods cannot be directly applied to HPC
scenarios due to data heterogeneity and lack of accuracy. This paper proposes a
novel framework, called ClusterRCA, to localize culprit nodes and determine
failure types by leveraging multimodal data. ClusterRCA extracts features from
topologically connected network interface controller (NIC) pairs to analyze the
diverse, multimodal data in HPC systems. To accurately localize culprit nodes
and determine failure types, ClusterRCA combines classifier-based and
graph-based approaches. A failure graph is constructed based on the output of
the state classifier, and then it performs a customized random walk on the
graph to localize the root cause. Experiments on datasets collected by a
top-tier global HPC device vendor show ClusterRCA achieves high accuracy in
diagnosing network failure for HPC systems. ClusterRCA also maintains robust
performance across different application scenarios.

</details>


### [202] [Utility-Driven Speculative Decoding for Mixture-of-Experts](https://arxiv.org/abs/2506.20675)
*Anish Saxena,Po-An Tsai,Hritvik Taneja,Aamer Jaleel,Moinuddin Qureshi*

Main category: cs.DC

TL;DR: GPU内存带宽是大型语言模型(LLM)低延迟推理的主要瓶颈。传统密集LLM中，推测解码通过轻量级草案器并行验证K个令牌提高吞吐量，但对新兴的混合专家模型(MoE)无效，因其会激活更多权重导致性能下降。论文提出Cascade框架，动态选择启用推测并调整K值，使MoE推理速度提升7-14%。


<details>
  <summary>Details</summary>
Motivation: 当前推测解码技术在密集LLM中广泛应用，但在MoE模型中因权重激活模式差异导致性能下降（甚至产生1.5倍减速）。需要开发能自适应MoE特性的动态推测解码方案。

Method: 提出Cascade框架：1) 使用'推测效用'指标（令牌增益/验证成本比）动态评估；2) 通过测试阶段和设定阶段周期性决策；3) 根据效用值自动禁用推测或选择最优K值。

Result: 在vLLM中实现Cascade，测试5种主流MoE模型：将最大减速控制在5%（原1.5倍），相比固定K值方案提升7-14%吞吐量，使推测解码在MoE中具备实用性。

Conclusion: 通过效用驱动的动态推测机制，Cascade首次将推测解码成功应用于MoE模型，解决了权重激活带来的数据移动瓶颈，为MoE推理加速提供了新范式。

Abstract: GPU memory bandwidth is the main bottleneck for low-latency Large Language
Model (LLM) inference. Speculative decoding leverages idle GPU compute by using
a lightweight drafter to propose K tokens, which the LLM verifies in parallel,
boosting token throughput. In conventional dense LLMs, all model weights are
fetched each iteration, so speculation adds no latency overhead. Emerging
Mixture of Experts (MoE) models activate only a subset of weights per token,
greatly reducing data movement. However, we show that speculation is
ineffective for MoEs: draft tokens collectively activate more weights,
increasing data movement and verification time by 2-3x. When token throughput
gains fail to offset this overhead, speculation causes slowdowns up to 1.5x,
making it infeasible. Even when useful, the optimal K varies by task, model,
and even between requests and iterations. Thus, despite widespread use in dense
LLMs, speculation remains impractical in leading MoEs.
  We present Cascade, a utility-driven framework that selectively enables
speculation to avoid slowdowns and dynamically tunes K to accelerate MoE
serving. Cascade uses a lightweight metric, speculation utility, the ratio of
token gains to verification cost, which shows iteration-level locality,
enabling periodic decisions via short test and longer set phases. For each
request, Cascade disables speculation if utility drops below one during
testing, and when utility exceeds one, tests multiple K-values to choose the
utility-maximizing K for the set phase. We implement Cascade in vLLM and
evaluate it on five popular MoEs with workloads spanning code, math,
extraction, and mixed tasks. Cascade limits slowdown to 5% (vs. 1.5x) and
improves throughput by 7-14% over static K, making speculative decoding
practical for MoEs.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [203] [Global and Local Contrastive Learning for Joint Representations from Cardiac MRI and ECG](https://arxiv.org/abs/2506.20683)
*Alexander Selivanov,Philip Müller,Özgün Turgut,Nil Stolt-Ansó,Daniel Rückert*

Main category: eess.IV

TL;DR: 提出PTACL框架，通过多模态对比学习增强心电图表征，整合心脏磁共振的时空信息，提升非侵入性心脏诊断能力。


<details>
  <summary>Details</summary>
Motivation: 心电图（ECG）虽广泛用于检测心脏电活动异常，但无法直接测量心室容积和射血分数等功能参数。心脏磁共振（CMR）虽为金标准，但昂贵且不易获取。因此，需要一种方法结合两者的优势。

Method: PTACL（患者和时间对齐对比学习）框架，通过全局患者级对比损失和局部时间级对比损失，整合ECG和CMR的时空信息，无需引入新的可学习权重。

Result: 在UK Biobank的27,951名受试者数据上，PTACL在检索相似心脏表型患者和预测CMR衍生心脏功能参数方面优于基线方法。

Conclusion: PTACL能够增强心电图的诊断信息，为非侵入性心脏诊断提供新潜力。

Abstract: An electrocardiogram (ECG) is a widely used, cost-effective tool for
detecting electrical abnormalities in the heart. However, it cannot directly
measure functional parameters, such as ventricular volumes and ejection
fraction, which are crucial for assessing cardiac function. Cardiac magnetic
resonance (CMR) is the gold standard for these measurements, providing detailed
structural and functional insights, but is expensive and less accessible. To
bridge this gap, we propose PTACL (Patient and Temporal Alignment Contrastive
Learning), a multimodal contrastive learning framework that enhances ECG
representations by integrating spatio-temporal information from CMR. PTACL uses
global patient-level contrastive loss and local temporal-level contrastive
loss. The global loss aligns patient-level representations by pulling ECG and
CMR embeddings from the same patient closer together, while pushing apart
embeddings from different patients. Local loss enforces fine-grained temporal
alignment within each patient by contrasting encoded ECG segments with
corresponding encoded CMR frames. This approach enriches ECG representations
with diagnostic information beyond electrical activity and transfers more
insights between modalities than global alignment alone, all without
introducing new learnable weights. We evaluate PTACL on paired ECG-CMR data
from 27,951 subjects in the UK Biobank. Compared to baseline approaches, PTACL
achieves better performance in two clinically relevant tasks: (1) retrieving
patients with similar cardiac phenotypes and (2) predicting CMR-derived cardiac
function parameters, such as ventricular volumes and ejection fraction. Our
results highlight the potential of PTACL to enhance non-invasive cardiac
diagnostics using ECG. The code is available at:
https://github.com/alsalivan/ecgcmr

</details>


### [204] [U-R-VEDA: Integrating UNET, Residual Links, Edge and Dual Attention, and Vision Transformer for Accurate Semantic Segmentation of CMRs](https://arxiv.org/abs/2506.20689)
*Racheal Mukisa,Arvind K. Bansal*

Main category: eess.IV

TL;DR: 提出了一种基于深度学习的U-R-Veda模型，用于心脏磁共振图像的精确语义分割，结合了卷积变换、视觉变换器、残差连接和注意力机制，显著提高了分割精度。


<details>
  <summary>Details</summary>
Motivation: 自动化精确的心脏图像分割是心脏疾病量化与自动化诊断的必要初始步骤，现有模型在右心室和左心室心肌的描绘上仍有提升空间。

Method: U-R-Veda模型整合了卷积变换、视觉变换器、残差连接、通道注意力、空间注意力及基于边缘检测的跳跃连接，通过深度嵌入注意力机制减少信息损失。

Result: 模型在DSC指标上达到95.2%的平均准确率，尤其在右心室和左心室心肌的分割上优于其他模型。

Conclusion: U-R-Veda模型显著提升了心脏磁共振图像的语义分割效果，为医学图像分析提供了更精确的工具。

Abstract: Artificial intelligence, including deep learning models, will play a
transformative role in automated medical image analysis for the diagnosis of
cardiac disorders and their management. Automated accurate delineation of
cardiac images is the first necessary initial step for the quantification and
automated diagnosis of cardiac disorders. In this paper, we propose a deep
learning based enhanced UNet model, U-R-Veda, which integrates convolution
transformations, vision transformer, residual links, channel-attention, and
spatial attention, together with edge-detection based skip-connections for an
accurate fully-automated semantic segmentation of cardiac magnetic resonance
(CMR) images. The model extracts local-features and their interrelationships
using a stack of combination convolution blocks, with embedded channel and
spatial attention in the convolution block, and vision transformers. Deep
embedding of channel and spatial attention in the convolution block identifies
important features and their spatial localization. The combined edge
information with channel and spatial attention as skip connection reduces
information-loss during convolution transformations. The overall model
significantly improves the semantic segmentation of CMR images necessary for
improved medical image analysis. An algorithm for the dual attention module
(channel and spatial attention) has been presented. Performance results show
that U-R-Veda achieves an average accuracy of 95.2%, based on DSC metrics. The
model outperforms the accuracy attained by other models, based on DSC and HD
metrics, especially for the delineation of right-ventricle and
left-ventricle-myocardium.

</details>


### [205] [A Novel Framework for Integrating 3D Ultrasound into Percutaneous Liver Tumour Ablation](https://arxiv.org/abs/2506.21162)
*Shuwei Xing,Derek W. Cool,David Tessier,Elvis C. S. Chen,Terry M. Peters,Aaron Fenster*

Main category: eess.IV

TL;DR: 该论文提出了一种将3D超声整合到肝脏肿瘤消融治疗中的新框架，通过2D超声-CT/MRI配准方法降低复杂性，并展示了有效的配准结果。


<details>
  <summary>Details</summary>
Motivation: 3D超声在肝脏肿瘤消融治疗中具有显著优势，但由于超声图像中肿瘤识别的挑战，其广泛应用受到限制。本研究旨在解决这一问题，推动3D超声在临床治疗中的应用。

Method: 提出了一种新颖的框架，包括一种临床可行的2D超声-CT/MRI配准方法，利用3D超声作为中介降低配准复杂性，并引入直观的多模态图像可视化技术进行验证。

Result: 2D超声-CT/MRI配准的标记距离误差约为2-4毫米，每对图像处理时间为0.22秒。非刚性配准比刚性配准平均对齐误差降低了约40%。

Conclusion: 该集成框架提升了3D超声在肝脏肿瘤消融治疗中的能力，展示了其在临床干预中扩展治疗作用的潜力。

Abstract: 3D ultrasound (US) imaging has shown significant benefits in enhancing the
outcomes of percutaneous liver tumour ablation. Its clinical integration is
crucial for transitioning 3D US into the therapeutic domain. However,
challenges of tumour identification in US images continue to hinder its broader
adoption. In this work, we propose a novel framework for integrating 3D US into
the standard ablation workflow. We present a key component, a clinically viable
2D US-CT/MRI registration approach, leveraging 3D US as an intermediary to
reduce registration complexity. To facilitate efficient verification of the
registration workflow, we also propose an intuitive multimodal image
visualization technique. In our study, 2D US-CT/MRI registration achieved a
landmark distance error of approximately 2-4 mm with a runtime of 0.22s per
image pair. Additionally, non-rigid registration reduced the mean alignment
error by approximately 40% compared to rigid registration. Results demonstrated
the efficacy of the proposed 2D US-CT/MRI registration workflow. Our
integration framework advanced the capabilities of 3D US imaging in improving
percutaneous tumour ablation, demonstrating the potential to expand the
therapeutic role of 3D US in clinical interventions.

</details>


### [206] [Exploring the Design Space of 3D MLLMs for CT Report Generation](https://arxiv.org/abs/2506.21535)
*Mohammed Baharoon,Jun Ma,Congyu Fang,Augustin Toma,Bo Wang*

Main category: eess.IV

TL;DR: 该论文系统研究了3D多模态大语言模型在放射学报告生成中的设计空间，并提出了两种基于知识的报告增强方法，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 研究多模态大语言模型（MLLMs）在自动化放射学报告生成（RRG）中的应用，特别是在3D CT报告生成中的设计空间和性能优化。

Method: 系统研究了3D MLLMs的设计空间，包括视觉输入表示、投影器、大语言模型（LLMs）和微调技术，并引入了两种基于知识的报告增强方法。

Result: 在MICCAI 2024 AMOS-MM挑战赛中取得了第二名，GREEN分数提升了10%。实验结果表明，RRG在相同训练协议下与LLM的规模无关，且更大的体积尺寸并不总是提升性能。

Conclusion: 3D MLLMs在放射学报告生成中具有潜力，设计空间和知识增强方法的优化可以显著提升性能，但需注意预训练体积尺寸的影响。

Abstract: Multimodal Large Language Models (MLLMs) have emerged as a promising way to
automate Radiology Report Generation (RRG). In this work, we systematically
investigate the design space of 3D MLLMs, including visual input
representation, projectors, Large Language Models (LLMs), and fine-tuning
techniques for 3D CT report generation. We also introduce two knowledge-based
report augmentation methods that improve performance on the GREEN score by up
to 10\%, achieving the 2nd place on the MICCAI 2024 AMOS-MM challenge. Our
results on the 1,687 cases from the AMOS-MM dataset show that RRG is largely
independent of the size of LLM under the same training protocol. We also show
that larger volume size does not always improve performance if the original ViT
was pre-trained on a smaller volume size. Lastly, we show that using a
segmentation mask along with the CT volume improves performance. The code is
publicly available at https://github.com/bowang-lab/AMOS-MM-Solution

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [207] [Evaluating PDE discovery methods for multiscale modeling of biological signals](https://arxiv.org/abs/2506.20694)
*Andréa Ducos,Audrey Denizot,Thomas Guyet,Hugues Berry*

Main category: q-bio.QM

TL;DR: 该论文提出了一种结合粒子模拟和偏微分方程发现的方法，用于从微观数据推断生物系统的宏观动力学特性，并在钙扩散实验中验证了五种先进方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 生物系统具有非线性、多尺度及部分未知的物理原理，使得其行为表征极具挑战性。研究旨在通过跨尺度机制解决这一难题。

Method: 结合基于粒子的模拟和偏微分方程（PDE）发现框架，在受控环境中进行初步实验，评估五种先进的PDE发现方法。

Result: 实验表明，部分方法能准确恢复扩散项，验证了PDE发现从微观数据捕捉宏观生物系统动力学的潜力。

Conclusion: PDE发现方法在跨尺度生物系统建模中展现出显著潜力，尤其在钙扩散等场景下能有效链接微观与宏观动力学。

Abstract: Biological systems are non-linear, include unobserved variables and the
physical principles that govern their dynamics are partly unknown. This makes
the characterization of their behavior very challenging. Notably, their
activity occurs on multiple interdependent spatial and temporal scales that
require linking mechanisms across scales. To address the challenge of bridging
gaps between scales, we leverage partial differential equations (PDE)
discovery. PDE discovery suggests meso-scale dynamics characteristics from
micro-scale data. In this article, we present our framework combining
particle-based simulations and PDE discovery and conduct preliminary
experiments to assess equation discovery in controlled settings. We evaluate
five state-of-the-art PDE discovery methods on particle-based simulations of
calcium diffusion in astrocytes. The performances of the methods are evaluated
on both the form of the discovered equation and the forecasted temporal
variations of calcium concentration. Our results show that several methods
accurately recover the diffusion term, highlighting the potential of PDE
discovery for capturing macroscopic dynamics in biological systems from
microscopic data.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [208] [DRAGON: Distributional Rewards Optimize Diffusion Generative Models](https://arxiv.org/abs/2504.15217)
*Yatong Bai,Jonah Casebeer,Somayeh Sojoudi,Nicholas J. Bryan*

Main category: cs.SD

TL;DR: DRAGON是一个灵活的框架，用于微调媒体生成模型，通过多样化的奖励函数优化生成结果，无需依赖人类偏好标注。


<details>
  <summary>Details</summary>
Motivation: 传统的人类反馈强化学习（RLHF）或直接偏好优化（DPO）方法在优化媒体生成模型时灵活性不足，DRAGON旨在提供更通用的奖励函数优化方案。

Method: DRAGON通过选择编码器和参考样本构建奖励函数，利用正负样本集的对比最大化奖励，支持跨模态和多类型奖励。

Result: 在20种不同奖励函数下，DRAGON平均胜率达81.45%，且基于范例集的奖励函数显著提升生成质量，人类评估胜率达60.95%。

Conclusion: DRAGON展示了通过设计多样化奖励函数优化生成模型的新途径，显著提升人类感知质量。

Abstract: We present Distributional RewArds for Generative OptimizatioN (DRAGON), a
versatile framework for fine-tuning media generation models towards a desired
outcome. Compared with traditional reinforcement learning with human feedback
(RLHF) or pairwise preference approaches such as direct preference optimization
(DPO), DRAGON is more flexible. It can optimize reward functions that evaluate
either individual examples or distributions of them, making it compatible with
a broad spectrum of instance-wise, instance-to-distribution, and
distribution-to-distribution rewards. Leveraging this versatility, we construct
novel reward functions by selecting an encoder and a set of reference examples
to create an exemplar distribution. When cross-modality encoders such as CLAP
are used, the reference examples may be of a different modality (e.g., text
versus audio). Then, DRAGON gathers online and on-policy generations, scores
them to construct a positive demonstration set and a negative set, and
leverages the contrast between the two sets to maximize the reward. For
evaluation, we fine-tune an audio-domain text-to-music diffusion model with 20
different reward functions, including a custom music aesthetics model, CLAP
score, Vendi diversity, and Frechet audio distance (FAD). We further compare
instance-wise (per-song) and full-dataset FAD settings while ablating multiple
FAD encoders and reference sets. Over all 20 target rewards, DRAGON achieves an
81.45% average win rate. Moreover, reward functions based on exemplar sets
indeed enhance generations and are comparable to model-based rewards. With an
appropriate exemplar set, DRAGON achieves a 60.95% human-voted music quality
win rate without training on human preference annotations. As such, DRAGON
exhibits a new approach to designing and optimizing reward functions for
improving human-perceived quality. Sound examples at
https://ml-dragon.github.io/web.

</details>


### [209] [Exploring Adapter Design Tradeoffs for Low Resource Music Generation](https://arxiv.org/abs/2506.21298)
*Atharva Mehta,Shivam Chauhan,Monojit Choudhury*

Main category: cs.SD

TL;DR: 该论文研究了在低资源音乐流派中，如何通过参数高效微调技术（PEFT）优化MusicGen和Mustango模型的适配器设计，以平衡计算资源与生成质量。


<details>
  <summary>Details</summary>
Motivation: 微调大规模音乐生成模型（如MusicGen和Mustango）通常需要大量计算资源。论文旨在探索如何通过适配器设计（如架构、位置和大小）在低资源音乐流派中实现高效微调。

Method: 论文通过分析MusicGen和Mustango模型在Hindustani古典音乐和土耳其Makam音乐上的不同适配器配置（如卷积基和Transformer基适配器），评估其性能和资源需求。

Result: 研究发现，卷积基适配器擅长捕捉局部音乐细节，而Transformer基适配器更擅长保持长程依赖关系。中等规模适配器（40M参数）在表达力和质量间达到最佳平衡。Mustango生成多样性更高但稳定性较差，而MusicGen训练更快且质量更高。

Conclusion: 适配器设计对音乐生成模型的微调效果有显著影响，需根据具体需求（如细节捕捉或长程依赖）选择合适配置。Mustango适合多样性要求高的场景，而MusicGen在效率和稳定性上表现更优。

Abstract: Fine-tuning large-scale music generation models, such as MusicGen and
Mustango, is a computationally expensive process, often requiring updates to
billions of parameters and, therefore, significant hardware resources.
Parameter-Efficient Fine-Tuning (PEFT) techniques, particularly adapter-based
methods, have emerged as a promising alternative, enabling adaptation with
minimal trainable parameters while preserving model performance. However, the
design choices for adapters, including their architecture, placement, and size,
are numerous, and it is unclear which of these combinations would produce
optimal adapters and why, for a given case of low-resource music genre. In this
paper, we attempt to answer this question by studying various adapter
configurations for two AI music models, MusicGen and Mustango, on two genres:
Hindustani Classical and Turkish Makam music.
  Our findings reveal distinct trade-offs: convolution-based adapters excel in
capturing fine-grained local musical details such as ornamentations and short
melodic phrases, while transformer-based adapters better preserve long-range
dependencies crucial for structured improvisation. Additionally, we analyze
computational resource requirements across different adapter scales,
demonstrating how mid-sized adapters (40M parameters) achieve an optimal
balance between expressivity and quality. Furthermore, we find that Mustango, a
diffusion-based model, generates more diverse outputs with better adherence to
the description in the input prompt while lacking in providing stability in
notes, rhythm alignment, and aesthetics. Also, it is computationally intensive
and requires significantly more time to train. In contrast, autoregressive
models like MusicGen offer faster training and are more efficient, and can
produce better quality output in comparison, but have slightly higher
redundancy in their generations.

</details>


### [210] [A Hierarchical Deep Learning Approach for Minority Instrument Detection](https://arxiv.org/abs/2506.21167)
*Dylan Sechet,Francesca Bugiotti,Matthieu Kowalski,Edouard d'Hérouville,Filip Langiewicz*

Main category: cs.SD

TL;DR: 该论文提出了一种基于Hornbostel-Sachs分类的层次化分类方法，用于识别音频片段中的乐器活动，并在MedleyDB数据集上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 音乐信息检索中，识别音频片段中的乐器活动对音乐分类和发现具有重要意义。然而，现有的深度学习方法主要关注数据充足的乐器类别，而忽略了细粒度乐器识别的挑战。

Method: 采用层次化分类策略，结合Hornbostel-Sachs分类体系，利用MedleyDB数据集的多样性，测试了新的层次化音乐预测模型。

Result: 研究表明，层次化分类方法能够在乐器级别注释有限的情况下，实现更可靠的粗粒度乐器检测。

Conclusion: 通过填补细粒度乐器识别与组别识别之间的空白，该研究为音乐信息检索领域的进一步进展奠定了基础。

Abstract: Identifying instrument activities within audio excerpts is vital in music
information retrieval, with significant implications for music cataloging and
discovery. Prior deep learning endeavors in musical instrument recognition have
predominantly emphasized instrument classes with ample data availability.
Recent studies have demonstrated the applicability of hierarchical
classification in detecting instrument activities in orchestral music, even
with limited fine-grained annotations at the instrument level. Based on the
Hornbostel-Sachs classification, such a hierarchical classification system is
evaluated using the MedleyDB dataset, renowned for its diversity and richness
concerning various instruments and music genres. This work presents various
strategies to integrate hierarchical structures into models and tests a new
class of models for hierarchical music prediction. This study showcases more
reliable coarse-level instrument detection by bridging the gap between detailed
instrument identification and group-level recognition, paving the way for
further advancements in this domain.

</details>


### [211] [Integrating Vehicle Acoustic Data for Enhanced Urban Traffic Management: A Study on Speed Classification in Suzhou](https://arxiv.org/abs/2506.21269)
*Pengfei Fan,Yuli Zhang,Xinheng Wang,Ruiyuan Jiang,Hankang Gu,Dongyao Jia,Shangbo Wang*

Main category: cs.SD

TL;DR: 该研究公开了苏州城市道路声学数据集（SZUR-Acoustic Dataset），并提出了一种双模态特征融合的深度卷积神经网络（BMCNN）来建模车辆噪声与行驶速度的耦合关系，用于智能城市交通管理系统中的实时噪声监测和速度估计。


<details>
  <summary>Details</summary>
Motivation: 研究旨在通过声学方法优化交通流量控制、减少路边噪声污染，并支持可持续城市规划。为此，需要一个公开的数据集和有效的算法来建模车辆噪声与速度的关系。

Method: 研究采用了自适应去噪和归一化策略进行预处理，并设计了一个双模态特征融合的深度卷积神经网络（BMCNN），通过并行分支提取MFCC和小波包能量特征，并利用跨模态注意力机制融合这些特征。

Result: BMCNN在SZUR-Acoustic数据集上的分类准确率为87.56%，在公开的IDMT-Traffic数据集上达到96.28%。消融实验和鲁棒性测试验证了各模块对性能提升和过拟合缓解的贡献。

Conclusion: 提出的基于声学的速度分类方法可集成到智能城市交通管理系统中，实现实时噪声监测和速度估计，从而优化交通流量控制、减少噪声污染，并支持可持续城市规划。

Abstract: This study presents and publicly releases the Suzhou Urban Road Acoustic
Dataset (SZUR-Acoustic Dataset), which is accompanied by comprehensive
data-acquisition protocols and annotation guidelines to ensure transparency and
reproducibility of the experimental workflow. To model the coupling between
vehicular noise and driving speed, we propose a bimodal-feature-fusion deep
convolutional neural network (BMCNN). During preprocessing, an adaptive
denoising and normalization strategy is applied to suppress environmental
background interference; in the network architecture, parallel branches extract
Mel-frequency cepstral coefficients (MFCCs) and wavelet-packet energy features,
which are subsequently fused via a cross-modal attention mechanism in the
intermediate feature space to fully exploit time-frequency information.
Experimental results demonstrate that BMCNN achieves a classification accuracy
of 87.56% on the SZUR-Acoustic Dataset and 96.28% on the public IDMT-Traffic
dataset. Ablation studies and robustness tests on the Suzhou dataset further
validate the contributions of each module to performance improvement and
overfitting mitigation. The proposed acoustics-based speed classification
method can be integrated into smart-city traffic management systems for
real-time noise monitoring and speed estimation, thereby optimizing traffic
flow control, reducing roadside noise pollution, and supporting sustainable
urban planning.

</details>


### [212] [SmoothSinger: A Conditional Diffusion Model for Singing Voice Synthesis with Multi-Resolution Architecture](https://arxiv.org/abs/2506.21478)
*Kehan Sui,Jinxu Xiang,Fang Jin*

Main category: cs.SD

TL;DR: SmoothSinger是一种基于条件扩散模型的歌唱语音合成方法，通过统一框架直接优化低质量音频，减少失真，提升自然度。


<details>
  <summary>Details</summary>
Motivation: 当前基于扩散模型的歌唱语音合成方法因复杂的声学和音乐特性，常产生降低自然度的伪影，且依赖声码器作为最终阶段引入失真。

Method: SmoothSinger采用参考引导的双分支架构，利用低质量音频引导去噪过程，并增强U-Net以捕捉音高轮廓和长期频谱依赖。

Result: 在Opencpop数据集上的实验表明，SmoothSinger在客观和主观评估中均达到最先进水平，有效减少伪影并提升合成语音的自然度。

Conclusion: SmoothSinger通过统一框架和结构优化，显著提升了歌唱语音合成的质量和自然度，为相关领域提供了新的解决方案。

Abstract: Singing voice synthesis (SVS) aims to generate expressive and high-quality
vocals from musical scores, requiring precise modeling of pitch, duration, and
articulation. While diffusion-based models have achieved remarkable success in
image and video generation, their application to SVS remains challenging due to
the complex acoustic and musical characteristics of singing, often resulting in
artifacts that degrade naturalness. In this work, we propose SmoothSinger, a
conditional diffusion model designed to synthesize high quality and natural
singing voices. Unlike prior methods that depend on vocoders as a final stage
and often introduce distortion, SmoothSinger refines low-quality synthesized
audio directly in a unified framework, mitigating the degradation associated
with two-stage pipelines. The model adopts a reference-guided dual-branch
architecture, using low-quality audio from any baseline system as a reference
to guide the denoising process, enabling more expressive and context-aware
synthesis. Furthermore, it enhances the conventional U-Net with a parallel
low-frequency upsampling path, allowing the model to better capture pitch
contours and long term spectral dependencies. To improve alignment during
training, we replace reference audio with degraded ground truth audio,
addressing temporal mismatch between reference and target signals. Experiments
on the Opencpop dataset, a large-scale Chinese singing corpus, demonstrate that
SmoothSinger achieves state-of-the-art results in both objective and subjective
evaluations. Extensive ablation studies confirm its effectiveness in reducing
artifacts and improving the naturalness of synthesized voices.

</details>


### [213] [Learnable Adaptive Time-Frequency Representation via Differentiable Short-Time Fourier Transform](https://arxiv.org/abs/2506.21440)
*Maxime Leiber,Yosra Marnissi,Axel Barrau,Sylvain Meignen,Laurent Massoulié*

Main category: cs.SD

TL;DR: 提出了一种可微分的短时傅里叶变换（STFT）方法，通过梯度优化参数，解决了传统方法依赖离散搜索的问题，并提升了时频表示和下游任务性能。


<details>
  <summary>Details</summary>
Motivation: 传统STFT的参数调整依赖手动或启发式方法，效果不佳且计算量大，需要一种更高效的参数优化方法。

Method: 提出了一种统一的、可微分的STFT公式，支持基于梯度的参数优化，并可无缝集成到神经网络中，联合优化STFT参数和网络权重。

Result: 实验表明，该方法在模拟和真实数据上均能有效优化时频表示，并提升下游任务的性能。

Conclusion: 可微分STFT提供了一种高效的参数优化方法，显著提升了时频分析和相关应用的性能。

Abstract: The short-time Fourier transform (STFT) is widely used for analyzing
non-stationary signals. However, its performance is highly sensitive to its
parameters, and manual or heuristic tuning often yields suboptimal results. To
overcome this limitation, we propose a unified differentiable formulation of
the STFT that enables gradient-based optimization of its parameters. This
approach addresses the limitations of traditional STFT parameter tuning
methods, which often rely on computationally intensive discrete searches. It
enables fine-tuning of the time-frequency representation (TFR) based on any
desired criterion. Moreover, our approach integrates seamlessly with neural
networks, allowing joint optimization of the STFT parameters and network
weights. The efficacy of the proposed differentiable STFT in enhancing TFRs and
improving performance in downstream tasks is demonstrated through experiments
on both simulated and real-world data.

</details>


<div id='q-bio.BM'></div>

# q-bio.BM [[Back]](#toc)

### [214] [CovDocker: Benchmarking Covalent Drug Design with Tasks, Datasets, and Solutions](https://arxiv.org/abs/2506.21085)
*Yangzhe Peng,Kaiyuan Gao,Liang He,Yuheng Cong,Haiguang Liu,Kun He,Lijun Wu*

Main category: q-bio.BM

TL;DR: 该论文介绍了CovDocker，一个用于共价对接的综合基准，旨在更好地捕捉共价结合的复杂性，并展示了其在预测相互作用位点和建模分子转化中的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的大多数对接方法和深度学习方法难以考虑共价键的形成及其相关的结构变化，这限制了共价药物设计的研究进展。

Method: 通过将共价对接过程分解为三个主要任务（反应位置预测、共价反应预测和共价对接），并采用Uni-Mol和Chemformer等先进模型，建立了基线性能。

Result: 基准测试在准确预测相互作用位点和建模共价结合中的分子转化方面表现出色，证实了其作为严格框架的潜力。

Conclusion: 该研究强调了数据驱动方法在加速选择性共价抑制剂发现中的潜力，并解决了治疗开发中的关键挑战。

Abstract: Molecular docking plays a crucial role in predicting the binding mode of
ligands to target proteins, and covalent interactions, which involve the
formation of a covalent bond between the ligand and the target, are
particularly valuable due to their strong, enduring binding nature. However,
most existing docking methods and deep learning approaches hardly account for
the formation of covalent bonds and the associated structural changes. To
address this gap, we introduce a comprehensive benchmark for covalent docking,
CovDocker, which is designed to better capture the complexities of covalent
binding. We decompose the covalent docking process into three main tasks:
reactive location prediction, covalent reaction prediction, and covalent
docking. By adapting state-of-the-art models, such as Uni-Mol and Chemformer,
we establish baseline performances and demonstrate the effectiveness of the
benchmark in accurately predicting interaction sites and modeling the molecular
transformations involved in covalent binding. These results confirm the role of
the benchmark as a rigorous framework for advancing research in covalent drug
design. It underscores the potential of data-driven approaches to accelerate
the discovery of selective covalent inhibitors and addresses critical
challenges in therapeutic development.

</details>


### [215] [MegaFold: System-Level Optimizations for Accelerating Protein Structure Prediction Models](https://arxiv.org/abs/2506.20686)
*Hoa La,Ahan Gupta,Alex Morehead,Jianlin Cheng,Minjia Zhang*

Main category: q-bio.BM

TL;DR: MegaFold是一个跨平台系统，旨在加速AlphaFold3的训练，通过减少内存使用和提高计算效率，显著提升了蛋白质折叠模型的可扩展性。


<details>
  <summary>Details</summary>
Motivation: AlphaFold3等蛋白质结构预测模型在生物分子建模方面取得了显著进展，但其计算和内存密集型操作、2D注意力机制以及检索增强的数据管道等问题严重影响了训练的可扩展性。因此，需要一种高效的系统来解决这些瓶颈。

Method: MegaFold通过提前缓存消除GPU空闲时间、使用Triton内核实现内存高效的EvoAttention，以及对AF3中常见且关键的小算子进行深度融合，来优化训练过程。

Result: 在NVIDIA H200和AMD MI250 GPU上的评估显示，MegaFold将AF3训练的峰值内存使用减少了1.23倍，每次迭代训练时间分别提高了1.73倍和1.62倍，并且能够处理比PyTorch基线长1.35倍的序列长度。

Conclusion: MegaFold显著提升了现代蛋白质折叠模型的可扩展性，使其能够更高效地处理更长的序列，为生物分子建模领域提供了重要的工具。

Abstract: Protein structure prediction models such as AlphaFold3 (AF3) push the
frontier of biomolecular modeling by incorporating science-informed
architectural changes to the transformer architecture. However, these advances
come at a steep system cost, introducing: compute- and memory-intensive
operators, 2D attention mechanisms, and retrieval-augmented data pipelines,
which collectively hinder the scalability of AF3 training. In this work, we
present MegaFold, a cross-platform system to accelerate AF3 training. MegaFold
tackles key bottlenecks through ahead-of-time caching to eliminate GPU idle
time from the retrieval-augmented data pipeline, Triton-based kernels for
memory-efficient EvoAttention on heterogeneous devices, and deep fusion for
common and critical small operators in AF3. Evaluation on both NVIDIA H200 and
AMD MI250 GPUs shows that MegaFold reduces peak memory usage of AF3 training by
up to 1.23$\times$ and improves per-iteration training time by up-to
1.73$\times$ and 1.62$\times$ respectively. More importantly, MegaFold enables
training on 1.35$\times$ longer sequence lengths compared to PyTorch baselines
without running out-of-memory, significantly improving the scalability of
modern protein folding models. We open source our code at
https://github.com/Supercomputing-System-AI-Lab/MegaFold/.

</details>


<div id='physics.med-ph'></div>

# physics.med-ph [[Back]](#toc)

### [216] [IMC-PINN-FE: A Physics-Informed Neural Network for Patient-Specific Left Ventricular Finite Element Modeling with Image Motion Consistency and Biomechanical Parameter Estimation](https://arxiv.org/abs/2506.20696)
*Siyu Mu,Wei Xuan Chan,Choon Hwai Yap*

Main category: physics.med-ph

TL;DR: 提出了一种结合物理信息神经网络和有限元建模的新方法IMC-PINN-FE，用于快速、个性化且图像一致的心脏生物力学建模。


<details>
  <summary>Details</summary>
Motivation: 传统有限元方法计算成本高且难以准确重现心脏运动，需要一种更高效、精确的方法来模拟心肌的生物力学行为。

Method: IMC-PINN-FE框架结合图像运动一致性和有限元建模，通过预训练网络或非监督网络估计心脏运动，并提取运动模式，快速估计心肌刚度和主动张力。

Result: IMC-PINN-FE显著提高了计算速度（75倍加速），并更准确地匹配图像位移（Dice系数从0.849提升至0.927），同时保持了真实的压力-体积行为。

Conclusion: IMC-PINN-FE为快速、个性化且图像一致的心脏生物力学建模提供了一种高效、稳健的方法，避免了大数据集需求并提高了患者特异性。

Abstract: Elucidating the biomechanical behavior of the myocardium is crucial for
understanding cardiac physiology, but cannot be directly inferred from clinical
imaging and typically requires finite element (FE) simulations. However,
conventional FE methods are computationally expensive and often fail to
reproduce observed cardiac motions. We propose IMC-PINN-FE, a physics-informed
neural network (PINN) framework that integrates imaged motion consistency (IMC)
with FE modeling for patient-specific left ventricular (LV) biomechanics.
Cardiac motion is first estimated from MRI or echocardiography using either a
pre-trained attention-based network or an unsupervised cyclic-regularized
network, followed by extraction of motion modes. IMC-PINN-FE then rapidly
estimates myocardial stiffness and active tension by fitting clinical pressure
measurements, accelerating computation from hours to seconds compared to
traditional inverse FE. Based on these parameters, it performs FE modeling
across the cardiac cycle at 75x speedup. Through motion constraints, it matches
imaged displacements more accurately, improving average Dice from 0.849 to
0.927, while preserving realistic pressure-volume behavior. IMC-PINN-FE
advances previous PINN-FE models by introducing back-computation of material
properties and better motion fidelity. Using motion from a single subject to
reconstruct shape modes also avoids the need for large datasets and improves
patient specificity. IMC-PINN-FE offers a robust and efficient approach for
rapid, personalized, and image-consistent cardiac biomechanical modeling.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [217] [Consistent Zero-shot 3D Texture Synthesis Using Geometry-aware Diffusion and Temporal Video Models](https://arxiv.org/abs/2506.20946)
*Donggoo Kang,Jangyeong Kim,Dasol Jeong,Junyoung Choi,Jeonga Wi,Hyunmin Lee,Joonho Gwon,Joonki Paik*

Main category: cs.GR

TL;DR: VideoTex利用视频生成模型解决3D纹理合成中的空间和时间不一致问题，通过几何感知条件和结构UV扩散策略提升纹理质量和稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有纹理合成方法因缺乏全局上下文和几何理解导致不一致性，而视频生成模型在时间一致性上表现优异，因此提出结合两者优势的VideoTex框架。

Method: VideoTex结合几何感知条件精确利用3D网格结构，并提出结构UV扩散策略增强遮挡区域的语义信息保留，实现平滑纹理过渡。

Result: 实验表明VideoTex在纹理保真度、接缝融合和稳定性上优于现有方法，适用于需要视觉质量和时间一致性的动态实时应用。

Conclusion: VideoTex通过视频生成模型和几何感知条件，显著提升了3D纹理合成的质量和一致性，为动态实时应用开辟了新途径。

Abstract: Current texture synthesis methods, which generate textures from fixed
viewpoints, suffer from inconsistencies due to the lack of global context and
geometric understanding. Meanwhile, recent advancements in video generation
models have demonstrated remarkable success in achieving temporally consistent
videos. In this paper, we introduce VideoTex, a novel framework for seamless
texture synthesis that leverages video generation models to address both
spatial and temporal inconsistencies in 3D textures. Our approach incorporates
geometry-aware conditions, enabling precise utilization of 3D mesh structures.
Additionally, we propose a structure-wise UV diffusion strategy, which enhances
the generation of occluded areas by preserving semantic information, resulting
in smoother and more coherent textures. VideoTex not only achieves smoother
transitions across UV boundaries but also ensures high-quality, temporally
stable textures across video frames. Extensive experiments demonstrate that
VideoTex outperforms existing methods in texture fidelity, seam blending, and
stability, paving the way for dynamic real-time applications that demand both
visual quality and temporal coherence.

</details>
