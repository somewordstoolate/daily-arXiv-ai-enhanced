<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 64]
- [cs.AI](#cs.AI) [Total: 12]
- [cs.LG](#cs.LG) [Total: 95]
- [cs.MA](#cs.MA) [Total: 3]
- [cs.CV](#cs.CV) [Total: 48]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 1]
- [cs.CE](#cs.CE) [Total: 3]
- [cs.CR](#cs.CR) [Total: 5]
- [math.PR](#math.PR) [Total: 1]
- [math.OC](#math.OC) [Total: 1]
- [q-bio.QM](#q-bio.QM) [Total: 2]
- [eess.IV](#eess.IV) [Total: 3]
- [cs.IR](#cs.IR) [Total: 2]
- [eess.SP](#eess.SP) [Total: 5]
- [cs.GR](#cs.GR) [Total: 3]
- [cs.NI](#cs.NI) [Total: 2]
- [eess.SY](#eess.SY) [Total: 1]
- [physics.med-ph](#physics.med-ph) [Total: 1]
- [cs.SI](#cs.SI) [Total: 1]
- [eess.AS](#eess.AS) [Total: 3]
- [cs.DC](#cs.DC) [Total: 4]
- [cs.NE](#cs.NE) [Total: 1]
- [cs.LO](#cs.LO) [Total: 1]
- [cs.SE](#cs.SE) [Total: 3]
- [cs.HC](#cs.HC) [Total: 3]
- [cs.SD](#cs.SD) [Total: 6]
- [q-bio.GN](#q-bio.GN) [Total: 1]
- [q-fin.ST](#q-fin.ST) [Total: 1]
- [q-bio.NC](#q-bio.NC) [Total: 1]
- [cs.RO](#cs.RO) [Total: 11]
- [stat.ML](#stat.ML) [Total: 7]
- [quant-ph](#quant-ph) [Total: 1]
- [cs.CY](#cs.CY) [Total: 3]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [LLM-as-a-qualitative-judge: automating error analysis in natural language generation](https://arxiv.org/abs/2506.09147)
*Nadezhda Chirkova,Tunde Oluwaseyi Ajayi,Seth Aycock,Zain Muhammad Mujahid,Vladana Perlić,Ekaterina Borisova,Markarit Vartampetian*

Main category: cs.CL

TL;DR: 提出LLM作为定性评判者，通过结构化报告分析NLG系统问题，提供改进见解。


<details>
  <summary>Details</summary>
Motivation: 现有LLM评估方法主要依赖数值评分，缺乏对NLG系统具体问题的定性分析，难以为开发者提供改进方向。

Method: 结合开放式逐实例问题分析与累积聚类算法，生成错误类型结构化报告，并通过人工标注数据验证方法有效性。

Result: LLM定性评估在2/3案例中正确识别问题，且生成报告与人工标注结果高度相似。

Conclusion: LLM作为定性评判者能有效识别NLG系统问题类型，为开发者提供可操作的改进建议，补充传统定量评估的不足。

Abstract: Prompting large language models (LLMs) to evaluate generated text, known as
LLM-as-a-judge, has become a standard evaluation approach in natural language
generation (NLG), but is primarily used as a quantitative tool, i.e. with
numerical scores as main outputs. In this work, we propose
LLM-as-a-qualitative-judge, an LLM-based evaluation approach with the main
output being a structured report of common issue types in the NLG system
outputs. Our approach is targeted at providing developers with meaningful
insights on what improvements can be done to a given NLG system and consists of
two main steps, namely open-ended per-instance issue analysis and clustering of
the discovered issues using an intuitive cumulative algorithm. We also
introduce a strategy for evaluating the proposed approach, coupled with ~300
annotations of issues in instances from 12 NLG datasets. Our results show that
LLM-as-a-qualitative-judge correctly recognizes instance-specific issues in 2/3
cases and is capable of producing error type reports resembling the reports
composed by human annotators. Our code and data are publicly available at
https://github.com/tunde-ajayi/llm-as-a-qualitative-judge.

</details>


### [2] [PHRASED: Phrase Dictionary Biasing for Speech Translation](https://arxiv.org/abs/2506.09175)
*Peidong Wang,Jian Xue,Rui Zhao,Junkun Chen,Aswin Shanmugam Subramanian,Jinyu Li*

Main category: cs.CL

TL;DR: 提出一种基于词典的短语偏置方法，显著提升流式语音翻译和多模态大语言模型的短语翻译效果。


<details>
  <summary>Details</summary>
Motivation: 由于训练数据中短语出现频率低，导致语音翻译任务中短语翻译准确率不足。

Method: 使用源语言-目标语言短语映射对的词典偏置方法，应用于流式转导器模型和多模态大语言模型。

Result: 流式模型相对提升21%性能，多模态模型实现85%的短语召回率相对提升。

Conclusion: 短语词典偏置能有效利用外部知识，显著改善不同模型架构的短语翻译能力。

Abstract: Phrases are essential to understand the core concepts in conversations.
However, due to their rare occurrence in training data, correct translation of
phrases is challenging in speech translation tasks. In this paper, we propose a
phrase dictionary biasing method to leverage pairs of phrases mapping from the
source language to the target language. We apply the phrase dictionary biasing
method to two types of widely adopted models, a transducer-based streaming
speech translation model and a multimodal large language model. Experimental
results show that the phrase dictionary biasing method outperforms phrase list
biasing by 21% relatively for the streaming speech translation model. In
addition, phrase dictionary biasing enables multimodal large language models to
use external phrase information, achieving 85% relative improvement in phrase
recall.

</details>


### [3] [A Technique for Isolating Lexically-Independent Phonetic Dependencies in Generative CNNs](https://arxiv.org/abs/2506.09218)
*Bruno Ferenc Šegedin*

Main category: cs.CL

TL;DR: 该研究探讨了生成式卷积神经网络（CNNs）在原始音频波形上的词汇不变泛化能力，并提出了一种通过绕过全连接层（FC）探测卷积层动态泛化能力的新方法。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探索深度神经网络（DNNs）能否在词汇学习之外实现音位规则的动态泛化，并验证全连接层瓶颈对模型泛化能力的影响。

Method: 通过将全连接层（FC）的通道数从1024压缩至8，并设计一种绕过FC层、向卷积块输入随机特征图的新技术，生成音频以分析模型的泛化能力。

Result: 实验表明，即使绕过FC层，生成的音频仍与使用FC层的输出具有相同的音位规则偏向，证明卷积层可动态泛化超出FC层词汇配置的语音依赖关系。

Conclusion: 卷积层能够独立于全连接层学到的词汇约束，动态捕获并泛化音位规则，揭示了CNN在语音表征中的深层泛化潜力。

Abstract: The ability of deep neural networks (DNNs) to represent phonotactic
generalizations derived from lexical learning remains an open question. This
study (1) investigates the lexically-invariant generalization capacity of
generative convolutional neural networks (CNNs) trained on raw audio waveforms
of lexical items and (2) explores the consequences of shrinking the
fully-connected layer (FC) bottleneck from 1024 channels to 8 before training.
Ultimately, a novel technique for probing a model's lexically-independent
generalizations is proposed that works only under the narrow FC bottleneck:
generating audio outputs by bypassing the FC and inputting randomized feature
maps into the convolutional block. These outputs are equally biased by a
phonotactic restriction in training as are outputs generated with the FC. This
result shows that the convolutional layers can dynamically generalize phonetic
dependencies beyond lexically-constrained configurations learned by the FC.

</details>


### [4] [Extrapolation by Association: Length Generalization Transfer in Transformers](https://arxiv.org/abs/2506.09251)
*Ziyang Cai,Nayoung Lee,Avi Schwarzschild,Samet Oymak,Dimitris Papailiopoulos*

Main category: cs.CL

TL;DR: 研究发现Transformer模型在相关任务间可迁移长度泛化能力，预训练提供的计算支架可促进下游外推，注意力头重用是潜在机制。


<details>
  <summary>Details</summary>
Motivation: 探索Transformer模型如何通过任务关联实现长度泛化（从短输入外推至长输入），揭示其泛化机制的本质。

Method: 在算术运算、字符串变换、迷宫导航等算法任务上，通过联合训练长序列辅助任务，分析预训练模型的下游表现及注意力头重用模式。

Result: 任务间存在长度泛化迁移效应；预训练模型具备可重用的计算支架；注意力头重用与迁移能力呈正相关。

Conclusion: Transformer通过跨任务组合性复用归纳偏差实现泛化，任务关联性驱动的结构重用是外推能力的重要来源。

Abstract: Transformer language models have demonstrated impressive generalization
capabilities in natural language domains, yet we lack a fine-grained
understanding of how such generalization arises. In this paper, we investigate
length generalization--the ability to extrapolate from shorter to longer
inputs--through the lens of \textit{task association}. We find that length
generalization can be \textit{transferred} across related tasks. That is,
training a model with a longer and related auxiliary task can lead it to
generalize to unseen and longer inputs from some other target task. We
demonstrate this length generalization transfer across diverse algorithmic
tasks, including arithmetic operations, string transformations, and maze
navigation. Our results show that transformer models can inherit generalization
capabilities from similar tasks when trained jointly. Moreover, we observe
similar transfer effects in pretrained language models, suggesting that
pretraining equips models with reusable computational scaffolding that
facilitates extrapolation in downstream settings. Finally, we provide initial
mechanistic evidence that length generalization transfer correlates with the
re-use of the same attention heads between the tasks. Together, our findings
deepen our understanding of how transformers generalize to out-of-distribution
inputs and highlight the compositional reuse of inductive structure across
tasks.

</details>


### [5] [Self-Anchored Attention Model for Sample-Efficient Classification of Prosocial Text Chat](https://arxiv.org/abs/2506.09259)
*Zhuofang Li,Rafal Kocielnik,Fereshteh Soltani,Penphob,Boonyarungsrit,Animashree Anandkumar,R. Michael Alvarez*

Main category: cs.CL

TL;DR: 本文提出一种新型自锚定注意力模型（SAAM），用于低资源环境下游戏聊天中亲社会行为的自动分类，相比现有技术提升7.9%，并首次在《使命召唤》中实现应用。


<details>
  <summary>Details</summary>
Motivation: 现有研究集中于检测游戏聊天毒性内容，但亲社会行为识别同样重要且缺乏数据与模型资源。识别亲社会行为可促进其分析与推广，推动平台治理从单纯惩罚转向正向激励。

Method: 结合无监督发现与游戏领域专家协作识别亲社会行为，提出SAAM模型，利用全训练集作为'锚点'缓解数据稀缺问题，构建首个游戏聊天亲社会行为自动分类系统。

Result: SAAM模型性能较现有最佳技术提升7.9%，在《使命召唤：现代战争II》中验证有效性，实现低资源环境下亲社会行为分类。

Conclusion: 该研究首次将NLP技术应用于游戏聊天亲社会行为识别，为在线平台治理提供从抑制毒性到鼓励正向互动的范式转变支持。

Abstract: Millions of players engage daily in competitive online games, communicating
through in-game chat. Prior research has focused on detecting relatively small
volumes of toxic content using various Natural Language Processing (NLP)
techniques for the purpose of moderation. However, recent studies emphasize the
importance of detecting prosocial communication, which can be as crucial as
identifying toxic interactions. Recognizing prosocial behavior allows for its
analysis, rewarding, and promotion. Unlike toxicity, there are limited
datasets, models, and resources for identifying prosocial behaviors in
game-chat text. In this work, we employed unsupervised discovery combined with
game domain expert collaboration to identify and categorize prosocial player
behaviors from game chat. We further propose a novel Self-Anchored Attention
Model (SAAM) which gives 7.9% improvement compared to the best existing
technique. The approach utilizes the entire training set as "anchors" to help
improve model performance under the scarcity of training data. This approach
led to the development of the first automated system for classifying prosocial
behaviors in in-game chats, particularly given the low-resource settings where
large-scale labeled data is not available. Our methodology was applied to one
of the most popular online gaming titles - Call of Duty(R): Modern
Warfare(R)II, showcasing its effectiveness. This research is novel in applying
NLP techniques to discover and classify prosocial behaviors in player in-game
chat communication. It can help shift the focus of moderation from solely
penalizing toxicity to actively encouraging positive interactions on online
platforms.

</details>


### [6] [Did I Faithfully Say What I Thought? Bridging the Gap Between Neural Activity and Self-Explanations in Large Language Models](https://arxiv.org/abs/2506.09277)
*Milan Bhan,Jean-Noel Vittaut,Nicolas Chesneau,Sarath Chandar,Marie-Jeanne Lesot*

Main category: cs.CL

TL;DR: 本文提出一种新框架，通过直接比较LLM生成的自我解释与模型内部隐藏状态的解释，定量评估其忠实性，为生成更可靠的解释提供基础。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要依赖行为测试或计算块识别，未深入分析模型推理时的神经活动，导致无法准确评估自我自然语言（self-NLE）是否真实反映LLM的决策过程。

Method: 引入灵活框架，通过对比self-NLE与模型内部隐藏状态的解释，建立两者直接关联，从而量化自我解释的忠实性。

Result: 该框架揭示了self-NLE与模型推理过程的内在联系，提供了对解释忠实性的深层见解，并为生成更忠实的解释提供技术基础。

Conclusion: 通过连接self-NLE与模型内部状态，该研究推进了对解释忠实性的理解，并为未来构建可信赖的LLM解释机制奠定基础。

Abstract: Large Language Models (LLM) have demonstrated the capability of generating
free text self Natural Language Explanation (self-NLE) to justify their
answers. Despite their logical appearance, self-NLE do not necessarily reflect
the LLM actual decision-making process, making such explanations unfaithful.
While existing methods for measuring self-NLE faithfulness mostly rely on
behavioral tests or computational block identification, none of them examines
the neural activity underlying the model's reasoning. This work introduces a
novel flexible framework for quantitatively measuring the faithfulness of
LLM-generated self-NLE by directly comparing the latter with interpretations of
the model's internal hidden states. The proposed framework is versatile and
provides deep insights into self-NLE faithfulness by establishing a direct
connection between self-NLE and model reasoning. This approach advances the
understanding of self-NLE faithfulness and provides building blocks for
generating more faithful self-NLE.

</details>


### [7] [$(RSA)^2$: A Rhetorical-Strategy-Aware Rational Speech Act Framework for Figurative Language Understanding](https://arxiv.org/abs/2506.09301)
*Cesare Spinoso-Di Piano,David Austin,Pablo Piantanida,Jackie Chi Kit Cheung*

Main category: cs.CL

TL;DR: 本文提出了一种结合修辞策略的RSA²框架，用于解释比喻性语言，无需建模说话者动机，结合LLM在讽刺理解任务中达到SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有RSA框架无法有效处理比喻性语言(如反讽/夸张)，且需针对具体情境建模非字面表达动机，限制了应用范围。

Method: 提出修辞策略感知的RSA²框架，通过建模说话者使用的修辞策略(而非具体动机)解释非字面表达，并与大语言模型(LLM)结合。

Result: 在自建讽刺数据集PragMega+的讽刺分集上，RSA²框架实现了与人类兼容的解释效果，并达到当前最佳性能。

Conclusion: RSA²框架通过修辞策略建模有效解决了比喻性语言理解问题，验证了其在不依赖具体动机建模情况下的理论优势与实践有效性。

Abstract: Figurative language (e.g., irony, hyperbole, understatement) is ubiquitous in
human communication, resulting in utterances where the literal and the intended
meanings do not match. The Rational Speech Act (RSA) framework, which
explicitly models speaker intentions, is the most widespread theory of
probabilistic pragmatics, but existing implementations are either unable to
account for figurative expressions or require modeling the implicit motivations
for using figurative language (e.g., to express joy or annoyance) in a
setting-specific way. In this paper, we introduce the Rhetorical-Strategy-Aware
RSA $(RSA)^2$ framework which models figurative language use by considering a
speaker's employed rhetorical strategy. We show that $(RSA)^2$ enables
human-compatible interpretations of non-literal utterances without modeling a
speaker's motivations for being non-literal. Combined with LLMs, it achieves
state-of-the-art performance on the ironic split of PragMega+, a new irony
interpretation dataset introduced in this study.

</details>


### [8] [Alzheimer's Dementia Detection Using Perplexity from Paired Large Language Models](https://arxiv.org/abs/2506.09315)
*Yao Xiao,Heidi Christensen,Stefan Goetze*

Main category: cs.CL

TL;DR: 本研究利用指令微调后的Mistral-7B大语言模型改进配对困惑度方法，显著提升阿尔茨海默病（AD）检测准确率，并揭示模型通过学习AD患者语言模式实现可解释决策。


<details>
  <summary>Details</summary>
Motivation: 阿尔茨海默病常导致语言能力衰退，但现有检测技术存在准确率不足及决策过程不透明的问题，需开发更优方法。

Method: 基于Mistral-7B指令微调模型扩展配对困惑度方法，通过对比模型生成响应与人类语言模式检测AD。

Result: 检测准确率较现有最佳方法提升3.33%，较ADReSS 2020基准方法提升6.35%，且决策边界清晰可解释。

Conclusion: 大语言模型能有效识别AD特殊语言模式，为模型解释与数据增强提供了新途径。

Abstract: Alzheimer's dementia (AD) is a neurodegenerative disorder with cognitive
decline that commonly impacts language ability. This work extends the paired
perplexity approach to detecting AD by using a recent large language model
(LLM), the instruction-following version of Mistral-7B. We improve accuracy by
an average of 3.33% over the best current paired perplexity method and by 6.35%
over the top-ranked method from the ADReSS 2020 challenge benchmark. Our
further analysis demonstrates that the proposed approach can effectively detect
AD with a clear and interpretable decision boundary in contrast to other
methods that suffer from opaque decision-making processes. Finally, by
prompting the fine-tuned LLMs and comparing the model-generated responses to
human responses, we illustrate that the LLMs have learned the special language
patterns of AD speakers, which opens up possibilities for novel methods of
model interpretation and data augmentation.

</details>


### [9] [Towards Efficient and Effective Alignment of Large Language Models](https://arxiv.org/abs/2506.09329)
*Yuxin Jiang*

Main category: cs.CL

TL;DR: 本论文针对大语言模型（LLM）与人类期望对齐的挑战，提出数据收集、训练优化及评估三方面的新方法，包括Lion对抗蒸馏框架、WebR自动合成数据、LTE元学习知识更新、BMC优化偏好数据相关性，以及FollowBench多级约束评估基准。


<details>
  <summary>Details</summary>
Motivation: 现有LLM对齐方法依赖人工数据集或专有模型，且训练优化效率不足，评估忽视约束遵循能力。需解决数据多样性、知识更新效率及细粒度评估的局限性。

Method: 1. Lion框架通过对抗蒸馏生成挑战性指令优化数据；2. WebR从原始网页自动合成指令数据；3. LTE利用元学习实现实时/批量知识更新；4. BMC改进DPO以捕捉词级相关性；5. FollowBench构建多级约束评估体系。

Result: Lion实现零样本推理SOTA，WebR提升数据多样性，LTE增强知识整合效率，BMC在QA和数学任务中表现更优，FollowBench揭示模型约束遵循的显著缺陷。

Conclusion: 所提方法系统性推进LLM对齐，暴露现有模型在复杂约束下的不足，为数据、训练与评估提供新方向。

Abstract: Large language models (LLMs) exhibit remarkable capabilities across diverse
tasks, yet aligning them efficiently and effectively with human expectations
remains a critical challenge. This thesis advances LLM alignment by introducing
novel methodologies in data collection, training, and evaluation. We first
address alignment data collection. Existing approaches rely heavily on manually
curated datasets or proprietary models. To overcome these limitations, we
propose Lion, an adversarial distillation framework that iteratively refines
training data by identifying and generating challenging instructions, enabling
state-of-the-art zero-shot reasoning. Additionally, we introduce Web
Reconstruction (WebR), a fully automated framework that synthesizes
instruction-tuning data directly from raw web documents, significantly
improving data diversity and scalability over existing synthetic data methods.
Next, we enhance alignment training through novel optimization techniques. We
develop Learning to Edit (LTE), a framework that enables LLMs to efficiently
integrate new knowledge while preserving existing information. LTE leverages
meta-learning to improve both real-time and batch knowledge updates.
Furthermore, we introduce Bridging and Modeling Correlations (BMC), a
refinement of Direct Preference Optimization (DPO) that explicitly captures
token-level correlations in preference data, leading to superior alignment
across QA and mathematical reasoning tasks. Finally, we tackle the challenge of
evaluating alignment. Existing benchmarks emphasize response quality but
overlook adherence to specific constraints. To bridge this gap, we introduce
FollowBench, a multi-level, fine-grained benchmark assessing LLMs' ability to
follow complex constraints across diverse instruction types. Our results expose
key weaknesses in current models' constraint adherence, offering insights for
future improvements.

</details>


### [10] [Multi-Agent Language Models: Advancing Cooperation, Coordination, and Adaptation](https://arxiv.org/abs/2506.09331)
*Arjun Vaithilingam Sudhakar*

Main category: cs.CL

TL;DR: 该论文探讨大型语言模型（LLM）是否具备心理理论（Theory of Mind），即推断他人意图的能力，并通过多智能体协作强化学习（MARL）框架研究其如何促进人机协作系统的开发。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM在文本任务中表现出色，但其是否真正理解人类意图尚不明确。理解意图是协作的基础，对多智能体（包括人类与AI）合作至关重要。

Method: 通过多智能体协作强化学习（MARL）框架，模拟人类社交推理过程，利用基于LLM的智能体进行自然语言交互，研究其协作与适应能力。

Result: 研究表明，LLM能够通过反复交互学习协作策略，增强与人工及人类伙伴的适应性合作，为构建混合人机协作系统提供基础。

Conclusion: 基于LLM的智能体在自然语言交互中展现出潜在的心理理论能力，未来或能推动无缝人机协作，对人类社会与AI交互具有深远影响。

Abstract: Modern Large Language Models (LLMs) exhibit impressive zero-shot and few-shot
generalization capabilities across complex natural language tasks, enabling
their widespread use as virtual assistants for diverse applications such as
translation and summarization. Despite being trained solely on large corpora of
text without explicit supervision on author intent, LLMs appear to infer the
underlying meaning of textual interactions. This raises a fundamental question:
can LLMs model and reason about the intentions of others, i.e., do they possess
a form of theory of mind? Understanding other's intentions is crucial for
effective collaboration, which underpins human societal success and is
essential for cooperative interactions among multiple agents, including humans
and autonomous systems. In this work, we investigate the theory of mind in LLMs
through the lens of cooperative multi-agent reinforcement learning (MARL),
where agents learn to collaborate via repeated interactions, mirroring human
social reasoning. Our approach aims to enhance artificial agent's ability to
adapt and cooperate with both artificial and human partners. By leveraging
LLM-based agents capable of natural language interaction, we move towards
creating hybrid human-AI systems that can foster seamless collaboration, with
broad implications for the future of human-artificial interaction.

</details>


### [11] [RePO: Replay-Enhanced Policy Optimization](https://arxiv.org/abs/2506.09340)
*Siheng Li,Zhanhui Zhou,Wai Lam,Chao Yang,Chaochao Lu*

Main category: cs.CL

TL;DR: 提出RePO方法，通过回放策略增强样本多样性，解决GRPO计算成本高、数据效率低的问题，显著提升大语言模型在数学推理任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 现有GRPO方法依赖每个提示的多个策略内样本进行优势估计，导致计算成本高、数据效率低。需更高效的方法优化大语言模型的强化学习过程。

Method: RePO引入多样化回放策略，从回放缓冲区检索策略外样本，结合策略内样本进行优化，扩大每个提示的样本覆盖范围。

Result: 在7个数学推理基准测试中，RePO使Qwen2.5-Math-1.5B和Qwen3-1.7B平均性能分别提升18.4和4.1个百分点。Qwen3-1.7B计算成本仅15%增长，有效优化步骤增加48%。

Conclusion: RePO通过策略内外样本混合优化，在较低计算开销下显著提升模型性能，证明回放策略增强在强化学习优化中的有效性。

Abstract: Reinforcement learning (RL) is vital for optimizing large language models
(LLMs). Recent Group Relative Policy Optimization (GRPO) estimates advantages
using multiple on-policy outputs per prompt, leading to high computational
costs and low data efficiency. To address this, we introduce Replay-Enhanced
Policy Optimization (RePO), which leverages diverse replay strategies to
retrieve off-policy samples from a replay buffer, allowing policy optimization
based on a broader and more diverse set of samples for each prompt. Experiments
on five LLMs across seven mathematical reasoning benchmarks demonstrate that
RePO achieves absolute average performance gains of $18.4$ and $4.1$ points for
Qwen2.5-Math-1.5B and Qwen3-1.7B, respectively, compared to GRPO. Further
analysis indicates that RePO increases computational cost by $15\%$ while
raising the number of effective optimization steps by $48\%$ for Qwen3-1.7B,
with both on-policy and off-policy sample numbers set to $8$. The repository
can be accessed at https://github.com/SihengLi99/RePO.

</details>


### [12] [Latent Multi-Head Attention for Small Language Models](https://arxiv.org/abs/2506.09342)
*Sushant Mehta,Raj Dandekar,Rajat Dandekar,Sreedath Panat*

Main category: cs.CL

TL;DR: 本文首次全面研究小型语言模型中潜在多头注意力（MLA）的效率-质量权衡，发现结合旋转位置嵌入（RoPE）的MLA（半秩维度）可减少45% KV缓存内存且仅增加0.3%验证损失，在内存受限场景实现帕累托优化。


<details>
  <summary>Details</summary>
Motivation: 探索小型语言模型中潜在多头注意力架构的效率与质量平衡，解决内存受限部署场景下传统注意力机制资源消耗高的问题。

Method: 在10万合成故事上训练30M参数GPT模型，对比标准多头注意力（MHA）、潜在多头注意力（MLA）及MLA+RoPE三种架构，分析不同潜在维度秩（r=d/2）对性能的影响。

Result: MLA+RoPE半秩模型KV缓存内存降低45%，验证损失仅增0.3%；A100 GPU速度提升1.4倍；GPT-4评估质量得分7.4/10（语法/创意/一致性最优）。无RoPE时MLA性能下降3-5%，加入后反超标准注意力2%。

Conclusion: MLA+RoPE在小型模型中实现内存-效率-质量的帕累托优化，RoPE对性能提升具有关键作用，半秩设计为资源受限场景提供有效解决方案。

Abstract: We present the first comprehensive study of latent multi-head attention (MLA)
for small language models, revealing interesting efficiency-quality trade-offs.
Training 30M-parameter GPT models on 100,000 synthetic stories, we benchmark
three architectural variants: standard multi-head attention (MHA), MLA, and MLA
with rotary positional embeddings (MLA+RoPE). Our key finding is that MLA+RoPE
with half-rank latent dimensions (r = d/2) achieves a 45% KV-cache memory
reduction while incurring only a 0.3% increase in validation loss (essentially
matching MHA quality)- a Pareto improvement for memory constrained deployment.
We further show that RoPE is crucial for MLA in small models: without it, MLA
underperforms vanilla attention by 3-5%, but with RoPE, it surpasses vanilla by
2%. Inference benchmarks on NVIDIA A100 GPUs reveal that MLA with r=d/2
achieves a 1.4 times speedup over full-rank MLA while maintaining the memory
savings. GPT-4 evaluations corroborate perplexity results, with ours achieving
the highest quality scores (7.4/10) across grammar, creativity, and consistency
metrics. Code and models will be released upon acceptance.

</details>


### [13] [OmniDRCA: Parallel Speech-Text Foundation Model via Dual-Resolution Speech Representations and Contrastive Alignment](https://arxiv.org/abs/2506.09349)
*Chao-Hong Tan,Qian Chen,Wen Wang,Chong Deng,Qinglin Zhang,Luyao Cheng,Hai Yu,Xin Zhang,Xiang Lv,Tianyu Zhao,Chong Zhang,Yukun Ma,Yafeng Chen,Hui Wang,Jiaqing Liu,Jieping Ye*

Main category: cs.CL

TL;DR: 本文提出基于联合自回归建模的并行语音-文本基础模型OmniDRCA，通过双分辨率语音表征和对比跨模态对齐，在口语问答任务中实现SOTA性能，并探索全双工会话场景的扩展潜力。


<details>
  <summary>Details</summary>
Motivation: 现有端到端语音生成方法存在两种局限：独立生成语音标记导致文本与语音割裂，或交错/并行生成模态间缺乏高效对齐。需开发能实现模态互感知且增强理解的模型。

Method: OmniDRCA采用并行语音-文本联合自回归框架，引入双分辨率语音表征捕捉细/粗粒度信息，并通过对比跨模态对齐提升音频理解能力。

Result: 在Spoken QA基准测试中，OmniDRCA在并行模型中达到SOTA，与交错模型性能相当，且初步验证了全双工会话场景的可行性。

Conclusion: OmniDRCA通过模态联合建模与对比对齐，平衡生成效率与跨模态理解，为多模态对话系统提供可扩展框架。

Abstract: Recent studies on end-to-end speech generation with large language models
(LLMs) have attracted significant community attention, with multiple works
extending text-based LLMs to generate discrete speech tokens. Existing
approaches primarily fall into two categories: (1) Methods that generate
discrete speech tokens independently without incorporating them into the LLM's
autoregressive process, resulting in text generation being unaware of
concurrent speech synthesis. (2) Models that generate interleaved or parallel
speech-text tokens through joint autoregressive modeling, enabling mutual
modality awareness during generation. This paper presents OmniDRCA, a parallel
speech-text foundation model based on joint autoregressive modeling, featuring
dual-resolution speech representations and contrastive cross-modal alignment.
Our approach processes speech and text representations in parallel while
enhancing audio comprehension through contrastive alignment. Experimental
results on Spoken Question Answering benchmarks demonstrate that OmniDRCA
establishes new state-of-the-art (SOTA) performance among parallel joint
speech-text modeling based foundation models, and achieves competitive
performance compared to interleaved models. Additionally, we explore the
potential of extending the framework to full-duplex conversational scenarios.

</details>


### [14] [DIVE into MoE: Diversity-Enhanced Reconstruction of Large Language Models from Dense into Mixture-of-Experts](https://arxiv.org/abs/2506.09351)
*Yuchen Feng,Bowen Shen,Naibin Gu,Jiaxuan Zhao,Peng Fu,Zheng Lin,Weiping Wang*

Main category: cs.CL

TL;DR: 本文提出DIVE方法，通过在不同校准数据集上剪枝并重组前馈网络模块，结合高效重训练策略，显著提升MoE架构大语言模型的训练效率，在保持精度的同时减少专家冗余。


<details>
  <summary>Details</summary>
Motivation: 现有MoE架构重建方法常忽视专家多样性导致冗余，而从头训练专家参数成本过高。通过观察不同剪枝数据集可增强专家多样性，提出以更低成本构建高效MoE模型的方法。

Method: 1) 领域亲和性挖掘确定专家方向 2) 基于剪枝的FFN模块专家重建 3) 对路由器、专家模块和归一化层进行高效重训练。通过剪枝重组实现参数复用，保留原始模型知识。

Result: 在Llama系列模型上的实验表明，DIVE以更少激活参数量超越现有剪枝和MoE重建方法，训练成本降低的同时精度损失最小（相同参数规模下优于基线）。

Conclusion: DIVE通过增强专家多样性，首次实现从预训练密集模型到MoE架构的高效转换，为低成本构建高性能MoE大模型提供了新范式。

Abstract: Large language models (LLMs) with the Mixture-of-Experts (MoE) architecture
achieve high cost-efficiency by selectively activating a subset of the
parameters. Despite the inference efficiency of MoE LLMs, the training of
extensive experts from scratch incurs substantial overhead, whereas
reconstructing a dense LLM into an MoE LLM significantly reduces the training
budget. However, existing reconstruction methods often overlook the diversity
among experts, leading to potential redundancy. In this paper, we come up with
the observation that a specific LLM exhibits notable diversity after being
pruned on different calibration datasets, based on which we present a
Diversity-Enhanced reconstruction method named DIVE. The recipe of DIVE
includes domain affinity mining, pruning-based expert reconstruction, and
efficient retraining. Specifically, the reconstruction includes pruning and
reassembly of the feed-forward network (FFN) module. After reconstruction, we
efficiently retrain the model on routers, experts and normalization modules. We
implement DIVE on Llama-style LLMs with open-source training corpora.
Experiments show that DIVE achieves training efficiency with minimal accuracy
trade-offs, outperforming existing pruning and MoE reconstruction methods with
the same number of activated parameters.

</details>


### [15] [Taming SQL Complexity: LLM-Based Equivalence Evaluation for Text-to-SQL](https://arxiv.org/abs/2506.09359)
*Qingyun Zeng,Simin Ma,Arash Niknafs,Ashish Basran,Carol Szabo*

Main category: cs.CL

TL;DR: 本文探讨利用大语言模型（LLM）评估文本到SQL生成结果的语义等效性，分析其常见模式及挑战。


<details>
  <summary>Details</summary>
Motivation: 现有文本到SQL系统在用户查询模糊性及多有效SQL变体场景下，语义等效性评估存在困难，需探索更实用的评估方法。

Method: 提出基于LLM的语义与弱语义等效性评估框架，并总结SQL等价/非等价模式的分类体系。

Result: 揭示了LLM评估SQL语义等效性的潜力与局限性，识别了包括子查询优化、语法变体等关键等价模式及典型错误模式。

Conclusion: LLM为SQL语义评估提供了新思路，但需解决逻辑推理偏差、领域知识依赖等挑战以提升评估可靠性。

Abstract: The rise of Large Language Models (LLMs) has significantly advanced
Text-to-SQL (NL2SQL) systems, yet evaluating the semantic equivalence of
generated SQL remains a challenge, especially given ambiguous user queries and
multiple valid SQL interpretations. This paper explores using LLMs to assess
both semantic and a more practical "weak" semantic equivalence. We analyze
common patterns of SQL equivalence and inequivalence, discuss challenges in
LLM-based evaluation.

</details>


### [16] [COGENT: A Curriculum-oriented Framework for Generating Grade-appropriate Educational Content](https://arxiv.org/abs/2506.09367)
*Zhengyuan Liu,Stella Xin Yin,Dion Hoe-Lian Goh,Nancy F. Chen*

Main category: cs.CL

TL;DR: 本文提出课程导向框架COGENT，通过整合课程要素、控制可读性及采用好奇驱动方法，生成符合年级标准的STEM教育内容，实验证明其效果优于或接近人工编写材料。


<details>
  <summary>Details</summary>
Motivation: 生成式AI在教育场景面临三大挑战：难以对齐课程标准、维持年级阅读水平一致性，以及STEM教育中科学术语与日常语言的平衡问题，需针对性解决方案。

Method: COGENT框架整合科学概念/核心思想/学习目标三要素，通过文本长度、词汇难度和句子复杂度控制可读性，并采用基于好奇心的内容设计提升学生参与度。

Result: 多维度评估（LLM评判+专家分析）显示，COGENT生成的内容在年级适配性上稳定优于或持平人工编写参考文本，验证了框架有效性。

Conclusion: 该研究为规模化生成自适应、高质量的STEM学习资源提供了可行路径，解决了生成式AI在教育场景的核心痛点。

Abstract: While Generative AI has demonstrated strong potential and versatility in
content generation, its application to educational contexts presents several
challenges. Models often fail to align with curriculum standards and maintain
grade-appropriate reading levels consistently. Furthermore, STEM education
poses additional challenges in balancing scientific explanations with everyday
language when introducing complex and abstract ideas and phenomena to younger
students. In this work, we propose COGENT, a curriculum-oriented framework for
generating grade-appropriate educational content. We incorporate three
curriculum components (science concepts, core ideas, and learning objectives),
control readability through length, vocabulary, and sentence complexity, and
adopt a ``wonder-based'' approach to increase student engagement and interest.
We conduct a multi-dimensional evaluation via both LLM-as-a-judge and human
expert analysis. Experimental results show that COGENT consistently produces
grade-appropriate passages that are comparable or superior to human references.
Our work establishes a viable approach for scaling adaptive and high-quality
learning resources.

</details>


### [17] [RuleReasoner: Reinforced Rule-based Reasoning via Domain-aware Dynamic Sampling](https://arxiv.org/abs/2506.08672)
*Yang Liu,Jiaqi Li,Zilong Zheng*

Main category: cs.CL

TL;DR: 本文提出RuleReasoner方法，通过动态采样和强化学习提升小型推理模型在规则推理任务中的泛化能力和效率，显著优于大型模型。


<details>
  <summary>Details</summary>
Motivation: 现有大型推理模型虽通过强化学习提升性能，但小型模型能否在多样化任务中实现鲁棒的规则推理仍存疑。规则格式、类型及复杂度的差异进一步加剧挑战。

Method: 提出RuleReasoner框架：1) 构建多领域任务库；2) 基于历史奖励动态调整领域采样权重，实现领域增强和在线学习调度，无需人工设计混合训练策略。

Result: 在ID和OOD任务中分别以4.1%和10.4%的优势超越前沿大型模型（如OpenAI-o1），且计算效率优于现有动态采样方法。

Conclusion: RuleReasoner证明小型模型可通过强化学习实现高效规则推理，其动态采样机制有效解决跨领域泛化问题，为轻量级推理系统提供新路径。

Abstract: Rule-based reasoning has been acknowledged as one of the fundamental problems
in reasoning, while deviations in rule formats, types, and complexity in
real-world applications pose severe challenges. Recent studies have shown that
large reasoning models (LRMs) have remarkable reasoning capabilities, and their
performance is substantially enhanced by reinforcement learning (RL). However,
it remains an open question whether small reasoning models (SRMs) can learn
rule-based reasoning effectively with robust generalization across diverse
tasks and domains. To address this, we introduce Reinforced Rule-based
Reasoning, a.k.a. RuleReasoner, a simple yet effective method to conduct
rule-based reasoning via a wide collection of curated tasks and a novel
domain-aware dynamic sampling approach. Specifically, RuleReasoner resamples
each training batch by updating the sampling weights of different domains based
on historical rewards. This facilitates domain augmentation and flexible online
learning schedules for RL, obviating the need for pre-hoc human-engineered
mix-training recipes used in existing methods. Empirical evaluations on
in-distribution (ID) and out-of-distribution (OOD) benchmarks reveal that
RuleReasoner outperforms frontier LRMs by a significant margin ($\Delta$4.1%
average points on eight ID tasks and $\Delta$10.4% average points on three OOD
tasks over OpenAI-o1). Notably, our approach also exhibits higher computational
efficiency compared to prior dynamic sampling methods for RL.

</details>


### [18] [CoLMbo: Speaker Language Model for Descriptive Profiling](https://arxiv.org/abs/2506.09375)
*Massa Baali,Shuo Han,Syed Abdul Hannan,Purusottam Samal,Karanveer Singh,Soham Deshmukh,Rita Singh,Bhiksha Raj*

Main category: cs.CL

TL;DR: 本文提出CoLMbo模型，通过结合说话人编码器和提示条件生成，解决了传统说话人识别系统无法生成详细特征描述的问题，并在零样本场景中表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统说话人识别系统仅能进行身份分类，无法结构化捕捉方言、性别、年龄等人口属性，且缺乏上下文丰富的描述能力。

Method: 整合说话人编码器与基于提示的动态调节机制，通过用户自定义提示生成定制化描述，支持方言变体和年龄特征等新特性适配。

Result: CoLMbo在多数据集零样本场景中显著提升说话人特征分析能力，可生成包含区域方言和年龄特征的细粒度描述。

Conclusion: 该模型突破了传统说话人分析的局限性，为领域提供了可动态扩展的上下文感知框架，是说话人识别领域的重要进展。

Abstract: Speaker recognition systems are often limited to classification tasks and
struggle to generate detailed speaker characteristics or provide context-rich
descriptions. These models primarily extract embeddings for speaker
identification but fail to capture demographic attributes such as dialect,
gender, and age in a structured manner. This paper introduces CoLMbo, a Speaker
Language Model (SLM) that addresses these limitations by integrating a speaker
encoder with prompt-based conditioning. This allows for the creation of
detailed captions based on speaker embeddings. CoLMbo utilizes user-defined
prompts to adapt dynamically to new speaker characteristics and provides
customized descriptions, including regional dialect variations and age-related
traits. This innovative approach not only enhances traditional speaker
profiling but also excels in zero-shot scenarios across diverse datasets,
marking a significant advancement in the field of speaker recognition.

</details>


### [19] [Binary classification for perceived quality of headlines and links on worldwide news websites, 2018-2024](https://arxiv.org/abs/2506.09381)
*Austin McCutcheon,Thiago E. A. de Oliveira,Aleksandr Zheleznov,Chris Brogly*

Main category: cs.CL

TL;DR: 研究通过传统集成方法与微调DistilBERT模型，验证了自动区分新闻标题/链接质量的可行性，准确率分别达88.1%和90.3%，但后者训练时间更长。


<details>
  <summary>Details</summary>
Motivation: 在线新闻的泛滥导致低质量标题/链接广泛传播，需探索自动区分新闻质量的方法。

Method: 使用12个机器学习模型，基于5754万条平衡数据集（含115个语言特征），利用专家共识标注质量标签，对比传统集成方法与深度学习模型。

Result: 传统集成方法（如Bagging分类器）准确率88.1%，而微调后的DistilBERT准确率最高（90.3%），但训练耗时更长。

Conclusion: 传统NLP特征与深度学习模型均能有效区分新闻质量，但需在预测性能与训练时间之间权衡。

Abstract: The proliferation of online news enables potential widespread publication of
perceived low-quality news headlines/links. As a result, we investigated
whether it was possible to automatically distinguish perceived lower-quality
news headlines/links from perceived higher-quality headlines/links. We
evaluated twelve machine learning models on a binary, balanced dataset of
57,544,214 worldwide news website links/headings from 2018-2024 (28,772,107 per
class) with 115 extracted linguistic features. Binary labels for each text were
derived from scores based on expert consensus regarding the respective news
domain quality. Traditional ensemble methods, particularly the bagging
classifier, had strong performance (88.1% accuracy, 88.3% F1, 80/20 train/test
split). Fine-tuned DistilBERT achieved the highest accuracy (90.3%, 80/20
train/test split) but required more training time. The results suggest that
both NLP features with traditional classifiers and deep learning models can
effectively differentiate perceived news headline/link quality, with some
trade-off between predictive performance and train time.

</details>


### [20] [Comparing human and LLM politeness strategies in free production](https://arxiv.org/abs/2506.09391)
*Haoran Zhao,Robert D. Hawkins*

Main category: cs.CL

TL;DR: 大型语言模型（LLMs）在礼貌用语上存在语境敏感性差异：大模型能复现人类语用偏好且生成内容更受青睐，但过度依赖负面礼貌策略可能导致误解。


<details>
  <summary>Details</summary>
Motivation: 探究LLMs是否像人类一样能根据语境灵活使用正/负面礼貌策略，平衡信息传递与社会目标，解决语用对齐问题。

Method: 通过约束性和开放式生成任务，对比人类与不同规模LLMs的回应，结合计算语用学理论框架与语言学分析。

Result: ≥70B参数模型能复现语用学偏好且人类更倾向其开放式回答，但语言分析显示模型在积极语境中仍过度使用负面策略（如委婉）。

Conclusion: LLMs已具备复杂礼貌策略处理能力，但正/负面策略的失衡揭示了AI系统语用对齐的潜在风险，需进一步研究其社会交互影响。

Abstract: Polite speech poses a fundamental alignment challenge for large language
models (LLMs). Humans deploy a rich repertoire of linguistic strategies to
balance informational and social goals -- from positive approaches that build
rapport (compliments, expressions of interest) to negative strategies that
minimize imposition (hedging, indirectness). We investigate whether LLMs employ
a similarly context-sensitive repertoire by comparing human and LLM responses
in both constrained and open-ended production tasks. We find that larger models
($\ge$70B parameters) successfully replicate key preferences from the
computational pragmatics literature, and human evaluators surprisingly prefer
LLM-generated responses in open-ended contexts. However, further linguistic
analyses reveal that models disproportionately rely on negative politeness
strategies even in positive contexts, potentially leading to
misinterpretations. While modern LLMs demonstrate an impressive handle on
politeness strategies, these subtle differences raise important questions about
pragmatic alignment in AI systems.

</details>


### [21] [A Hierarchical Probabilistic Framework for Incremental Knowledge Tracing in Classroom Settings](https://arxiv.org/abs/2506.09393)
*Xinyi Gao,Qiucheng Wu,Yang Zhang,Xuechen Liu,Kaizhi Qian,Ying Xu,Shiyu Chang*

Main category: cs.CL

TL;DR: 提出基于知识树的KT²框架，利用层次化知识概念和隐马尔可夫树模型，在低资源场景下实现高效知识追踪与动态更新。


<details>
  <summary>Details</summary>
Motivation: 现有知识追踪方法在数据稀疏的课堂场景中表现受限，需在线更新能力。层次化知识概念作为先验信息可缓解数据不足问题。

Method: KT²框架通过树状知识层级建立隐马尔可夫模型，采用EM算法估计知识掌握度，并设计增量更新机制实现个性化预测。

Result: 实验表明KT²在在线低资源场景中持续优于基线模型，验证了树状知识结构与动态更新机制的有效性。

Conclusion: KT²通过结构化知识先验与概率建模，为低资源环境下的知识追踪提供了高效解决方案，具有实际教学场景适用性。

Abstract: Knowledge tracing (KT) aims to estimate a student's evolving knowledge state
and predict their performance on new exercises based on performance history.
Many realistic classroom settings for KT are typically low-resource in data and
require online updates as students' exercise history grows, which creates
significant challenges for existing KT approaches. To restore strong
performance under low-resource conditions, we revisit the hierarchical
knowledge concept (KC) information, which is typically available in many
classroom settings and can provide strong prior when data are sparse. We
therefore propose Knowledge-Tree-based Knowledge Tracing (KT$^2$), a
probabilistic KT framework that models student understanding over a
tree-structured hierarchy of knowledge concepts using a Hidden Markov Tree
Model. KT$^2$ estimates student mastery via an EM algorithm and supports
personalized prediction through an incremental update mechanism as new
responses arrive. Our experiments show that KT$^2$ consistently outperforms
strong baselines in realistic online, low-resource settings.

</details>


### [22] [Token Constraint Decoding Improves Robustness on Question Answering for Large Language Models](https://arxiv.org/abs/2506.09408)
*Jui-Ming Yao,Hao-Yuan Chen,Zi-Xian Tang,Bing-Jia Tan,Sheng-Wei Peng,Bing-Cheng Xie,Shun-Feng Su*

Main category: cs.CL

TL;DR: 本文提出Token Constraint Decoding (TCD)方法通过强制token级预测对齐，结合提示工程，显著提升大语言模型在噪声环境下的鲁棒性，尤其使小模型性能恢复达+39%。


<details>
  <summary>Details</summary>
Motivation: 大语言模型(LLMs)在多项选择题基准测试中表现优异，但对输入微小扰动高度敏感，限制了其在现实噪声场景中的可靠性应用。

Method: 提出推理时算法TCD，通过约束token级预测一致性增强鲁棒性，并与提示工程结合优化。通过惩罚机制调节模型过度自信输出。

Result: 在CommonsenseQA、MMLU等数据集上，TCD使Gemma3 1B等弱模型性能恢复最高达39%。惩罚参数分析揭示不同模型需差异化调度策略。

Conclusion: TCD作为模型无关方法，通过推理时正则化有效提升LLMs在现实缺陷下的推理稳定性，为安全关键场景部署奠定技术基础。

Abstract: Large Language Models (LLMs) have demonstrated impressive performance on
multiple-choice question answering (MCQA) benchmarks, yet they remain highly
vulnerable to minor input perturbations. In this paper, we introduce and
evaluate Token Constraint Decoding (TCD). This simple yet effective
inference-time algorithm enforces alignment between token-level predictions to
enhance robustness in noisy settings. Through extensive experiments on
CommonsenseQA, MMLU, and MMLU-Pro, we show that TCD, especially when paired
with prompt engineering (PE) fixes, significantly restores performance degraded
by input noise, yielding up to +39\% absolute gains for weaker models like
Gemma3 1B. Penalty sweep analyses further reveal that TCD implicitly
regularizes overconfident outputs, with different models requiring distinct
penalty schedules to maximize resilience. Our findings establish TCD as a
practical, model-agnostic approach for improving reasoning stability under
real-world imperfections and pave the way for more reliable deployment of LLMs
in safety-critical or user-facing applications.

</details>


### [23] [PGDA-KGQA: A Prompt-Guided Generative Framework with Multiple Data Augmentation Strategies for Knowledge Graph Question Answering](https://arxiv.org/abs/2506.09414)
*Xiujun Zhou,Pingjian Zhang,Deyou Tang*

Main category: cs.CL

TL;DR: PGDA-KGQA提出了一种基于提示引导的多策略数据增强框架，通过生成单跳伪问题、语义保留改写和答案引导反向路径探索，有效提升知识图谱问答的语义解析与多跳推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的KGQA方法受限于标注数据多样性不足和多跳推理样本稀缺，传统数据增强方法易导致语义失真或忽视多跳推理，导致模型泛化能力受限。

Method: 采用统一提示设计范式，结合三种数据增强策略：(1)生成单跳伪问题对齐KG关系；(2)语义保留式问题改写增强语言变体鲁棒性；(3)答案引导反向路径探索生成多跳问题，并通过增强-生成-检索流程提升逻辑形式生成精度。

Result: 在WebQSP数据集上F1、Hits@1和准确率分别提升2.8%、1.2%、3.1%，在ComplexWebQuestions上分别提升1.8%、1.1%、2.4%，均超越现有最优方法。

Conclusion: PGDA-KGQA通过多策略数据增强有效解决了语义对齐与多跳推理的平衡问题，显著提升了KGQA任务的性能，验证了提示工程与混合增强策略的有效性。

Abstract: Knowledge Graph Question Answering (KGQA) is a crucial task in natural
language processing that requires reasoning over knowledge graphs (KGs) to
answer natural language questions. Recent methods utilizing large language
models (LLMs) have shown remarkable semantic parsing capabilities but are
limited by the scarcity of diverse annotated data and multi-hop reasoning
samples. Traditional data augmentation approaches are focus mainly on
single-hop questions and prone to semantic distortion, while LLM-based methods
primarily address semantic distortion but usually neglect multi-hop reasoning,
thus limiting data diversity. The scarcity of multi-hop samples further weakens
models' generalization. To address these issues, we propose PGDA-KGQA, a
prompt-guided generative framework with multiple data augmentation strategies
for KGQA. At its core, PGDA-KGQA employs a unified prompt-design paradigm: by
crafting meticulously engineered prompts that integrate the provided textual
content, it leverages LLMs to generate large-scale (question, logical form)
pairs for model training. Specifically, PGDA-KGQA enriches its training set by:
(1) generating single-hop pseudo questions to improve the alignment of question
semantics with KG relations; (2) applying semantic-preserving question
rewriting to improve robustness against linguistic variations; (3) employing
answer-guided reverse path exploration to create realistic multi-hop questions.
By adopting an augment-generate-retrieve semantic parsing pipeline, PGDA-KGQA
utilizes the augmented data to enhance the accuracy of logical form generation
and thus improve answer retrieval performance. Experiments demonstrate that
outperforms state-of-the-art methods on standard KGQA datasets, achieving
improvements on WebQSP by 2.8%, 1.2%, and 3.1% and on ComplexWebQuestions by
1.8%, 1.1%, and 2.4% in F1, Hits@1, and Accuracy, respectively.

</details>


### [24] [Hidden in Plain Sight: Evaluation of the Deception Detection Capabilities of LLMs in Multimodal Settings](https://arxiv.org/abs/2506.09424)
*Md Messal Monem Miah,Adrita Anika,Xi Shi,Ruihong Huang*

Main category: cs.CL

TL;DR: 研究评估了大型语言模型（LLMs）和多模态模型（LMMs）在跨领域欺骗检测中的表现，发现微调后的LLMs在文本检测中表现优异，而LMMs难以有效利用多模态线索。实验分析了上下文示例选择、辅助特征及提示策略的影响。


<details>
  <summary>Details</summary>
Motivation: 数字时代，欺骗检测至关重要但极具挑战。研究旨在探索LLMs和LMMs在跨模态欺骗检测中的潜力与局限，为实际应用提供依据。

Method: 使用三个数据集（RLTD、MU3D、OpSpam），评估开源与商用LLMs的零样本/少样本学习能力，分析随机或相似性上下文示例选择、非语言特征（手势、视频摘要）及不同提示策略（直接标签生成、思维链推理）的效果。

Result: 微调LLMs在文本欺骗检测中达到最优性能，LMMs未能充分利用跨模态信息。辅助特征对提升检测效果有限，直接标签生成策略优于思维链推理。

Conclusion: LLMs在文本欺骗检测中潜力显著，但LMMs需改进跨模态整合能力。实际应用中需结合任务特性选择模型与策略，同时关注多模态线索的局限性。

Abstract: Detecting deception in an increasingly digital world is both a critical and
challenging task. In this study, we present a comprehensive evaluation of the
automated deception detection capabilities of Large Language Models (LLMs) and
Large Multimodal Models (LMMs) across diverse domains. We assess the
performance of both open-source and commercial LLMs on three distinct datasets:
real life trial interviews (RLTD), instructed deception in interpersonal
scenarios (MU3D), and deceptive reviews (OpSpam). We systematically analyze the
effectiveness of different experimental setups for deception detection,
including zero-shot and few-shot approaches with random or similarity-based
in-context example selection. Our results show that fine-tuned LLMs achieve
state-of-the-art performance on textual deception detection tasks, while LMMs
struggle to fully leverage cross-modal cues. Additionally, we analyze the
impact of auxiliary features, such as non-verbal gestures and video summaries,
and examine the effectiveness of different prompting strategies, including
direct label generation and chain-of-thought reasoning. Our findings provide
key insights into how LLMs process and interpret deceptive cues across
modalities, highlighting their potential and limitations in real-world
deception detection applications.

</details>


### [25] [Improved Supervised Fine-Tuning for Large Language Models to Mitigate Catastrophic Forgetting](https://arxiv.org/abs/2506.09428)
*Fei Ding,Baiqiao Wang*

Main category: cs.CL

TL;DR: 本文提出一种新型监督微调方法，通过重建指令分布与多模型筛选数据，在提升任务性能的同时减少灾难性遗忘，且无需原始训练数据。


<details>
  <summary>Details</summary>
Motivation: 传统监督微调(SFT)在增强大模型指令遵循能力时会导致通用能力下降，且第三方使用开源模型时因无法获取原始数据加剧遗忘问题。

Method: 1. 重建基础模型的SFT指令分布；2. 多模型筛选最优数据；3. 混合新旧数据进行监督微调。

Result: 实验表明该方法在保持通用领域能力的同时提升了特定任务表现，有效缓解了灾难性遗忘。

Conclusion: 提出的低成本SFT方法通过数据重建与混合机制，平衡了任务性能与模型通用性，为第三方优化开源模型提供了可行方案。

Abstract: Supervised Fine-Tuning (SFT), while enhancing large language models(LLMs)'
instruction-following capabilities and domain-specific task adaptability, often
diminishes their general capabilities. Moreover, due to the inaccessibility of
original pre-training data, catastrophic forgetting tends to be exacerbated
when third-party practitioners implement SFT on open-sourced models. To address
this challenge, we propose a novel, more cost-effective SFT method which could
effectively reduce the risk of catastrophic forgetting without access to
original SFT data. Our approach begins by reconstructing the likely SFT
instruction distribution of the base model, followed by a multi-model screening
process to select optimal data, which is then mixed with new data for SFT.
Experimental results demonstrate that our method preserves generalization
capabilities in general domains while improving task-specific performance.

</details>


### [26] [GigaChat Family: Efficient Russian Language Modeling Through Mixture of Experts Architecture](https://arxiv.org/abs/2506.09440)
*GigaChat team,Mamedov Valentin,Evgenii Kosarev,Gregory Leleytner,Ilya Shchuckin,Valeriy Berezovskiy,Daniil Smirnov,Dmitry Kozlov,Sergei Averkiev,Lukyanenko Ivan,Aleksandr Proshunin,Ainur Israfilova,Ivan Baskov,Artem Chervyakov,Emil Shakirov,Mikhail Kolesov,Daria Khomich,Darya Latortseva,Sergei Porkhun,Yury Fedorov,Oleg Kutuzov,Polina Kudriavtseva,Sofiia Soldatova,Kolodin Egor,Stanislav Pyatkin,Dzmitry Menshykh,Grafov Sergei,Eldar Damirov,Karlov Vladimir,Ruslan Gaitukiev,Arkadiy Shatenov,Alena Fenogenova,Nikita Savushkin,Fedor Minkin*

Main category: cs.CL

TL;DR: 本文介绍了专为俄语设计的GigaChat大语言模型系列，包括不同规模的基座模型及指令调优版本，并开源了三个模型以促进俄语NLP研究和应用。


<details>
  <summary>Details</summary>
Motivation: 俄语大语言模型开发因计算资源需求高而受限，需构建专用基础模型以支持俄语NLP研究与工业应用。

Method: 设计不同规模的GigaChat模型架构，详细报告预训练过程及实验设计，提供API、Telegram机器人及网页接口，并开源部分模型。

Result: GigaChat在俄语和英语基准测试中表现优异，与多语言模型对比验证其性能，开源模型已发布于Hugging Face平台。

Conclusion: GigaChat填补了俄语大模型的空白，开源模型及多样化访问方式将推动俄语NLP研究及工业解决方案发展。

Abstract: Generative large language models (LLMs) have become crucial for modern NLP
research and applications across various languages. However, the development of
foundational models specifically tailored to the Russian language has been
limited, primarily due to the significant computational resources required.
This paper introduces the GigaChat family of Russian LLMs, available in various
sizes, including base models and instruction-tuned versions. We provide a
detailed report on the model architecture, pre-training process, and
experiments to guide design choices. In addition, we evaluate their performance
on Russian and English benchmarks and compare GigaChat with multilingual
analogs. The paper presents a system demonstration of the top-performing models
accessible via an API, a Telegram bot, and a Web interface. Furthermore, we
have released three open GigaChat models in open-source
(https://huggingface.co/ai-sage), aiming to expand NLP research opportunities
and support the development of industrial solutions for the Russian language.

</details>


### [27] [UniToMBench: Integrating Perspective-Taking to Improve Theory of Mind in LLMs](https://arxiv.org/abs/2506.09450)
*Prameshwar Thiyagarajan,Vaishnavi Parimi,Shamant Sai,Soumil Garg,Zhangir Meirbek,Nitin Yarlagadda,Kevin Zhu,Chris Kim*

Main category: cs.CL

TL;DR: 本文提出统一基准UniToMBench，结合SimToM和TOMBENCH优势，通过多交互任务和动态场景评估大语言模型的心理理论能力。实验显示GPT-4o等模型在情感/信念任务中准确率超80%，但知识任务表现波动较大。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在心理理论（ToM）任务中难以准确预测人类心理状态，需系统性改进评估方法以揭示模型在社交推理中的局限性。

Method: 构建UniToMBench基准，整合多交互任务设计与动态故事情节，使用超1000个手工场景数据集，结合视角采择技术及多维评估指标，刺激模型社会认知能力。

Result: GPT-4o等模型在情感/信念相关任务中准确率超80%，但知识型任务表现存在显著差异，突显当前模型在ToM任务中的能力不均衡性。

Conclusion: UniToMBench揭示了现有模型的ToM能力边界，其综合性评估框架为未来模型优化提供重要工具，代码已开源以促进社区研究。

Abstract: Theory of Mind (ToM), the ability to understand the mental states of oneself
and others, remains a challenging area for large language models (LLMs), which
often fail to predict human mental states accurately. In this paper, we
introduce UniToMBench, a unified benchmark that integrates the strengths of
SimToM and TOMBENCH to systematically improve and assess ToM capabilities in
LLMs by integrating multi-interaction task designs and evolving story
scenarios. Supported by a custom dataset of over 1,000 hand-written scenarios,
UniToMBench combines perspective-taking techniques with diverse evaluation
metrics to better stimulate social cognition in LLMs. Through evaluation, we
observe that while models like GPT-4o and GPT-4o Mini show consistently high
accuracy in tasks involving emotional and belief-related scenarios, with
results usually above 80%, there is significant variability in their
performance across knowledge-based tasks. These results highlight both the
strengths and limitations of current LLMs in ToM-related tasks, underscoring
the value of UniToMBench as a comprehensive tool for future development. Our
code is publicly available here:
https://github.com/Shamant/unifiedtombenchmark.

</details>


### [28] [Towards Bridging the Reward-Generation Gap in Direct Alignment Algorithms](https://arxiv.org/abs/2506.09457)
*Zeguan Xiao,Yun Chen,Guanhua Chen*

Main category: cs.CL

TL;DR: 本文提出POET方法，通过截断正负样本至等长，解决直接对齐算法（DAAs）中奖励-生成差距问题，实验显示其在DPO和SimPO中显著提升生成性能。


<details>
  <summary>Details</summary>
Motivation: 直接对齐算法（DAAs）存在奖励-生成差距，即训练目标与推理生成表现不匹配，主要原因是模型生成过程中前缀词的重要性未在隐式奖励函数中正确体现。

Method: 提出POET方法：将正负样本响应截断至较短样本的长度，使DAAs优化目标在所有位置收敛，从而更关注前缀词的优化。

Result: 在DPO和SimPO中应用POET后，AlpacaEval 2得分最高提升15.6分，下游任务性能普遍提升。

Conclusion: POET通过解决奖励-生成差距，验证了优化过程中关注前缀词的重要性，为DAAs的改进提供了有效方向。

Abstract: Direct Alignment Algorithms (DAAs), such as Direct Preference Optimization
(DPO) and Simple Preference Optimization (SimPO), have emerged as efficient
alternatives to Reinforcement Learning from Human Feedback (RLHF) algorithms
for aligning large language models (LLMs) with human preferences. However, DAAs
suffer from a fundamental limitation we identify as the "reward-generation gap"
-- a misalignment between optimization objectives during training and actual
generation performance during inference. In this paper, we find a contributor
to the reward-generation gap is the mismatch between the inherent importance of
prefix tokens during the LLM generation process and how this importance is
reflected in the implicit reward functions of DAAs. To bridge the gap, we
introduce a simple yet effective approach called Prefix-Oriented Equal-length
Training (POET), which truncates both preferred and dispreferred responses to
match the shorter one's length. Training with POET, where both responses in
each sample are truncated to equal length, resulting in diverse truncated
lengths across samples, the optimization of DAAs objective is implicitly
constrained to converge across all positions, thus paying more attention to
prefix tokens than the standard DAAs. We conduct experiments with DPO and
SimPO, two representative DAAs, demonstrating that POET improves over their
standard implementations, achieving up to 15.6 points in AlpacaEval 2 and
overall improvements across downstream tasks. Our results highlight the
importance of addressing the misalignment between reward optimization and
generation performance in DAAs.

</details>


### [29] [Bridging Online Behavior and Clinical Insight: A Longitudinal LLM-based Study of Suicidality on YouTube Reveals Novel Digital Markers](https://arxiv.org/abs/2506.09495)
*Ilanit Sobol,Shir Lissak,Refael Tikochinski,Tal Nakash,Anat Brunstein Klomek,Eyal Fruchter,Roi Reichart*

Main category: cs.CL

TL;DR: 该研究通过结合自下而上的计算模型、混合方法和专家驱动的自上而下分析，发现YouTube平台特有的用户参与度指标与自杀行为相关，且自杀未遂者上传视频的动机存在时间差异，揭示了数字行为与临床认知间的鸿沟。


<details>
  <summary>Details</summary>
Motivation: 自杀仍是西方国家主要死因，传统研究方法需突破。随着社交媒体成为生活核心，数字足迹为研究自杀行为提供了新视角。研究聚焦YouTube上自杀未遂者的视频上传行为，探索其与专家认知的差异。

Method: 采用三阶段混合方法：1) 自下而上使用LLM主题建模分析181个自杀未遂者与134个对照组频道，识别行为指标；2) 混合方法由临床专家评估LLM生成主题；3) 自上而下通过心理评估分析自杀叙事差异。

Result: 自下而上方法发现5个与自杀相关主题，其中『心理健康挣扎』和『YouTube参与度』呈现显著时间变化(p<.01)。专家未识别出平台特有的参与度指标。心理评估显示自杀时间与视频上传动机显著相关：事前未遂者旨在帮助他人(β=-1.69)，事中上传者则视为个人康复部分(β=1.08)。

Conclusion: 整合多维度方法揭示了数字行为特征与临床认知的差异，强调自下而上方法在发现平台特异性指标的价值。自杀未遂者的内容创作动机存在时间维度差异，为数字时代自杀预防提供了新视角。

Abstract: Suicide remains a leading cause of death in Western countries, underscoring
the need for new research approaches. As social media becomes central to daily
life, digital footprints offer valuable insight into suicidal behavior.
Focusing on individuals who attempted suicide while uploading videos to their
channels, we investigate: How do suicidal behaviors manifest on YouTube, and
how do they differ from expert knowledge? We applied complementary approaches:
computational bottom-up, hybrid, and expert-driven top-down, on a novel
longitudinal dataset of 181 YouTube channels from individuals with
life-threatening attempts, alongside 134 control channels. In the bottom-up
approach, we applied LLM-based topic modeling to identify behavioral
indicators. Of 166 topics, five were associated with suicide-attempt, with two
also showing temporal attempt-related changes ($p<.01$) - Mental Health
Struggles ($+0.08$)* and YouTube Engagement ($+0.1$)*. In the hybrid approach,
a clinical expert reviewed LLM-derived topics and flagged 19 as
suicide-related. However, none showed significant attempt-related temporal
effects beyond those identified bottom-up. Notably, YouTube Engagement, a
platform-specific indicator, was not flagged by the expert, underscoring the
value of bottom-up discovery. In the top-down approach, psychological
assessment of suicide attempt narratives revealed that the only significant
difference between individuals who attempted before and those attempted during
their upload period was the motivation to share this experience: the former
aimed to Help Others ($\beta=-1.69$, $p<.01$), while the latter framed it as
part of their Personal Recovery ($\beta=1.08$, $p<.01$). By integrating these
approaches, we offer a nuanced understanding of suicidality, bridging digital
behavior and clinical insights.
  * Within-group changes in relation to the suicide attempt.

</details>


### [30] [Give Me FP32 or Give Me Death? Challenges and Solutions for Reproducible Reasoning](https://arxiv.org/abs/2506.09501)
*Jiayi Yuan,Hao Li,Xinheng Ding,Wenya Xie,Yu-Jhe Li,Wentian Zhao,Kun Wan,Jing Shi,Xia Hu,Zirui Liu*

Main category: cs.CL

TL;DR: 研究发现大语言模型（LLMs）的性能评估因浮点运算精度问题存在显著不可复现性，尤其在推理任务中，硬件配置差异会导致准确率波动高达9%。作者提出轻量级推理框架LayerCast，在保持16位存储的同时使用FP32计算以平衡效率与稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM评估常忽视数值精度对结果的影响，导致基准测试结果不可靠。本文旨在揭示浮点运算的非结合性在有限精度下如何引发模型输出的不一致性，并探索解决方案。

Method: 通过控制实验分析不同硬件、软件配置及精度设置对模型输出影响，提出LayerCast框架：以16位存储权重，但所有计算采用FP32精度。

Result: 实验显示，在bfloat16精度下，推理模型因GPU数量/类型和批次大小的差异，准确率波动达9%，响应长度差异达9000词。LayerCast在保持内存效率的同时显著提升数值稳定性。

Conclusion: LLM评估需重视数值精度对可复现性的影响。LayerCast通过分离存储与计算精度，为平衡效率与稳定性提供了可行方案，代码已发布。

Abstract: Large Language Models (LLMs) are now integral across various domains and have
demonstrated impressive performance. Progress, however, rests on the premise
that benchmark scores are both accurate and reproducible. We demonstrate that
the reproducibility of LLM performance is fragile: changing system
configuration such as evaluation batch size, GPU count, and GPU version can
introduce significant difference in the generated responses. This issue is
especially pronounced in reasoning models, where minor rounding differences in
early tokens can cascade into divergent chains of thought, ultimately affecting
accuracy. For instance, under bfloat16 precision with greedy decoding, a
reasoning model like DeepSeek-R1-Distill-Qwen-7B can exhibit up to 9% variation
in accuracy and 9,000 tokens difference in response length due to differences
in GPU count, type, and evaluation batch size. We trace the root cause of this
variability to the non-associative nature of floating-point arithmetic under
limited numerical precision. This work presents the first systematic
investigation into how numerical precision affects reproducibility in LLM
inference. Through carefully controlled experiments across various hardware,
software, and precision settings, we quantify when and how model outputs
diverge. Our analysis reveals that floating-point precision -- while critical
for reproducibility -- is often neglected in evaluation practices. Inspired by
this, we develop a lightweight inference pipeline, dubbed LayerCast, that
stores weights in 16-bit precision but performs all computations in FP32,
balancing memory efficiency with numerical stability. Code is available at
https://github.com/nanomaoli/llm_reproducibility.

</details>


### [31] [TransXSSM: A Hybrid Transformer State Space Model with Unified Rotary Position Embedding](https://arxiv.org/abs/2506.09507)
*Bingheng Wu,Jingze Shi,Yifan Wu,Nan Tang,Yuyu Luo*

Main category: cs.CL

TL;DR: 本文提出了一种统一的旋转位置编码方法（RoPE），将Transformer与状态空间模型（SSM）结合，解决了二者位置编码机制不兼容的问题，实现了高效、高性能的长上下文建模。


<details>
  <summary>Details</summary>
Motivation: Transformer依赖显式旋转位置嵌入（RoPE），而SSM通过卷积隐式编码位置，二者整合时因位置编码机制不兼容，导致性能下降。需解决这一矛盾以实现高效混合建模。

Method: 提出统一旋转位置编码框架（RoPE），构建混合架构\model，在自注意力和状态空间组件中采用一致的位置编码方案，实现Transformer与SSM层的协同整合。

Result: 在4K序列长度下，\model训练/推理速度比标准Transformer快42.3%/29.5%，语言建模准确率提升超4%。1.3B版本比320M版本平均准确率增益达7.22%，优于同类模型。

Conclusion: 统一位置编码消除了混合模型的位置不兼容性，证明其能效与性能优势，为长上下文建模提供了高效解决方案。

Abstract: Transformers exhibit proficiency in capturing long-range dependencies,
whereas State Space Models (SSMs) facilitate linear-time sequence modeling.
Notwithstanding their synergistic potential, the integration of these
architectures presents a significant challenge, primarily attributable to a
fundamental incongruity in their respective positional encoding mechanisms:
Transformers rely on explicit Rotary Position Embeddings (RoPE), while SSMs
leverage implicit positional representations via convolutions. This divergence
often precipitates discontinuities and suboptimal performance. To address this
impediment, we propose a unified rotary position embedding (\textbf{\ourRoPE})
methodology, thereby establishing a consistent positional encoding framework
for both self-attention and state-space components. Using this \ourRoPE, we
introduce \textbf{\model}, a hybrid architecture that coherently integrates the
Transformer and SSM layers under this unified positional encoding scheme. At a
4K sequence length, \model exhibits training and inference speeds that are
\textbf{42.3\% and 29.5\% faster}, respectively, relative to standard
Transformer models. It also delivers higher accuracy: under comparable
settings, it surpasses a Transformer baseline by over 4\% on language modeling
benchmarks. \model furthermore scales more effectively: \model-1.3B gains
\textbf{7.22\%} in average accuracy over its 320M version (versus about 6\%
gains for equivalent Transformers or SSMs). Our results show that unified
positional encoding resolves positional incompatibility in hybrid models,
enabling efficient, high-performance long-context modeling.

</details>


### [32] [ReasonMed: A 370K Multi-Agent Generated Dataset for Advancing Medical Reasoning](https://arxiv.org/abs/2506.09513)
*Yu Sun,Xingyu Qian,Weiwen Xu,Hao Zhang,Chenghao Xiao,Long Li,Yu Rong,Wenbing Huang,Qifeng Bai,Tingyang Xu*

Main category: cs.CL

TL;DR: 研究者构建了最大医学推理数据集ReasonMed，通过多智能体验证和精炼流程提升数据质量，并基于此训练出高效小模型ReasonMed-7B，在医学问答任务中超越大模型。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在数学和编程领域表现优异，但其在知识密集型医学问答中的潜力尚未充分挖掘，需针对性优化推理能力。

Method: 提出多智能体验证框架：1) 从170万初始推理路径蒸馏出37万高质量样本；2) 设计错误修正器(Error Refiner)迭代修正验证器标记的错误步骤。

Result: ReasonMed-7B在sub-10B模型中创性能新高，超越前最佳模型4.17%，在PubMedQA上以7B参数量超越LLaMA3.1-70B达4.60%。

Conclusion: 结合详细思维链与精简答案摘要的微调策略，配合高质量数据集构建方法，可使小模型在医学推理任务中实现超越大模型的性能突破。

Abstract: Though reasoning-based large language models (LLMs) have excelled in
mathematics and programming, their capabilities in knowledge-intensive medical
question answering remain underexplored. To address this, we introduce
ReasonMed, the largest medical reasoning dataset, comprising 370k high-quality
examples distilled from 1.7 million initial reasoning paths generated by
various LLMs. ReasonMed is constructed through a \textit{multi-agent
verification and refinement process}, where we design an \textit{Error Refiner}
to enhance the reasoning paths by identifying and correcting error-prone steps
flagged by a verifier. Leveraging ReasonMed, we systematically investigate best
practices for training medical reasoning models and find that combining
detailed Chain-of-Thought (CoT) reasoning with concise answer summaries yields
the most effective fine-tuning strategy. Based on this strategy, we train
ReasonMed-7B, which sets a new benchmark for sub-10B models, outperforming the
prior best by 4.17\% and even exceeding LLaMA3.1-70B on PubMedQA by 4.60\%.

</details>


### [33] [KG-Infused RAG: Augmenting Corpus-Based RAG with External Knowledge Graphs](https://arxiv.org/abs/2506.09542)
*Dingjun Wu,Yukun Yan,Zhenghao Liu,Zhiyuan Liu,Maosong Sun*

Main category: cs.CL

TL;DR: KG-Infused RAG框架通过整合知识图谱(KG)与扩散激活认知机制，结合结构化与非结构化知识源，并利用偏好学习优化关键阶段，显著提升RAG模型在QA任务中的性能表现。


<details>
  <summary>Details</summary>
Motivation: 现有RAG方法依赖单一知识源（非结构化文本或结构化知识），且缺乏认知科学启发的知识激活机制，导致多源知识融合不足与推理能力受限。

Method: 提出KG-Infused RAG框架：1) 检索KG事实并扩展查询 2) 通过扩散激活机制实现语义关联推理 3) 结合语料库段落与结构化事实生成答案 4) 在流程关键阶段引入偏好学习优化

Result: 在5个QA基准测试中，KG-Infused RAG相比基础RAG提升3.8%-13.8%，集成至Self-RAG后进一步获得性能增益，验证其作为即插即用增强模块的有效性与通用性。

Conclusion: 通过融合知识图谱的语义结构与认知启发的扩散激活机制，KG-Infused RAG实现了可解释的多源检索，为基于语料的RAG方法提供了可扩展的性能增强方案。

Abstract: Retrieval-Augmented Generation (RAG) improves factual accuracy by grounding
responses in external knowledge. However, existing methods typically rely on a
single source, either unstructured text or structured knowledge. Moreover, they
lack cognitively inspired mechanisms for activating relevant knowledge. To
address these issues, we propose KG-Infused RAG, a framework that integrates
KGs into RAG systems to implement spreading activation, a cognitive process
that enables concept association and inference. KG-Infused RAG retrieves KG
facts, expands the query accordingly, and enhances generation by combining
corpus passages with structured facts, enabling interpretable, multi-source
retrieval grounded in semantic structure. We further improve KG-Infused RAG via
preference learning on sampled key stages in the pipeline. Experiments on five
QA benchmarks show that KG-Infused RAG consistently outperforms vanilla RAG (by
3.8% to 13.8%). Additionally, when integrated into Self-RAG, KG-Infused RAG
brings further performance gains, demonstrating its effectiveness and
versatility as a plug-and-play enhancement module for corpus-based RAG methods.

</details>


### [34] [MEDUSA: A Multimodal Deep Fusion Multi-Stage Training Framework for Speech Emotion Recognition in Naturalistic Conditions](https://arxiv.org/abs/2506.09556)
*Georgios Chatzichristodoulou,Despoina Kosmopoulou,Antonios Kritikos,Anastasia Poulopoulou,Efthymios Georgiou,Athanasios Katsamanis,Vassilis Katsouros,Alexandros Potamianos*

Main category: cs.CL

TL;DR: MEDUSA是一个多模态框架，通过四阶段训练流程解决语音情感识别中的类别不平衡和情感模糊性问题，结合深度跨模态变换器融合和元分类器优化，在Interspeech 2025挑战赛中取得第一名。


<details>
  <summary>Details</summary>
Motivation: 语音情感识别（SER）因人类情绪的主观性及自然条件下数据分布不均而具有挑战性，需开发能处理类别不平衡和情感模糊性的方法。

Method: 提出四阶段训练框架：前两阶段利用DeepSER（基于预训练声学/语言表征的跨模态变换器）训练集成分类器，并采用Manifold MixUp正则化；后两阶段通过多任务学习和软目标优化元分类器。

Result: MEDUSA在Interspeech 2025挑战赛的『自然条件下分类情感识别』任务中排名第一。

Conclusion: MEDUSA通过多模态融合、集成学习及结合人类标注的软目标，有效提升自然条件下SER性能，验证了框架的优越性。

Abstract: SER is a challenging task due to the subjective nature of human emotions and
their uneven representation under naturalistic conditions. We propose MEDUSA, a
multimodal framework with a four-stage training pipeline, which effectively
handles class imbalance and emotion ambiguity. The first two stages train an
ensemble of classifiers that utilize DeepSER, a novel extension of a deep
cross-modal transformer fusion mechanism from pretrained self-supervised
acoustic and linguistic representations. Manifold MixUp is employed for further
regularization. The last two stages optimize a trainable meta-classifier that
combines the ensemble predictions. Our training approach incorporates human
annotation scores as soft targets, coupled with balanced data sampling and
multitask learning. MEDUSA ranked 1st in Task 1: Categorical Emotion
Recognition in the Interspeech 2025: Speech Emotion Recognition in Naturalistic
Conditions Challenge.

</details>


### [35] [Gender Bias in English-to-Greek Machine Translation](https://arxiv.org/abs/2506.09558)
*Eleni Gkovedarou,Joke Daems,Luna De Bruyne*

Main category: cs.CL

TL;DR: 研究分析了Google Translate和DeepL在英译希腊语中的性别偏见，发现两者在性别明确时表现良好，但在模糊情况下无法生成包容性翻译。GPT-4o虽能提供性别中立替代方案，但仍存在残余偏见。


<details>
  <summary>Details</summary>
Motivation: 随着包容性语言需求增长，机器翻译系统可能强化性别刻板印象的问题引发关注。研究聚焦未被充分探索的英译希腊语场景，评估商业系统在男性偏见、职业刻板印象及反刻板翻译错误中的表现。

Method: 构建人工标注的双语数据集GendEL（含240个性别明确/模糊句子），测试两大商用MT系统及GPT-4o在性别中立翻译生成能力，特别关注提示工程对偏见缓解的作用。

Result: DeepL在女性性别明确句子上优于Google和GPT-4o，但所有系统在性别模糊时均无法生成中立翻译。GPT-4o能为74%模糊案例生成合适替代方案，但仍存在残余刻板印象。

Conclusion: 现有商用MT系统尚未实现性别包容翻译，GPT-4o通过提示工程展现偏见缓解潜力，但需进一步优化以消除残余偏见。研究强调开发系统性偏见检测与缓解方案的必要性。

Abstract: As the demand for inclusive language increases, concern has grown over the
susceptibility of machine translation (MT) systems to reinforce gender
stereotypes. This study investigates gender bias in two commercial MT systems,
Google Translate and DeepL, focusing on the understudied English-to-Greek
language pair. We address three aspects of gender bias: i) male bias, ii)
occupational stereotyping, and iii) errors in anti-stereotypical translations.
Additionally, we explore the potential of prompted GPT-4o as a bias mitigation
tool that provides both gender-explicit and gender-neutral alternatives when
necessary. To achieve this, we introduce GendEL, a manually crafted bilingual
dataset of 240 gender-ambiguous and unambiguous sentences that feature
stereotypical occupational nouns and adjectives. We find persistent gender bias
in translations by both MT systems; while they perform well in cases where
gender is explicitly defined, with DeepL outperforming both Google Translate
and GPT-4o in feminine gender-unambiguous sentences, they are far from
producing gender-inclusive or neutral translations when the gender is
unspecified. GPT-4o shows promise, generating appropriate gendered and neutral
alternatives for most ambiguous cases, though residual biases remain evident.

</details>


### [36] [Towards Open Foundation Language Model and Corpus for Macedonian: A Low-Resource Language](https://arxiv.org/abs/2506.09560)
*Stefan Krsteski,Matea Tashkovska,Borjan Sazdov,Hristijan Gjoreski,Branislav Gerazov*

Main category: cs.CL

TL;DR: 本文为马其顿语构建了大规模语料库、指令数据集和评估套件，并训练出性能优于同规模模型的8B参数LLM，促进低资源地区技术应用。


<details>
  <summary>Details</summary>
Motivation: 全球技术应用需要适应低资源语言，但现有大语言模型对马其顿语等低资源语言支持不足，限制了相关国家的技术发展。

Method: 收集40GB马其顿语语料库(3.5B词)和10.6万条文化相关指令数据，构建包含7个基准的评估体系，训练8B参数domestic-yak模型。

Result: 模型在8B参数级别全面超越基线，性能接近大10倍的模型。母语者评估显示其语法正确性和文化适配性优于大型模型。

Conclusion: 公开数据集、代码和模型权重为低资源语言LLM研究奠定基础，证明通过针对性优化可实现小模型的高效文化适配。

Abstract: The increase in technological adoption worldwide comes with demands for novel
tools to be used by the general population. Large Language Models (LLMs)
provide a great opportunity in this respect, but their capabilities remain
limited for low-resource languages, restricting applications in countries where
such languages are spoken. We create several resources to facilitate the
adoption of LLMs and to support research advancements for Macedonian. We
collect the largest Macedonian corpus to date, consisting of 40GB of textual
data and totaling 3.5B words. To support conversational applications, we
collect a 106k-instance instruction dataset, carefully built to be culturally
grounded. For evaluation, we construct a Macedonian evaluation suite covering
seven benchmarks. Finally, we train domestic-yak, a state-of-the-art
8B-parameter model, on our curated datasets and evaluate it against eight
baseline models using the newly constructed benchmark suite. Our model
outperforms all existing models in the 8B parameter range across all
benchmarks, and achieves performance comparable to models up to 10x larger.
Furthermore, a qualitative analysis with native speakers reveals that our model
is preferred over larger counterparts, receiving higher ratings for grammatical
correctness and cultural appropriateness. All datasets, code, and model weights
are openly released, setting a foundation for advancing LLMs in similarly
underrepresented languages. These resources are publicly available at
github.com/LVSTCK for source code, and at huggingface.co/LVSTCK for pretrained
model weights and data.

</details>


### [37] [From Symbolic to Neural and Back: Exploring Knowledge Graph-Large Language Model Synergies](https://arxiv.org/abs/2506.09566)
*Blaž Škrlj,Boshko Koloski,Senja Pollak,Nada Lavrač*

Main category: cs.CL

TL;DR: 本文系统综述知识图谱（KG）与大型语言模型（LLM）的协同方法，提出KG增强LLM和LLM优化KG的双向框架，强调可扩展性、计算性能与数据质量，并指出未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 整合知识图谱的结构化知识到LLM中，以增强模型的事实推理能力、减少幻觉，同时利用LLM提升KG的构建与查询效率，解决现有研究中动态更新、数据可靠性等关键问题。

Method: 将现有方法分为两类：KG增强的LLM（提升推理、复杂问答）和LLM增强的KG（支持KG构建与补全），并通过系统性分析对比技术路径。

Result: 揭示KG与LLM融合的互惠潜力，识别可扩展性、计算效率等核心挑战，提出神经符号整合、动态更新、伦理等未来方向。

Conclusion: KG与LLM的深度协同可推动智能系统处理复杂现实任务，未来需关注动态知识更新、数据质量及伦理问题。

Abstract: Integrating structured knowledge from Knowledge Graphs (KGs) into Large
Language Models (LLMs) enhances factual grounding and reasoning capabilities.
This survey paper systematically examines the synergy between KGs and LLMs,
categorizing existing approaches into two main groups: KG-enhanced LLMs, which
improve reasoning, reduce hallucinations, and enable complex question
answering; and LLM-augmented KGs, which facilitate KG construction, completion,
and querying. Through comprehensive analysis, we identify critical gaps and
highlight the mutual benefits of structured knowledge integration. Compared to
existing surveys, our study uniquely emphasizes scalability, computational
efficiency, and data quality. Finally, we propose future research directions,
including neuro-symbolic integration, dynamic KG updating, data reliability,
and ethical considerations, paving the way for intelligent systems capable of
managing more complex real-world knowledge tasks.

</details>


### [38] [Memorization in Language Models through the Lens of Intrinsic Dimension](https://arxiv.org/abs/2506.09591)
*Stefan Arnold*

Main category: cs.CL

TL;DR: 研究发现语言模型（LM）的记忆行为受序列内在维度（ID）调控，高ID序列在过参数化模型和稀疏暴露下更不易被记忆，揭示了模型规模、数据暴露与结构复杂性对记忆的交互影响。


<details>
  <summary>Details</summary>
Motivation: 现有研究关注上下文长度、参数规模等对LM无意记忆的影响，但潜在结构（如序列复杂性）如何调节记忆尚不明确。本文旨在探索ID作为结构复杂性的几何指标如何抑制记忆。

Method: 通过分析序列在潜在空间中的内在维度（ID），研究其与记忆率的关系，重点关注过参数化模型和稀疏暴露条件下的表现。

Result: 高ID序列比低ID序列更难被记忆，尤其在模型参数过多或数据暴露不足时，ID成为抑制记忆的关键信号。

Conclusion: 模型规模、数据暴露频率和序列结构复杂性（ID）共同塑造LM的记忆行为，ID的抑制效应为理解记忆机制提供了新视角。

Abstract: Language Models (LMs) are prone to memorizing parts of their data during
training and unintentionally emitting them at generation time, raising concerns
about privacy leakage and disclosure of intellectual property. While previous
research has identified properties such as context length, parameter size, and
duplication frequency, as key drivers of unintended memorization, little is
known about how the latent structure modulates this rate of memorization. We
investigate the role of Intrinsic Dimension (ID), a geometric proxy for the
structural complexity of a sequence in latent space, in modulating
memorization. Our findings suggest that ID acts as a suppressive signal for
memorization: compared to low-ID sequences, high-ID sequences are less likely
to be memorized, particularly in overparameterized models and under sparse
exposure. These findings highlight the interaction between scale, exposure, and
complexity in shaping memorization.

</details>


### [39] [Benchmarking Debiasing Methods for LLM-based Parameter Estimates](https://arxiv.org/abs/2506.09627)
*Nicolas Audinet de Pieuchon,Adel Daoud,Connor T. Jerzak,Moa Johansson,Richard Johansson*

Main category: cs.CL

TL;DR: 研究比较了两种结合大语言模型（LLM）与专家标注的去偏方法（DSL和PPI），发现DSL在有限样本下通常偏差更小、效率更高，但稳定性较差，需权衡偏差与方差。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLM）标注成本低但存在不一致性，其误差会影响下游参数估计（如回归系数）。现有去偏方法（如DSL和PPI）在理论上有效，但实际有限样本中的表现尚不明确。

Method: 通过分析不同专家标注数量下DSL和PPI的性能变化，并在多任务中比较两者表现，评估其偏差、效率及稳定性。

Result: DSL在多数任务中比PPI更有效降低偏差且效率更高，但其表现随数据集波动较大；两种方法在大样本下均较低偏差，但小样本中受LLM偏差和专家标注量影响显著。

Conclusion: 去偏方法存在偏差-方差权衡，需开发新指标量化其有限样本效率，并进一步研究提升DSL跨数据集稳定性。

Abstract: Large language models (LLMs) offer an inexpensive yet powerful way to
annotate text, but are often inconsistent when compared with experts. These
errors can bias downstream estimates of population parameters such as
regression coefficients and causal effects. To mitigate this bias, researchers
have developed debiasing methods such as Design-based Supervised Learning (DSL)
and Prediction-Powered Inference (PPI), which promise valid estimation by
combining LLM annotations with a limited number of expensive expert
annotations. Although these methods produce consistent estimates under
theoretical assumptions, it is unknown how they compare in finite samples of
sizes encountered in applied research. We make two contributions: First, we
study how each method's performance scales with the number of expert
annotations, highlighting regimes where LLM bias or limited expert labels
significantly affect results. Second, we compare DSL and PPI across a range of
tasks, finding that although both achieve low bias with large datasets, DSL
often outperforms PPI on bias reduction and empirical efficiency, but its
performance is less consistent across datasets. Our findings indicate that
there is a bias-variance tradeoff at the level of debiasing methods, calling
for more research on developing metrics for quantifying their efficiency in
finite samples.

</details>


### [40] [Modeling Probabilistic Reduction using Information Theory and Naive Discriminative Learning](https://arxiv.org/abs/2506.09641)
*Anna Stein,Kevin Tang*

Main category: cs.CL

TL;DR: 研究比较了基于信息论的概率预测器与朴素判别学习（NDL）预测器在声学词时长建模中的表现，发现N-gram模型优于NDL模型，但信息论公式可提升NDL效果，强调需结合频率、上下文预测性及信息论指标。


<details>
  <summary>Details</summary>
Motivation: 验证NDL因认知动机而更有效的假设，并探索信息论公式对NDL模型的改进潜力，以优化声学缩减建模。

Method: 使用Buckeye语料库，构建三种模型：信息论公式增强的NDL模型、传统NDL模型、N-gram概率模型，对比其预测性能。

Result: N-gram模型表现最佳，传统NDL模型最弱；但信息论公式显著提升了NDL模型性能，表明其与判别学习信息的结合具有价值。

Conclusion: 声学缩减建模需综合频率、上下文预测性及平均预测性，同时整合信息论指标与判别学习信息，以提升模型解释力与效果。

Abstract: This study compares probabilistic predictors based on information theory with
Naive Discriminative Learning (NDL) predictors in modeling acoustic word
duration, focusing on probabilistic reduction. We examine three models using
the Buckeye corpus: one with NDL-derived predictors using information-theoretic
formulas, one with traditional NDL predictors, and one with N-gram
probabilistic predictors. Results show that the N-gram model outperforms both
NDL models, challenging the assumption that NDL is more effective due to its
cognitive motivation. However, incorporating information-theoretic formulas
into NDL improves model performance over the traditional model. This research
highlights a) the need to incorporate not only frequency and contextual
predictability but also average contextual predictability, and b) the
importance of combining information-theoretic metrics of predictability and
information derived from discriminative learning in modeling acoustic
reduction.

</details>


### [41] [Using Sign Language Production as Data Augmentation to enhance Sign Language Translation](https://arxiv.org/abs/2506.09643)
*Harry Walsh,Maksym Ivashechkin,Richard Bowden*

Main category: cs.CL

TL;DR: 该论文提出利用手语生成技术（Sign Language Production）增强低资源手语数据集，通过生成多样化手语视频提升手语翻译模型性能，实验显示最高提升19%。


<details>
  <summary>Details</summary>
Motivation: 手语作为低资源语言，数据集规模远小于口语语言，且数据收集面临成本、稀缺性和隐私限制。现有手语翻译模型因数据不足而性能受限，需通过数据增强解决资源瓶颈。

Method: 结合三种技术：1）基于骨架的手语生成方法；2）手语片段拼接（sign stitching）；3）两种逼真生成模型（SignGAN和SignSplat），通过改变手语者的外观和骨骼运动生成多样化数据。

Result: 实验表明，所提方法能有效扩充数据集，使手语翻译模型性能最高提升19%。

Conclusion: 该方法为资源受限环境下构建更鲁棒、准确的手语翻译系统提供了可行路径，验证了手语生成技术对翻译任务的数据增强潜力。

Abstract: Machine learning models fundamentally rely on large quantities of
high-quality data. Collecting the necessary data for these models can be
challenging due to cost, scarcity, and privacy restrictions. Signed languages
are visual languages used by the deaf community and are considered low-resource
languages. Sign language datasets are often orders of magnitude smaller than
their spoken language counterparts. Sign Language Production is the task of
generating sign language videos from spoken language sentences, while Sign
Language Translation is the reverse translation task. Here, we propose
leveraging recent advancements in Sign Language Production to augment existing
sign language datasets and enhance the performance of Sign Language Translation
models. For this, we utilize three techniques: a skeleton-based approach to
production, sign stitching, and two photo-realistic generative models, SignGAN
and SignSplat. We evaluate the effectiveness of these techniques in enhancing
the performance of Sign Language Translation models by generating variation in
the signer's appearance and the motion of the skeletal data. Our results
demonstrate that the proposed methods can effectively augment existing datasets
and enhance the performance of Sign Language Translation models by up to 19%,
paving the way for more robust and accurate Sign Language Translation systems,
even in resource-constrained environments.

</details>


### [42] [Learning Efficient and Generalizable Graph Retriever for Knowledge-Graph Question Answering](https://arxiv.org/abs/2506.09645)
*Tianjun Yao,Haoxuan Li,Zhiqiang Shen,Pan Li,Tongliang Liu,Kun Zhang*

Main category: cs.CL

TL;DR: 提出RAPL框架，通过两阶段标注、模型无关的图转换和路径推理策略，提升知识图谱问答（KGQA）中图检索的效率和泛化能力，实验显示其显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于检索增强生成（RAG）的方法依赖非结构化文本，导致可解释性和结构化推理受限；而基于知识图谱的检索器虽表现良好，但泛化能力不足。需解决检索效率与泛化性之间的平衡问题。

Method: RAPL框架包含三部分：1）结合启发式信号与参数模型的两阶段标注策略，提供因果监督；2）模型无关的图转换方法，捕获三元组内/间交互以增强表示；3）基于路径的推理策略，利用结构化输入支持下游推理。

Result: RAPL在KGQA任务中超越SOTA方法2.66%-20.34%，显著缩小小模型与强大LLM间的性能差距，并在跨数据集场景下保持优势，验证其检索能力与泛化性。

Conclusion: RAPL通过结构化检索增强，有效提升KGQA性能，减少不同规模LLM间的表现差异，为知识图谱与LLM的融合提供了高效且通用的解决方案。

Abstract: Large Language Models (LLMs) have shown strong inductive reasoning ability
across various domains, but their reliability is hindered by the outdated
knowledge and hallucinations. Retrieval-Augmented Generation mitigates these
issues by grounding LLMs with external knowledge; however, most existing RAG
pipelines rely on unstructured text, limiting interpretability and structured
reasoning. Knowledge graphs, which represent facts as relational triples, offer
a more structured and compact alternative. Recent studies have explored
integrating knowledge graphs with LLMs for knowledge graph question answering
(KGQA), with a significant proportion adopting the retrieve-then-reasoning
paradigm. In this framework, graph-based retrievers have demonstrated strong
empirical performance, yet they still face challenges in generalization
ability. In this work, we propose RAPL, a novel framework for efficient and
effective graph retrieval in KGQA. RAPL addresses these limitations through
three aspects: (1) a two-stage labeling strategy that combines heuristic
signals with parametric models to provide causally grounded supervision; (2) a
model-agnostic graph transformation approach to capture both intra- and
inter-triple interactions, thereby enhancing representational capacity; and (3)
a path-based reasoning strategy that facilitates learning from the injected
rational knowledge, and supports downstream reasoner through structured inputs.
Empirically, RAPL outperforms state-of-the-art methods by $2.66\%-20.34\%$, and
significantly reduces the performance gap between smaller and more powerful
LLM-based reasoners, as well as the gap under cross-dataset settings,
highlighting its superior retrieval capability and generalizability. Codes are
available at: https://github.com/tianyao-aka/RAPL.

</details>


### [43] [Bridging the Gap Between Open-Source and Proprietary LLMs in Table QA](https://arxiv.org/abs/2506.09657)
*Nikolas Evkarpidi,Elena Tutubalina*

Main category: cs.CL

TL;DR: 本文提出了一种结合文本到SQL/代码生成、自校正机制、检索增强生成（RAG）和端到端模块的表格问答系统，通过大语言模型协调各组件，在SemEval 2025 Task 8中获得80%准确率（Top-13/38），开源模型性能显著提升且接近商业模型。


<details>
  <summary>Details</summary>
Motivation: 针对表格数据问答任务中开源模型性能不足的问题，探索通过模块化集成与LLM协调机制提升准确率，缩小与专有大语言模型的差距。

Method: 整合文本到SQL/代码生成模块、自校正机制、RAG和端到端模块，通过大语言模型进行流程协调，并采用消融实验分析组件贡献度。

Result: 系统在竞赛评估中获得80%准确率（排名13/38），开源模型准确率显著提升，性能接近专有LLM的表格处理水平，代码已开源。

Conclusion: 模块化架构与LLM协调机制有效提升表格问答性能，但领域仍存在未解决挑战，需进一步研究复杂表格结构的语义解析能力。

Abstract: This paper presents a system developed for SemEval 2025 Task 8: Question
Answering (QA) over tabular data. Our approach integrates several key
components: text-to-SQL and text-to-code generation modules, a self-correction
mechanism, and a retrieval-augmented generation (RAG). Additionally, it
includes an end-to-end (E2E) module, all orchestrated by a large language model
(LLM). Through ablation studies, we analyzed the effects of different parts of
our pipeline and identified the challenges that are still present in this
field. During the evaluation phase of the competition, our solution achieved an
accuracy of 80%, resulting in a top-13 ranking among the 38 participating
teams. Our pipeline demonstrates a significant improvement in accuracy for
open-source models and achieves a performance comparable to proprietary LLMs in
QA tasks over tables. The code is available at GitHub repository.

</details>


### [44] [Query-Level Uncertainty in Large Language Models](https://arxiv.org/abs/2506.09669)
*Lihu Chen,Gaël Varoquaux*

Main category: cs.CL

TL;DR: 本文提出一种无需训练的“内部置信度”方法，通过查询级别的不确定性检测大语言模型的知识边界，以优化自适应推理机制，在保持性能的同时降低推理成本。


<details>
  <summary>Details</summary>
Motivation: 大语言模型需明确自身知识边界以进行自适应推理（如调用RAG、深度思考或弃答机制），从而提升AI的可靠性和效率。

Method: 基于查询级别不确定性检测，提出无训练的“内部置信度”方法，利用模型各层及token的自我评估判断能否回答查询。

Result: 在事实QA和数学推理任务中，内部置信度优于基线方法，且能有效用于RAG和模型级联，降低推理成本并保持性能。

Conclusion: 该方法通过检测知识边界实现了高效自适应推理，为构建高效可信的AI系统提供了可行方案。

Abstract: It is important for Large Language Models to be aware of the boundary of
their knowledge, the mechanism of identifying known and unknown queries. This
type of awareness can help models perform adaptive inference, such as invoking
RAG, engaging in slow and deep thinking, or adopting the abstention mechanism,
which is beneficial to the development of efficient and trustworthy AI. In this
work, we propose a method to detect knowledge boundaries via Query-Level
Uncertainty, which aims to determine if the model is able to address a given
query without generating any tokens. To this end, we introduce a novel and
training-free method called \emph{Internal Confidence}, which leverages
self-evaluations across layers and tokens. Empirical results on both factual QA
and mathematical reasoning tasks demonstrate that our internal confidence can
outperform several baselines. Furthermore, we showcase that our proposed method
can be used for efficient RAG and model cascading, which is able to reduce
inference costs while maintaining performance.

</details>


### [45] [Is Fine-Tuning an Effective Solution? Reassessing Knowledge Editing for Unstructured Data](https://arxiv.org/abs/2506.09672)
*Hao Xiong,Chuanyuan Tan,Wenliang Chen*

Main category: cs.CL

TL;DR: 本文针对非结构化知识编辑(UKE）任务中存在的局部性评估缺失和微调方法异常失效问题，提出通过构建扩展数据集评估模型局部性，并确定影响微调性能的关键因素，最终开发出性能优于现有方法的FT-UKE方法。


<details>
  <summary>Details</summary>
Motivation: 现有非结构化知识编辑方法存在两个主要问题：(1)缺乏对模型编辑后局部性（Locality）的系统评估；(2)基于微调（FT）的方法在UKE任务中表现异常失效。

Method: 1. 通过扩展现有UKE数据集构建UnKEBench-Loc和AKEW-Loc(CF)，从非结构化和结构化视角评估局部性；2. 识别影响FT方法性能的四个因素，并通过实验确定最优训练配置（FT-UKE）。

Result: FT-UKE在单样本和批量编辑场景下均显著优于现有SOTA方法，批量编辑时平均指标优势从+6.78%扩大至+10.80%，且优势随批量增大而增强。

Conclusion: 通过系统性局部性评估框架和优化的微调配置，FT-UKE证明了传统微调方法在UKE任务中的潜力，为后续研究提供了可复现的训练方案和基准数据集。

Abstract: Unstructured Knowledge Editing (UKE) is crucial for updating the relevant
knowledge of large language models (LLMs). It focuses on unstructured inputs,
such as long or free-form texts, which are common forms of real-world
knowledge. Although previous studies have proposed effective methods and tested
them, some issues exist: (1) Lack of Locality evaluation for UKE, and (2)
Abnormal failure of fine-tuning (FT) based methods for UKE. To address these
issues, we first construct two datasets, UnKEBench-Loc and AKEW-Loc (CF), by
extending two existing UKE datasets with locality test data from the
unstructured and structured views. This enables a systematic evaluation of the
Locality of post-edited models. Furthermore, we identify four factors that may
affect the performance of FT-based methods. Based on these factors, we conduct
experiments to determine how the well-performing FT-based methods should be
trained for the UKE task, providing a training recipe for future research. Our
experimental results indicate that the FT-based method with the optimal setting
(FT-UKE) is surprisingly strong, outperforming the existing state-of-the-art
(SOTA). In batch editing scenarios, FT-UKE shows strong performance as well,
with its advantage over SOTA methods increasing as the batch size grows,
expanding the average metric lead from +6.78% to +10.80%

</details>


### [46] [Inv-Entropy: A Fully Probabilistic Framework for Uncertainty Quantification in Language Models](https://arxiv.org/abs/2506.09684)
*Haoyi Song,Ruihan Ji,Naichen Shi,Fan Lai,Raed Al Kontar*

Main category: cs.CL

TL;DR: 本文提出了一种基于逆模型的概率框架Inv-Entropy，通过系统性扰动量化LLMs的不确定性，并开发了遗传算法扰动策略GAAP和评估指标TSU，实验证明其优于现有语义不确定性量化方法。


<details>
  <summary>Details</summary>
Motivation: 现有LLMs不确定性量化方法多为启发式且缺乏概率基础，需建立理论支撑的灵活框架。

Method: 1. 建立双随机游走理论模型，将输入输出建模为马尔可夫链；2. 基于逆模型构建概率框架，通过扰动输入空间定义Inv-Entropy不确定性度量；3. 提出遗传算法扰动策略GAAP和直接评估指标TSU。

Result: Inv-Entropy在实验中超越现有语义UQ方法，TSU指标有效绕过正确性代理直接量化不确定性。

Conclusion: 该框架兼具理论严谨性与灵活性，支持多维度扩展，GAAP扰动与TSU指标为LLMs可靠性评估提供了新范式。

Abstract: Large language models (LLMs) have transformed natural language processing,
but their reliable deployment requires effective uncertainty quantification
(UQ). Existing UQ methods are often heuristic and lack a probabilistic
foundation. This paper begins by providing a theoretical justification for the
role of perturbations in UQ for LLMs. We then introduce a dual random walk
perspective, modeling input-output pairs as two Markov chains with transition
probabilities defined by semantic similarity. Building on this, we propose a
fully probabilistic framework based on an inverse model, which quantifies
uncertainty by evaluating the diversity of the input space conditioned on a
given output through systematic perturbations. Within this framework, we define
a new uncertainty measure, Inv-Entropy. A key strength of our framework is its
flexibility: it supports various definitions of uncertainty measures,
embeddings, perturbation strategies, and similarity metrics. We also propose
GAAP, a perturbation algorithm based on genetic algorithms, which enhances the
diversity of sampled inputs. In addition, we introduce a new evaluation metric,
Temperature Sensitivity of Uncertainty (TSU), which directly assesses
uncertainty without relying on correctness as a proxy. Extensive experiments
demonstrate that Inv-Entropy outperforms existing semantic UQ methods. The code
to reproduce the results can be found at
https://github.com/UMDataScienceLab/Uncertainty-Quantification-for-LLMs.

</details>


### [47] [ComfyUI-R1: Exploring Reasoning Models for Workflow Generation](https://arxiv.org/abs/2506.09790)
*Zhenran Xu,Yiyu Wang,Xue Yang,Longyue Wang,Weihua Luo,Kaifu Zhang,Baotian Hu,Min Zhang*

Main category: cs.CL

TL;DR: 本文提出ComfyUI-R1模型，通过两阶段训练实现AI艺术创作流程自动化生成，显著超越GPT-4o等闭源模型，验证长链思维推理与代码化工作流表示的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有AI生成内容平台（如ComfyUI）需要用户手动编排复杂模块化工作流，存在专业知识门槛高、学习曲线陡峭的问题。

Method: 1) 基于4K工作流数据集构建长链思维推理数据；2) 两阶段训练框架：先进行CoT微调冷启动，再通过规则-指标混合奖励的强化学习提升推理能力。

Result: 7B参数模型实现97%格式正确率，在通过率、节点/图级F1分数上显著超越GPT-4o和Claude系列，能合成含多样化节点的复杂工作流。

Conclusion: 长链思维过程与工作流代码化转换是提升因素，证明结构化推理在AI艺术创作中的潜力，为复杂创意流程自动化提供新范式。

Abstract: AI-generated content has evolved from monolithic models to modular workflows,
particularly on platforms like ComfyUI, enabling customization in creative
pipelines. However, crafting effective workflows requires great expertise to
orchestrate numerous specialized components, presenting a steep learning curve
for users. To address this challenge, we introduce ComfyUI-R1, the first large
reasoning model for automated workflow generation. Starting with our curated
dataset of 4K workflows, we construct long chain-of-thought (CoT) reasoning
data, including node selection, workflow planning, and code-level workflow
representation. ComfyUI-R1 is trained through a two-stage framework: (1) CoT
fine-tuning for cold start, adapting models to the ComfyUI domain; (2)
reinforcement learning for incentivizing reasoning capability, guided by a
fine-grained rule-metric hybrid reward, ensuring format validity, structural
integrity, and node-level fidelity. Experiments show that our 7B-parameter
model achieves a 97\% format validity rate, along with high pass rate,
node-level and graph-level F1 scores, significantly surpassing prior
state-of-the-art methods that employ leading closed-source models such as
GPT-4o and Claude series. Further analysis highlights the critical role of the
reasoning process and the advantage of transforming workflows into code.
Qualitative comparison reveals our strength in synthesizing intricate workflows
with diverse nodes, underscoring the potential of long CoT reasoning in AI art
creation.

</details>


### [48] [Do LLMs Give Psychometrically Plausible Responses in Educational Assessments?](https://arxiv.org/abs/2506.09796)
*Andreas Säuberli,Diego Frassinelli,Barbara Plank*

Main category: cs.CL

TL;DR: 研究评估大型语言模型（LLM）作为教育测评试点参与者的可行性，发现校准后模型反应更接近人类，但零样本场景下仍不适用。


<details>
  <summary>Details</summary>
Motivation: 传统教育测评需大量真人前导研究，若LLM能模拟人类答题行为，可加速测评开发并提升效度。

Method: 使用心理测量学经典测试理论和项目反应理论，分析18个指令调优LLM在阅读、美国历史、经济学科目多选题数据集上的反应。

Result: 大模型易过度自信，经温度校准后反应分布更拟人；阅读类题目与人类相关性较高，但整体相关性较弱。

Conclusion: 零样本场景下LLM暂不适合直接用于教育测评前导研究，需进一步优化模型与人类反应的匹配度。

Abstract: Knowing how test takers answer items in educational assessments is essential
for test development, to evaluate item quality, and to improve test validity.
However, this process usually requires extensive pilot studies with human
participants. If large language models (LLMs) exhibit human-like response
behavior to test items, this could open up the possibility of using them as
pilot participants to accelerate test development. In this paper, we evaluate
the human-likeness or psychometric plausibility of responses from 18
instruction-tuned LLMs with two publicly available datasets of multiple-choice
test items across three subjects: reading, U.S. history, and economics. Our
methodology builds on two theoretical frameworks from psychometrics which are
commonly used in educational assessment, classical test theory and item
response theory. The results show that while larger models are excessively
confident, their response distributions can be more human-like when calibrated
with temperature scaling. In addition, we find that LLMs tend to correlate
better with humans in reading comprehension items compared to other subjects.
However, the correlations are not very strong overall, indicating that LLMs
should not be used for piloting educational assessments in a zero-shot setting.

</details>


### [49] [CoRT: Code-integrated Reasoning within Thinking](https://arxiv.org/abs/2506.09820)
*Chengpeng Li,Zhengyang Tang,Ziniu Li,Mingfeng Xue,Keqin Bao,Tian Ding,Ruoyu Sun,Benyou Wang,Xiang Wang,Junyang Lin,Dayiheng Liu*

Main category: cs.CL

TL;DR: CoRT框架通过提示工程合成代码集成推理数据，结合监督微调、拒绝微调和强化学习，有效提升大型推理模型在数学任务中的准确性和效率，并减少计算资源消耗。


<details>
  <summary>Details</summary>
Motivation: 现有大型推理模型（LRMs）在复杂数学运算中效率低且准确性不足，直接结合外部代码解释器（CI）会导致知识不兼容和效率问题。

Method: 提出CoRT后训练框架：1) 通过提示工程（Hint-Engineering）在关键位置插入代码提示，合成代码集成推理数据；2) 基于30个高质量样本，对1.5B-32B参数模型进行监督微调、拒绝微调和强化学习。

Result: 在5个数学推理数据集上，32B和1.5B模型准确率分别提升4%和8%，推理token消耗减少30%-50%。

Conclusion: CoRT通过优化模型与代码解释器的交互机制，显著提升数学推理性能并降低计算成本，验证了代码集成后训练框架的有效性。

Abstract: Large Reasoning Models (LRMs) like o1 and DeepSeek-R1 have shown remarkable
progress in natural language reasoning with long chain-of-thought (CoT), yet
they remain inefficient or inaccurate when handling complex mathematical
operations. Addressing these limitations through computational tools (e.g.,
computation libraries and symbolic solvers) is promising, but it introduces a
technical challenge: Code Interpreter (CI) brings external knowledge beyond the
model's internal text representations, thus the direct combination is not
efficient. This paper introduces CoRT, a post-training framework for teaching
LRMs to leverage CI effectively and efficiently. As a first step, we address
the data scarcity issue by synthesizing code-integrated reasoning data through
Hint-Engineering, which strategically inserts different hints at appropriate
positions to optimize LRM-CI interaction. We manually create 30 high-quality
samples, upon which we post-train models ranging from 1.5B to 32B parameters,
with supervised fine-tuning, rejection fine-tuning and reinforcement learning.
Our experimental results demonstrate that Hint-Engineering models achieve 4\%
and 8\% absolute improvements on DeepSeek-R1-Distill-Qwen-32B and
DeepSeek-R1-Distill-Qwen-1.5B respectively, across five challenging
mathematical reasoning datasets. Furthermore, Hint-Engineering models use about
30\% fewer tokens for the 32B model and 50\% fewer tokens for the 1.5B model
compared with the natural language models. The models and code are available at
https://github.com/ChengpengLi1003/CoRT.

</details>


### [50] [EmoNet-Voice: A Fine-Grained, Expert-Verified Benchmark for Speech Emotion Detection](https://arxiv.org/abs/2506.09827)
*Christoph Schuhmann,Robert Kaczmarczyk,Gollam Rabby,Felix Friedrich,Maurice Kraus,Kourosh Nadi,Huu Nguyen,Kristian Kersting,Sören Auer*

Main category: cs.CL

TL;DR: 本文提出EmoNet-Voice，包含大规模预训练数据集和专家标注的基准测试集，通过合成语音与心理学专家验证构建隐私安全的细粒度40类情感语音识别资源，并开发出与人类专家高一致性的Empathic Insight Voice模型，发现高唤醒情绪（如愤怒）比低唤醒状态（如专注）更易检测。


<details>
  <summary>Details</summary>
Motivation: 现有语音情感数据集存在情感粒度不足、隐私风险及依赖表演性数据等缺陷，需构建更精细、隐私安全且覆盖敏感情绪的资源以推动语音情感理解评估体系发展。

Method: 利用先进语音合成技术生成模拟特定情感场景的合成音频片段，构建包含4,500+小时、11种音色、40种情感、4种语言的预训练数据集(EmoNet-Voice Big)和专家标注的基准集(EmoNet-Voice Bench)，并由心理学专家进行强度标注验证。

Result: Empathic Insight Voice模型在语音情感识别中达到与人类专家高度一致的新标准，评估发现高唤醒情绪（如愤怒）检测准确率显著高于低唤醒状态（如专注）。

Conclusion: EmoNet-Voice通过合成数据与专家验证解决了现有数据集的局限性，为细粒度情感识别提供新基准，同时证明合成方法在隐私敏感场景中的有效性，揭示了不同情绪唤醒度对检测难度的影响规律。

Abstract: The advancement of text-to-speech and audio generation models necessitates
robust benchmarks for evaluating the emotional understanding capabilities of AI
systems. Current speech emotion recognition (SER) datasets often exhibit
limitations in emotional granularity, privacy concerns, or reliance on acted
portrayals. This paper introduces EmoNet-Voice, a new resource for speech
emotion detection, which includes EmoNet-Voice Big, a large-scale pre-training
dataset (featuring over 4,500 hours of speech across 11 voices, 40 emotions,
and 4 languages), and EmoNet-Voice Bench, a novel benchmark dataset with human
expert annotations. EmoNet-Voice is designed to evaluate SER models on a
fine-grained spectrum of 40 emotion categories with different levels of
intensities. Leveraging state-of-the-art voice generation, we curated synthetic
audio snippets simulating actors portraying scenes designed to evoke specific
emotions. Crucially, we conducted rigorous validation by psychology experts who
assigned perceived intensity labels. This synthetic, privacy-preserving
approach allows for the inclusion of sensitive emotional states often absent in
existing datasets. Lastly, we introduce Empathic Insight Voice models that set
a new standard in speech emotion recognition with high agreement with human
experts. Our evaluations across the current model landscape exhibit valuable
findings, such as high-arousal emotions like anger being much easier to detect
than low-arousal states like concentration.

</details>


### [51] [Error-Guided Pose Augmentation: Enhancing Rehabilitation Exercise Assessment through Targeted Data Generation](https://arxiv.org/abs/2506.09833)
*Omar Sherif,Ali Hamdi*

Main category: cs.CL

TL;DR: 本文提出了一种名为EGPA的康复评估方法，通过生成模拟临床运动错误的合成骨骼数据，结合注意力图卷积网络，显著提升了动作质量评估的准确性和可解释性，适用于家庭和临床康复场景。


<details>
  <summary>Details</summary>
Motivation: 现有康复评估系统存在数据不平衡、难以检测细微动作错误的问题，尤其在家庭场景中限制了患者进展监测的有效性。

Method: 提出错误引导姿态增强(EGPA)方法，针对生物力学错误生成合成骨骼数据，并与基于图卷积的注意力网络结合，增强对关键关节和运动阶段的识别能力。

Result: 实验显示平均绝对误差降低27.6%，错误分类准确率提升45.8%，注意力可视化验证了模型对临床关键运动阶段的聚焦能力。

Conclusion: EGPA通过合成针对性错误数据与可解释模型设计，为自动化康复评估提供了高精度、可解释的解决方案，具有临床实用价值。

Abstract: Effective rehabilitation assessment is essential for monitoring patient
progress, particularly in home-based settings. Existing systems often face
challenges such as data imbalance and difficulty detecting subtle movement
errors. This paper introduces Error-Guided Pose Augmentation (EGPA), a method
that generates synthetic skeleton data by simulating clinically relevant
movement mistakes. Unlike standard augmentation techniques, EGPA targets
biomechanical errors observed in rehabilitation. Combined with an
attention-based graph convolutional network, EGPA improves performance across
multiple evaluation metrics. Experiments demonstrate reductions in mean
absolute error of up to 27.6 percent and gains in error classification accuracy
of 45.8 percent. Attention visualizations show that the model learns to focus
on clinically significant joints and movement phases, enhancing both accuracy
and interpretability. EGPA offers a promising approach for improving automated
movement quality assessment in both clinical and home-based rehabilitation
contexts.

</details>


### [52] [Dataset of News Articles with Provenance Metadata for Media Relevance Assessment](https://arxiv.org/abs/2506.09847)
*Tomas Peterka,Matyas Bohacek*

Main category: cs.CL

TL;DR: 现有方法检测媒体操纵时仅关注图像语义与文本的对应，易漏检。本文提出新闻媒体来源数据集，定义来源地（LOR）和来源时间（DTOR）相关性任务，测试发现LLMs在LOR上表现良好，但DTOR效果差，需进一步研究。


<details>
  <summary>Details</summary>
Motivation: 当前错误/虚假信息中，上下文不符或错误归属的图像是主要操纵形式。现有方法仅验证图像语义与文本是否一致，但若图像内容与叙述部分相关即通过，导致漏检。需更全面检测图像来源的时空准确性。

Method: 构建新闻媒体来源数据集（含来源标签的新闻图片），定义LOR（图像来源地是否相关）和DTOR（图像时间是否相关）任务，并在6个大语言模型（LLMs）上测试基线性能。

Result: 零样本学习下，LLMs在LOR任务表现良好，但DTOR任务效果显著落后，表明现有模型对时间相关性的捕捉能力不足，需针对性优化架构。

Conclusion: 时空来源检测对打击媒体操纵至关重要。LOR的潜力与DTOR的不足凸显需开发专用模型，未来可结合多模态信息或设计时间推理模块提升效果。

Abstract: Out-of-context and misattributed imagery is the leading form of media
manipulation in today's misinformation and disinformation landscape. The
existing methods attempting to detect this practice often only consider whether
the semantics of the imagery corresponds to the text narrative, missing
manipulation so long as the depicted objects or scenes somewhat correspond to
the narrative at hand. To tackle this, we introduce News Media Provenance
Dataset, a dataset of news articles with provenance-tagged images. We formulate
two tasks on this dataset, location of origin relevance (LOR) and date and time
of origin relevance (DTOR), and present baseline results on six large language
models (LLMs). We identify that, while the zero-shot performance on LOR is
promising, the performance on DTOR hinders, leaving room for specialized
architectures and future work.

</details>


### [53] [Causal Sufficiency and Necessity Improves Chain-of-Thought Reasoning](https://arxiv.org/abs/2506.09853)
*Xiangning Yu,Zhuohan Wang,Linyi Yang,Haoxuan Li,Anjie Liu,Xiao Xue,Jun Wang,Mengyue Yang*

Main category: cs.CL

TL;DR: 本文提出基于因果框架的CoT推理方法，通过量化推理步骤的充分性与必要性，自动增删步骤以提升效率，在保持准确性的同时减少计算开销。


<details>
  <summary>Details</summary>
Motivation: 针对Chain-of-Thought提示法存在的两个核心问题——推理步骤的充分性(是否全面支撑结论)和必要性(是否存在冗余步骤)，需要建立量化评估与优化机制。

Method: 构建因果推理框架，结合概率充分性(PS)与必要性(PN)指标，通过反事实干预量化各步骤对最终结论的实际影响，实现自动化的步骤增补与剪枝。

Result: 在数学推理与常识推理基准测试中，该方法显著提升推理效率(平均减少15-20%推理步骤)，降低token消耗(最高达30%)且保持原有准确率。

Conclusion: 通过因果推理框架系统解决CoT的充分性与必要性问题，为提升大语言模型推理效率与成本效益提供了可量化的技术路径。

Abstract: Chain-of-Thought (CoT) prompting plays an indispensable role in endowing
large language models (LLMs) with complex reasoning capabilities. However, CoT
currently faces two fundamental challenges: (1) Sufficiency, which ensures that
the generated intermediate inference steps comprehensively cover and
substantiate the final conclusion; and (2) Necessity, which identifies the
inference steps that are truly indispensable for the soundness of the resulting
answer. We propose a causal framework that characterizes CoT reasoning through
the dual lenses of sufficiency and necessity. Incorporating causal Probability
of Sufficiency and Necessity allows us not only to determine which steps are
logically sufficient or necessary to the prediction outcome, but also to
quantify their actual influence on the final reasoning outcome under different
intervention scenarios, thereby enabling the automated addition of missing
steps and the pruning of redundant ones. Extensive experimental results on
various mathematical and commonsense reasoning benchmarks confirm substantial
improvements in reasoning efficiency and reduced token usage without
sacrificing accuracy. Our work provides a promising direction for improving LLM
reasoning performance and cost-effectiveness.

</details>


### [54] [Attention Head Embeddings with Trainable Deep Kernels for Hallucination Detection in LLMs](https://arxiv.org/abs/2506.09886)
*Rodion Oblovatny,Alexandra Bazarova,Alexey Zaytsev*

Main category: cs.CL

TL;DR: 提出通过分析提示与响应隐状态分布的概率差异来检测大语言模型幻觉的新方法，发现幻觉响应偏离度更小，并利用可学习深度核提升检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有幻觉检测方法依赖外部知识或辅助模型，本文旨在探索模型内在特征，通过隐状态分布差异实现无外部依赖的检测。

Method: 基于提示-响应隐状态分布的概率偏离度构建检测指标，采用可自适应捕捉分布几何差异的深度核提升检测敏感性。

Result: 在多个基准测试中达到SOTA性能，即使不训练核函数仍保持竞争力，证明方法的鲁棒性和可扩展性。

Conclusion: 揭示幻觉产生于表层复述而非深层推理的本质，提出首个完全基于模型内在特征的检测框架，为实际应用提供高效解决方案。

Abstract: We present a novel approach for detecting hallucinations in large language
models (LLMs) by analyzing the probabilistic divergence between prompt and
response hidden-state distributions. Counterintuitively, we find that
hallucinated responses exhibit smaller deviations from their prompts compared
to grounded responses, suggesting that hallucinations often arise from
superficial rephrasing rather than substantive reasoning. Leveraging this
insight, we propose a model-intrinsic detection method that uses distributional
distances as principled hallucination scores, eliminating the need for external
knowledge or auxiliary models. To enhance sensitivity, we employ deep learnable
kernels that automatically adapt to capture nuanced geometric differences
between distributions. Our approach outperforms existing baselines,
demonstrating state-of-the-art performance on several benchmarks. The method
remains competitive even without kernel training, offering a robust, scalable
solution for hallucination detection.

</details>


### [55] [The Emergence of Abstract Thought in Large Language Models Beyond Any Language](https://arxiv.org/abs/2506.09890)
*Yuxin Chen,Yiran Zhao,Yang Zhang,An Zhang,Kenji Kawaguchi,Shafiq Joty,Junnan Li,Tat-Seng Chua,Michael Qizhe Shieh,Wenxuan Zhang*

Main category: cs.CL

TL;DR: 研究发现大型语言模型（LLMs）通过发展核心语言无关参数空间，形成跨语言抽象思维。共享神经元随模型发展重要性增加，而语言专属神经元影响减弱。基于此，提出神经元特异性训练策略，实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 针对LLMs是否依赖英语进行『思考』的争议，探索其多语言能力的内在机制，尤其是模型参数如何支持跨语言泛化与抽象思维。

Method: 识别语言相关神经元（共享型与专属型），分析其随模型发展的动态变化，并设计针对不同发展阶段语言无关程度的神经元特异性训练策略。

Result: 发现核心语言无关参数空间（极小但关键），共享神经元比例与功能重要性随模型升级显著提升，专属神经元影响力下降。所提训练策略在多类LLMs中验证有效。

Conclusion: LLMs通过共享神经元构建语言无关的抽象思维基础，其多语言能力源于核心参数空间的演化。针对性训练策略可优化模型跨语言泛化能力。

Abstract: As large language models (LLMs) continue to advance, their capacity to
function effectively across a diverse range of languages has shown marked
improvement. Preliminary studies observe that the hidden activations of LLMs
often resemble English, even when responding to non-English prompts. This has
led to the widespread assumption that LLMs may "think" in English. However,
more recent results showing strong multilingual performance, even surpassing
English performance on specific tasks in other languages, challenge this view.
In this work, we find that LLMs progressively develop a core language-agnostic
parameter space-a remarkably small subset of parameters whose deactivation
results in significant performance degradation across all languages. This
compact yet critical set of parameters underlies the model's ability to
generalize beyond individual languages, supporting the emergence of abstract
thought that is not tied to any specific linguistic system. Specifically, we
identify language-related neurons-those are consistently activated during the
processing of particular languages, and categorize them as either shared
(active across multiple languages) or exclusive (specific to one). As LLMs
undergo continued development over time, we observe a marked increase in both
the proportion and functional importance of shared neurons, while exclusive
neurons progressively diminish in influence. These shared neurons constitute
the backbone of the core language-agnostic parameter space, supporting the
emergence of abstract thought. Motivated by these insights, we propose
neuron-specific training strategies tailored to LLMs' language-agnostic levels
at different development stages. Experiments across diverse LLM families
support our approach.

</details>


### [56] [PersonaLens: A Benchmark for Personalization Evaluation in Conversational AI Assistants](https://arxiv.org/abs/2506.09902)
*Zheng Zhao,Clara Vania,Subhradeep Kayal,Naila Khan,Shay B. Cohen,Emine Yilmaz*

Main category: cs.CL

TL;DR: 本文提出PersonaLens基准，用于系统评估任务导向AI助手在个性化适应能力上的表现，发现现有模型存在显著差异。


<details>
  <summary>Details</summary>
Motivation: 现有个性化评估方法局限于闲聊、非对话任务或狭窄领域，无法有效衡量任务导向AI助手在复杂场景下的个性化能力。

Method: 构建包含多样化用户偏好与交互历史的PersonaLens基准，并设计两个基于LLM的代理：用户代理模拟任务对话，法官代理通过LLM-as-a-Judge范式评估个性化、响应质量及任务成功率。

Result: 实验表明当前LLM助手在个性化能力上存在显著差异，为改进对话AI提供了数据支持。

Conclusion: PersonaLens为推进对话AI系统的个性化能力提供了关键评估工具与洞见。

Abstract: Large language models (LLMs) have advanced conversational AI assistants.
However, systematically evaluating how well these assistants apply
personalization--adapting to individual user preferences while completing
tasks--remains challenging. Existing personalization benchmarks focus on
chit-chat, non-conversational tasks, or narrow domains, failing to capture the
complexities of personalized task-oriented assistance. To address this, we
introduce PersonaLens, a comprehensive benchmark for evaluating personalization
in task-oriented AI assistants. Our benchmark features diverse user profiles
equipped with rich preferences and interaction histories, along with two
specialized LLM-based agents: a user agent that engages in realistic
task-oriented dialogues with AI assistants, and a judge agent that employs the
LLM-as-a-Judge paradigm to assess personalization, response quality, and task
success. Through extensive experiments with current LLM assistants across
diverse tasks, we reveal significant variability in their personalization
capabilities, providing crucial insights for advancing conversational AI
systems.

</details>


### [57] [Aspect-Based Opinion Summarization with Argumentation Schemes](https://arxiv.org/abs/2506.09917)
*Wendi Zhou,Ameer Saadat-Yazd,Nadin Kokciyan*

Main category: cs.CL

TL;DR: 本文提出ASESUM系统，通过提取产品关键方面的观点并评估其显著性和有效性，实现无需预定义方面的跨领域自适应评论摘要生成，实验证明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 在线评论摘要对消费者决策至关重要，但现有方法（抽取式/生成式）难以自动生成基于产品方面的可靠摘要，且无法适应不同领域的需求。

Method: 提出ASESUM框架：通过提取方面中心论点，结合显著性验证机制，动态识别产品关键方面并生成支持性证据的摘要，无需预定义方面集合。

Result: 在真实数据集上的实验表明，该方法能有效捕捉原始评论的多样化视角，在生成基于方面的摘要任务中优于新旧基线方法。

Conclusion: ASESUM系统通过结合方面中心论点的提取与验证机制，成功实现了跨领域自适应的评论摘要生成，为自动化观点归纳提供了新解决方案。

Abstract: Reviews are valuable resources for customers making purchase decisions in
online shopping. However, it is impractical for customers to go over the vast
number of reviews and manually conclude the prominent opinions, which prompts
the need for automated opinion summarization systems. Previous approaches,
either extractive or abstractive, face challenges in automatically producing
grounded aspect-centric summaries. In this paper, we propose a novel
summarization system that not only captures predominant opinions from an aspect
perspective with supporting evidence, but also adapts to varying domains
without relying on a pre-defined set of aspects. Our proposed framework,
ASESUM, summarizes viewpoints relevant to the critical aspects of a product by
extracting aspect-centric arguments and measuring their salience and validity.
We conduct experiments on a real-world dataset to demonstrate the superiority
of our approach in capturing diverse perspectives of the original reviews
compared to new and existing methods.

</details>


### [58] [VerIF: Verification Engineering for Reinforcement Learning in Instruction Following](https://arxiv.org/abs/2506.09942)
*Hao Peng,Yunjia Qi,Xiaozhi Wang,Bin Xu,Lei Hou,Juanzi Li*

Main category: cs.CL

TL;DR: 本文提出VerIF方法，结合规则代码与大型推理模型的验证，构建高质量指令数据集VerInstruct，通过强化学习训练显著提升模型在指令跟随任务中的性能，并开源相关资源。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习在指令跟随任务中验证工程的最佳实践尚未充分探索，需解决验证信号的有效整合问题以优化模型性能。

Method: 提出VerIF验证框架，融合基于规则的代码验证与大型推理模型（如QwQ-32B）的LLM验证，并构建含22,000条验证信号的指令数据集VerInstruct。

Result: 训练模型在多个指令跟随基准测试中达到同规模模型SOTA，泛化至未见约束时表现良好，且通用能力未受影响。

Conclusion: VerIF可无缝整合至现有强化学习流程，提升模型整体性能，其方法及开源资源为后续研究提供基础。

Abstract: Reinforcement learning with verifiable rewards (RLVR) has become a key
technique for enhancing large language models (LLMs), with verification
engineering playing a central role. However, best practices for RL in
instruction following remain underexplored. In this work, we explore the
verification challenge in RL for instruction following and propose VerIF, a
verification method that combines rule-based code verification with LLM-based
verification from a large reasoning model (e.g., QwQ-32B). To support this
approach, we construct a high-quality instruction-following dataset,
VerInstruct, containing approximately 22,000 instances with associated
verification signals. We apply RL training with VerIF to two models, achieving
significant improvements across several representative instruction-following
benchmarks. The trained models reach state-of-the-art performance among models
of comparable size and generalize well to unseen constraints. We further
observe that their general capabilities remain unaffected, suggesting that RL
with VerIF can be integrated into existing RL recipes to enhance overall model
performance. We have released our datasets, codes, and models to facilitate
future research at https://github.com/THU-KEG/VerIF.

</details>


### [59] [Query-Focused Retrieval Heads Improve Long-Context Reasoning and Re-ranking](https://arxiv.org/abs/2506.09944)
*Wuwei Zhang,Fangcong Yin,Howard Yen,Danqi Chen,Xi Ye*

Main category: cs.CL

TL;DR: 本文提出QRHEAD和QR-RETRIEVER，通过聚合查询相关注意力分数改进长上下文检索，在多项任务中性能显著优于基线方法，并增强模型长上下文能力的可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有长上下文语言模型中的检索头（retrieval heads）在真实任务（如长上下文QA）中检索效率有限，需改进其检索能力以提升模型性能。

Method: 1. 基于输入查询的注意力分数聚合识别QRHEAD；2. 设计QR-RETRIEVER，利用QRHEAD的累积注意力质量作为检索分数，筛选最相关上下文片段进行推理。

Result: 在多跳推理任务LongMemEval和CLIPPER上性能提升超10%，BEIR基准测试中击性能优于RankGPT等LLM重排器，且查询-上下文注意力评分与任务选择对模型效果关键。

Conclusion: QR-RETRIEVER作为通用检索工具有效提升长上下文推理性能，同时为语言模型长上下文能力的机制提供可解释性分析框架。

Abstract: Recent work has identified retrieval heads (Wu et al., 2025b), a subset of
attention heads responsible for retrieving salient information in long-context
language models (LMs), as measured by their copy-paste behavior in
Needle-in-a-Haystack tasks. In this paper, we introduce QRHEAD (Query-Focused
Retrieval Head), an improved set of attention heads that enhance retrieval from
long context. We identify QRHEAD by aggregating attention scores with respect
to the input query, using a handful of examples from real-world tasks (e.g.,
long-context QA). We further introduce QR- RETRIEVER, an efficient and
effective retriever that uses the accumulated attention mass of QRHEAD as
retrieval scores. We use QR- RETRIEVER for long-context reasoning by selecting
the most relevant parts with the highest retrieval scores. On multi-hop
reasoning tasks LongMemEval and CLIPPER, this yields over 10% performance gains
over full context and outperforms strong dense retrievers. We also evaluate
QRRETRIEVER as a re-ranker on the BEIR benchmark and find that it achieves
strong zero-shot performance, outperforming other LLM-based re-rankers such as
RankGPT. Further analysis shows that both the querycontext attention scoring
and task selection are crucial for identifying QRHEAD with strong downstream
utility. Overall, our work contributes a general-purpose retriever and offers
interpretability insights into the long-context capabilities of LMs.

</details>


### [60] [Resa: Transparent Reasoning Models via SAEs](https://arxiv.org/abs/2506.09967)
*Shangshang Wang,Julian Asilis,Ömer Faruk Akgül,Enes Burak Bilgin,Ollie Liu,Deqing Fu,Willie Neiswanger*

Main category: cs.CL

TL;DR: Resa模型通过稀疏自编码器调优（SAE-Tuning）方法，以极低成本（约1美元、20分钟）在1.5B参数模型中实现接近RL训练的性能（保留97%），并展示推理能力的通用性与模块化特性。


<details>
  <summary>Details</summary>
Motivation: 探索如何通过模型底层表征高效且低成本地激发语言模型的强推理能力，解决传统强化学习（RL）训练方法成本高、耗时长的问题。

Method: 提出SAE-Tuning方法：1) 用稀疏自编码器（SAE）从源模型提取推理能力；2) 利用SAE指导目标模型的监督微调，仅需验证过的问答数据（无需推理过程数据）。

Result: 1) 成本降低2000倍至1美元，时间缩短450倍至20分钟，性能保留97%；2) 在AIME24/AMC23上分别达43.33%/90% Pass@1；3) 提取的推理能力具备跨数据集通用性和跨模型模块化特性。

Conclusion: SAE-Tuning显著降低推理能力激发成本，验证了模型表征中可迁移的通用推理模块存在，为高效构建小规模推理模型提供新路径。

Abstract: How cost-effectively can we elicit strong reasoning in language models by
leveraging their underlying representations? We answer this question with Resa,
a family of 1.5B reasoning models trained via a novel and efficient sparse
autoencoder tuning (SAE-Tuning) procedure. This method first trains an SAE to
capture reasoning abilities from a source model, and then uses the trained SAE
to guide a standard supervised fine-tuning process to elicit such abilities in
a target model, all using verified question-answer data without any reasoning
traces. Notably, when applied to certain base models before further RL
post-training, SAE-Tuning retains >97% of its RL-trained counterpart's
reasoning performance while reducing training costs by >2000x to roughly \$1
and training time by >450x to around 20 minutes. Furthermore, when applied to
lightly RL-trained models (e.g., within 1 hour on 2 GPUs), it enables reasoning
performance such as 43.33% Pass@1 on AIME24 and 90% Pass@1 on AMC23 for only
around \$1 additional cost. Surprisingly, the reasoning abilities extracted via
SAEs are potentially both generalizable and modular. Generality means abilities
extracted from one dataset still elevate performance on a larger and
overlapping corpus. Modularity means abilities extracted from Qwen or Qwen-Math
can be attached to the R1-Distill model at test time, without any retraining,
and yield comparable gains. Extensive ablations validate these findings and all
artifacts are fully open-sourced.

</details>


### [61] [When Detection Fails: The Power of Fine-Tuned Models to Generate Human-Like Social Media Text](https://arxiv.org/abs/2506.09975)
*Hillary Dawkins,Kathleen C. Fraser,Svetlana Kiritchenko*

Main category: cs.CL

TL;DR: 研究探讨在社交媒体上检测AI生成文本的挑战，发现当攻击者使用未公开的微调模型时，现有检测方法效果显著下降，且微调模型对检测算法具有普遍威胁。


<details>
  <summary>Details</summary>
Motivation: 社交媒体因短文本和非正式语言特性，成为AI生成内容（如影响舆论的批量帖子）的攻击载体，需研究其检测难点及防御手段。

Method: 以威胁方视角构建包含505,159条AI生成社交媒体帖的数据集，涵盖开源、闭源及微调模型，测试不同检测算法在模型未公开场景下的表现，并通过人类实验验证。

Result: 若攻击者不公开微调模型，现有检测方法准确率大幅下降；消融实验表明检测算法普遍对微调模型脆弱，人类实验进一步确认此结论。

Conclusion: 微调模型作为LLM的常见应用场景，显著降低检测效果，此问题对检测领域具有广泛影响，需重新评估现有检测方法的实际有效性。

Abstract: Detecting AI-generated text is a difficult problem to begin with; detecting
AI-generated text on social media is made even more difficult due to the short
text length and informal, idiosyncratic language of the internet. It is
nonetheless important to tackle this problem, as social media represents a
significant attack vector in online influence campaigns, which may be bolstered
through the use of mass-produced AI-generated posts supporting (or opposing)
particular policies, decisions, or events. We approach this problem with the
mindset and resources of a reasonably sophisticated threat actor, and create a
dataset of 505,159 AI-generated social media posts from a combination of
open-source, closed-source, and fine-tuned LLMs, covering 11 different
controversial topics. We show that while the posts can be detected under
typical research assumptions about knowledge of and access to the generating
models, under the more realistic assumption that an attacker will not release
their fine-tuned model to the public, detectability drops dramatically. This
result is confirmed with a human study. Ablation experiments highlight the
vulnerability of various detection algorithms to fine-tuned LLMs. This result
has implications across all detection domains, since fine-tuning is a generally
applicable and realistic LLM use case.

</details>


### [62] [Step-by-step Instructions and a Simple Tabular Output Format Improve the Dependency Parsing Accuracy of LLMs](https://arxiv.org/abs/2506.09983)
*Hiroshi Matsuda,Chunpeng Ma,Masayuki Asahara*

Main category: cs.CL

TL;DR: 本文提出一种分步指令策略及简化输出格式，显著提升大语言模型在依存句法分析中的准确率，并在17种语言数据集上实现无幻觉或污染的SOTA效果，同时多语言微调增强跨语言泛化能力。


<details>
  <summary>Details</summary>
Motivation: 标准提示方法在生成结构有效且准确的依存句法分析结果时存在不足，尤其在输出格式一致性方面面临挑战。

Method: 采用分步指令策略（先通用词性标注，后预测句法头与依存标签）及简化的类CoNLL-U输出格式，结合多语言微调技术。

Result: 在17种语言的Universal Dependencies数据集上达到SOTA准确率，多语言微调显著提升跨语言泛化性能，且无幻觉或数据污染问题。

Conclusion: 显式推理步骤能有效提升基于LLM的句法分析效果，所提出的格式一致方法为基于括号标注的解析提供了可扩展替代方案。

Abstract: Recent advances in large language models (LLMs) have enabled impressive
performance in various tasks. However, standard prompting often struggles to
produce structurally valid and accurate outputs, especially in dependency
parsing. We propose a novel step-by-step instruction strategy, where universal
part-of-speech tagging precedes the prediction of syntactic heads and
dependency labels, and a simplified CoNLL-U like output format, our method
achieves state-of-the-art accuracy on Universal Dependencies datasets across 17
languages without hallucination or contamination. We further show that
multilingual fine-tuning simultaneously improves cross-language generalization
performance. Our results highlight the effectiveness of explicit reasoning
steps in LLM-based parsing and offer a scalable, format-consistent alternative
to bracket-based approaches.

</details>


### [63] [Large Language Models for Toxic Language Detection in Low-Resource Balkan Languages](https://arxiv.org/abs/2506.09992)
*Amel Muminovic,Amela Kadric Muminovic*

Main category: cs.CL

TL;DR: 研究评估了四种大语言模型在塞尔维亚语、克罗地亚语和波斯尼亚语等低资源语言的有毒评论检测效果，发现添加上下文片段可提升召回率，其中上下文增强模式下的Gemini模型综合表现最佳（F1=0.82），零样本GPT-4.1误报率最低。


<details>
  <summary>Details</summary>
Motivation: 针对巴尔干地区低资源语言社区缺乏有效内容审核工具的问题，研究大语言模型在有限标注数据环境下对有毒语言的检测能力。

Method: 构建4,500条手动标注的YouTube/TikTok多领域评论数据集，测试GPT-3.5 Turbo、GPT-4.1、Gemini 1.5 Pro和Claude 3 Opus在零样本/上下文增强两种模式下的性能指标（精确率、召回率、F1、准确率、误报率）。

Result: 上下文增强使平均召回率提升0.12，F1最高提升0.10（Gemini达0.82），但可能增加误报。零样本GPT-4.1保持最高精确率和最低误报率。

Conclusion: 通过优化提示设计和阈值校准，无需大量标注数据即可显著提升低资源语言的有毒内容检测效果，为巴尔干语言社区提供了实用解决方案。

Abstract: Online toxic language causes real harm, especially in regions with limited
moderation tools. In this study, we evaluate how large language models handle
toxic comments in Serbian, Croatian, and Bosnian, languages with limited
labeled data. We built and manually labeled a dataset of 4,500 YouTube and
TikTok comments drawn from videos across diverse categories, including music,
politics, sports, modeling, influencer content, discussions of sexism, and
general topics. Four models (GPT-3.5 Turbo, GPT-4.1, Gemini 1.5 Pro, and Claude
3 Opus) were tested in two modes: zero-shot and context-augmented. We measured
precision, recall, F1 score, accuracy and false positive rates. Including a
short context snippet raised recall by about 0.12 on average and improved F1
score by up to 0.10, though it sometimes increased false positives. The best
balance came from Gemini in context-augmented mode, reaching an F1 score of
0.82 and accuracy of 0.82, while zero-shot GPT-4.1 led on precision and had the
lowest false alarms. We show how adding minimal context can improve toxic
language detection in low-resource settings and suggest practical strategies
such as improved prompt design and threshold calibration. These results show
that prompt design alone can yield meaningful gains in toxicity detection for
underserved Balkan language communities.

</details>


### [64] [From Judgment to Interference: Early Stopping LLM Harmful Outputs via Streaming Content Monitoring](https://arxiv.org/abs/2506.09996)
*Yang Li,Qiang Sheng,Yehan Yang,Xueyao Zhang,Juan Cao*

Main category: cs.CL

TL;DR: 本文提出一种流式内容监控器（SCM），通过构建细粒度标注数据集FineHarm和双监督训练，实现在LLM生成过程中仅观察部分输出即可高效检测有害内容，同时提升模型无害性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM安全审核方法存在延迟高（全检测）或训练-推理不一致（部分检测）的问题，需设计原生支持部分检测的数据与模型联合方案。

Method: 1. 构建含29K细粒度标注的FineHarm数据集；2. 提出流式内容监控器SCM，采用响应级和词元级双监督训练，实时跟踪LLM输出流进行判断。

Result: SCM仅需平均观察18%的输出词元即达到0.95+宏F1值，性能媲美全检测；作为伪标注器可增强安全对齐，无害性评分超越DPO。

Conclusion: SCM通过数据与模型协同设计，在低延迟下实现高效有害内容检测，并为LLM安全对齐提供新思路。

Abstract: Though safety alignment has been applied to most large language models
(LLMs), LLM service providers generally deploy a subsequent moderation as the
external safety guardrail in real-world products. Existing moderators mainly
practice a conventional full detection, which determines the harmfulness based
on the complete LLM output, causing high service latency. Recent works pay more
attention to partial detection where moderators oversee the generation midway
and early stop the output if harmfulness is detected, but they directly apply
moderators trained with the full detection paradigm to incomplete outputs,
introducing a training-inference gap that lowers the performance. In this
paper, we explore how to form a data-and-model solution that natively supports
partial detection. For the data, we construct FineHarm, a dataset consisting of
29K prompt-response pairs with fine-grained annotations to provide reasonable
supervision for token-level training. Then, we propose the streaming content
monitor, which is trained with dual supervision of response- and token-level
labels and can follow the output stream of LLM to make a timely judgment of
harmfulness. Experiments show that SCM gains 0.95+ in macro F1 score that is
comparable to full detection, by only seeing the first 18% of tokens in
responses on average. Moreover, the SCM can serve as a pseudo-harmfulness
annotator for improving safety alignment and lead to a higher harmlessness
score than DPO.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [65] [Robot-Gated Interactive Imitation Learning with Adaptive Intervention Mechanism](https://arxiv.org/abs/2506.09176)
*Haoyuan Cai,Zhenghao Peng,Bolei Zhou*

Main category: cs.AI

TL;DR: 本文提出自适应干预机制（AIM），通过动态调整人类示范请求标准，显著降低交互式模仿学习中的人类监控成本，并提升学习效率与数据质量。


<details>
  <summary>Details</summary>
Motivation: 现有交互式模仿学习（IIL）方法对人类的认知负担较高，需频繁请求专家干预。为此，需开发一种能自适应减少专家监控需求的方法。

Method: AIM采用机器人门控机制，利用代理Q函数模拟人类干预规则，通过评估智能体与专家动作的实时对齐程度，动态调整干预请求阈值。Q函数在智能体偏离专家行为时赋予高值，随其能力提升逐步降低。

Result: 实验表明，AIM在连续/离散控制任务中减少专家监控工作量，相比基线Thrifty-DAgger，人类接管成本降低40%，且能识别安全关键状态，收集更高质量示范数据，减少总体交互需求。

Conclusion: AIM通过自适应干预机制有效降低人力成本，提升学习效率与安全性，为需人类干预的强化学习任务提供高效解决方案。

Abstract: Interactive Imitation Learning (IIL) allows agents to acquire desired
behaviors through human interventions, but current methods impose high
cognitive demands on human supervisors. We propose the Adaptive Intervention
Mechanism (AIM), a novel robot-gated IIL algorithm that learns an adaptive
criterion for requesting human demonstrations. AIM utilizes a proxy Q-function
to mimic the human intervention rule and adjusts intervention requests based on
the alignment between agent and human actions. By assigning high Q-values when
the agent deviates from the expert and decreasing these values as the agent
becomes proficient, the proxy Q-function enables the agent to assess the
real-time alignment with the expert and request assistance when needed. Our
expert-in-the-loop experiments reveal that AIM significantly reduces expert
monitoring efforts in both continuous and discrete control tasks. Compared to
the uncertainty-based baseline Thrifty-DAgger, our method achieves a 40%
improvement in terms of human take-over cost and learning efficiency.
Furthermore, AIM effectively identifies safety-critical states for expert
assistance, thereby collecting higher-quality expert demonstrations and
reducing overall expert data and environment interactions needed. Code and demo
video are available at https://github.com/metadriverse/AIM.

</details>


### [66] [Comment on The Illusion of Thinking: Understanding the Strengths and Limitations of Reasoning Models via the Lens of Problem Complexity](https://arxiv.org/abs/2506.09250)
*C. Opus,A. Lawsen*

Main category: cs.AI

TL;DR: 本文反驳Shojaee等人(2025)关于大型推理模型存在『准确性崩溃』的结论，指出其结论源于实验设计缺陷而非模型根本性推理失败。通过修正实验条件，模型在河内塔问题上展现高准确度。


<details>
  <summary>Details</summary>
Motivation: 针对已有研究将大型推理模型(LRMs)在复杂规划任务中的表现归因于根本性推理缺陷，本文旨在揭示其结论实际源于实验设计的三方面方法论局限，强调严谨评估框架的重要性。

Method: 通过分析原研究的河内塔token限制问题、自动评估框架缺陷，以及River Crossing基准测试中数学不可解实例的误用，改进实验条件：要求模型生成函数而非完整步骤列表，并排除无效测试案例。

Result: 在控制实验条件后，多个模型在曾被报告完全失败的河内塔实例上展现出高准确率，证明原结论主要反映实验设计缺陷而非模型能力局限。

Conclusion: 评估AI推理能力时需严格控制实验条件，区分真实推理失败与方法论缺陷。现有关于LRMs存在『准确性崩溃』的论断可能高估了模型缺陷，低估了实验设计对结果的影响。

Abstract: Shojaee et al. (2025) report that Large Reasoning Models (LRMs) exhibit
"accuracy collapse" on planning puzzles beyond certain complexity thresholds.
We demonstrate that their findings primarily reflect experimental design
limitations rather than fundamental reasoning failures. Our analysis reveals
three critical issues: (1) Tower of Hanoi experiments systematically exceed
model output token limits at reported failure points, with models explicitly
acknowledging these constraints in their outputs; (2) The authors' automated
evaluation framework fails to distinguish between reasoning failures and
practical constraints, leading to misclassification of model capabilities; (3)
Most concerningly, their River Crossing benchmarks include mathematically
impossible instances for N > 5 due to insufficient boat capacity, yet models
are scored as failures for not solving these unsolvable problems. When we
control for these experimental artifacts, by requesting generating functions
instead of exhaustive move lists, preliminary experiments across multiple
models indicate high accuracy on Tower of Hanoi instances previously reported
as complete failures. These findings highlight the importance of careful
experimental design when evaluating AI reasoning capabilities.

</details>


### [67] [Ming-Omni: A Unified Multimodal Model for Perception and Generation](https://arxiv.org/abs/2506.09344)
*Inclusion AI,Biao Gong,Cheng Zou,Chuanyang Zheng,Chunluan Zhou,Canxiang Yan,Chunxiang Jin,Chunjie Shen,Dandan Zheng,Fudong Wang,Furong Xu,GuangMing Yao,Jun Zhou,Jingdong Chen,Jianxin Sun,Jiajia Liu,Jianjiang Zhu,Jun Peng,Kaixiang Ji,Kaiyou Song,Kaimeng Ren,Libin Wang,Lixiang Ru,Lele Xie,Longhua Tan,Lyuxin Xue,Lan Wang,Mochen Bai,Ning Gao,Pei Chen,Qingpei Guo,Qinglong Zhang,Qiang Xu,Rui Liu,Ruijie Xiong,Sirui Gao,Tinghao Liu,Taisong Li,Weilong Chai,Xinyu Xiao,Xiaomei Wang,Xiaoxue Chen,Xiao Lu,Xiaoyu Li,Xingning Dong,Xuzheng Yu,Yi Yuan,Yuting Gao,Yunxiao Sun,Yipeng Chen,Yifei Wu,Yongjie Lyu,Ziping Ma,Zipeng Feng,Zhijiang Fang,Zhihao Qiu,Ziyuan Huang,Zhengyu He*

Main category: cs.AI

TL;DR: Ming-Omni是一个统一的多模态模型，支持图像、文本、音频和视频处理，兼具语音与图像生成能力，采用MoE架构与模态专用路由机制，首次开源实现与GPT-4o多模态支持对齐。


<details>
  <summary>Details</summary>
Motivation: 传统多模态模型常需多个专用模型或任务微调，缺乏统一框架且生成能力受限。本文旨在构建单一模型实现多模态感知与生成，突破音频/图像生成的技术限制。

Method: 1. 专用编码器提取多模态特征；2. 基于MoE的Ling架构配合模态专用路由融合信息；3. 集成先进音频解码器与Ming-Lite-Uni模块实现语音/图像生成；4. 统一框架支持跨模态任务无需结构调整。

Result: 实验验证模型在多模态感知与生成任务中的高效性，支持上下文对话、文本转语音、图像编辑等功能，成为首个开源且多模态支持对标GPT-4o的模型。

Conclusion: Ming-Omni通过统一架构实现全模态处理与生成，代码与模型权重开源促进社区研究，为多模态AI提供高效解决方案。

Abstract: We propose Ming-Omni, a unified multimodal model capable of processing
images, text, audio, and video, while demonstrating strong proficiency in both
speech and image generation. Ming-Omni employs dedicated encoders to extract
tokens from different modalities, which are then processed by Ling, an MoE
architecture equipped with newly proposed modality-specific routers. This
design enables a single model to efficiently process and fuse multimodal inputs
within a unified framework, thereby facilitating diverse tasks without
requiring separate models, task-specific fine-tuning, or structural redesign.
Importantly, Ming-Omni extends beyond conventional multimodal models by
supporting audio and image generation. This is achieved through the integration
of an advanced audio decoder for natural-sounding speech and Ming-Lite-Uni for
high-quality image generation, which also allow the model to engage in
context-aware chatting, perform text-to-speech conversion, and conduct
versatile image editing. Our experimental results showcase Ming-Omni offers a
powerful solution for unified perception and generation across all modalities.
Notably, our proposed Ming-Omni is the first open-source model we are aware of
to match GPT-4o in modality support, and we release all code and model weights
to encourage further research and development in the community.

</details>


### [68] [Beyond Nash Equilibrium: Bounded Rationality of LLMs and humans in Strategic Decision-making](https://arxiv.org/abs/2506.09390)
*Kehan Zheng,Jinfeng Zhou,Hongning Wang*

Main category: cs.AI

TL;DR: 研究通过行为博弈论实验发现，大语言模型在战略决策中能模仿人类启发式策略（如结果导向策略切换、未来互动促进合作），但策略应用更僵化且环境动态适应性较弱，仅部分实现了类人有限理性。


<details>
  <summary>Details</summary>
Motivation: 探究大语言模型在战略决策中是否表现出与人类相似的有限理性特征，及其与人类行为模式的异同。

Method: 采用行为博弈论经典实验范式，将LLMs置于与人类相同的实验条件下，通过石头剪刀布和囚徒困境游戏进行策略行为对比分析。

Result: LLMs复现了人类策略切换/合作增强等模式，但策略应用机械性强、环境动态敏感度低；模型架构特征显著影响战略行为，推理模型在适应性场景中表现欠佳。

Conclusion: 当前LLMs仅部分捕捉人类有限理性特征，需开发增强对手建模能力和情境感知的训练方法以提升战略决策灵活性。

Abstract: Large language models are increasingly used in strategic decision-making
settings, yet evidence shows that, like humans, they often deviate from full
rationality. In this study, we compare LLMs and humans using experimental
paradigms directly adapted from behavioral game-theory research. We focus on
two well-studied strategic games, Rock-Paper-Scissors and the Prisoner's
Dilemma, which are well known for revealing systematic departures from rational
play in human subjects. By placing LLMs in identical experimental conditions,
we evaluate whether their behaviors exhibit the bounded rationality
characteristic of humans. Our findings show that LLMs reproduce familiar human
heuristics, such as outcome-based strategy switching and increased cooperation
when future interaction is possible, but they apply these rules more rigidly
and demonstrate weaker sensitivity to the dynamic changes in the game
environment. Model-level analyses reveal distinctive architectural signatures
in strategic behavior, and even reasoning models sometimes struggle to find
effective strategies in adaptive situations. These results indicate that
current LLMs capture only a partial form of human-like bounded rationality and
highlight the need for training methods that encourage flexible opponent
modeling and stronger context awareness.

</details>


### [69] [A Call for Collaborative Intelligence: Why Human-Agent Systems Should Precede AI Autonomy](https://arxiv.org/abs/2506.09420)
*Henry Peng Zou,Wei-Chieh Huang,Yaozu Wu,Chunyu Miao,Dongyuan Li,Aiwei Liu,Yue Zhou,Yankai Chen,Weizhi Zhang,Yangning Li,Liancheng Fang,Renhe Jiang,Philip S. Yu*

Main category: cs.AI

TL;DR: 论文质疑完全自主AI代理的发展方向，提出LLM-HAS人机协作系统，强调人类参与可提升可靠性、透明度和适应性，并通过多领域案例证明协作优于纯AI方案。


<details>
  <summary>Details</summary>
Motivation: 现有完全自主AI系统存在可靠性、透明度不足，且无法真正理解人类需求，作者认为应探索人类与AI协作的路径而非追求完全自动化。

Method: 提出LLM-HAS框架，通过保持人类在决策环中的核心地位（提供反馈、解答问题、控制权保留），结合医疗/金融/软件开发案例进行验证，并设计协作系统构建方案。

Result: 案例研究表明人机协作系统在复杂任务中表现优于纯AI方案，同时提出解决系统透明度、责任归属等挑战的具体技术方案。

Conclusion: AI发展应聚焦人机协作能力而非自主性，未来方向是构建增强人类能力的伙伴式AI系统，其价值体现在与人类形成互补而非替代关系。

Abstract: Recent improvements in large language models (LLMs) have led many researchers
to focus on building fully autonomous AI agents. This position paper questions
whether this approach is the right path forward, as these autonomous systems
still have problems with reliability, transparency, and understanding the
actual requirements of human. We suggest a different approach: LLM-based
Human-Agent Systems (LLM-HAS), where AI works with humans rather than replacing
them. By keeping human involved to provide guidance, answer questions, and
maintain control, these systems can be more trustworthy and adaptable. Looking
at examples from healthcare, finance, and software development, we show how
human-AI teamwork can handle complex tasks better than AI working alone. We
also discuss the challenges of building these collaborative systems and offer
practical solutions. This paper argues that progress in AI should not be
measured by how independent systems become, but by how well they can work with
humans. The most promising future for AI is not in systems that take over human
roles, but in those that enhance human capabilities through meaningful
partnership.

</details>


### [70] [Fast Monte Carlo Tree Diffusion: 100x Speedup via Parallel Sparse Planning](https://arxiv.org/abs/2506.09498)
*Jaesik Yoon,Hyeonseo Cho,Yoshua Bengio,Sungjin Ahn*

Main category: cs.AI

TL;DR: 本文提出Fast-MCTD，通过并行化和稀疏化技术显著提升MCTD的推理速度，同时保持规划性能，成为扩散模型推理时的高效解决方案。


<details>
  <summary>Details</summary>
Motivation: 现有MCTD方法虽在复杂规划任务中表现优异，但其树搜索的序列化特性和迭代去噪过程导致计算开销巨大，限制了实际应用中的效率和可扩展性。

Method: 提出Fast-MCTD框架，包含并行MCTD（通过延迟树更新和冗余感知选择实现并行推演）与稀疏MCTD（通过轨迹粗化缩短推演长度）两项核心技术。

Result: 实验显示Fast-MCTD在保持或提升规划性能的同时，速度比标准MCTD提升达100倍，部分任务中推理速度甚至超过无需搜索的Diffuser方法。

Conclusion: Fast-MCTD通过高效搜索机制实现了扩散模型推理时推理速度与规划质量的平衡，为复杂长程推理任务提供了实用化解决方案。

Abstract: Diffusion models have recently emerged as a powerful approach for trajectory
planning. However, their inherently non-sequential nature limits their
effectiveness in long-horizon reasoning tasks at test time. The recently
proposed Monte Carlo Tree Diffusion (MCTD) offers a promising solution by
combining diffusion with tree-based search, achieving state-of-the-art
performance on complex planning problems. Despite its strengths, our analysis
shows that MCTD incurs substantial computational overhead due to the sequential
nature of tree search and the cost of iterative denoising. To address this, we
propose Fast-MCTD, a more efficient variant that preserves the strengths of
MCTD while significantly improving its speed and scalability. Fast-MCTD
integrates two techniques: Parallel MCTD, which enables parallel rollouts via
delayed tree updates and redundancy-aware selection; and Sparse MCTD, which
reduces rollout length through trajectory coarsening. Experiments show that
Fast-MCTD achieves up to 100x speedup over standard MCTD while maintaining or
improving planning performance. Remarkably, it even outperforms Diffuser in
inference speed on some tasks, despite Diffuser requiring no search and
yielding weaker solutions. These results position Fast-MCTD as a practical and
scalable solution for diffusion-based inference-time reasoning.

</details>


### [71] [DipLLM: Fine-Tuning LLM for Strategic Decision-making in Diplomacy](https://arxiv.org/abs/2506.09655)
*Kaixuan Xu,Jiajun Chai,Sicheng Li,Yuqian Fu,Yuanheng Zhu,Dongbin Zhao*

Main category: cs.AI

TL;DR: 本文提出DipLLM，一种基于微调大型语言模型（LLM）的智能体，通过自回归分解简化复杂多单位动作分配，仅用1.5%的数据即超越现有最佳模型Cicero，展示了LLM在复杂多人策略游戏中的潜力。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖均衡搜索生成海量游戏数据训练，计算成本高；而LLM虽能利用预训练知识减少数据需求，但外交游戏的指数级动作组合与复杂策略交互仍构成挑战。

Method: DipLLM采用自回归分解框架，将多单位动作分配转化为序列化单元级决策，并以均衡策略为学习目标，通过小规模微调实现高效训练。

Result: 仅使用Cicero模型1.5%的训练数据，DipLLM性能即超过Cicero，验证了方法的有效性。

Conclusion: 研究表明，微调LLM能高效解决复杂多人策略决策问题，为类似场景提供了轻量级解决方案的可行路径。

Abstract: Diplomacy is a complex multiplayer game that requires both cooperation and
competition, posing significant challenges for AI systems. Traditional methods
rely on equilibrium search to generate extensive game data for training, which
demands substantial computational resources. Large Language Models (LLMs) offer
a promising alternative, leveraging pre-trained knowledge to achieve strong
performance with relatively small-scale fine-tuning. However, applying LLMs to
Diplomacy remains challenging due to the exponential growth of possible action
combinations and the intricate strategic interactions among players. To address
this challenge, we propose DipLLM, a fine-tuned LLM-based agent that learns
equilibrium policies for Diplomacy. DipLLM employs an autoregressive
factorization framework to simplify the complex task of multi-unit action
assignment into a sequence of unit-level decisions. By defining an equilibrium
policy within this framework as the learning objective, we fine-tune the model
using only 1.5% of the data required by the state-of-the-art Cicero model,
surpassing its performance. Our results demonstrate the potential of fine-tuned
LLMs for tackling complex strategic decision-making in multiplayer games.

</details>


### [72] [Application-Driven Value Alignment in Agentic AI Systems: Survey and Perspectives](https://arxiv.org/abs/2506.09656)
*Wei Zeng,Hengshu Zhu,Chuan Qin,Han Wu,Yihang Cheng,Sirui Zhang,Xiaowei Jin,Yinuo Shen,Zhenxing Wang,Feimin Zhong,Hui Xiong*

Main category: cs.AI

TL;DR: 本文综述了Agentic AI阶段下多智能体系统的价值对齐问题，从分层价值原则、应用场景分类、评估方法及多智能体协调等方面展开分析，并提出未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着大模型驱动的AI应用复杂化，其情境性与系统性风险加剧，需确保智能体目标、行为与人类价值观及社会规范一致，以应对社会治理需求与技术发展的挑战。

Method: 采用分层视角组织宏观-中观-微观三层价值原则，按从通用到具体的分类梳理应用场景，系统审查价值对齐评估方法及数据集，并探讨多智能体间的价值协调机制。

Result: 构建了多层次价值原则框架，完成场景分类与评估方法总结，提出多智能体协调中的价值冲突解决方案，并整理相关数据集与对齐技术。

Conclusion: 价值对齐是复杂多智能体系统落地的关键，未来需进一步研究动态环境下的对齐机制、跨文化价值协调及可解释性评估方法。

Abstract: The ongoing evolution of AI paradigms has propelled AI research into the
Agentic AI stage. Consequently, the focus of research has shifted from single
agents and simple applications towards multi-agent autonomous decision-making
and task collaboration in complex environments. As Large Language Models (LLMs)
advance, their applications become more diverse and complex, leading to
increasingly situational and systemic risks. This has brought significant
attention to value alignment for AI agents, which aims to ensure that an
agent's goals, preferences, and behaviors align with human values and societal
norms. This paper reviews value alignment in agent systems within specific
application scenarios. It integrates the advancements in AI driven by large
models with the demands of social governance. Our review covers value
principles, agent system application scenarios, and agent value alignment
evaluation. Specifically, value principles are organized hierarchically from a
top-down perspective, encompassing macro, meso, and micro levels. Agent system
application scenarios are categorized and reviewed from a general-to-specific
viewpoint. Agent value alignment evaluation systematically examines datasets
for value alignment assessment and relevant value alignment methods.
Additionally, we delve into value coordination among multiple agents within
agent systems. Finally, we propose several potential research directions in
this field.

</details>


### [73] [Intent Factored Generation: Unleashing the Diversity in Your Language Model](https://arxiv.org/abs/2506.09659)
*Eltayeb Ahmed,Uljad Berdica,Martha Elliott,Danijela Horak,Jakob N. Foerster*

Main category: cs.AI

TL;DR: 本文提出意图分解生成（IFG）方法，通过两阶段采样（首先生成多样化意图，再基于意图生成连贯响应）提升大语言模型的生成多样性，同时保持质量。该方法在数学推理、代码生成、对话多样性等任务中均取得显著效果。


<details>
  <summary>Details</summary>
Motivation: 现有方法通过词汇级改写增加多样性，但导致重复响应和推理任务探索不足。需要一种能在概念层面提升多样性，同时保持输出质量的方法。

Method: IFG方法分两阶段：1) 高温度采样语义密集的意图（如摘要/关键词），促进概念多样性；2) 低温度基于原始提示+意图生成最终响应，确保连贯性。在思维链推理时显式生成每步意图。

Result: 在数学/代码任务中提升pass@k和强化学习效果；结合DPO提升对话多样性且不牺牲奖励；在新闻评论数据集上实现更高多样性。开源新收集的读者评论和新闻文章数据集。

Conclusion: IFG通过简单调整提示和温度参数，即可在保持性能的同时显著提升LLM样本多样性。该方法易于集成到现有算法，适用于推理、对话、通用语言建模等多种场景。

Abstract: Obtaining multiple meaningfully diverse, high quality samples from Large
Language Models for a fixed prompt remains an open challenge. Current methods
for increasing diversity often only operate at the token-level, paraphrasing
the same response. This is problematic because it leads to poor exploration on
reasoning problems and to unengaging, repetitive conversational agents. To
address this we propose Intent Factored Generation (IFG), factorising the
sampling process into two stages. First, we sample a semantically dense intent,
e.g., a summary or keywords. Second, we sample the final response conditioning
on both the original prompt and the intent from the first stage. This allows us
to use a higher temperature during the intent step to promote conceptual
diversity, and a lower temperature during the final generation to ensure the
outputs are coherent and self-consistent. Additionally, we find that prompting
the model to explicitly state its intent for each step of the chain-of-thought
before generating the step is beneficial for reasoning tasks. We demonstrate
our method's effectiveness across a diverse set of tasks. We show this method
improves both pass@k and Reinforcement Learning from Verifier Feedback on maths
and code tasks. For instruction-tuning, we combine IFG with Direct Preference
Optimisation to increase conversational diversity without sacrificing reward.
Finally, we achieve higher diversity while maintaining the quality of
generations on a general language modelling task, using a new dataset of reader
comments and news articles that we collect and open-source. In summary, we
present a simple method of increasing the sample diversity of LLMs while
maintaining performance. This method can be implemented by changing the prompt
and varying the temperature during generation, making it easy to integrate into
many algorithms for gains across various applications.

</details>


### [74] [How Do People Revise Inconsistent Beliefs? Examining Belief Revision in Humans with User Studies](https://arxiv.org/abs/2506.09977)
*Stylianos Loukas Vasileiou,Antonio Rago,Maria Vanina Martinez,William Yeoh*

Main category: cs.AI

TL;DR: 研究发现人类在信念修正中更倾向于基于解释的非最小化调整，与经典理论不符，这对设计符合人类认知的AI系统具有启示。


<details>
  <summary>Details</summary>
Motivation: 理解人类如何根据新信息修正信念，以开发能有效建模并与人推理对齐的AI系统，因现有理论框架与人类实际认知模式存在差异。

Method: 通过三个系统性用户实验，研究人们在面对矛盾信息时如何利用解释（无论是否被提供）进行信念修正。

Result: 实验表明人类普遍偏好基于解释的信念修正，此类修正常导致非最小化调整，且该现象被经典信念变化理论所忽视。

Conclusion: AI系统需引入基于解释的、可能非最小化的信念修正机制，以更贴合人类实际认知过程。

Abstract: Understanding how humans revise their beliefs in light of new information is
crucial for developing AI systems which can effectively model, and thus align
with, human reasoning. While theoretical belief revision frameworks rely on a
set of principles that establish how these operations are performed, empirical
evidence from cognitive psychology suggests that people may follow different
patterns when presented with conflicting information. In this paper, we present
three comprehensive user studies showing that people consistently prefer
explanation-based revisions, i.e., those which are guided by explanations, that
result in changes to their belief systems that are not necessarily captured by
classical belief change theory. Our experiments systematically investigate how
people revise their beliefs with explanations for inconsistencies, whether they
are provided with them or left to formulate them themselves, demonstrating a
robust preference for what may seem non-minimal revisions across different
types of scenarios. These findings have implications for AI systems designed to
model human reasoning or interact with humans, suggesting that such systems
should accommodate explanation-based, potentially non-minimal belief revision
operators to better align with human cognitive processes.

</details>


### [75] [V-JEPA 2: Self-Supervised Video Models Enable Understanding, Prediction and Planning](https://arxiv.org/abs/2506.09985)
*Mido Assran,Adrien Bardes,David Fan,Quentin Garrido,Russell Howes,Mojtaba,Komeili,Matthew Muckley,Ammar Rizvi,Claire Roberts,Koustuv Sinha,Artem Zholus,Sergio Arnaud,Abha Gejji,Ada Martin,Francois Robert Hogan,Daniel Dugas,Piotr Bojanowski,Vasil Khalidov,Patrick Labatut,Francisco Massa,Marc Szafraniec,Kapil Krishnakumar,Yong Li,Xiaodong Ma,Sarath Chandar,Franziska Meier,Yann LeCun,Michael Rabbat,Nicolas Ballas*

Main category: cs.AI

TL;DR: 本文提出自监督模型V-JEPA 2，通过海量视频与少量机器人交互数据预训练，实现视频理解、预测及机器人零样本规划，无需任务特定训练。


<details>
  <summary>Details</summary>
Motivation: 解决AI通过观察学习物理世界理解与行动的问题，探索结合互联网视频与少量机器人轨迹数据的自监督方法。

Method: 1. 基于超100万小时视频预训练无动作联合嵌入预测架构V-JEPA 2；2. 对齐大语言模型增强问答能力；3. 用<62小时机器人视频微调动作条件世界模型V-JEPA 2-AC。

Result: V-JEPA 2在动作理解（SSv2 77.3%）、任务预测（Epic-Kitchens 39.7%）及视频问答（PerceptionTest 84.0%）达SOTA；V-JEPA 2-AC在Franka机械臂实现零样本物体抓取规划。

Conclusion: 自监督学习结合网络规模数据与少量机器人交互数据，可构建无需环境特定训练的世界模型，实现物理世界中的规划能力。

Abstract: A major challenge for modern AI is to learn to understand the world and learn
to act largely by observation. This paper explores a self-supervised approach
that combines internet-scale video data with a small amount of interaction data
(robot trajectories), to develop models capable of understanding, predicting,
and planning in the physical world. We first pre-train an action-free
joint-embedding-predictive architecture, V-JEPA 2, on a video and image dataset
comprising over 1 million hours of internet video. V-JEPA 2 achieves strong
performance on motion understanding (77.3 top-1 accuracy on Something-Something
v2) and state-of-the-art performance on human action anticipation (39.7
recall-at-5 on Epic-Kitchens-100) surpassing previous task-specific models.
Additionally, after aligning V-JEPA 2 with a large language model, we
demonstrate state-of-the-art performance on multiple video question-answering
tasks at the 8 billion parameter scale (e.g., 84.0 on PerceptionTest, 76.9 on
TempCompass). Finally, we show how self-supervised learning can be applied to
robotic planning tasks by post-training a latent action-conditioned world
model, V-JEPA 2-AC, using less than 62 hours of unlabeled robot videos from the
Droid dataset. We deploy V-JEPA 2-AC zero-shot on Franka arms in two different
labs and enable picking and placing of objects using planning with image goals.
Notably, this is achieved without collecting any data from the robots in these
environments, and without any task-specific training or reward. This work
demonstrates how self-supervised learning from web-scale data and a small
amount of robot interaction data can yield a world model capable of planning in
the physical world.

</details>


### [76] [Meta-Adaptive Prompt Distillation for Few-Shot Visual Question Answering](https://arxiv.org/abs/2506.06905)
*Akash Gupta,Amos Storkey,Mirella Lapata*

Main category: cs.AI

TL;DR: 本文提出一种元学习方法，通过从任务相关图像特征中提取可调整的软提示，结合注意力映射模块，提升小型多模态模型在少样本场景下的任务适应能力，实验表明该方法在视觉问答任务中优于传统上下文学习和提示调优方法。


<details>
  <summary>Details</summary>
Motivation: 大型多模态模型（LMMs）的上下文学习（ICL）在少样本场景下表现不稳定，尤其小型模型易受图像嵌入中冗余信息干扰，导致示例数量与性能提升非单调相关。

Method: 提出基于元学习的软提示蒸馏方法：1) 从任务相关图像特征中提取固定软提示集；2) 引入可集成至LLaVA v1.5架构的注意力映射模块；3) 联合优化软提示与模块参数，实现低数据量下的快速任务适应。

Result: 在VL-ICL Bench评估中，本方法在视觉问答任务上持续超越ICL及现有提示调优方法，图像扰动场景下仍保持优势，任务诱导准确率提升3.2%，推理能力显著增强。

Conclusion: 通过元学习驱动的软提示蒸馏与注意力机制融合，有效缓解多模态信息过载问题，为少样本场景下的LMM任务适应提供了可扩展解决方案。

Abstract: Large Multimodal Models (LMMs) often rely on in-context learning (ICL) to
perform new tasks with minimal supervision. However, ICL performance,
especially in smaller LMMs, is inconsistent and does not always improve
monotonically with increasing examples. We hypothesize that this occurs due to
the LMM being overwhelmed by additional information present in the image
embeddings, which is not required for the downstream task. To address this, we
propose a meta-learning approach that provides an alternative for inducing
few-shot capabilities in LMMs, using a fixed set of soft prompts that are
distilled from task-relevant image features and can be adapted at test time
using a few examples. To facilitate this distillation, we introduce an
attention-mapper module that can be easily integrated with the popular LLaVA
v1.5 architecture and is jointly learned with soft prompts, enabling task
adaptation in LMMs under low-data regimes with just a few gradient steps.
Evaluation on the VL-ICL Bench shows that our method consistently outperforms
ICL and related prompt-tuning approaches, even under image perturbations,
improving task induction and reasoning across visual question answering tasks.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [77] [Llama-Affinity: A Predictive Antibody Antigen Binding Model Integrating Antibody Sequences with Llama3 Backbone Architecture](https://arxiv.org/abs/2506.09052)
*Delower Hossain,Ehsan Saghapour,Kevin Song,Jake Y. Chen*

Main category: cs.LG

TL;DR: 本文提出基于Llama 3架构的抗体-抗原结合亲和力预测模型LlamaAffinity，利用OAS数据库数据，在精度、效率等指标上显著超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统抗体亲和力测量方法耗时昂贵，而人工智能（尤其是大语言模型）为抗体表征和AI驱动设计提供了新可能。

Method: 采用开源Llama 3模型作为主干网络，整合Observed Antibody Space（OAS）数据库的抗体序列数据进行训练。

Result: 模型在准确率（0.9640）、F1分数（0.9643）、AUC-ROC（0.9936）等指标超越AntiFormer等SOTA方法，且训练时间仅0.46小时（五折平均）。

Conclusion: LlamaAffinity通过高效计算框架实现了抗体亲和力预测的突破，为AI驱动的抗体药物开发提供了新范式。

Abstract: Antibody-facilitated immune responses are central to the body's defense
against pathogens, viruses, and other foreign invaders. The ability of
antibodies to specifically bind and neutralize antigens is vital for
maintaining immunity. Over the past few decades, bioengineering advancements
have significantly accelerated therapeutic antibody development. These
antibody-derived drugs have shown remarkable efficacy, particularly in treating
cancer, SARS-CoV-2, autoimmune disorders, and infectious diseases.
Traditionally, experimental methods for affinity measurement have been
time-consuming and expensive. With the advent of artificial intelligence, in
silico medicine has been revolutionized; recent developments in machine
learning, particularly the use of large language models (LLMs) for representing
antibodies, have opened up new avenues for AI-based design and improved
affinity prediction. Herein, we present an advanced antibody-antigen binding
affinity prediction model (LlamaAffinity), leveraging an open-source Llama 3
backbone and antibody sequence data sourced from the Observed Antibody Space
(OAS) database. The proposed approach shows significant improvement over
existing state-of-the-art (SOTA) methods (AntiFormer, AntiBERTa, AntiBERTy)
across multiple evaluation metrics. Specifically, the model achieved an
accuracy of 0.9640, an F1-score of 0.9643, a precision of 0.9702, a recall of
0.9586, and an AUC-ROC of 0.9936. Moreover, this strategy unveiled higher
computational efficiency, with a five-fold average cumulative training time of
only 0.46 hours, significantly lower than in previous studies.

</details>


### [78] [FinHEAR: Human Expertise and Adaptive Risk-Aware Temporal Reasoning for Financial Decision-Making](https://arxiv.org/abs/2506.09080)
*Jiaxiang Chen,Mingxi Zou,Zhuo Wang,Qifan Wang,Dongning Sun,Chi Zhang,Zenglin Xu*

Main category: cs.LG

TL;DR: FinHEAR是一个基于多智能体的框架，结合人类专业知识和自适应风险感知推理，通过历史趋势分析、实时事件解读及专家先例检索，提升语言模型在金融决策中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型（LLMs）在金融决策中缺乏对人类行为模式（如信息不对称下的专家依赖、损失厌恶敏感性、反馈驱动调整）的捕捉能力，需解决动态事件响应与风险适应性问题。

Method: 提出FinHEAR框架：基于行为经济学理论，整合专家引导检索、信心调整头寸规模、结果驱动的优化机制，通过事件驱动的多智能体协作分析历史与实时数据。

Result: 在金融数据集上，FinHEAR在趋势预测和交易任务中优于基线模型，实现更高准确率和风险调整后收益。

Conclusion: FinHEAR通过融合专家知识与动态风险感知，增强了金融决策的鲁棒性和可解释性，验证了多智能体框架在复杂金融场景中的有效性。

Abstract: Financial decision-making presents unique challenges for language models,
demanding temporal reasoning, adaptive risk assessment, and responsiveness to
dynamic events. While large language models (LLMs) show strong general
reasoning capabilities, they often fail to capture behavioral patterns central
to human financial decisions-such as expert reliance under information
asymmetry, loss-averse sensitivity, and feedback-driven temporal adjustment. We
propose FinHEAR, a multi-agent framework for Human Expertise and Adaptive
Risk-aware reasoning. FinHEAR orchestrates specialized LLM-based agents to
analyze historical trends, interpret current events, and retrieve
expert-informed precedents within an event-centric pipeline. Grounded in
behavioral economics, it incorporates expert-guided retrieval,
confidence-adjusted position sizing, and outcome-based refinement to enhance
interpretability and robustness. Empirical results on curated financial
datasets show that FinHEAR consistently outperforms strong baselines across
trend prediction and trading tasks, achieving higher accuracy and better
risk-adjusted returns.

</details>


### [79] [Enhanced Whole Page Optimization via Mixed-Grained Reward Mechanism-Adapted Language Models](https://arxiv.org/abs/2506.09084)
*Xinyuan Wang,Liang Wu,Yanjie Fu*

Main category: cs.LG

TL;DR: 本文提出PageLLM方法，通过结合页面级和项目级奖励机制，利用用户反馈微调大语言模型，优化搜索结果和推荐页面的整体质量与关键项准确性，在真实场景中实现GMV增长。


<details>
  <summary>Details</summary>
Motivation: 传统基于人工标注数据的LLM微调方法成本过高，且用户反馈天然存在噪声。如何在保证WPO（全页优化）效果的同时降低标注成本，成为大规模系统优化的关键挑战。

Method: 提出PageLLM框架：1) 使用用户行为数据替代人工标注；2) 设计混合粒度奖励机制（页面级评估整体连贯性，项目级聚焦核心推荐准确性）；3) 通过双重奖励信号联合优化模型。

Result: 在公开数据集和工业场景验证中超越基线模型，线上A/B测试显示GMV提升0.44%（覆盖千万级用户），证明其在实际系统中的有效性。

Conclusion: PageLLM通过噪声鲁棒的双重奖励设计，成功将LLM应用于复杂WPO任务，在降低标注成本的同时提升商业指标，为大规模推荐系统优化提供新范式。

Abstract: Optimizing the presentation of search and recommendation results is crucial
to enhancing user experience and engagement. Whole Page Optimization (WPO)
plays a pivotal role in this process, as it directly influences how information
is surfaced to users. While Pre-trained Large Language Models (LLMs) have
demonstrated remarkable capabilities in generating coherent and contextually
relevant content, fine-tuning these models for complex tasks like WPO presents
challenges. Specifically, the need for extensive human-annotated data to
mitigate issues such as hallucinations and model instability can be
prohibitively expensive, especially in large-scale systems that interact with
millions of items daily. In this work, we address the challenge of fine-tuning
LLMs for WPO by using user feedback as the supervision. Unlike manually labeled
datasets, user feedback is inherently noisy and less precise. To overcome this,
we propose a reward-based fine-tuning approach, PageLLM, which employs a
mixed-grained reward mechanism that combines page-level and item-level rewards.
The page-level reward evaluates the overall quality and coherence, while the
item-level reward focuses on the accuracy and relevance of key recommendations.
This dual-reward structure ensures that both the holistic presentation and the
critical individual components are optimized. We validate PageLLM on both
public and industrial datasets. PageLLM outperforms baselines and achieves a
0.44\% GMV increase in an online A/B test with over 10 million users,
demonstrating its real-world impact.

</details>


### [80] [LLM-ML Teaming: Integrated Symbolic Decoding and Gradient Search for Valid and Stable Generative Feature Transformation](https://arxiv.org/abs/2506.09085)
*Xinyuan Wang,Haoyue Bai,Nanxu Gong,Wangyang Ying,Sixun Dong,Xiquan Cui,Yanjie Fu*

Main category: cs.LG

TL;DR: 提出结合大语言模型（LLM）与机器学习（ML）的协作框架，通过四步流程解决特征转换中的稳定生成和有效生成挑战。实验表明该方法在下游任务性能提升5%，错误案例减少近半，并验证了LLM对原始数据的理解能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法（传统机器学习有效性低、大语言模型不稳定）无法同时解决特征转换中的稳定生成（输出一致性）和有效生成（无错误序列）问题，需结合LLM的语法保证能力与机器学习的梯度引导搜索稳定性优势。

Method: 提出协作框架：1）教师LLM生成高质量样本；2）特征转换序列嵌入与潜在空间搜索；3）学生LLM知识蒸馏；4）LLM与ML解码器协作，结合符号生成与梯度优化概率，确保生成结果稳定且有效。

Result: 实验显示协作策略使下游性能提升5%，错误减少近50%，且方法高效鲁棒。进一步发现LLM具备理解原始数据的能力。

Conclusion: LLM与ML的协作框架有效结合双方优势，在提升生成稳定性和有效性同时，验证了LLM的数据理解潜力，为特征转换任务提供了新方向。

Abstract: Feature transformation enhances data representation by deriving new features
from the original data. Generative AI offers potential for this task, but faces
challenges in stable generation (consistent outputs) and valid generation
(error-free sequences). Existing methods--traditional MLs' low validity and
LLMs' instability--fail to resolve both. We find that LLMs ensure valid syntax,
while ML's gradient-steered search stabilizes performance. To bridge this gap,
we propose a teaming framework combining LLMs' symbolic generation with ML's
gradient optimization. This framework includes four steps: (1) golden examples
generation, aiming to prepare high-quality samples with the ground knowledge of
the teacher LLM; (2) feature transformation sequence embedding and search,
intending to uncover potentially superior embeddings within the latent space;
(3) student LLM feature transformation, aiming to distill knowledge from the
teacher LLM; (4) LLM-ML decoder teaming, dedicating to combine ML and the
student LLM probabilities for valid and stable generation. The experiments on
various datasets show that the teaming policy can achieve 5\% improvement in
downstream performance while reducing nearly half of the error cases. The
results also demonstrate the efficiency and robustness of the teaming policy.
Additionally, we also have exciting findings on LLMs' capacity to understand
the original data.

</details>


### [81] [Spiking Neural Models for Decision-Making Tasks with Learning](https://arxiv.org/abs/2506.09087)
*Sophie Jaffard,Giulia Mezzadri,Patricia Reynaud-Bouret,Etienne Tanré*

Main category: cs.LG

TL;DR: 本文提出了一种基于霍克斯过程的尖峰神经网络模型，将生物神经元机制融入认知决策模型，弥补了传统漂移扩散模型和泊松计数器缺乏学习机制的缺陷，并通过实验验证了模型的有效性。


<details>
  <summary>Details</summary>
Motivation: 传统决策认知模型（如漂移扩散模型和泊松计数器）缺乏生物神经元的学习机制，且仅适用于先验类别已知的任务。为弥合认知模型与生物神经机制之间的鸿沟，需开发具有生物合理性的学习模型。

Method: 1) 证明漂移扩散模型与泊松计数器的等效性；2) 提出基于霍克斯过程的尖峰神经网络模型，包含局部学习规则；3) 设计在线分类任务验证模型预测。

Result: 1) 漂移扩散模型可被泊松尖峰神经元近似；2) 具有相关噪声的漂移扩散模型可由霍克斯网络导出；3) 实验验证了模型在动态学习场景中的适应性。

Conclusion: 该模型通过整合生物神经元动态与学习机制，为理解神经活动与行为关联提供了新框架，推动了认知模型与生物机制的统一。

Abstract: In cognition, response times and choices in decision-making tasks are
commonly modeled using Drift Diffusion Models (DDMs), which describe the
accumulation of evidence for a decision as a stochastic process, specifically a
Brownian motion, with the drift rate reflecting the strength of the evidence.
In the same vein, the Poisson counter model describes the accumulation of
evidence as discrete events whose counts over time are modeled as Poisson
processes, and has a spiking neurons interpretation as these processes are used
to model neuronal activities. However, these models lack a learning mechanism
and are limited to tasks where participants have prior knowledge of the
categories. To bridge the gap between cognitive and biological models, we
propose a biologically plausible Spiking Neural Network (SNN) model for
decision-making that incorporates a learning mechanism and whose neurons
activities are modeled by a multivariate Hawkes process. First, we show a
coupling result between the DDM and the Poisson counter model, establishing
that these two models provide similar categorizations and reaction times and
that the DDM can be approximated by spiking Poisson neurons. To go further, we
show that a particular DDM with correlated noise can be derived from a Hawkes
network of spiking neurons governed by a local learning rule. In addition, we
designed an online categorization task to evaluate the model predictions. This
work provides a significant step toward integrating biologically relevant
neural mechanisms into cognitive models, fostering a deeper understanding of
the relationship between neural activity and behavior.

</details>


### [82] [Integrating Asynchronous AdaBoost into Federated Learning: Five Real World Applications](https://arxiv.org/abs/2506.09090)
*Arthur Oghlukyan,Nuria Gomez Blas*

Main category: cs.LG

TL;DR: 本文提出一种增强型异步AdaBoost联邦学习框架，通过自适应通信调度与延迟权重补偿，在降低同步频率与通信开销的同时提升模型精度，并在边缘设备计算机视觉、区块链模型透明化等五个领域验证其高效性。实验显示训练时间减少20-35%，通信开销降低30-40%，收敛所需迭代轮数显著减少。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中频繁的全局同步导致高通信开销与低效率，传统AdaBoost方法在异构分布式场景下存在收敛慢、鲁棒性不足的问题。需通过优化同步机制与补偿策略，提升跨领域联邦学习效率。

Method: 提出自适应通信调度规则（基于误差驱动的动态同步触发阈值）与延迟权重补偿机制，结合异步参数更新策略，减少客户端-服务器同步频率。通过数学建模定义调度规则与补偿函数，支持多场景参数适配。

Result: 在五类场景中对比基线AdaBoost：训练时间减少20-35%，通信开销降低30-40%，收敛所需迭代轮数减少15-25轮。分类精度保持或提升（如医疗诊断场景F1-score提高2.1%），各领域改进效果显著（图表量化展示）。

Conclusion: 增强型异步AdaBoost框架通过动态调度与补偿机制，在多种联邦学习场景中实现效率与鲁棒性协同优化，验证了跨领域普适性，为资源受限的分布式AI部署提供有效解决方案。

Abstract: This paper presents a comprehensive analysis of an enhanced asynchronous
AdaBoost framework for federated learning (FL), focusing on its application
across five distinct domains: computer vision on edge devices, blockchain-based
model transparency, on-device mobile personalization, IoT anomaly detection,
and federated healthcare diagnostics. The proposed algorithm incorporates
adaptive communication scheduling and delayed weight compensation to reduce
synchronization frequency and communication overhead while preserving or
improving model accuracy. We examine how these innovations improve
communication efficiency, scalability, convergence, and robustness in each
domain. Comparative metrics including training time, communication overhead,
convergence iterations, and classification accuracy are evaluated using data
and estimates derived from Oghlukyan's enhanced AdaBoost framework. Empirical
results show, for example, training time reductions on the order of 20-35% and
communication overhead reductions of 30-40% compared to baseline AdaBoost, with
convergence achieved in significantly fewer boosting rounds. Tables and charts
summarize these improvements by domain. Mathematical formulations of the
adaptive scheduling rule and error-driven synchronization thresholds are
provided. Overall, the enhanced AdaBoost exhibits markedly improved efficiency
and robustness across diverse FL scenarios, suggesting broad applicability of
the approach.

</details>


### [83] [Variational Inference Optimized Using the Curved Geometry of Coupled Free Energy](https://arxiv.org/abs/2506.09091)
*Kenric Nelson,Igor Oliveira,Amenah Al-Najafi,Fode Zhang,Hon Keung Tony Ng*

Main category: cs.LG

TL;DR: 本文提出了一种基于耦合自由能的变分推断优化框架，通过扩展指数族几何结构处理重尾分布，并设计耦合变分自编码器（CVAE），在图像重建任务中性能提升3%。


<details>
  <summary>Details</summary>
Motivation: 传统变分推断在处理重尾分布（如广义帕累托、学生t分布）时存在局限性，需改进模型在弯曲几何空间中的准确性与鲁棒性。

Method: 引入耦合自由能作为变分下界，结合耦合指数族的几何结构（修正Fisher信息度量与仿射连接），设计CVAE模型，通过重尾潜在分布采样降低异常值。

Result: 在CelebA数据集上，CVAE的Wasserstein-2/Fréchet Inception Distance指标较传统VAE提升3%（5轮训练后），且重建损失保持均方误差形式。

Conclusion: 耦合自由能框架有效提升变分推断对重尾分布的建模能力，CVAE通过尾部高惩罚机制减少异常样本，增强模型训练稳定性。

Abstract: We introduce an optimization framework for variational inference based on the
coupled free energy, extending variational inference techniques to account for
the curved geometry of the coupled exponential family. This family includes
important heavy-tailed distributions such as the generalized Pareto and the
Student's t. By leveraging the coupled free energy, which is equal to the
coupled evidence lower bound (ELBO) of the inverted probabilities, we improve
the accuracy and robustness of the learned model. The coupled generalization of
Fisher Information metric and the affine connection. The method is applied to
the design of a coupled variational autoencoder (CVAE). By using the coupling
for both the distributions and cost functions, the reconstruction metric is
derived to still be the mean-square average loss with modified constants. The
novelty comes from sampling the heavy-tailed latent distribution with its
associated coupled probability, which has faster decaying tails. The result is
the ability to train a model with high penalties in the tails, while assuring
that the training samples have a reduced number of outliers. The Wasserstein-2
or Fr\'echet Inception Distance of the reconstructed CelebA images shows the
CVAE has a 3\% improvement over the VAE after 5 epochs of training.

</details>


### [84] [CUDA-LLM: LLMs Can Write Efficient CUDA Kernels](https://arxiv.org/abs/2506.09092)
*Wentao Chen,Jiace Zhu,Qi Fan,Yehan Ma,An Zou*

Main category: cs.LG

TL;DR: 本文提出FSR框架，结合大语言模型（LLM）自动生成并迭代优化高性能CUDA代码，其生成的GPU内核在正确性和执行速度上显著优于人工编写代码。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在通用代码生成中表现优异，但针对GPU等硬件架构的高性能、硬件感知代码生成仍存在挑战，需兼顾功能正确性、编译优化与运行时效率。

Method: 提出特征搜索与强化（FSR）框架，通过多样化测试验证功能正确性，结合实际GPU执行延迟反馈，联合优化代码语义正确性和运行时性能，使LLM生成适应GPU架构的高效代码。

Result: FSR框架生成的CUDA内核正确率稳定，在AI负载和计算密集型任务中，执行速度最高达人工代码的179倍。

Conclusion: LLM与性能强化方法结合可有效生成硬件敏感的高性能GPU代码，为自动化GPU编程提供了新方向。

Abstract: Large Language Models (LLMs) have demonstrated strong capabilities in
general-purpose code generation. However, generating the code which is deeply
hardware-specific, architecture-aware, and performance-critical, especially for
massively parallel GPUs, remains a complex challenge. In this work, we explore
the use of LLMs for the automated generation and optimization of CUDA programs,
with the goal of producing high-performance GPU kernels that fully exploit the
underlying hardware. To address this challenge, we propose a novel framework
called \textbf{Feature Search and Reinforcement (FSR)}. FSR jointly optimizes
compilation and functional correctness, as well as the runtime performance,
which are validated through extensive and diverse test cases, and measured by
actual kernel execution latency on the target GPU, respectively. This approach
enables LLMs not only to generate syntactically and semantically correct CUDA
code but also to iteratively refine it for efficiency, tailored to the
characteristics of the GPU architecture. We evaluate FSR on representative CUDA
kernels, covering AI workloads and computational intensive algorithms. Our
results show that LLMs augmented with FSR consistently guarantee correctness
rates. Meanwhile, the automatically generated kernels can outperform general
human-written code by a factor of up to 179$\times$ in execution speeds. These
findings highlight the potential of combining LLMs with performance
reinforcement to automate GPU programming for hardware-specific,
architecture-sensitive, and performance-critical applications.

</details>


### [85] [Merging Smarter, Generalizing Better: Enhancing Model Merging on OOD Data](https://arxiv.org/abs/2506.09093)
*Bingjie Zhang,Hongkang Li,Changlong Shi,Guowei Rong,He Zhao,Dongsheng Wang,Dandan Guo,Meng Wang*

Main category: cs.LG

TL;DR: 本文提出LwPTV方法，通过分层剪枝任务向量中的冗余参数，提升多任务学习模型在领域外(OOD)任务上的性能，同时保持领域内(ID)任务能力，并与现有模型合并方法兼容。


<details>
  <summary>Details</summary>
Motivation: 当前多任务学习模型合并方法主要关注领域内(ID)数据集性能，但忽视了领域外(OOD)数据集的有效性。需要一种能提升OOD任务表现且不依赖训练数据的通用方法。

Method: 构建参数冗余显著性评分，通过分层剪枝任务向量(Layer-wise Pruning Task Vector)，仅保留预训练模型参数，生成任务掩码向量。该方法可灵活集成到现有模型合并框架中。

Result: 大量实验表明，该方法在保持ID任务能力的同时，显著提升了OOD任务的性能表现，且与主流模型合并方法具有良好兼容性。

Conclusion: LwPTV通过参数冗余分析和分层剪枝机制，有效平衡了ID/OOD任务性能，扩展了多任务学习模型的实际应用范围。

Abstract: Multi-task learning (MTL) concurrently trains a model on diverse task
datasets to exploit common features, thereby improving overall performance
across the tasks. Recent studies have dedicated efforts to merging multiple
independent model parameters into a unified model for MTL, thus circumventing
the need for training data and expanding the scope of applicable scenarios of
MTL. However, current approaches to model merging predominantly concentrate on
enhancing performance within in-domain (ID) datasets, often overlooking their
efficacy on out-of-domain (OOD) datasets. In this work, we proposed LwPTV
(Layer-wise Pruning Task Vector) by building a saliency score, measuring the
redundancy of parameters in task vectors. Designed in this way ours can achieve
mask vector for each task and thus perform layer-wise pruning on the task
vectors, only keeping the pre-trained model parameters at the corresponding
layer in merged model. Owing to its flexibility, our method can be seamlessly
integrated with most of existing model merging methods to improve their
performance on OOD tasks. Extensive experiments demonstrate that the
application of our method results in substantial enhancements in OOD
performance while preserving the ability on ID tasks.

</details>


### [86] [Intra-Trajectory Consistency for Reward Modeling](https://arxiv.org/abs/2506.09096)
*Chaoyang Zhou,Shunyu Liu,Zengmao Wang,Di Wang,Rong-Cheng Tu,Bo Du,Dacheng Tao*

Main category: cs.LG

TL;DR: 本文提出一种基于生成概率的奖励一致性正则化方法（ICRM），通过细粒度信号提升奖励模型的泛化能力，改善策略对齐和推理验证效果。


<details>
  <summary>Details</summary>
Motivation: 现有奖励模型依赖粗粒度的响应级评分，难以识别与评分真正相关的响应轨迹部分，导致对未见响应的泛化能力差。

Method: 利用生成概率建立响应轨迹内过程间奖励一致性，通过贝叶斯框架设计轨迹内一致性正则化，强制高生成概率的相邻过程保持奖励一致。

Result: 改进后的奖励模型在RewardBench上表现更优，且能诱导更好的DPO对齐策略和最佳N推理验证结果。

Conclusion: 通过细粒度奖励一致性信号增强，可有效提升奖励模型性能，促进更优的策略对齐和推理时验证效果。

Abstract: Reward models are critical for improving large language models (LLMs),
particularly in reinforcement learning from human feedback (RLHF) or
inference-time verification. Current reward modeling typically relies on scores
of overall responses to learn the outcome rewards for the responses. However,
since the response-level scores are coarse-grained supervision signals, the
reward model struggles to identify the specific components within a response
trajectory that truly correlate with the scores, leading to poor generalization
on unseen responses. In this paper, we propose to leverage generation
probabilities to establish reward consistency between processes in the response
trajectory, which allows the response-level supervisory signal to propagate
across processes, thereby providing additional fine-grained signals for reward
learning. Building on analysis under the Bayesian framework, we develop an
intra-trajectory consistency regularization to enforce that adjacent processes
with higher next-token generation probability maintain more consistent rewards.
We apply the proposed regularization to the advanced outcome reward model,
improving its performance on RewardBench. Besides, we show that the reward
model trained with the proposed regularization induces better DPO-aligned
policies and achieves better best-of-N (BON) inference-time verification
results. Our code is provided in https://github.com/chaoyang101/ICRM.

</details>


### [87] [Too Big to Think: Capacity, Memorization, and Generalization in Pre-Trained Transformers](https://arxiv.org/abs/2506.09099)
*Joshua Barron,Devin White*

Main category: cs.LG

TL;DR: 研究发现大型语言模型的记忆与泛化能力存在容量依赖的权衡：小模型擅长算术外推但无法记忆事实，大模型反之；中等模型向记忆偏移。联合训练时所有模型均无法外推，表明预训练可能固有偏向某类学习模式。


<details>
  <summary>Details</summary>
Motivation: 探索大语言模型中记忆化与泛化的关系，现有研究表明二者深度交织但机制不明。通过控制实验分离两种能力，揭示模型容量对学习模式的影响。

Method: 使用合成字符级任务（算术外推测泛化/事实回忆测记忆），从头预训练不同容量Transformer模型。单独及联合训练两种任务，观察模型表现差异。

Result: 容量与能力呈明确权衡：小模型仅能泛化，大模型仅能记忆，中等模型随训练转向记忆。联合训练时所有模型均丧失泛化能力。

Conclusion: 预训练可能本质优先记忆化学习。研究通过受控实验揭示容量塑造学习行为的机制，为小模型设计提供新视角：需针对性平衡不同学习目标。

Abstract: The relationship between memorization and generalization in large language
models (LLMs) remains an open area of research, with growing evidence that the
two are deeply intertwined. In this work, we investigate this relationship by
pre-training a series of capacity-limited Transformer models from scratch on
two synthetic character-level tasks designed to separately probe generalization
(via arithmetic extrapolation) and memorization (via factual recall). We
observe a consistent trade-off: small models extrapolate to unseen arithmetic
cases but fail to memorize facts, while larger models memorize but fail to
extrapolate. An intermediate-capacity model exhibits a similar shift toward
memorization. When trained on both tasks jointly, no model (regardless of size)
succeeds at extrapolation. These findings suggest that pre-training may
intrinsically favor one learning mode over the other. By isolating these
dynamics in a controlled setting, our study offers insight into how model
capacity shapes learning behavior and offers broader implications for the
design and deployment of small language models.

</details>


### [88] [An Interpretable N-gram Perplexity Threat Model for Large Language Model Jailbreaks](https://arxiv.org/abs/2410.16222)
*Valentyn Boreiko,Alexander Panfilov,Vaclav Voracek,Matthias Hein,Jonas Geiping*

Main category: cs.LG

TL;DR: 本文提出了一种基于N-gram语言模型的统一威胁模型，用于系统评估大语言模型越狱攻击方法的有效性，发现离散优化攻击优于基于LLM的方法，且有效攻击依赖罕见bigrams。


<details>
  <summary>Details</summary>
Motivation: 现有越狱攻击方法在流畅性和计算成本上差异显著，但缺乏统一的评估标准。研究旨在建立可解释的威胁模型，实现攻击方法的公平比较。

Method: 构建基于1T tokens的N-gram语言模型，替代传统困惑度指标，以LLM无关、非参数化的方式评估攻击在文本分布中的可能性，并调整主流攻击方法进行基准测试。

Result: 现代安全调整模型的攻击成功率低于预期；离散优化攻击显著优于LLM生成攻击；有效攻击通过利用Reddit/代码数据集中的罕见bigrams实现突破。

Conclusion: 统一威胁模型揭示了越狱攻击的本质特征，为防御机制设计提供新视角，并证明基于统计语言模型的评估具有可解释性和跨模型适用性。

Abstract: A plethora of jailbreaking attacks have been proposed to obtain harmful
responses from safety-tuned LLMs. These methods largely succeed in coercing the
target output in their original settings, but their attacks vary substantially
in fluency and computational effort. In this work, we propose a unified threat
model for the principled comparison of these methods. Our threat model checks
if a given jailbreak is likely to occur in the distribution of text. For this,
we build an N-gram language model on 1T tokens, which, unlike model-based
perplexity, allows for an LLM-agnostic, nonparametric, and inherently
interpretable evaluation. We adapt popular attacks to this threat model, and,
for the first time, benchmark these attacks on equal footing with it. After an
extensive comparison, we find attack success rates against safety-tuned modern
models to be lower than previously presented and that attacks based on discrete
optimization significantly outperform recent LLM-based attacks. Being
inherently interpretable, our threat model allows for a comprehensive analysis
and comparison of jailbreak attacks. We find that effective attacks exploit and
abuse infrequent bigrams, either selecting the ones absent from real-world text
or rare ones, e.g., specific to Reddit or code datasets.

</details>


### [89] [Feature Shift Localization Network](https://arxiv.org/abs/2506.09101)
*Míriam Barrabés,Daniel Mas Montserrat,Kapal Dev,Alexander G. Ioannidis*

Main category: cs.LG

TL;DR: 提出FSL-Net神经网络，用于快速准确定位大规模高维数据集中的特征偏移，无需重新训练即可处理新数据。


<details>
  <summary>Details</summary>
Motivation: 异构数据源、噪声测量及不一致的数据处理流程会导致特征偏移，现有方法在定位偏移特征时存在准确性低或扩展性差的问题。

Method: 设计特征偏移定位网络（FSL-Net），通过大量数据集训练学习统计特性，可直接应用于新数据集和未知偏移的定位。

Result: FSL-Net能高效定位高维数据中的特征偏移，其预训练模型在未见过数据上表现良好，代码和模型已开源。

Conclusion: FSL-Net解决了特征偏移定位的准确性与可扩展性挑战，为提升数据质量和下游分析效果提供了有效工具。

Abstract: Feature shifts between data sources are present in many applications
involving healthcare, biomedical, socioeconomic, financial, survey, and
multi-sensor data, among others, where unharmonized heterogeneous data sources,
noisy data measurements, or inconsistent processing and standardization
pipelines can lead to erroneous features. Localizing shifted features is
important to address the underlying cause of the shift and correct or filter
the data to avoid degrading downstream analysis. While many techniques can
detect distribution shifts, localizing the features originating them is still
challenging, with current solutions being either inaccurate or not scalable to
large and high-dimensional datasets. In this work, we introduce the Feature
Shift Localization Network (FSL-Net), a neural network that can localize
feature shifts in large and high-dimensional datasets in a fast and accurate
manner. The network, trained with a large number of datasets, learns to extract
the statistical properties of the datasets and can localize feature shifts from
previously unseen datasets and shifts without the need for re-training. The
code and ready-to-use trained model are available at
https://github.com/AI-sandbox/FSL-Net.

</details>


### [90] [Unifying Block-wise PTQ and Distillation-based QAT for Progressive Quantization toward 2-bit Instruction-Tuned LLMs](https://arxiv.org/abs/2506.09104)
*Jung Hyun Lee,Seungjae Shin,Vinnam Kim,Jaeseong You,An Chen*

Main category: cs.LG

TL;DR: 本文提出了一种统一渐进量化框架（UPQ），首次将指令调优的LLMs量化至INT2，无需专有数据，并在MMLU和IFEval基准测试中达到最优性能。


<details>
  <summary>Details</summary>
Motivation: 现有低比特量化研究集中于预训练模型，而指令调优模型因微调后参数敏感性更高，量化误差更大，亟需一种兼顾精度与效率的量化方法。

Method: UPQ分两步骤：1）通过块级后训练量化（PTQ）将FP16模型先降至INT4以减少误差；2）采用基于蒸馏的量化感知训练（Distill-QAT），最小化广义Jensen-Shannon散度，使INT2模型输出与原始模型一致。

Result: 实验表明，UPQ在开源指令调优模型上首次实现INT2量化，且在MMLU和IFEval基准测试中取得当前最优结果，验证了方法的有效性。

Conclusion: UPQ通过渐进式量化策略有效平衡了指令调优LLMs的低比特量化误差与性能，为资源受限设备部署提供了可行方案。

Abstract: As the rapid scaling of large language models (LLMs) poses significant
challenges for deployment on resource-constrained devices, there is growing
interest in extremely low-bit quantization, such as 2-bit. Although prior works
have shown that 2-bit large models are pareto-optimal over their 4-bit smaller
counterparts in both accuracy and latency, these advancements have been limited
to pre-trained LLMs and have not yet been extended to instruction-tuned models.
To bridge this gap, we propose Unified Progressive Quantization (UPQ)$-$a novel
progressive quantization framework (FP16$\rightarrow$INT4$\rightarrow$INT2)
that unifies block-wise post-training quantization (PTQ) with
distillation-based quantization-aware training (Distill-QAT) for INT2
instruction-tuned LLM quantization. UPQ first quantizes FP16 instruction-tuned
models to INT4 using block-wise PTQ to significantly reduce the quantization
error introduced by subsequent INT2 quantization. Next, UPQ applies Distill-QAT
to enable INT2 instruction-tuned LLMs to generate responses consistent with
their original FP16 counterparts by minimizing the generalized Jensen-Shannon
divergence (JSD) between the two. To the best of our knowledge, we are the
first to demonstrate that UPQ can quantize open-source instruction-tuned LLMs
to INT2 without relying on proprietary post-training data, while achieving
state-of-the-art performances on MMLU and IFEval$-$two of the most
representative benchmarks for evaluating instruction-tuned LLMs.

</details>


### [91] [MetaTT: A Global Tensor-Train Adapter for Parameter-Efficient Fine-Tuning](https://arxiv.org/abs/2506.09105)
*Javier Lopez-Piqueres,Pranav Deshpande,Archan Ray,Mattia J. Villani,Marco Pistoia,Niraj Kumar*

Main category: cs.LG

TL;DR: MetaTT提出了一种基于Tensor Train分解的统一适配器框架，用于预训练Transformer的全局低秩微调。相比LoRA等方法，MetaTT通过共享TT结构显著减少参数，同时保持或超越现有方法的精度，并支持多任务扩展。


<details>
  <summary>Details</summary>
Motivation: 现有方法如LoRA独立微调每个权重矩阵，导致参数随模式乘积增长。MetaTT旨在通过共享TT分解所有子模块（如注意力层、前馈层），以更低的参数量实现全局微调，并简化多任务适配器设计。

Method: MetaTT使用单一共享TT张量分解所有Transformer子模块，通过索引层、矩阵类型等结构轴组织参数。支持动态扩展模式（如任务/头数），并采用DMRG式秩自适应优化和Adam训练策略。

Result: 在标准语言建模基准测试中，MetaTT参数减少幅度最大，精度与LoRA相当且优于其他张量分解方法。TT分解的成熟优化流程（如秩自适应）简化了训练过程。

Conclusion: MetaTT通过TT分解实现了参数高效、可扩展的全局微调，其统一结构支持多任务共享适配器，且优化过程更稳定，为低秩自适应提供了新的张量网络解决方案。

Abstract: We present MetaTT, a unified Tensor Train (TT) adapter framework for global
low-rank fine-tuning of pre-trained transformers. Unlike LoRA, which fine-tunes
each weight matrix independently, MetaTT uses a single shared TT to factorize
all transformer sub-modules -- query, key, value, projection, and feed-forward
layers -- by indexing the structural axes like layer and matrix type, and
optionally heads and tasks. For a given rank, while LoRA adds parameters
proportional to the product across modes, MetaTT only adds parameters
proportional to the sum across modes leading to a significantly compressed
final adapter. Our benchmarks compare MetaTT with LoRA along with recent
state-of-the-art matrix and tensor decomposition based fine-tuning schemes. We
observe that when tested on standard language modeling benchmarks, MetaTT leads
to the most reduction in the parameters while maintaining similar accuracy to
LoRA and even outperforming other tensor-based methods. Unlike CP or other
rank-factorizations, the TT ansatz benefits from mature optimization routines
-- e.g., DMRG-style rank adaptive minimization in addition to Adam, which we
find simplifies training. Because new modes can be appended cheaply, MetaTT
naturally extends to shared adapters across many tasks without redesigning the
core tensor.

</details>


### [92] [SensorLM: Learning the Language of Wearable Sensors](https://arxiv.org/abs/2506.09108)
*Yuwei Zhang,Kumar Ayush,Siyuan Qiao,A. Ali Heydari,Girish Narayanswamy,Maxwell A. Xu,Ahmed A. Metwally,Shawn Xu,Jake Garrison,Xuhai Xu,Tim Althoff,Yun Liu,Pushmeet Kohli,Jiening Zhan,Mark Malhotra,Shwetak Patel,Cecilia Mascolo,Xin Liu,Daniel McDuff,Yuzhe Yang*

Main category: cs.LG

TL;DR: SensorLM是一个传感器-语言基础模型家族，通过自然语言理解可穿戴传感器数据，解决了传感器数据与语言对齐的挑战，并在多项任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有未标注的真实世界可穿戴传感器数据缺乏成对的文本描述，导致传感器数据与自然语言对齐和解释困难。

Method: 提出分层标题生成流程，捕捉传感器数据的统计、结构和语义信息，并扩展多模态预训练架构（如CLIP、CoCa），构建通用模型框架。

Result: 创建了最大传感器-语言数据集（覆盖10.3万人、5970万小时数据），在零样本识别、小样本学习和跨模态检索等任务中优于现有方法，并展示了扩展性、标签效率等新能力。

Conclusion: SensorLM验证了传感器-语言模型在真实场景中的潜力，其零样本泛化和任务适应性为可穿戴数据分析提供了新方向。

Abstract: We present SensorLM, a family of sensor-language foundation models that
enable wearable sensor data understanding with natural language. Despite its
pervasive nature, aligning and interpreting sensor data with language remains
challenging due to the lack of paired, richly annotated sensor-text
descriptions in uncurated, real-world wearable data. We introduce a
hierarchical caption generation pipeline designed to capture statistical,
structural, and semantic information from sensor data. This approach enabled
the curation of the largest sensor-language dataset to date, comprising over
59.7 million hours of data from more than 103,000 people. Furthermore, SensorLM
extends prominent multimodal pretraining architectures (e.g., CLIP, CoCa) and
recovers them as specific variants within a generic architecture. Extensive
experiments on real-world tasks in human activity analysis and healthcare
verify the superior performance of SensorLM over state-of-the-art in zero-shot
recognition, few-shot learning, and cross-modal retrieval. SensorLM also
demonstrates intriguing capabilities including scaling behaviors, label
efficiency, sensor captioning, and zero-shot generalization to unseen tasks.

</details>


### [93] [CodeBrain: Bridging Decoupled Tokenizer and Multi-Scale Architecture for EEG Foundation Model](https://arxiv.org/abs/2506.09110)
*Jingying Ma,Feng Wu,Qika Lin,Yucheng Xing,Chenyu Liu,Ziyu Jia,Mengling Feng*

Main category: cs.LG

TL;DR: 本文提出CodeBrain，一种高效且结构上与大脑组织对齐的脑电图基础模型，通过双阶段训练解决现有模型在异质表示和多尺度依赖捕捉上的不足，并在多个数据集上验证了其泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统任务特异性脑电图模型因通道配置、序列长度和任务目标的差异导致迁移性受限，现有脑电图基础模型（EFMs）在异质表示能力和多尺度脑依赖捕捉效率上存在不足。

Method: 分两阶段训练：(1) TFDual-Tokenizer独立标记化时频域成分以扩展离散表示空间；(2) EEGSSM结合全局卷积与滑动窗口注意力机制，联合建模稀疏长程与局部依赖，通过掩码自监督学习预测标记索引。

Result: 在10个公开脑电图数据集上的线性探测实验表明CodeBrain具有显著泛化能力，模型代码与预训练权重将后续开源。

Conclusion: CodeBrain通过生物学启发的可解释建模为神经科学研究奠定基础，其结构设计更贴合大脑小世界拓扑特性，能高效捕捉脑电图的多尺度固有结构。

Abstract: Electroencephalography (EEG) provides real-time insights into brain activity
and is widely used in neuroscience. However, variations in channel
configurations, sequence lengths, and task objectives limit the transferability
of traditional task-specific models. Although recent EEG foundation models
(EFMs) aim to learn generalizable representations, they struggle with limited
heterogeneous representation capacity and inefficiency in capturing multi-scale
brain dependencies. To address these challenges, we propose CodeBrain, an
efficient EFM structurally aligned with brain organization, trained in two
stages. (1) We introduce a TFDual-Tokenizer that independently tokenizes
heterogeneous temporal and frequency components, enabling a quadratic expansion
of the discrete representation space. This also offers a degree of
interpretability through cross-domain token analysis. (2) We propose the
EEGSSM, which combines a structured global convolution architecture and a
sliding window attention mechanism to jointly model sparse long-range and local
dependencies. Unlike fully connected Transformer models, EEGSSM better reflects
the brain's small-world topology and efficiently captures EEG's inherent
multi-scale structure. EEGSSM is trained with a masked self-supervised learning
objective to predict token indices obtained in TFDual-Tokenizer. Comprehensive
experiments on 10 public EEG datasets demonstrate the generalizability of
CodeBrain with linear probing. By offering biologically informed and
interpretable EEG modeling, CodeBrain lays the foundation for future
neuroscience research. Both code and pretraining weights will be released in
the future version.

</details>


### [94] [TRACE: Grounding Time Series in Context for Multimodal Embedding and Retrieval](https://arxiv.org/abs/2506.09114)
*Jialin Chen,Ziyu Zhao,Gaukhar Nurbek,Aosong Feng,Ali Maatouk,Leandros Tassiulas,Yifeng Gao,Rex Ying*

Main category: cs.LG

TL;DR: 本文提出TRACE多模态检索器，通过文本与时间序列的细粒度对齐及硬负样本挖掘，实现跨模态检索，提升下游任务的准确性和可解释性，并作为独立编码器在预测与分类任务中达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 动态数据（如医疗、气象）的跨模态检索需求日益增长，但现有方法存在语义缺失、模态对齐困难、多通道处理能力不足等问题，阻碍了时间序列基础模型的发展。

Method: 提出TRACE框架：1）通过文本上下文对齐时间序列嵌入；2）细粒度通道级对齐与硬负样本挖掘；3）支持文本-时序双向检索；4）轻量级任务调优保持跨模态对齐。

Result: TRACE在预测与分类任务中达到SOTA性能，检索增强模型显著提升下游任务准确率，多领域实验验证其作为通用检索器与独立编码器的双重有效性。

Conclusion: TRACE兼具检索增强与独立编码能力，通过跨模态对齐与轻量调优，为时间序列模型提供语义丰富的上下文，推动领域特定动态数据的实用化分析。

Abstract: The ubiquity of dynamic data in domains such as weather, healthcare, and
energy underscores a growing need for effective interpretation and retrieval of
time-series data. These data are inherently tied to domain-specific contexts,
such as clinical notes or weather narratives, making cross-modal retrieval
essential not only for downstream tasks but also for developing robust
time-series foundation models by retrieval-augmented generation (RAG). Despite
the increasing demand, time-series retrieval remains largely underexplored.
Existing methods often lack semantic grounding, struggle to align heterogeneous
modalities, and have limited capacity for handling multi-channel signals. To
address this gap, we propose TRACE, a generic multimodal retriever that grounds
time-series embeddings in aligned textual context. TRACE enables fine-grained
channel-level alignment and employs hard negative mining to facilitate
semantically meaningful retrieval. It supports flexible cross-modal retrieval
modes, including Text-to-Timeseries and Timeseries-to-Text, effectively linking
linguistic descriptions with complex temporal patterns. By retrieving
semantically relevant pairs, TRACE enriches downstream models with informative
context, leading to improved predictive accuracy and interpretability. Beyond a
static retrieval engine, TRACE also serves as a powerful standalone encoder,
with lightweight task-specific tuning that refines context-aware
representations while maintaining strong cross-modal alignment. These
representations achieve state-of-the-art performance on downstream forecasting
and classification tasks. Extensive experiments across multiple domains
highlight its dual utility, as both an effective encoder for downstream
applications and a general-purpose retriever to enhance time-series models.

</details>


### [95] [Scalable Spatiotemporal Inference with Biased Scan Attention Transformer Neural Processes](https://arxiv.org/abs/2506.09163)
*Daniel Jenson,Jhonathan Navott,Piotr Grynfelder,Mengyan Zhang,Makkunda Sharma,Elizaveta Semenova,Seth Flaxman*

Main category: cs.LG

TL;DR: 本文提出了一种新型神经过程模型BSA-TNP，通过引入核回归块、组不变注意力偏差和高效内存的偏置扫描注意力机制，在保持扩展性的同时显著提升模型精度，适用于时空演化过程建模，并支持百万级数据的高效推理。


<details>
  <summary>Details</summary>
Motivation: 现有神经过程模型在应对复杂应用时面临扩展性与精度的矛盾，尤其在处理平移不变过程时，传统方法常以精度换取效率。本文旨在证明这种权衡并非必要，并提出更优解决方案。

Method: 提出BSA-TNP架构，核心包含：1) 核回归块实现多分辨率学习；2) 组不变注意力偏差保持平移不变性；3) 内存优化的偏置扫描注意力机制；4) 支持高维固定效应和时空演化过程建模。

Result: BSA-TNP在精度上超越现有最佳模型，训练时间大幅缩短，单GPU实现百万测试点+十万上下文点的一分钟内推理，且具备多分辨率学习能力和时空建模特性。

Conclusion: BSA-TNP有效解决了神经过程模型扩展性与精度的矛盾，为地质学、流行病学等数据密集型领域提供了高效、透明的建模工具，证明了架构创新可突破传统权衡限制。

Abstract: Neural Processes (NPs) are a rapidly evolving class of models designed to
directly model the posterior predictive distribution of stochastic processes.
While early architectures were developed primarily as a scalable alternative to
Gaussian Processes (GPs), modern NPs tackle far more complex and data hungry
applications spanning geology, epidemiology, climate, and robotics. These
applications have placed increasing pressure on the scalability of these
models, with many architectures compromising accuracy for scalability. In this
paper, we demonstrate that this tradeoff is often unnecessary, particularly
when modeling fully or partially translation invariant processes. We propose a
versatile new architecture, the Biased Scan Attention Transformer Neural
Process (BSA-TNP), which introduces Kernel Regression Blocks (KRBlocks),
group-invariant attention biases, and memory-efficient Biased Scan Attention
(BSA). BSA-TNP is able to: (1) match or exceed the accuracy of the best models
while often training in a fraction of the time, (2) exhibit translation
invariance, enabling learning at multiple resolutions simultaneously, (3)
transparently model processes that evolve in both space and time, (4) support
high dimensional fixed effects, and (5) scale gracefully -- running inference
with over 1M test points with 100K context points in under a minute on a single
24GB GPU.

</details>


### [96] [Improving LLM Agent Planning with In-Context Learning via Atomic Fact Augmentation and Lookahead Search](https://arxiv.org/abs/2506.09171)
*Samuel Holt,Max Ruiz Luyten,Thomas Pouplin,Mihaela van der Schaar*

Main category: cs.LG

TL;DR: 提出一种基于原子事实增强和递归前瞻搜索的LLM智能体框架，通过上下文学习提升复杂交互环境中的规划能力，无需微调即可在线优化决策。


<details>
  <summary>Details</summary>
Motivation: 现有LLM方法在动态适应新信息、高效利用历史经验进行多步推理方面存在不足，需通过微调或大量交互指导才能有效工作。

Method: 结合原子事实提取（从交互轨迹中获取关键事实）与深度受限递归搜索：动态增强LLM组件提示，通过模拟潜在轨迹和结果评估进行规划。

Result: 在TextFrozenLake和ALFWorld等任务中验证，代理随经验积累表现出更优行为，理论证明性能与事实抽象质量及LLM模拟精度正相关。

Conclusion: 该框架通过上下文经验积累实现了LLM在交互任务中的持续性能提升，为无参数更新的在线学习提供了新思路。

Abstract: Large Language Models (LLMs) are increasingly capable but often require
significant guidance or extensive interaction history to perform effectively in
complex, interactive environments. Existing methods may struggle with adapting
to new information or efficiently utilizing past experiences for multi-step
reasoning without fine-tuning. We introduce a novel LLM agent framework that
enhances planning capabilities through in-context learning, facilitated by
atomic fact augmentation and a recursive lookahead search. Our agent learns to
extract task-critical ``atomic facts'' from its interaction trajectories. These
facts dynamically augment the prompts provided to LLM-based components
responsible for action proposal, latent world model simulation, and state-value
estimation. Planning is performed via a depth-limited lookahead search, where
the LLM simulates potential trajectories and evaluates their outcomes, guided
by the accumulated facts and interaction history. This approach allows the
agent to improve its understanding and decision-making online, leveraging its
experience to refine its behavior without weight updates. We provide a
theoretical motivation linking performance to the quality of fact-based
abstraction and LLM simulation accuracy. Empirically, our agent demonstrates
improved performance and adaptability on challenging interactive tasks,
achieving more optimal behavior as it accumulates experience, showcased in
tasks such as TextFrozenLake and ALFWorld.

</details>


### [97] [MultiNet: An Open-Source Software Toolkit \& Benchmark Suite for the Evaluation and Adaptation of Multimodal Action Models](https://arxiv.org/abs/2506.09172)
*Pranav Guruprasad,Yangyue Wang,Harshvardhan Sikka*

Main category: cs.LG

TL;DR: 论文提出MultiNet——一个开源的多模态动作模型基准及生态系统，用于评估视觉-语言-动作模型的跨领域能力，并提供包含1.3万亿标记的复合数据集。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言模型(VLMs)和视觉-语言-动作模型(VLAs)缺乏统一的评估标准与跨模态任务支持，需建立开放基准以推动通用智能体系统发展。

Method: 构建开源框架MultiNet，包含标准化评估协议、数据/模型下载工具，以及整合图像描述、机器人控制、游戏交互等任务的复合多模态数据集。

Result: 成功开发包含1.3万亿标记的跨领域数据集及配套工具链，并已应用于研究视觉-语言-动作模型泛化能力的局限性。

Conclusion: MultiNet为多模态动作模型提供了可扩展的评估基础设施，其开放生态将加速通用智能体系统的研究进程。

Abstract: Recent innovations in multimodal action models represent a promising
direction for developing general-purpose agentic systems, combining visual
understanding, language comprehension, and action generation. We introduce
MultiNet - a novel, fully open-source benchmark and surrounding software
ecosystem designed to rigorously evaluate and adapt models across vision,
language, and action domains. We establish standardized evaluation protocols
for assessing vision-language models (VLMs) and vision-language-action models
(VLAs), and provide open source software to download relevant data, models, and
evaluations. Additionally, we provide a composite dataset with over 1.3
trillion tokens of image captioning, visual question answering, commonsense
reasoning, robotic control, digital game-play, simulated
locomotion/manipulation, and many more tasks. The MultiNet benchmark,
framework, toolkit, and evaluation harness have been used in downstream
research on the limitations of VLA generalization.

</details>


### [98] [The Curious Language Model: Strategic Test-Time Information Acquisition](https://arxiv.org/abs/2506.09173)
*Michael Cooper,Rohan Wadhawan,John Michael Giorgi,Chenhao Tan,Davis Liang*

Main category: cs.LG

TL;DR: 本文提出CuriosiTree方法，通过贪心树搜索平衡信息增益与成本，实现大语言模型在零样本场景下的高效信息获取，临床诊断实验中优于基线策略。


<details>
  <summary>Details</summary>
Motivation: 决策者在信息不足时需通过不同成本渠道获取信息，如何选择高性价比的信息获取策略成为关键挑战。

Method: 基于启发式贪心树搜索策略，动态评估行动预期信息增益与成本，实现零样本信息获取的序列化决策。

Result: 临床诊断模拟验证显示，该方法能有效整合异构信息源，在诊断准确率与成本效益上超越基线策略。

Conclusion: CuriosiTree为LLM提供了可扩展的实时信息获取框架，在复杂决策场景中展现出显著的实际应用价值。

Abstract: Decision-makers often possess insufficient information to render a confident
decision. In these cases, the decision-maker can often undertake actions to
acquire the necessary information about the problem at hand, e.g., by
consulting knowledgeable authorities or by conducting experiments. Importantly,
different levers of information acquisition come with different costs, posing
the challenge of selecting the actions that are both informative and
cost-effective. In this work, we propose CuriosiTree, a heuristic-based,
test-time policy for zero-shot information acquisition in large language models
(LLMs). CuriosiTree employs a greedy tree search to estimate the expected
information gain of each action and strategically chooses actions based on a
balance of anticipated information gain and associated cost. Empirical
validation in a clinical diagnosis simulation shows that CuriosiTree enables
cost-effective integration of heterogenous sources of information, and
outperforms baseline action selection strategies in selecting action sequences
that enable accurate diagnosis.

</details>


### [99] [Multivariate Long-term Time Series Forecasting with Fourier Neural Filter](https://arxiv.org/abs/2506.09174)
*Chenheng Xu,Dan Wu,Yixin Zhu,Ying Nian Wu*

Main category: cs.LG

TL;DR: 本文提出FNF和DBD架构，通过统一时频域信息处理与优化梯度流，解决了多元长期时间序列预测中时空建模的挑战，无需辅助技术即在多领域数据集上达到SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有方法多借用NLP/CV领域通用架构（如Transformer），缺乏针对时间序列特性（如周期性）的专用骨干网络，依赖辅助技术补偿模型不足。

Method: 提出FNF骨干网络（融合局部时域与全局频域信息）和DBD架构（基于信息瓶颈理论优化梯度流与表示能力），形成端到端时空建模框架。

Result: 在5个领域11个基准数据集上验证，统一超参即达到SOTA性能，且无需信号分解等辅助技术，证明架构本身可捕获时序固有特性。

Conclusion: 专用神经架构能有效建模时序内在属性，可能推动科学和工业领域的时序建模范式转变，减少对领域无关架构+辅助技术的依赖。

Abstract: Multivariate long-term time series forecasting has been suffering from the
challenge of capturing both temporal dependencies within variables and spatial
correlations across variables simultaneously. Current approaches predominantly
repurpose backbones from natural language processing or computer vision (e.g.,
Transformers), which fail to adequately address the unique properties of time
series (e.g., periodicity). The research community lacks a dedicated backbone
with temporal-specific inductive biases, instead relying on domain-agnostic
backbones supplemented with auxiliary techniques (e.g., signal decomposition).
We introduce FNF as the backbone and DBD as the architecture to provide
excellent learning capabilities and optimal learning pathways for
spatio-temporal modeling, respectively. Our theoretical analysis proves that
FNF unifies local time-domain and global frequency-domain information
processing within a single backbone that extends naturally to spatial modeling,
while information bottleneck theory demonstrates that DBD provides superior
gradient flow and representation capacity compared to existing unified or
sequential architectures. Our empirical evaluation across 11 public benchmark
datasets spanning five domains (energy, meteorology, transportation,
environment, and nature) confirms state-of-the-art performance with consistent
hyperparameter settings. Notably, our approach achieves these results without
any auxiliary techniques, suggesting that properly designed neural
architectures can capture the inherent properties of time series, potentially
transforming time series modeling in scientific and industrial applications.

</details>


### [100] [Multi-Task Reward Learning from Human Ratings](https://arxiv.org/abs/2506.09183)
*Mingkang Wu,Devin White,Evelyn Rose,Vernon Lawhern,Nicholas R Waytowich,Yongcan Cao*

Main category: cs.LG

TL;DR: 本文提出一种新的强化学习方法，通过联合考虑分类和回归任务，动态平衡策略权重以模拟人类决策的不确定性，实验表明其优于现有基于评分的RL方法。


<details>
  <summary>Details</summary>
Motivation: 现有RLHF方法将人类决策简化为独立任务（如分类/回归），忽略了实际决策中多策略整合的特点。需更贴近人类真实推理机制。

Method: 在无奖励环境中利用人类评分推导奖励函数，引入可学习权重平衡分类与回归模型的贡献，使模型自适应调整策略侧重。

Result: 使用合成人类评分数据的实验表明，该方法在基于评分的RL方法中表现最优，部分场景甚至超越传统RL方法。

Conclusion: 通过多任务联合建模捕捉人类决策的不确定性，能有效提升模型行为对齐效果，验证了动态权重机制的有效性。

Abstract: Reinforcement learning from human feeback (RLHF) has become a key factor in
aligning model behavior with users' goals. However, while humans integrate
multiple strategies when making decisions, current RLHF approaches often
simplify this process by modeling human reasoning through isolated tasks such
as classification or regression. In this paper, we propose a novel
reinforcement learning (RL) method that mimics human decision-making by jointly
considering multiple tasks. Specifically, we leverage human ratings in
reward-free environments to infer a reward function, introducing learnable
weights that balance the contributions of both classification and regression
models. This design captures the inherent uncertainty in human decision-making
and allows the model to adaptively emphasize different strategies. We conduct
several experiments using synthetic human ratings to validate the effectiveness
of the proposed approach. Results show that our method consistently outperforms
existing rating-based RL methods, and in some cases, even surpasses traditional
RL approaches.

</details>


### [101] [LaDCast: A Latent Diffusion Model for Medium-Range Ensemble Weather Forecasting](https://arxiv.org/abs/2506.09193)
*Yilin Zhuang,Karthik Duraisamy*

Main category: cs.LG

TL;DR: LaDCast是首个基于潜在扩散的全球中期集合预报框架，通过自编码器压缩数据并在潜在空间生成小时级集合预报，结合几何位置编码与双流注意力机制，在降低计算成本的同时达到接近欧洲中期预报中心IFS-ENS的精度，并显著提升极端天气事件追踪能力。


<details>
  <summary>Details</summary>
Motivation: 传统集合数值天气预报(NWP)与机器学习方法在概率预报的准确性和不确定性量化方面存在效率与精度双重挑战，需开发兼顾计算效率与预报性能的新方法。

Method: 使用自编码器压缩ERA5再分析场至潜在空间，构建基于Transformer的扩散模型进行潜在状态序列预测；引入GeoRoPE编码地球球面几何特征，采用双流注意力机制实现高效条件建模，结合正弦时间嵌入捕捉季节周期性。

Result: 在未显式扰动情况下，LaDCast的确定性/概率性预报技能接近IFS-ENS，对气旋等极端事件轨迹预测优于现有模型，潜在空间计算使存储和算力需求降低数个量级。

Conclusion: LaDCast通过潜在空间建模实现了高效高精度天气预报，为实时公里级分辨率预报提供了可行路径，其开源代码将促进相关研究发展。

Abstract: Accurate probabilistic weather forecasting demands both high accuracy and
efficient uncertainty quantification, challenges that overburden both ensemble
numerical weather prediction (NWP) and recent machine-learning methods. We
introduce LaDCast, the first global latent-diffusion framework for medium-range
ensemble forecasting, which generates hourly ensemble forecasts entirely in a
learned latent space. An autoencoder compresses high-dimensional ERA5
reanalysis fields into a compact representation, and a transformer-based
diffusion model produces sequential latent updates with arbitrary hour
initialization. The model incorporates Geometric Rotary Position Embedding
(GeoRoPE) to account for the Earth's spherical geometry, a dual-stream
attention mechanism for efficient conditioning, and sinusoidal temporal
embeddings to capture seasonal patterns. LaDCast achieves deterministic and
probabilistic skill close to that of the European Centre for Medium-Range
Forecast IFS-ENS, without any explicit perturbations. Notably, LaDCast
demonstrates superior performance in tracking rare extreme events such as
cyclones, capturing their trajectories more accurately than established models.
By operating in latent space, LaDCast reduces storage and compute by orders of
magnitude, demonstrating a practical path toward forecasting at kilometer-scale
resolution in real time. We open-source our code and models and provide the
training and evaluation pipelines at: https://github.com/tonyzyl/ladcast.

</details>


### [102] [FLoRIST: Singular Value Thresholding for Efficient and Accurate Federated Fine-Tuning of Large Language Models](https://arxiv.org/abs/2506.09199)
*Hariharan Ramesh,Jyotikrishna Dass*

Main category: cs.LG

TL;DR: 本文提出FLoRIST框架，通过高效分解本地适配器并引入可调奇异值阈值，在联邦学习中实现低通信、低计算开销的LoRA聚合，平衡了效率与性能。


<details>
  <summary>Details</summary>
Motivation: 现有联邦LoRA方法存在聚合噪声、通信效率低或计算开销大的问题，尤其在异构客户端场景下难以平衡效率与准确性。

Method: FLoRIST采用分离式奇异值分解流程，在紧凑中间空间聚合本地LoRA信息，通过服务器端最优秩选择构建全局低秩适配器。

Result: 多数据集和LLM实验表明，FLoRIST在同构/异构场景下均实现最佳通信效率与性能平衡，且计算成本显著降低。

Conclusion: FLoRIST为联邦学习中的参数高效微调提供了数学精确、通信高效且计算轻量的解决方案，推动了LLM联邦调优的实用化进程。

Abstract: Integrating Low-Rank Adaptation (LoRA) into federated learning offers a
promising solution for parameter-efficient fine-tuning of Large Language Models
(LLMs) without sharing local data. However, several methods designed for
federated LoRA present significant challenges in balancing communication
efficiency, model accuracy, and computational cost, particularly among
heterogeneous clients. These methods either rely on simplistic averaging of
local adapters, which introduces aggregation noise, require transmitting large
stacked local adapters, leading to poor communication efficiency, or
necessitate reconstructing memory-dense global weight-update matrix and
performing computationally expensive decomposition to design client-specific
low-rank adapters. In this work, we propose FLoRIST, a federated fine-tuning
framework that achieves mathematically accurate aggregation without incurring
high communication or computational overhead. Instead of constructing the full
global weight-update matrix at the server, FLoRIST employs an efficient
decomposition pipeline by performing singular value decomposition on stacked
local adapters separately. This approach operates within a compact intermediate
space to represent the accumulated information from local LoRAs. We introduce
tunable singular value thresholding for server-side optimal rank selection to
construct a pair of global low-rank adapters shared by all clients. Extensive
empirical evaluations across multiple datasets and LLMs demonstrate that
FLoRIST consistently strikes the best balance between superior communication
efficiency and competitive performance in both homogeneous and heterogeneous
setups.

</details>


### [103] [FedRAG: A Framework for Fine-Tuning Retrieval-Augmented Generation Systems](https://arxiv.org/abs/2506.09200)
*Val Andrei Fajardo,David B. Emerson,Amandeep Singh,Veronica Chatrath,Marcelo Lotif,Ravi Theja,Alex Cheung,Izuki Matsubi*

Main category: cs.LG

TL;DR: FedRAG是一个支持集中式和联邦架构下微调RAG系统的框架，提供高效接口并填补工具空白。


<details>
  <summary>Details</summary>
Motivation: 现有RAG系统虽能缓解大语言模型依赖参数记忆的缺陷，但跨架构（集中式/联邦）的微调工具仍不足，需统一解决方案。

Method: 提出FedRAG框架，支持前沿微调方法，提供直观接口，实现集中式到联邦训练任务的无缝转换，并深度集成现代RAG生态。

Result: FedRAG成功填补了现有工具在跨架构微调RAG系统时的关键缺口，支持高效灵活的模型优化。

Conclusion: FedRAG通过统一架构和工具集成，显著提升了RAG系统在集中式与联邦环境中的微调能力。

Abstract: Retrieval-augmented generation (RAG) systems have been shown to be effective
in addressing many of the drawbacks of relying solely on the parametric memory
of large language models. Recent work has demonstrated that RAG systems can be
improved via fine-tuning of their retriever and generator models. In this work,
we introduce FedRAG, a framework for fine-tuning RAG systems across centralized
and federated architectures. FedRAG supports state-of-the-art fine-tuning
methods, offering a simple and intuitive interface and a seamless conversion
from centralized to federated training tasks. FedRAG is also deeply integrated
with the modern RAG ecosystem, filling a critical gap in available tools.

</details>


### [104] [Policy-Based Trajectory Clustering in Offline Reinforcement Learning](https://arxiv.org/abs/2506.09202)
*Hao Hu,Xinqi Wang,Simon Shaolei Du*

Main category: cs.LG

TL;DR: 本文提出两种新方法（PG-Kmeans和CAAE）用于离线强化学习轨迹聚类，通过策略生成概率和潜在表示引导实现有效聚类，并在理论与实验中验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 针对离线强化学习数据集中轨迹可能由不同策略生成的特点，研究如何通过聚类识别各轨迹对应的生成策略，以解决策略诱导分布混合导致的聚类模糊性问题。

Method: 提出策略引导K均值（PG-Kmeans）和码本吸引自编码器（CAAE）：前者通过行为克隆策略迭代分配轨迹，后者采用类VQ-VAE框架引导轨迹潜在表示靠近码本条目。

Result: 在D4RL数据集和GridWorld环境中验证，两种方法均能有效划分轨迹为有意义簇，且PG-Kmeans被证明的有限步收敛性支持其理论可靠性。

Conclusion: 所提方法为策略导向的轨迹聚类提供了可行方案，其解的不唯一性揭示了离线聚类本质挑战，在离线强化学习等领域具有广泛应用潜力。

Abstract: We introduce a novel task of clustering trajectories from offline
reinforcement learning (RL) datasets, where each cluster center represents the
policy that generated its trajectories. By leveraging the connection between
the KL-divergence of offline trajectory distributions and a mixture of
policy-induced distributions, we formulate a natural clustering objective. To
solve this, we propose Policy-Guided K-means (PG-Kmeans) and Centroid-Attracted
Autoencoder (CAAE). PG-Kmeans iteratively trains behavior cloning (BC) policies
and assigns trajectories based on policy generation probabilities, while CAAE
resembles the VQ-VAE framework by guiding the latent representations of
trajectories toward the vicinity of specific codebook entries to achieve
clustering. Theoretically, we prove the finite-step convergence of PG-Kmeans
and identify a key challenge in offline trajectory clustering: the inherent
ambiguity of optimal solutions due to policy-induced conflicts, which can
result in multiple equally valid but structurally distinct clusterings.
Experimentally, we validate our methods on the widely used D4RL dataset and
custom GridWorld environments. Our results show that both PG-Kmeans and CAAE
effectively partition trajectories into meaningful clusters. They offer a
promising framework for policy-based trajectory clustering, with broad
applications in offline RL and beyond.

</details>


### [105] [mLaSDI: Multi-stage latent space dynamics identification](https://arxiv.org/abs/2506.09207)
*William Anderson,Kevin Chung,Youngsoo Choi*

Main category: cs.LG

TL;DR: 本文提出多阶段潜在空间动态识别（mLaSDI）方法，通过分阶段训练多个自编码器逐步修正误差，以提升对偏微分方程降阶模型的预测精度和训练效率。


<details>
  <summary>Details</summary>
Motivation: 现有LaSDI方法在复杂或高频场景下，自编码器难以同时保证训练数据重建精度与潜在空间动态约束的平衡，导致模型性能受限。

Method: 采用多阶段自编码器训练策略，每个阶段的自编码器学习修正前一阶段的误差，结合低维常微分方程系统建模潜在空间动态。

Result: mLaSDI相比LaSDI显著降低了预测与重建误差，同时减少训练时间，且使用更小的自编码器即可实现更优性能。

Conclusion: 多阶段误差修正机制有效解决了自编码器在复杂动态建模中的权衡问题，为高精度高效降阶模型提供了新思路。

Abstract: Determining accurate numerical solutions of partial differential equations
(PDEs) is an important task in many scientific disciplines. However, solvers
can be computationally expensive, leading to the development of reduced-order
models (ROMs). Recently, Latent Space Dynamics Identification (LaSDI) was
proposed as a data-driven, non-intrusive ROM framework. LaSDI compresses the
training data using an autoencoder and learns a system of user-chosen ordinary
differential equations (ODEs), which govern the latent space dynamics. This
allows for rapid predictions by interpolating and evolving the low-dimensional
ODEs in the latent space. While LaSDI has produced effective ROMs for numerous
problems, the autoencoder can have difficulty accurately reconstructing
training data while also satisfying the imposed dynamics in the latent space,
particularly in complex or high-frequency regimes. To address this, we propose
multi-stage Latent Space Dynamics Identification (mLaSDI). With mLaSDI, several
autoencoders are trained sequentially in stages, where each autoencoder learns
to correct the error of the previous stages. We find that applying mLaSDI with
small autoencoders results in lower prediction and reconstruction errors, while
also reducing training time compared to LaSDI.

</details>


### [106] [Robust Noise Attenuation via Adaptive Pooling of Transformer Outputs](https://arxiv.org/abs/2506.09215)
*Greyson Brothers*

Main category: cs.LG

TL;DR: 本文研究了Transformer嵌入模型的池化方法设计，指出传统方法（AvgPool、MaxPool、ClsToken）在输入信噪比（SNR）波动时易出现性能崩溃，并提出一种基于注意力的自适应池化方法，理论证明其近似信号保留最优解，实验验证其在合成数据、关系推理、多智能体强化学习及含噪声视觉任务中的鲁棒性优势。


<details>
  <summary>Details</summary>
Motivation: 现有池化方法（如平均池化、最大池化、CLS标记）在输入信号与噪声混合时，因无法动态适应信噪比（SNR）变化，导致下游任务性能下降。研究旨在设计一种对SNR波动鲁棒的池化策略。

Method: 将池化建模为最小化信号损失的向量量化问题，提出基于注意力的自适应池化方法，理论推导其逼近信号最优量化器的误差界，并通过监督实验和实际任务（关系推理、多智能体强化学习、含噪声视觉任务）验证。

Result: 理论分析表明自适应池化在任意SNR下可逼近最优信号保留量化器；合成数据集实验验证其抗SNR波动能力，实际任务中Transformer结合自适应池化在噪声观测下表现更鲁棒。

Conclusion: 基于注意力的自适应池化通过动态权衡信号与噪声，显著提升Transformer在低SNR场景下的鲁棒性，为含噪声输入的嵌入聚合提供了理论保障与实用解决方案。

Abstract: We investigate the design of pooling methods used to summarize the outputs of
transformer embedding models, primarily motivated by reinforcement learning and
vision applications. This work considers problems where a subset of the input
vectors contains requisite information for a downstream task (signal) while the
rest are distractors (noise). By framing pooling as vector quantization with
the goal of minimizing signal loss, we demonstrate that the standard methods
used to aggregate transformer outputs, AvgPool, MaxPool, and ClsToken, are
vulnerable to performance collapse as the signal-to-noise ratio (SNR) of inputs
fluctuates. We then show that an attention-based adaptive pooling method can
approximate the signal-optimal vector quantizer within derived error bounds for
any SNR. Our theoretical results are first validated by supervised experiments
on a synthetic dataset designed to isolate the SNR problem, then generalized to
standard relational reasoning, multi-agent reinforcement learning, and vision
benchmarks with noisy observations, where transformers with adaptive pooling
display superior robustness across tasks.

</details>


### [107] [SoK: Machine Unlearning for Large Language Models](https://arxiv.org/abs/2506.09227)
*Jie Ren,Yue Xing,Yingqian Cui,Charu C. Aggarwal,Hui Liu*

Main category: cs.LG

TL;DR: 本文提出一种基于意图的大语言模型遗忘学习分类法，探讨真实知识移除的必要性、评估方法的不足及实际挑战，为未来研究和政策制定提供框架。


<details>
  <summary>Details</summary>
Motivation: 现有遗忘学习方法分类多关注技术特征，忽视核心意图差异（真实移除知识 vs. 抑制行为表现），需建立意图导向的体系以支持研究与实践。

Method: 构建意图导向的新分类法，系统分析三类问题：真实移除的可行性、评估策略的局限性、实际部署的障碍（如可扩展性）。

Result: 发现多数移除方法可能仅实现行为抑制而非真实知识消除，现有评估指标与基准存在偏差，且实际应用中存在序列遗忘等技术瓶颈。

Conclusion: 提出的分类框架为生成式AI遗忘学习提供系统性分析，强调需开发意图对齐的评估方法并解决实际部署挑战，支撑数据隐私治理决策。

Abstract: Large language model (LLM) unlearning has become a critical topic in machine
learning, aiming to eliminate the influence of specific training data or
knowledge without retraining the model from scratch. A variety of techniques
have been proposed, including Gradient Ascent, model editing, and re-steering
hidden representations. While existing surveys often organize these methods by
their technical characteristics, such classifications tend to overlook a more
fundamental dimension: the underlying intention of unlearning--whether it seeks
to truly remove internal knowledge or merely suppress its behavioral effects.
In this SoK paper, we propose a new taxonomy based on this intention-oriented
perspective. Building on this taxonomy, we make three key contributions. First,
we revisit recent findings suggesting that many removal methods may
functionally behave like suppression, and explore whether true removal is
necessary or achievable. Second, we survey existing evaluation strategies,
identify limitations in current metrics and benchmarks, and suggest directions
for developing more reliable and intention-aligned evaluations. Third, we
highlight practical challenges--such as scalability and support for sequential
unlearning--that currently hinder the broader deployment of unlearning methods.
In summary, this work offers a comprehensive framework for understanding and
advancing unlearning in generative AI, aiming to support future research and
guide policy decisions around data removal and privacy.

</details>


### [108] [Agent-based Condition Monitoring Assistance with Multimodal Industrial Database Retrieval Augmented Generation](https://arxiv.org/abs/2506.09247)
*Karl Löwenmark,Daniel Strömbergsson,Chang Liu,Marcus Liwicki,Fredrik Sandin*

Main category: cs.LG

TL;DR: 本文提出MindRAG框架，通过整合多模态检索增强生成（RAG）与定制化向量存储结构，结合大语言模型（LLM）推理代理，优化工业状态监测（CM）系统的误报率、故障严重性评估及决策支持，并提升系统可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有计算机化维护系统在故障严重性估计和维护决策上依赖人工，存在高误报率与不确定性，导致效率低下。需通过自动化方法减少人工负担并提高决策可靠性。

Method: 开发MindRAG模块化框架：1) 构建面向CM数据的半结构化多模态向量存储；2) 设计专用多模态RAG技术；3) 开发实际推理代理处理工业CM查询；4) 建立实验框架评估真实场景性能。利用现有标注与工单数据作为监督学习标签，解决无标注噪声数据训练难题。

Result: 经资深分析师验证的初步结果表明，MindRAG能有效管理告警并提供有意义的决策支持，降低误报率，同时增强CM系统的可解释性。

Conclusion: MindRAG通过LLM与领域数据融合，为工业CM系统提供自动化、可解释的决策支持，显著提升告警管理效率，验证了无标注数据场景下的模型训练挑战。

Abstract: Condition monitoring (CM) plays a crucial role in ensuring reliability and
efficiency in the process industry. Although computerised maintenance systems
effectively detect and classify faults, tasks like fault severity estimation,
and maintenance decisions still largely depend on human expert analysis. The
analysis and decision making automatically performed by current systems
typically exhibit considerable uncertainty and high false alarm rates, leading
to increased workload and reduced efficiency.
  This work integrates large language model (LLM)-based reasoning agents with
CM workflows to address analyst and industry needs, namely reducing false
alarms, enhancing fault severity estimation, improving decision support, and
offering explainable interfaces. We propose MindRAG, a modular framework
combining multimodal retrieval-augmented generation (RAG) with novel vector
store structures designed specifically for CM data. The framework leverages
existing annotations and maintenance work orders as surrogates for labels in a
supervised learning protocol, addressing the common challenge of training
predictive models on unlabelled and noisy real-world datasets.
  The primary contributions include: (1) an approach for structuring industry
CM data into a semi-structured multimodal vector store compatible with
LLM-driven workflows; (2) developing multimodal RAG techniques tailored for CM
data; (3) developing practical reasoning agents capable of addressing
real-world CM queries; and (4) presenting an experimental framework for
integrating and evaluating such agents in realistic industrial scenarios.
Preliminary results, evaluated with the help of an experienced analyst,
indicate that MindRAG provide meaningful decision support for more efficient
management of alarms, thereby improving the interpretability of CM systems.

</details>


### [109] [CFMI: Flow Matching for Missing Data Imputation](https://arxiv.org/abs/2506.09258)
*Vaidotas Simkus,Michael U. Gutmann*

Main category: cs.LG

TL;DR: 本文提出了一种基于条件流匹配的缺失数据填补方法CFMI，结合连续归一化流、流匹配和共享条件建模，在多种数据集上表现优于传统及现代方法，计算效率高且适用于不同维度和数据类型。


<details>
  <summary>Details</summary>
Motivation: 传统多重填补方法在处理高维数据时存在计算复杂性和效率问题，需开发一种通用又高效的填补方法以应对不同数据场景。

Method: CFMI整合连续归一化流（CNF）、流匹配（flow-matching）及共享条件建模，通过条件概率建模绕过传统方法的计算瓶颈。

Result: 在24个中低维表格数据集上，CFMI在多项指标中优于9种现有方法；在时间序列零样本填补中，其精度与扩散方法相当但计算效率更高。

Conclusion: CFMI在低维数据上与传统方法相当，在高维场景下扩展性强，综合性能优于其他深度学习方法，成为广泛数据类型的首选填补工具。

Abstract: We introduce conditional flow matching for imputation (CFMI), a new
general-purpose method to impute missing data. The method combines continuous
normalising flows, flow-matching, and shared conditional modelling to deal with
intractabilities of traditional multiple imputation. Our comparison with nine
classical and state-of-the-art imputation methods on 24 small to
moderate-dimensional tabular data sets shows that CFMI matches or outperforms
both traditional and modern techniques across a wide range of metrics. Applying
the method to zero-shot imputation of time-series data, we find that it matches
the accuracy of a related diffusion-based method while outperforming it in
terms of computational efficiency. Overall, CFMI performs at least as well as
traditional methods on lower-dimensional data while remaining scalable to
high-dimensional settings, matching or exceeding the performance of other deep
learning-based approaches, making it a go-to imputation method for a wide range
of data types and dimensionalities.

</details>


### [110] [Uncertainty Prioritized Experience Replay](https://arxiv.org/abs/2506.09270)
*Rodrigo Carrasco-Davis,Sebastian Lee,Claudia Clopath,Will Dabney*

Main category: cs.LG

TL;DR: 提出使用认知不确定性指导经验回放优先级，解决传统TD误差方法易受噪声干扰的问题，在Atari任务中超越基准。


<details>
  <summary>Details</summary>
Motivation: 传统基于时序差分误差的优先级采样易偏向噪声样本（类似探索中的'嘈杂电视问题'），需减少不可预测随机过程产生的噪声对价值估计的影响。

Method: 通过认知不确定性量化可被学习减少的估计不确定性，优先选择非随机噪声产生的经验转换，应用于回放缓冲区优先级排序。

Result: 在简单多臂老虎机、噪声网格世界及Atari测试中，其性能超越分位数回归深度Q学习基准。

Conclusion: 认知不确定性优先级回放为强化学习样本效率提升开辟新路径，有效抑制噪声干扰并优化学习过程。

Abstract: Prioritized experience replay, which improves sample efficiency by selecting
relevant transitions to update parameter estimates, is a crucial component of
contemporary value-based deep reinforcement learning models. Typically,
transitions are prioritized based on their temporal difference error. However,
this approach is prone to favoring noisy transitions, even when the value
estimation closely approximates the target mean. This phenomenon resembles the
noisy TV problem postulated in the exploration literature, in which
exploration-guided agents get stuck by mistaking noise for novelty. To mitigate
the disruptive effects of noise in value estimation, we propose using epistemic
uncertainty estimation to guide the prioritization of transitions from the
replay buffer. Epistemic uncertainty quantifies the uncertainty that can be
reduced by learning, hence reducing transitions sampled from the buffer
generated by unpredictable random processes. We first illustrate the benefits
of epistemic uncertainty prioritized replay in two tabular toy models: a simple
multi-arm bandit task, and a noisy gridworld. Subsequently, we evaluate our
prioritization scheme on the Atari suite, outperforming quantile regression
deep Q-learning benchmarks; thus forging a path for the use of uncertainty
prioritized replay in reinforcement learning agents.

</details>


### [111] [G-Sim: Generative Simulations with Large Language Models and Gradient-Free Calibration](https://arxiv.org/abs/2506.09272)
*Samuel Holt,Max Ruiz Luyten,Antonin Berthon,Mihaela van der Schaar*

Main category: cs.LG

TL;DR: 提出G-Sim混合框架，通过结合LLM驱动的结构设计与实证校准，构建可靠模拟器以支持复杂决策。


<details>
  <summary>Details</summary>
Motivation: 现有模拟器构建方法在泛化性、准确性及数据对齐方面存在不足，难以支撑医疗、物流等关键领域的系统级干预。

Method: 使用LLM迭代生成模拟器因果结构，结合梯度自由优化和模拟推断技术进行参数校准，处理非可微分/随机系统。

Result: G-Sim通过融合领域知识与数据，构建出因果驱动的可靠模拟器，缓解数据低效问题并支持鲁棒决策。

Conclusion: 整合先验知识与灵活校准的混合框架，显著提升了模拟器在复杂现实场景中的实用性和解释性。

Abstract: Constructing robust simulators is essential for asking "what if?" questions
and guiding policy in critical domains like healthcare and logistics. However,
existing methods often struggle, either failing to generalize beyond historical
data or, when using Large Language Models (LLMs), suffering from inaccuracies
and poor empirical alignment. We introduce G-Sim, a hybrid framework that
automates simulator construction by synergizing LLM-driven structural design
with rigorous empirical calibration. G-Sim employs an LLM in an iterative loop
to propose and refine a simulator's core components and causal relationships,
guided by domain knowledge. This structure is then grounded in reality by
estimating its parameters using flexible calibration techniques. Specifically,
G-Sim can leverage methods that are both likelihood-free and gradient-free with
respect to the simulator, such as gradient-free optimization for direct
parameter estimation or simulation-based inference for obtaining a posterior
distribution over parameters. This allows it to handle non-differentiable and
stochastic simulators. By integrating domain priors with empirical evidence,
G-Sim produces reliable, causally-informed simulators, mitigating
data-inefficiency and enabling robust system-level interventions for complex
decision-making.

</details>


### [112] [Learning The Minimum Action Distance](https://arxiv.org/abs/2506.09276)
*Lorenzo Steccanella,Joshua B. Evans,Özgür Şimşek,Anders Jonsson*

Main category: cs.LG

TL;DR: 本文提出一种仅从状态轨迹学习MDP状态表示的自监督框架，通过最小动作距离（MAD）捕捉环境结构，支持目标导向强化学习等任务，并在多环境中验证其优越性。


<details>
  <summary>Details</summary>
Motivation: 现有状态表示方法常依赖奖励信号或动作信息，本文旨在仅通过状态序列学习环境底层结构，解决无奖励/动作场景下的表示学习问题。

Method: 提出最小动作距离（MAD）作为状态间转换难度的核心指标，设计自监督嵌入模型使空间距离反映MAD，兼容对称/非对称近似与不同动态特性。

Result: 在包含确定性/随机、离散/连续、含噪声观测的多样化环境中，该方法准确学习MAD表示且显著优于基线，验证了框架的鲁棒性与通用性。

Conclusion: MAD作为几何意义明确的密集进度度量，能有效支持下游任务，自监督学习方法为无监督强化学习提供了新的状态表示范式。

Abstract: This paper presents a state representation framework for Markov decision
processes (MDPs) that can be learned solely from state trajectories, requiring
neither reward signals nor the actions executed by the agent. We propose
learning the minimum action distance (MAD), defined as the minimum number of
actions required to transition between states, as a fundamental metric that
captures the underlying structure of an environment. MAD naturally enables
critical downstream tasks such as goal-conditioned reinforcement learning and
reward shaping by providing a dense, geometrically meaningful measure of
progress. Our self-supervised learning approach constructs an embedding space
where the distances between embedded state pairs correspond to their MAD,
accommodating both symmetric and asymmetric approximations. We evaluate the
framework on a comprehensive suite of environments with known MAD values,
encompassing both deterministic and stochastic dynamics, as well as discrete
and continuous state spaces, and environments with noisy observations.
Empirical results demonstrate that the proposed approach not only efficiently
learns accurate MAD representations across these diverse settings but also
significantly outperforms existing state representation methods in terms of
representation quality.

</details>


### [113] [A Topic Modeling Analysis of Stigma Dimensions, Social, and Related Behavioral Circumstances in Clinical Notes Among Patients with HIV](https://arxiv.org/abs/2506.09279)
*Ziyi Chen,Yiyang Liu,Mattia Prosperi,Krishna Vaddiparti,Robert L Cook,Jiang Bian,Yi Guo,Yonghui Wu*

Main category: cs.LG

TL;DR: 利用自然语言处理分析HIV感染者电子健康记录，识别与耻辱感相关的多维度主题及社会行为因素，发现不同年龄组差异，提供高效评估方法。


<details>
  <summary>Details</summary>
Motivation: 传统问卷评估HIV相关耻辱感存在局限性，需通过电子健康记录（EHR）实现可扩展、时效性强的自动化分析以改善患者结局。

Method: 从9,140例HIV感染者EHR中提取临床笔记，采用LDA主题建模结合关键词雪球扩展策略，通过专家标注验证主题，并进行年龄/性别亚组差异分析。

Result: 识别出'心理健康与耻辱感''社会支持''医疗资源限制''拒绝治疗与隔离'等主题，不同年龄组间主题分布差异显著。

Conclusion: 基于EHR的NLP方法能有效捕捉HIV相关耻辱感多维度特征，克服传统评估局限，为精准干预提供数据支持。

Abstract: Objective: To characterize stigma dimensions, social, and related behavioral
circumstances in people living with HIV (PLWHs) seeking care, using natural
language processing methods applied to a large collection of electronic health
record (EHR) clinical notes from a large integrated health system in the
southeast United States. Methods: We identified 9,140 cohort of PLWHs from the
UF Health IDR and performed topic modeling analysis using Latent Dirichlet
Allocation (LDA) to uncover stigma dimensions, social, and related behavioral
circumstances. Domain experts created a seed list of HIV-related stigma
keywords, then applied a snowball strategy to iteratively review notes for
additional terms until saturation was reached. To identify more target topics,
we tested three keyword-based filtering strategies. Domain experts manually
reviewed the detected topics using the prevalent terms and key discussion
topics. Word frequency analysis was used to highlight the prevalent terms
associated with each topic. In addition, we conducted topic variation analysis
among subgroups to examine differences across age and sex-specific
demographics. Results and Conclusion: Topic modeling on sentences containing at
least one keyword uncovered a wide range of topic themes associated with
HIV-related stigma, social, and related behaviors circumstances, including
"Mental Health Concern and Stigma", "Social Support and Engagement", "Limited
Healthcare Access and Severe Illness", "Treatment Refusal and Isolation" and so
on. Topic variation analysis across age subgroups revealed differences.
Extracting and understanding the HIV-related stigma dimensions, social, and
related behavioral circumstances from EHR clinical notes enables scalable,
time-efficient assessment, overcoming the limitations of traditional
questionnaires and improving patient outcomes.

</details>


### [114] [Causal Graph Recovery in Neuroimaging through Answer Set Programming](https://arxiv.org/abs/2506.09286)
*Mohammadsajad Abavisani,Kseniya Solovyeva,David Danks,Vince Calhoun,Sergey Plis*

Main category: cs.LG

TL;DR: 本文提出一种基于答案集编程（ASP）的约束优化方法，用于从子采样时间序列数据中学习因果图结构。该方法通过整合子采样效应和图论剪枝，显著提高了因果图重建的准确性和效率，并在模拟和真实数据实验中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统方法在测量频率与因果时间尺度不匹配时，因信息丢失导致生成多个可能的因果图。研究旨在解决子采样带来的不确定性，提升因果图重建的准确性和可解释性。

Method: 采用答案集编程（ASP）进行约束优化，结合图论剪枝策略，生成最优因果图集合。该方法不仅能识别最可能的因果图，还能提供等价类供专家选择。

Result: 实验表明，该方法在模拟数据和脑结构连接数据中优于现有方法，F1分数平均提升12%，且在子采样率较高时仍保持鲁棒性。同时实现了因果图重建的精度和召回率最优结果。

Conclusion: 所提出的ASP方法通过显式建模子采样效应，显著提高了因果图推断的准确性和效率，并可作为现有方法的元框架进一步提升性能，尤其在高度子采样场景下表现突出。

Abstract: Learning graphical causal structures from time series data presents
significant challenges, especially when the measurement frequency does not
match the causal timescale of the system. This often leads to a set of equally
possible underlying causal graphs due to information loss from sub-sampling
(i.e., not observing all possible states of the system throughout time). Our
research addresses this challenge by incorporating the effects of sub-sampling
in the derivation of causal graphs, resulting in more accurate and intuitive
outcomes. We use a constraint optimization approach, specifically answer set
programming (ASP), to find the optimal set of answers. ASP not only identifies
the most probable underlying graph, but also provides an equivalence class of
possible graphs for expert selection. In addition, using ASP allows us to
leverage graph theory to further prune the set of possible solutions, yielding
a smaller, more accurate answer set significantly faster than traditional
approaches. We validate our approach on both simulated data and empirical
structural brain connectivity, and demonstrate its superiority over established
methods in these experiments. We further show how our method can be used as a
meta-approach on top of established methods to obtain, on average, 12%
improvement in F1 score. In addition, we achieved state of the art results in
terms of precision and recall of reconstructing causal graph from sub-sampled
time series data. Finally, our method shows robustness to varying degrees of
sub-sampling on realistic simulations, whereas other methods perform worse for
higher rates of sub-sampling.

</details>


### [115] [On-the-Fly Adaptive Distillation of Transformer to Dual-State Linear Attention](https://arxiv.org/abs/2506.09316)
*Yeonju Ro,Zhenyu Zhang,Souvik Kundu,Zhangyang Wang,Aditya Akella*

Main category: cs.LG

TL;DR: 本文提出双状态线性注意力（DSLA）和自适应蒸馏框架SERVE，通过平衡历史与近期token的依赖关系，在提升长文本处理效率的同时保持模型性能。实验显示，SERVE推理速度比Llama2-7B快2.3倍，且任务表现接近原模型。


<details>
  <summary>Details</summary>
Motivation: 传统线性注意力方法因过度关注近期token导致历史信息丢失，而大语言模型处理长文本时计算成本过高。需在效率与准确性间取得平衡。

Method: 1. DSLA设计：维护历史状态与近期状态，缓解线性注意力的短程偏差；2. SERVE框架：基于敏感度动态替换Transformer层为DSLA层，并通过链式微调保持层间一致性。

Result: SERVE推理速度比Llama2-7B快2.3倍，比Zamba-7B快3.0倍，下游任务性能接近原模型。消融实验验证DSLA能同时捕捉全局与局部依赖。

Conclusion: DSLA通过双状态机制有效解决线性注意力的历史信息缺失问题，SERVE框架实现动态负载下的效率-精度平衡，为长文本处理提供高效方案。

Abstract: Large language models (LLMs) excel at capturing global token dependencies via
self-attention but face prohibitive compute and memory costs on lengthy inputs.
While sub-quadratic methods (e.g., linear attention) can reduce these costs,
they often degrade accuracy due to overemphasizing recent tokens. In this work,
we first propose \textit{dual-state linear attention} (\textbf{\dsla}), a novel
design that maintains two specialized hidden states-one for preserving
historical context and one for tracking recency-thereby mitigating the
short-range bias typical of linear-attention architectures. To further balance
efficiency and accuracy under dynamic workload conditions, we introduce
\textbf{\serve}, an online \textit{adaptive distillation} framework that
progressively replaces Transformer layers with DSLA layers at inference time,
guided by a sensitivity-based layer ordering. \serve\ uses a chained
fine-tuning strategy to ensure that each newly converted DSLA layer remains
consistent with previously replaced layers, preserving the overall quality.
Extensive evaluations on commonsense reasoning, long-context QA, and text
summarization demonstrate that \serve\ yields \textbf{2.3x} faster inference
than Llama2-7B and \textbf{3.0x} faster than the hybrid Zamba-7B, while
retaining comparable performance across downstream tasks. Our ablation studies
show that DSLA's dual states capture both global and local dependencies,
addressing the historical-token underrepresentation seen in prior linear
attentions. Codes are available at https://github.com/utnslab/DSLA-Serve.

</details>


### [116] [Natural Language Guided Ligand-Binding Protein Design](https://arxiv.org/abs/2506.09332)
*Zhenqiao Song,Ramith Hettiarachchi,Chuan Li,Jianwen Xie,Lei Li*

Main category: cs.LG

TL;DR: 本文提出InstructPro蛋白质生成模型，能根据自然语言指令和配体公式设计功能匹配的配体结合蛋白。通过构建大规模数据集InstructProBench，训练1B/3B参数模型，在对接成功率和结构偏差指标上显著超越现有基线。


<details>
  <summary>Details</summary>
Motivation: 传统基于蛋白质-配体复合物数据的AI模型受限于实验数据稀缺性，而人工整理的蛋白质功能文本描述与配体化学式数据更丰富。利用文本指令指导蛋白质设计可突破数据瓶颈。

Method: 开发支持自然语言输入的InstructPro模型架构，构建包含959万(功能描述,配体SMILES,蛋白序列)三元组的InstructProBench数据集，训练1B/3B参数模型，通过文本指令与配体公式联合生成功能匹配的蛋白质序列。

Result: InstructPro-1B获得81.52%对接成功率(中等置信度)和4.026Å平均RMSD；3B版本进一步降至2.527Å，在对接成功率和结构保真度上全面超越ProGen2、ESM3等基线模型。

Conclusion: InstructPro证明通过自然语言指令设计功能蛋白质的可行性，其文本驱动的生成范式能有效利用丰富的文本知识资源，为精准蛋白质工程开辟新路径。

Abstract: Can AI protein models follow human language instructions and design proteins
with desired functions (e.g. binding to a ligand)? Designing proteins that bind
to a given ligand is crucial in a wide range of applications in biology and
chemistry. Most prior AI models are trained on protein-ligand complex data,
which is scarce due to the high cost and time requirements of laboratory
experiments. In contrast, there is a substantial body of human-curated text
descriptions about protein-ligand interactions and ligand formula. In this
paper, we propose InstructPro, a family of protein generative models that
follow natural language instructions to design ligand-binding proteins. Given a
textual description of the desired function and a ligand formula in SMILES,
InstructPro generates protein sequences that are functionally consistent with
the specified instructions. We develop the model architecture, training
strategy, and a large-scale dataset, InstructProBench, to support both training
and evaluation. InstructProBench consists of 9,592,829 triples of (function
description, ligand formula, protein sequence). We train two model variants:
InstructPro-1B (with 1 billion parameters) and InstructPro-3B~(with 3 billion
parameters). Both variants consistently outperform strong baselines, including
ProGen2, ESM3, and Pinal. Notably, InstructPro-1B achieves the highest docking
success rate (81.52% at moderate confidence) and the lowest average root mean
square deviation (RMSD) compared to ground truth structures (4.026{\AA}).
InstructPro-3B further descreases the average RMSD to 2.527{\AA}, demonstrating
InstructPro's ability to generate ligand-binding proteins that align with the
functional specifications.

</details>


### [117] [ErrorEraser: Unlearning Data Bias for Improved Continual Learning](https://arxiv.org/abs/2506.09347)
*Xuemei Cao,Hanlin Gu,Xin Yang,Bingjun Wei,Haoyang Liang,Xiangkun Wang,Tianrui Li*

Main category: cs.LG

TL;DR: 本文提出ErrorEraser插件，通过主动遗忘数据偏差导致的错误记忆，提升持续学习在防止遗忘和知识迁移中的性能。该方法包含错误识别与擦除模块，结合增量特征分布学习策略，有效降低数据偏差的负面影响。


<details>
  <summary>Details</summary>
Motivation: 现有持续学习方法忽视现实数据中的偏差，导致模型学习虚假相关性，并在任务间传递放大，削弱知识保留与迁移能力。需通过主动遗忘消除偏差影响。

Method: ErrorEraser包含两个模块：1) 错误识别模块通过无先验知识的特征空间概率密度分布识别偏差样本；2) 错误擦除模块通过调整决策空间仅消除错误知识。另设计增量特征分布学习策略降低下游任务资源消耗。

Result: 实验表明，ErrorEraser在三种持续学习方法中显著提升准确率（最高达15.7%）、降低遗忘率（最高达18.3%），且计算开销仅增加约1%。代码已开源。

Conclusion: 主动遗忘机制可有效缓解持续学习中的数据偏差问题。ErrorEraser作为通用插件，无需修改原模型结构即能增强新旧任务表现，为偏差处理提供新思路。

Abstract: Continual Learning (CL) primarily aims to retain knowledge to prevent
catastrophic forgetting and transfer knowledge to facilitate learning new
tasks. Unlike traditional methods, we propose a novel perspective: CL not only
needs to prevent forgetting, but also requires intentional forgetting.This
arises from existing CL methods ignoring biases in real-world data, leading the
model to learn spurious correlations that transfer and amplify across tasks.
From feature extraction and prediction results, we find that data biases
simultaneously reduce CL's ability to retain and transfer knowledge. To address
this, we propose ErrorEraser, a universal plugin that removes erroneous
memories caused by biases in CL, enhancing performance in both new and old
tasks. ErrorEraser consists of two modules: Error Identification and Error
Erasure. The former learns the probability density distribution of task data in
the feature space without prior knowledge, enabling accurate identification of
potentially biased samples. The latter ensures only erroneous knowledge is
erased by shifting the decision space of representative outlier samples.
Additionally, an incremental feature distribution learning strategy is designed
to reduce the resource overhead during error identification in downstream
tasks. Extensive experimental results show that ErrorEraser significantly
mitigates the negative impact of data biases, achieving higher accuracy and
lower forgetting rates across three types of CL methods. The code is available
at https://github.com/diadai/ErrorEraser.

</details>


### [118] [Adversarial Surrogate Risk Bounds for Binary Classification](https://arxiv.org/abs/2506.09348)
*Natalie S. Frank*

Main category: cs.LG

TL;DR: 本文研究对抗训练中替代风险与分类风险收敛速率的关系，提出替代风险界限量化收敛速度，并在标准学习中推导分布相关的风险界限。


<details>
  <summary>Details</summary>
Motivation: 现有对抗一致性研究未解决替代风险最小化序列下对抗分类风险收敛至最优值的速率问题，需量化该速率以提升对抗训练的理论基础。

Method: 通过理论分析推导对抗场景下的替代风险收敛速率界限，并扩展至非对抗场景的分布相关风险界限。

Result: 提出对抗分类风险的替代风险收敛速率量化界限，同时在标准学习场景中获得独立价值的分布相关风险界限。

Conclusion: 研究填补对抗一致性理论中收敛速率分析的空白，所提风险界限为优化对抗训练算法提供理论支持，非对抗场景结果亦具独立意义。

Abstract: A central concern in classification is the vulnerability of machine learning
models to adversarial attacks. Adversarial training is one of the most popular
techniques for training robust classifiers, which involves minimizing an
adversarial surrogate risk. Recent work characterized when a minimizing
sequence of an adversarial surrogate risk is also a minimizing sequence of the
adversarial classification risk for binary classification -- a property known
as adversarial consistency. However, these results do not address the rate at
which the adversarial classification risk converges to its optimal value for
such a sequence of functions that minimize the adversarial surrogate. This
paper provides surrogate risk bounds that quantify that convergence rate.
Additionally, we derive distribution-dependent surrogate risk bounds in the
standard (non-adversarial) learning setting, that may be of independent
interest.

</details>


### [119] [Anomaly Detection and Generation with Diffusion Models: A Survey](https://arxiv.org/abs/2506.09368)
*Yang Liu,Jing Liu,Chengfang Li,Rui Xi,Wenchao Li,Liang Cao,Jin Wang,Laurence T. Yang,Junsong Yuan,Wei Zhou*

Main category: cs.LG

TL;DR: 这篇综述探讨了扩散模型在异常检测与生成中的协同应用，提出了分类法分析现有方法，并指出未来研究方向如高效架构和多模态整合。


<details>
  <summary>Details</summary>
Motivation: 传统异常检测常面临异常数据稀缺的问题，而扩散模型凭借其强大的数据生成能力，能够通过生成合成数据弥补这一缺陷，同时检测反馈提升生成质量，形成互补循环。

Method: 论文系统综述了基于扩散模型的异常检测与生成方法，构建了以异常评分机制、条件策略和架构设计为核心的分类体系，并分析各类方法的优缺点。

Result: 研究揭示了检测与生成任务的协同效应，建立了统一的理论框架，同时指出当前方法在计算效率、可扩展性等方面的局限性。

Conclusion: 扩散模型为异常检测提供了创新解决方案，未来应聚焦高效架构设计、多模态条件策略以及与视觉-语言大模型的集成，推动跨领域应用。

Abstract: Anomaly detection (AD) plays a pivotal role across diverse domains, including
cybersecurity, finance, healthcare, and industrial manufacturing, by
identifying unexpected patterns that deviate from established norms in
real-world data. Recent advancements in deep learning, specifically diffusion
models (DMs), have sparked significant interest due to their ability to learn
complex data distributions and generate high-fidelity samples, offering a
robust framework for unsupervised AD. In this survey, we comprehensively review
anomaly detection and generation with diffusion models (ADGDM), presenting a
tutorial-style analysis of the theoretical foundations and practical
implementations and spanning images, videos, time series, tabular, and
multimodal data. Crucially, unlike existing surveys that often treat anomaly
detection and generation as separate problems, we highlight their inherent
synergistic relationship. We reveal how DMs enable a reinforcing cycle where
generation techniques directly address the fundamental challenge of anomaly
data scarcity, while detection methods provide critical feedback to improve
generation fidelity and relevance, advancing both capabilities beyond their
individual potential. A detailed taxonomy categorizes ADGDM methods based on
anomaly scoring mechanisms, conditioning strategies, and architectural designs,
analyzing their strengths and limitations. We final discuss key challenges
including scalability and computational efficiency, and outline promising
future directions such as efficient architectures, conditioning strategies, and
integration with foundation models (e.g., visual-language models and large
language models). By synthesizing recent advances and outlining open research
questions, this survey aims to guide researchers and practitioners in
leveraging DMs for innovative AD solutions across diverse applications.

</details>


### [120] [LPO: Towards Accurate GUI Agent Interaction via Location Preference Optimization](https://arxiv.org/abs/2506.09373)
*Jiaqi Tang,Yu Xia,Yi-Feng Wu,Yuwei Hu,Yuhui Chen,Qing-Guo Chen,Xiaogang Xu,Xiangyu Wu,Hao Lu,Yanqing Ma,Shiyin Lu,Qifeng Chen*

Main category: cs.LG

TL;DR: 本文提出了一种基于位置偏好优化（LPO）的新方法，通过信息熵和动态位置奖励函数提升GUI代理的交互精度，解决了现有方法在空间定位中的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有GUI代理的监督微调（SFT）方法在位置感知能力上存在不足，而强化学习策略难以有效评估位置准确性，导致交互效果受限。

Method: LPO结合信息熵预测高信息密度交互区域，引入动态位置奖励函数反映位置重要性，并通过GRPO支持对GUI环境的广泛探索。

Result: 实验表明LPO在离线和在线评估中均达到SOTA性能，显著提升了交互精度。

Conclusion: LPO通过优化位置偏好和动态奖励机制，为GUI代理的交互提供了更高效的解决方案，代码即将公开。

Abstract: The advent of autonomous agents is transforming interactions with Graphical
User Interfaces (GUIs) by employing natural language as a powerful
intermediary. Despite the predominance of Supervised Fine-Tuning (SFT) methods
in current GUI agents for achieving spatial localization, these methods face
substantial challenges due to their limited capacity to accurately perceive
positional data. Existing strategies, such as reinforcement learning, often
fail to assess positional accuracy effectively, thereby restricting their
utility. In response, we introduce Location Preference Optimization (LPO), a
novel approach that leverages locational data to optimize interaction
preferences. LPO uses information entropy to predict interaction positions by
focusing on zones rich in information. Besides, it further introduces a dynamic
location reward function based on physical distance, reflecting the varying
importance of interaction positions. Supported by Group Relative Preference
Optimization (GRPO), LPO facilitates an extensive exploration of GUI
environments and significantly enhances interaction precision. Comprehensive
experiments demonstrate LPO's superior performance, achieving SOTA results
across both offline benchmarks and real-world online evaluations. Our code will
be made publicly available soon, at https://github.com/AIDC-AI/LPO.

</details>


### [121] [Revisiting Diffusion Models: From Generative Pre-training to One-Step Generation](https://arxiv.org/abs/2506.09376)
*Bowen Zheng,Tianming Yang*

Main category: cs.LG

TL;DR: 扩散蒸馏技术存在训练成本高、学生模型性能下降的问题，研究发现单独使用GAN目标可克服这些限制，将扩散模型转化为高效一步生成器。扩散训练可视为生成式预训练，通过轻量级GAN微调即可解锁其能力，实验仅用0.2M图像即实现强性能。


<details>
  <summary>Details</summary>
Motivation: 传统扩散蒸馏方法因师生模型的步长/参数不匹配导致收敛到不同局部最优解，直接模仿效果受限。尽管引入GAN目标可缓解问题，但其机制尚不明确，需揭示扩散模型与高效单步生成的关系。

Method: 1) 识别扩散蒸馏的局限性本质 2) 证明独立GAN目标可突破限制 3) 提出扩散训练作为生成预训练框架 4) 冻结85%预训练参数进行轻量微调 5) 通过频域分析解释单步生成能力来源。

Result: 仅用0.2M图像微调即实现强性能，5M图像时接近SOTA。频域分析表明扩散训练使模型学习到频率空间特征，这为单步生成提供了基础。

Conclusion: 扩散训练本质是强大的生成预训练过程，其习得的能力可通过轻量级GAN微调快速转化为高效单步生成模型，为构建快速生成系统提供了新范式。

Abstract: Diffusion distillation is a widely used technique to reduce the sampling cost
of diffusion models, yet it often requires extensive training, and the student
performance tends to be degraded. Recent studies show that incorporating a GAN
objective may alleviate these issues, yet the underlying mechanism remains
unclear. In this work, we first identify a key limitation of distillation:
mismatched step sizes and parameter numbers between the teacher and the student
model lead them to converge to different local minima, rendering direct
imitation suboptimal. We further demonstrate that a standalone GAN objective,
without relying a distillation loss, overcomes this limitation and is
sufficient to convert diffusion models into efficient one-step generators.
Based on this finding, we propose that diffusion training may be viewed as a
form of generative pre-training, equipping models with capabilities that can be
unlocked through lightweight GAN fine-tuning. Supporting this view, we create a
one-step generation model by fine-tuning a pre-trained model with 85% of
parameters frozen, achieving strong performance with only 0.2M images and
near-SOTA results with 5M images. We further present a frequency-domain
analysis that may explain the one-step generative capability gained in
diffusion training. Overall, our work provides a new perspective for diffusion
training, highlighting its role as a powerful generative pre-training process,
which can be the basis for building efficient one-step generation models.

</details>


### [122] [Efficient Prediction of SO(3)-Equivariant Hamiltonian Matrices via SO(2) Local Frames](https://arxiv.org/abs/2506.09398)
*Haiyang Yu,Yuchao Lin,Xuan Zhang,Xiaofeng Qian,Shuiwang Ji*

Main category: cs.LG

TL;DR: 本文提出了一种名为QHNetV2的新型高效方法，通过引入SO(2)-等变操作并在SO(2)局部框架内进行特征更新，实现了全局SO(3)等变性，避免了高成本的SO(3)张量积，显著提升了电子结构计算的预测性能。


<details>
  <summary>Details</summary>
Motivation: 电子结构计算中的哈密顿矩阵预测在物理、化学和材料科学中至关重要，但传统方法依赖计算昂贵的SO(3)张量积。研究旨在通过利用哈密顿矩阵非对角块与SO(2)局部框架的内在关联，设计更高效的对称性感知模型。

Method: 提出QHNetV2网络，通过设计SO(2)-等变操作，在SO(2)局部框架内完成非对角块特征更新和消息传递，避免SO(3)张量积；引入连续SO(2)张量积进行节点特征融合，模拟对称收缩操作。

Result: 在QH9和MD17数据集上的实验表明，模型在多种分子结构和轨迹预测中表现优异，展现出强泛化能力，验证了SO(2)局部框架操作的有效性。

Conclusion: 基于SO(2)局部框架的操作为可扩展、对称性感知的电子结构学习提供了新方向，模型性能优越且计算高效，代码将通过AIRS库开源。

Abstract: We consider the task of predicting Hamiltonian matrices to accelerate
electronic structure calculations, which plays an important role in physics,
chemistry, and materials science. Motivated by the inherent relationship
between the off-diagonal blocks of the Hamiltonian matrix and the SO(2) local
frame, we propose a novel and efficient network, called QHNetV2, that achieves
global SO(3) equivariance without the costly SO(3) Clebsch-Gordan tensor
products. This is achieved by introducing a set of new efficient and powerful
SO(2)-equivariant operations and performing all off-diagonal feature updates
and message passing within SO(2) local frames, thereby eliminating the need of
SO(3) tensor products. Moreover, a continuous SO(2) tensor product is performed
within the SO(2) local frame at each node to fuse node features, mimicking the
symmetric contraction operation. Extensive experiments on the large QH9 and
MD17 datasets demonstrate that our model achieves superior performance across a
wide range of molecular structures and trajectories, highlighting its strong
generalization capability. The proposed SO(2) operations on SO(2) local frames
offer a promising direction for scalable and symmetry-aware learning of
electronic structures. Our code will be released as part of the AIRS library
https://github.com/divelab/AIRS.

</details>


### [123] [Synergizing Reinforcement Learning and Genetic Algorithms for Neural Combinatorial Optimization](https://arxiv.org/abs/2506.09404)
*Shengda Gu,Kai Li,Junliang Xing,Yifan Zhang,Jian Cheng*

Main category: cs.LG

TL;DR: 本文提出了一种结合深度强化学习（DRL）与遗传算法（GA）优势的进化增强机制（EAM），通过融合DRL的学习效率和GA的全局搜索能力，显著提升了组合优化问题的求解质量和训练效率。


<details>
  <summary>Details</summary>
Motivation: 组合优化问题因离散结构和庞大解空间而极具挑战性。现有DRL方法存在探索不足和易陷局部最优的缺陷，而GA虽具全局搜索能力但样本效率低且计算成本高。因此，结合两者优势以互补不足成为核心动机。

Method: 提出进化增强机制（EAM）：1）利用DRL策略生成初始解；2）通过领域特异性遗传操作（如交叉、变异）优化解；3）将进化后的解选择性反馈至策略训练循环，增强探索并加速收敛。理论分析证明进化解分布与策略分布的KL散度上界，确保更新有效性。

Result: 在TSP、CVRP等基准问题上，EAM显著优于现有方法（如Attention Model、POMO），提升解质量和训练效率。模型无关性使其可无缝集成至主流DRL求解器。

Conclusion: EAM通过协同DRL与GA，解决了单一方法的局限性，为组合优化提供了高效、稳定的通用框架。理论保障和实验验证表明其广泛适用性和显著性能提升。

Abstract: Combinatorial optimization problems are notoriously challenging due to their
discrete structure and exponentially large solution space. Recent advances in
deep reinforcement learning (DRL) have enabled the learning heuristics directly
from data. However, DRL methods often suffer from limited exploration and
susceptibility to local optima. On the other hand, evolutionary algorithms such
as Genetic Algorithms (GAs) exhibit strong global exploration capabilities but
are typically sample inefficient and computationally intensive. In this work,
we propose the Evolutionary Augmentation Mechanism (EAM), a general and
plug-and-play framework that synergizes the learning efficiency of DRL with the
global search power of GAs. EAM operates by generating solutions from a learned
policy and refining them through domain-specific genetic operations such as
crossover and mutation. These evolved solutions are then selectively reinjected
into the policy training loop, thereby enhancing exploration and accelerating
convergence. We further provide a theoretical analysis that establishes an
upper bound on the KL divergence between the evolved solution distribution and
the policy distribution, ensuring stable and effective policy updates. EAM is
model-agnostic and can be seamlessly integrated with state-of-the-art DRL
solvers such as the Attention Model, POMO, and SymNCO. Extensive results on
benchmark problems (e.g., TSP, CVRP, PCTSP, and OP) demonstrate that EAM
significantly improves both solution quality and training efficiency over
competitive baselines.

</details>


### [124] [Mitigating Spurious Correlations in LLMs via Causality-Aware Post-Training](https://arxiv.org/abs/2506.09433)
*Shurui Gui,Shuiwang Ji*

Main category: cs.LG

TL;DR: 该研究提出了一种因果感知的后训练方法（CAPT），通过分解预测步骤减少大语言模型的预训练偏见，提升其在分布内外任务上的泛化能力，实验证明该方法在少量样本下效果显著。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）在分布外（OOD）样本上表现不佳，主要因其预训练过程中习得的伪相关性（spurious correlations）。研究旨在通过因果干预减少此类偏见，提升模型泛化性。

Method: 提出CAPT框架，将带偏见的预测分解为无偏的“事件估计”和“事件干预”两步，避免微调引入新偏见，仅需100个分布内（ID）样本进行微调。

Result: 在CLadder因果推理基准和PrOntoQA逻辑推理数据集上，3B规模的CAPT微调后模型在ID和OOD任务中均超越传统监督微调（SFT）及更大规模LLMs。

Conclusion: CAPT通过因果分解有效缓解预训练偏见，兼具高样本效率与强泛化能力，为小规模LLMs在复杂推理任务中的应用提供了新方向。

Abstract: While large language models (LLMs) have demonstrated remarkable capabilities
in language modeling, recent studies reveal that they often fail on
out-of-distribution (OOD) samples due to spurious correlations acquired during
pre-training. Here, we aim to mitigate such spurious correlations through
causality-aware post-training (CAPT). By decomposing a biased prediction into
two unbiased steps, known as \textit{event estimation} and \textit{event
intervention}, we reduce LLMs' pre-training biases without incurring additional
fine-tuning biases, thus enhancing the model's generalization ability.
Experiments on the formal causal inference benchmark CLadder and the logical
reasoning dataset PrOntoQA show that 3B-scale language models fine-tuned with
CAPT can outperform both traditional SFT and larger LLMs on in-distribution
(ID) and OOD tasks using only 100 ID fine-tuning samples, demonstrating the
effectiveness and sample efficiency of CAPT.

</details>


### [125] [Generalization Error Analysis for Attack-Free and Byzantine-Resilient Decentralized Learning with Data Heterogeneity](https://arxiv.org/abs/2506.09438)
*Haoxiang Ye,Tao Sun,Qing Ling*

Main category: cs.LG

TL;DR: 本文针对去中心化学习的泛化误差进行了细粒度分析，涵盖无攻击和拜占庭容错场景下的异构数据，揭示了数据异质性、模型初始化及梯度噪声对泛化的影响，并通过实验验证理论。


<details>
  <summary>Details</summary>
Motivation: 现有研究多集中于去中心化学习的优化误差，但其泛化误差（决定模型在真实场景中的扩展性）尚未充分探索，尤其在数据异构和拜占庭攻击场景下，需系统性分析其影响因素。

Method: 在温和假设下，结合理论分析与数值实验（包括凸与非凸任务），研究异构数据场景下无攻击及拜占庭容错去中心化学习的泛化误差，对比传统同质数据与严格梯度假设的局限性。

Result: 发现泛化误差受数据异质性、初始化及梯度噪声显著影响；拜占庭攻击的负面影响与数据异质性相关但与样本量无关，实验验证了理论结论的普适性。

Conclusion: 去中心化学习的泛化误差需综合考虑数据分布、攻击容错机制及优化动态，研究结果为实际部署中模型鲁棒性设计提供了理论依据。

Abstract: Decentralized learning, which facilitates joint model training across
geographically scattered agents, has gained significant attention in the field
of signal and information processing in recent years. While the optimization
errors of decentralized learning algorithms have been extensively studied,
their generalization errors remain relatively under-explored. As the
generalization errors reflect the scalability of trained models on unseen data
and are crucial in determining the performance of trained models in real-world
applications, understanding the generalization errors of decentralized learning
is of paramount importance. In this paper, we present fine-grained
generalization error analysis for both attack-free and Byzantine-resilient
decentralized learning with heterogeneous data as well as under mild
assumptions, in contrast to prior studies that consider homogeneous data and/or
rely on a stringent bounded stochastic gradient assumption. Our results shed
light on the impact of data heterogeneity, model initialization and stochastic
gradient noise -- factors that have not been closely investigated before -- on
the generalization error of decentralized learning. We also reveal that
Byzantine attacks performed by malicious agents largely affect the
generalization error, and their negative impact is inherently linked to the
data heterogeneity while remaining independent on the sample size. Numerical
experiments on both convex and non-convex tasks are conducted to validate our
theoretical findings.

</details>


### [126] [Safe Screening Rules for Group SLOPE](https://arxiv.org/abs/2506.09451)
*Runxue Bao,Quanchao Lu,Yanfu Zhang*

Main category: cs.LG

TL;DR: 本文针对Group SLOPE在高维稀疏学习中存在的块不可分组效应导致计算成本高、内存占用大的问题，提出了一种安全筛选规则，有效识别零系数的不活跃组，显著提升计算效率且不损失准确性。


<details>
  <summary>Details</summary>
Motivation: Group SLOPE在自适应选择预测变量组时，因块不可分组效应导致现有方法效率低下或无效，尤其在高维场景下计算成本和内存消耗显著增加，亟需改进。

Method: 提出一种针对Group SLOPE模型的安全筛选规则，通过处理块不可分组效应，高效识别零系数的不活跃组，并将其排除在训练过程外，且可无缝集成至现有批量/随机算法求解器中。

Result: 实验表明该方法能有效检测不活跃特征组，显著提升计算效率（理论证明其安全性），且与原始方法结果一致，未影响模型性能。

Conclusion: 所提出的安全筛选规则解决了Group SLOPE的块不可分性挑战，在保证结果一致性的前提下大幅优化计算资源消耗，为高维稀疏学习提供了实用工具。

Abstract: Variable selection is a challenging problem in high-dimensional sparse
learning, especially when group structures exist. Group SLOPE performs well for
the adaptive selection of groups of predictors. However, the block
non-separable group effects in Group SLOPE make existing methods either invalid
or inefficient. Consequently, Group SLOPE tends to incur significant
computational costs and memory usage in practical high-dimensional scenarios.
To overcome this issue, we introduce a safe screening rule tailored for the
Group SLOPE model, which efficiently identifies inactive groups with zero
coefficients by addressing the block non-separable group effects. By excluding
these inactive groups during training, we achieve considerable gains in
computational efficiency and memory usage. Importantly, the proposed screening
rule can be seamlessly integrated into existing solvers for both batch and
stochastic algorithms. Theoretically, we establish that our screening rule can
be safely employed with existing optimization algorithms, ensuring the same
results as the original approaches. Experimental results confirm that our
method effectively detects inactive feature groups and significantly boosts
computational efficiency without compromising accuracy.

</details>


### [127] [Learning Obfuscations Of LLM Embedding Sequences: Stained Glass Transform](https://arxiv.org/abs/2506.09452)
*Jay Roberts,Kyle Mylonakis,Sidhartha Roy,Kaan Kale*

Main category: cs.LG

TL;DR: 本文提出一种名为Stained Glass Transform的方法，通过对LLM词嵌入进行随机序列变换，在信息论层面保护输入隐私的同时保持模型性能，解决了共享计算环境中敏感数据使用的隐私问题。


<details>
  <summary>Details</summary>
Motivation: 企业因共享/多租户计算基础设施需明文处理数据，导致敏感数据所有者对使用LLM部署存在顾虑。现有部署模式无法在保证隐私的前提下实现模型效用。

Method: 采用基于高斯混合模型互信息理论的Stained Glass Transform方法，对LLM词嵌入进行学习型、随机且序列相关的变换，实现输入数据的信息论隐私保护。

Result: 通过互信息后验隐私估计和标准LLM基准测试验证，该方法在token级隐私指标和模型性能上均达到隐私-效用的平衡。

Conclusion: Stained Glass Transform为共享计算环境中的敏感数据使用提供了理论支撑与实践方案，通过可证明的隐私保护机制扩展了LLM在受限场景下的适用范围。

Abstract: The high cost of ownership of AI compute infrastructure and challenges of
robust serving of large language models (LLMs) has led to a surge in managed
Model-as-a-service deployments. Even when enterprises choose on-premises
deployments, the compute infrastructure is typically shared across many teams
in order to maximize the return on investment. In both scenarios the deployed
models operate only on plaintext data, and so enterprise data owners must allow
their data to appear in plaintext on a shared or multi-tenant compute
infrastructure. This results in data owners with private or sensitive data
being hesitant or restricted in what data they use with these types of
deployments. In this work we introduce the Stained Glass Transform, a learned,
stochastic, and sequence dependent transformation of the word embeddings of an
LLM which information theoretically provides privacy to the input of the LLM
while preserving the utility of model. We theoretically connect a particular
class of Stained Glass Transforms to the theory of mutual information of
Gaussian Mixture Models. We then calculate a-postiori privacy estimates, based
on mutual information, and verify the privacy and utility of instances of
transformed embeddings through token level metrics of privacy and standard LLM
performance benchmarks.

</details>


### [128] [NDCG-Consistent Softmax Approximation with Accelerated Convergence](https://arxiv.org/abs/2506.09454)
*Yuanhao Pu,Defu Lian,Xiaolong Chen,Xu Huang,Jin Chen,Enhong Chen*

Main category: cs.LG

TL;DR: 本文针对大规模相似性学习中Softmax损失的计算开销与扩展性问题，提出两种基于泰勒展开的新型损失函数（RG²和RG×），结合ALS优化方法，在保证排序质量的同时显著提升计算效率。


<details>
  <summary>Details</summary>
Motivation: Softmax损失虽广泛用于列表排序任务，但其在大规模对象空间中的计算开销和可扩展性限制成为主要瓶颈，亟需更高效的替代方案。

Method: 通过Softmax损失的泰勒展开推导出RG²（平方损失）和RG×（交互损失），揭示加权平方损失与采样机制的关联，并与ALS优化方法结合以提升计算效率。

Result: 实验表明新方法在真实数据集上达到或超越Softmax损失的排序性能，且收敛速度显著加快，验证了理论分析与计算效率的平衡。

Conclusion: 该框架为相似性学习提供理论解释与高效工具，适用于需权衡排序质量与计算效率的广泛任务，推动大规模排序方法的实际应用。

Abstract: Ranking tasks constitute fundamental components of extreme similarity
learning frameworks, where extremely large corpora of objects are modeled
through relative similarity relationships adhering to predefined ordinal
structures. Among various ranking surrogates, Softmax (SM) Loss has been widely
adopted due to its natural capability to handle listwise ranking via global
negative comparisons, along with its flexibility across diverse application
scenarios. However, despite its effectiveness, SM Loss often suffers from
significant computational overhead and scalability limitations when applied to
large-scale object spaces. To address this challenge, we propose novel loss
formulations that align directly with ranking metrics: the
Ranking-Generalizable \textbf{squared} (RG$^2$) Loss and the
Ranking-Generalizable interactive (RG$^\times$) Loss, both derived through
Taylor expansions of the SM Loss. Notably, RG$^2$ reveals the intrinsic
mechanisms underlying weighted squared losses (WSL) in ranking methods and
uncovers fundamental connections between sampling-based and non-sampling-based
loss paradigms. Furthermore, we integrate the proposed RG losses with the
highly efficient Alternating Least Squares (ALS) optimization method, providing
both generalization guarantees and convergence rate analyses. Empirical
evaluations on real-world datasets demonstrate that our approach achieves
comparable or superior ranking performance relative to SM Loss, while
significantly accelerating convergence. This framework offers the similarity
learning community both theoretical insights and practically efficient tools,
with methodologies applicable to a broad range of tasks where balancing ranking
quality and computational efficiency is essential.

</details>


### [129] [On a few pitfalls in KL divergence gradient estimation for RL](https://arxiv.org/abs/2506.09477)
*Yunhao Tang,Rémi Munos*

Main category: cs.LG

TL;DR: 本文指出在LLM强化学习训练中实现KL散度梯度估计的常见错误，包括错误微分KL估计值及忽略序列性，并通过实验展示正确实现方法。


<details>
  <summary>Details</summary>
Motivation: 针对开源项目及论文中KL散度梯度估计的常见实现错误，揭示其对强化学习训练效果的影响，并提出修正方案。

Method: 通过理论分析指出两类主要错误（直接微分KL估计值、忽略序列性梯度计算），设计表格实验和LLM实验进行验证，并展示正确梯度计算方法。

Result: 实验证明错误实现无法获得有效KL梯度，而修正后的方法能正确反映KL散度的梯度方向，显著改善训练效果。

Conclusion: 正确实现KL梯度需避免直接对估计值微分，并考虑序列决策特性，这对LLM强化学习训练稳定性至关重要。

Abstract: We point out a few pitfalls in implementing gradient estimation for KL
divergence in RL training for LLM, as seen in a number of open source projects
and papers. The first major pitfall is to differentiate through the KL estimate
as loss functions to minimize KL divergence. We show that such implementations
are generally incorrect and do not produce the desired KL gradient. Secondly,
we show that some implementations do not account for the sequential nature of
the estimation problem and produce a partial gradient at best. We demonstrate
the impact of such issues with illustrative tabular and LLM experiments, and
show the correct way to implement the KL gradient.

</details>


### [130] [EnerBridge-DPO: Energy-Guided Protein Inverse Folding with Markov Bridges and Direct Preference Optimization](https://arxiv.org/abs/2506.09496)
*Dingyi Rong,Haotian Lu,Wenzhuo Zheng,Fan Zhang,Shuangjia Zheng,Ning Liu*

Main category: cs.LG

TL;DR: 本文开发了EnerBridge-DPO框架，通过结合马尔可夫桥与直接偏好优化（DPO），并引入显式能量约束损失，直接生成低能量、高稳定性的蛋白质序列，解决了现有方法忽视序列能量的问题。


<details>
  <summary>Details</summary>
Motivation: 现有蛋白质逆折叠的深度学习方法主要关注序列恢复率，但忽视了生成序列的能量稳定性。本文旨在设计能直接生成低能量、高稳定性蛋白质序列的模型。

Method: 提出EnerBridge-DPO框架：1) 将马尔可夫桥与DPO结合，利用基于能量的偏好微调模型，从信息丰富的先验序列生成候选；2) 引入显式能量约束损失，增强模型从先验知识中学习能量表示并直接预测序列能量值。

Result: EnerBridge-DPO设计的蛋白质复合物序列能量更低，同时保持与前沿模型相当的序列恢复率，并能准确预测不同序列间的ΔΔG值。

Conclusion: 该框架通过整合先验优化与能量驱动约束，有效克服现有方法局限，实现了能量稳定且结构合理的蛋白质序列设计，为能量景观的定量分析提供了新工具。

Abstract: Designing protein sequences with optimal energetic stability is a key
challenge in protein inverse folding, as current deep learning methods are
primarily trained by maximizing sequence recovery rates, often neglecting the
energy of the generated sequences. This work aims to overcome this limitation
by developing a model that directly generates low-energy, stable protein
sequences. We propose EnerBridge-DPO, a novel inverse folding framework focused
on generating low-energy, high-stability protein sequences. Our core innovation
lies in: First, integrating Markov Bridges with Direct Preference Optimization
(DPO), where energy-based preferences are used to fine-tune the Markov Bridge
model. The Markov Bridge initiates optimization from an information-rich prior
sequence, providing DPO with a pool of structurally plausible sequence
candidates. Second, an explicit energy constraint loss is introduced, which
enhances the energy-driven nature of DPO based on prior sequences, enabling the
model to effectively learn energy representations from a wealth of prior
knowledge and directly predict sequence energy values, thereby capturing
quantitative features of the energy landscape. Our evaluations demonstrate that
EnerBridge-DPO can design protein complex sequences with lower energy while
maintaining sequence recovery rates comparable to state-of-the-art models, and
accurately predicts $\Delta \Delta G$ values between various sequences.

</details>


### [131] [A Unified Theory of Compositionality, Modularity, and Interpretability in Markov Decision Processes](https://arxiv.org/abs/2506.09499)
*Thomas J. Ringstrom,Paul R. Schrater*

Main category: cs.LG

TL;DR: 本文提出Option Kernel Bellman Equations (OKBEs)，通过状态-时间选项核(STOK)直接优化策略，实现组合性、模块化与可解释的长时程规划，避免传统奖励最大化与这些特性的冲突。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习的奖励最大化可能与组合性、模块化及可解释性存在冲突。本文旨在通过无奖励MDP框架，支持可验证的长时程规划与内在动机机制，适应高维动态环境。

Method: 构建状态-时间选项核(STOK)作为策略预测映射，利用Chapman-Kolmogorov方程组合局部STOK与目标策略，形成因子化目标核，实现高维空间中的目标级前向规划。

Result: 智能体能快速合成元策略、跨任务复用规划表示，并通过内在动机函数(如empowerment)验证目标，解决高维复杂规划问题。

Conclusion: OKBEs通过放弃奖励最大化，促进组合性/模块化/可解释性，支持可扩展的验证与内在动机驱动的高维动态规划。

Abstract: We introduce Option Kernel Bellman Equations (OKBEs) for a new reward-free
Markov Decision Process. Rather than a value function, OKBEs directly construct
and optimize a predictive map called a state-time option kernel (STOK) to
maximize the probability of completing a goal while avoiding constraint
violations. STOKs are compositional, modular, and interpretable
initiation-to-termination transition kernels for policies in the Options
Framework of Reinforcement Learning. This means: 1) STOKs can be composed using
Chapman-Kolmogorov equations to make spatiotemporal predictions for multiple
policies over long horizons, 2) high-dimensional STOKs can be represented and
computed efficiently in a factorized and reconfigurable form, and 3) STOKs
record the probabilities of semantically interpretable goal-success and
constraint-violation events, needed for formal verification. Given a
high-dimensional state-transition model for an intractable planning problem, we
can decompose it with local STOKs and goal-conditioned policies that are
aggregated into a factorized goal kernel, making it possible to forward-plan at
the level of goals in high-dimensions to solve the problem. These properties
lead to highly flexible agents that can rapidly synthesize meta-policies, reuse
planning representations across many tasks, and justify goals using
empowerment, an intrinsic motivation function. We argue that
reward-maximization is in conflict with the properties of compositionality,
modularity, and interpretability. Alternatively, OKBEs facilitate these
properties to support verifiable long-horizon planning and intrinsic motivation
that scales to dynamic high-dimensional world-models.

</details>


### [132] [Efficient Preference-Based Reinforcement Learning: Randomized Exploration Meets Experimental Design](https://arxiv.org/abs/2506.09508)
*Andreas Schlaginhaufen,Reda Ouhamma,Maryam Kamgarpour*

Main category: cs.LG

TL;DR: 本文提出了一种基于随机探索的元算法，通过轨迹级偏好比较实现高效强化学习，结合批量查询与最优实验设计降低查询复杂度，并在理论与实证层面验证其竞争力。


<details>
  <summary>Details</summary>
Motivation: 在基于人类反馈的强化学习中，如何设计既能高效选择信息量大的偏好查询、又具备理论可靠性的算法存在挑战。现有乐观方法常面临计算复杂度高的问题，需探索更高效的替代方案。

Method: 提出随机探索的元算法框架，避免乐观方法的计算负担；改进算法引入批量轨迹对收集与最优实验设计筛选高信息量查询，支持并行化偏好反馈收集。

Result: 算法在温和的强化学习假设下获得遗憾界与最终迭代保证，实证显示其仅需少量查询即可达到基于显式奖励的RL性能，批量处理显著提升查询效率。

Conclusion: 随机探索与批量最优设计结合，平衡了计算效率与查询成本，为实际部署中并行化人类反馈收集提供了理论支持与实用价值。

Abstract: We study reinforcement learning from human feedback in general Markov
decision processes, where agents learn from trajectory-level preference
comparisons. A central challenge in this setting is to design algorithms that
select informative preference queries to identify the underlying reward while
ensuring theoretical guarantees. We propose a meta-algorithm based on
randomized exploration, which avoids the computational challenges associated
with optimistic approaches and remains tractable. We establish both regret and
last-iterate guarantees under mild reinforcement learning oracle assumptions.
To improve query complexity, we introduce and analyze an improved algorithm
that collects batches of trajectory pairs and applies optimal experimental
design to select informative comparison queries. The batch structure also
enables parallelization of preference queries, which is relevant in practical
deployment as feedback can be gathered concurrently. Empirical evaluation
confirms that the proposed method is competitive with reward-based
reinforcement learning while requiring a small number of preference queries.

</details>


### [133] [Neural Functions for Learning Periodic Signal](https://arxiv.org/abs/2506.09526)
*Woojin Cho,Minju Jo,Kookjin Lee,Noseong Park*

Main category: cs.LG

TL;DR: 提出一种新型网络架构，通过提取信号中的周期性模式来增强基于坐标的多层感知机（MLP）的泛化能力和外推性能，实验证明其在处理周期性信号时的有效性。


<details>
  <summary>Details</summary>
Motivation: 基于坐标的MLP在学习连续信号表示时存在过拟合和泛化能力不足的问题，尤其在处理具有周期性特征的信号时，其外推表现较差。

Method: 设计了一种新网络架构，从测量数据中提取周期性模式，并利用这些信息进行信号表示，以提升模型对周期性信号的建模能力。

Result: 在微分方程周期解学习、真实数据集的时间序列插值与预测任务中，所提方法显著优于传统MLP，尤其在信号外推场景下表现突出。

Conclusion: 通过显式建模周期性特征，新方法有效解决了传统MLP在信号外推中的局限性，为周期性信号的高效学习提供了新思路。

Abstract: As function approximators, deep neural networks have served as an effective
tool to represent various signal types. Recent approaches utilize multi-layer
perceptrons (MLPs) to learn a nonlinear mapping from a coordinate to its
corresponding signal, facilitating the learning of continuous neural
representations from discrete data points. Despite notable successes in
learning diverse signal types, coordinate-based MLPs often face issues of
overfitting and limited generalizability beyond the training region, resulting
in subpar extrapolation performance. This study addresses scenarios where the
underlying true signals exhibit periodic properties, either spatially or
temporally. We propose a novel network architecture, which extracts periodic
patterns from measurements and leverages this information to represent the
signal, thereby enhancing generalization and improving extrapolation
performance. We demonstrate the efficacy of the proposed method through
comprehensive experiments, including the learning of the periodic solutions for
differential equations, and time series imputation (interpolation) and
forecasting (extrapolation) on real-world datasets.

</details>


### [134] [Athena: Enhancing Multimodal Reasoning with Data-efficient Process Reward Models](https://arxiv.org/abs/2506.09532)
*Shuai Wang,Zhenhua Liu,Jiaheng Wei,Xuanwu Yin,Dong Li,Emad Barsoum*

Main category: cs.LG

TL;DR: 本文提出Athena-PRM多模态过程奖励模型，通过弱/强推理结果一致性生成高质量过程标注数据，显著降低标注成本，并在多个基准测试中实现最优性能。


<details>
  <summary>Details</summary>
Motivation: 传统过程奖励模型依赖高成本人工标注步骤级数据，且自动标注方法存在噪声大、计算开销高的问题。需开发高效生成高质量过程标注数据的方法。

Method: 1. 利用弱/强推理模型预测一致性筛选可靠过程标签
2. 提出ORM初始化与负数据上采样策略
3. 在测试时扩展验证、推理步骤评估、奖励排序微调三场景验证方法

Result: 1. Qwen2.5-VL-7B模型在WeMath/MathVista分别提升10.2/7.1分
2. VisualProcessBench刷新SOTA（F1提升3.9）
3. Athena-7B在5个基准测试中显著超越基线

Conclusion: Athena-PRM通过高效数据标注策略与模型优化方法，在复杂推理任务中实现低成本、高精度的过程评估，为多模态推理系统提供有效奖励建模框架。

Abstract: We present Athena-PRM, a multimodal process reward model (PRM) designed to
evaluate the reward score for each step in solving complex reasoning problems.
Developing high-performance PRMs typically demands significant time and
financial investment, primarily due to the necessity for step-level annotations
of reasoning steps. Conventional automated labeling methods, such as Monte
Carlo estimation, often produce noisy labels and incur substantial
computational costs. To efficiently generate high-quality process-labeled data,
we propose leveraging prediction consistency between weak and strong completers
as a criterion for identifying reliable process labels. Remarkably, Athena-PRM
demonstrates outstanding effectiveness across various scenarios and benchmarks
with just 5,000 samples. Furthermore, we also develop two effective strategies
to improve the performance of PRMs: ORM initialization and up-sampling for
negative data. We validate our approach in three specific scenarios:
verification for test time scaling, direct evaluation of reasoning step
correctness, and reward ranked fine-tuning. Our Athena-PRM consistently
achieves superior performance across multiple benchmarks and scenarios.
Notably, when using Qwen2.5-VL-7B as the policy model, Athena-PRM enhances
performance by 10.2 points on WeMath and 7.1 points on MathVista for test time
scaling. Furthermore, Athena-PRM sets the state-of-the-art (SoTA) results in
VisualProcessBench and outperforms the previous SoTA by 3.9 F1-score,
showcasing its robust capability to accurately assess the correctness of the
reasoning step. Additionally, utilizing Athena-PRM as the reward model, we
develop Athena-7B with reward ranked fine-tuning and outperforms baseline with
a significant margin on five benchmarks.

</details>


### [135] [STOAT: Spatial-Temporal Probabilistic Causal Inference Network](https://arxiv.org/abs/2506.09544)
*Yang Yang,Du Yin,Hao Xue,Flora Salim*

Main category: cs.LG

TL;DR: 提出STOAT框架，通过结合空间关系矩阵和因果推断，提升时空因果时间序列的概率预测能力，并在COVID-19数据中验证其优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 现有方法独立建模时空动态且忽略因果驱动的概率预测，导致预测能力受限。需解决空间依赖性与不确定性建模的联合优化问题。

Method: STOAT引入空间关系矩阵编码区域间依赖（如邻近性），结合深度概率模型估计分布参数，支持多种输出分布（高斯、t分布等）以捕捉区域特异性。

Result: 在六国COVID-19数据实验中，STOAT在关键指标上超越DeepAR等先进模型，尤其在强空间依赖性区域表现显著。

Conclusion: STOAT通过融合因果推断与地理空间概率预测，为疫情管理等复杂时空任务提供可泛化框架，验证了空间信息增强因果效应估计的有效性。

Abstract: Spatial-temporal causal time series (STC-TS) involve region-specific temporal
observations driven by causally relevant covariates and interconnected across
geographic or network-based spaces. Existing methods often model spatial and
temporal dynamics independently and overlook causality-driven probabilistic
forecasting, limiting their predictive power. To address this, we propose STOAT
(Spatial-Temporal Probabilistic Causal Inference Network), a novel framework
for probabilistic forecasting in STC-TS. The proposed method extends a causal
inference approach by incorporating a spatial relation matrix that encodes
interregional dependencies (e.g. proximity or connectivity), enabling spatially
informed causal effect estimation. The resulting latent series are processed by
deep probabilistic models to estimate the parameters of the distributions,
enabling calibrated uncertainty modeling. We further explore multiple output
distributions (e.g., Gaussian, Student's-$t$, Laplace) to capture
region-specific variability. Experiments on COVID-19 data across six countries
demonstrate that STOAT outperforms state-of-the-art probabilistic forecasting
models (DeepAR, DeepVAR, Deep State Space Model, etc.) in key metrics,
particularly in regions with strong spatial dependencies. By bridging causal
inference and geospatial probabilistic forecasting, STOAT offers a
generalizable framework for complex spatial-temporal tasks, such as epidemic
management.

</details>


### [136] [MOORL: A Framework for Integrating Offline-Online Reinforcement Learning](https://arxiv.org/abs/2506.09574)
*Gaurav Chaudhary,Wassim Uddin Mondal,Laxmidhar Behera*

Main category: cs.LG

TL;DR: 本文提出MOORL框架，通过结合离线与在线强化学习的优势，利用元策略无缝切换两种模式，有效提升样本效率与探索能力。实验表明该方法在多个基准任务中优于现有方法，且计算开销低。


<details>
  <summary>Details</summary>
Motivation: 离线强化学习受限于分布外动作问题，传统混合方法依赖复杂设计且计算成本高。本文旨在通过统一框架结合离线数据的鲁棒初始化与在线交互的高效探索，解决现有方法的局限性。

Method: 提出MOORL框架，引入元策略动态适应离线和在线轨迹，无需额外复杂组件。通过理论证明混合数据可增强探索，并设计稳定Q函数避免计算复杂度增加。

Result: 在D4RL和V-D4RL的28个任务中，MOORL性能显著优于当前最优方法，且计算开销极小，验证了其高效性与泛化能力。

Conclusion: MOORL通过统一离线和在线强化学习机制，在保持低计算成本的同时实现高效探索与稳定策略优化，为实际复杂场景提供了实用解决方案。

Abstract: Sample efficiency and exploration remain critical challenges in Deep
Reinforcement Learning (DRL), particularly in complex domains. Offline RL,
which enables agents to learn optimal policies from static, pre-collected
datasets, has emerged as a promising alternative. However, offline RL is
constrained by issues such as out-of-distribution (OOD) actions that limit
policy performance and generalization. To overcome these limitations, we
propose Meta Offline-Online Reinforcement Learning (MOORL), a hybrid framework
that unifies offline and online RL for efficient and scalable learning. While
previous hybrid methods rely on extensive design components and added
computational complexity to utilize offline data effectively, MOORL introduces
a meta-policy that seamlessly adapts across offline and online trajectories.
This enables the agent to leverage offline data for robust initialization while
utilizing online interactions to drive efficient exploration. Our theoretical
analysis demonstrates that the hybrid approach enhances exploration by
effectively combining the complementary strengths of offline and online data.
Furthermore, we demonstrate that MOORL learns a stable Q-function without added
complexity. Extensive experiments on 28 tasks from the D4RL and V-D4RL
benchmarks validate its effectiveness, showing consistent improvements over
state-of-the-art offline and hybrid RL baselines. With minimal computational
overhead, MOORL achieves strong performance, underscoring its potential for
practical applications in real-world scenarios.

</details>


### [137] [Beyond Overconfidence: Foundation Models Redefine Calibration in Deep Neural Networks](https://arxiv.org/abs/2506.09593)
*Achim Hekler,Lukas Kuhn,Florian Buettner*

Main category: cs.LG

TL;DR: 本文系统研究了基础模型（如ConvNeXt、EVA、BEiT）的校准特性，发现其在校准行为上与传统认知存在矛盾：分布内预测欠自信导致高校准误差，而分布偏移下校准效果反而提升。后处理方法在分布内有效，但在严重偏移时可能失效甚至恶化结果。


<details>
  <summary>Details</summary>
Motivation: 基础模型在预测性能上的突破与其校准特性研究不足形成对比，而可靠的不确定性校准对高风险场景部署至关重要。现有研究未充分揭示其校准行为与分布偏移的关系，需挑战传统持续改进的范式假设。

Method: 通过实证分析基础模型的校准行为，对比其在分布内/外场景下的置信度表现，并评估多种后处理校准技术在不同分布偏移程度下的有效性变化。

Result: 基础模型在分布内呈现系统性欠自信（ECE↑），分布偏移时校准改善；后处理方法可有效缓解分布内欠自信，但在严重偏移时可靠性下降甚至产生负面效果；架构创新对校准产生非单调影响。

Conclusion: 基础模型的校准特性颠覆传统认知，其与分布偏移的复杂关系及后处理技术的局限性表明：模型创新需同步评估校准行为，单纯性能提升不能保证安全部署，需开发更鲁棒的校准方法。

Abstract: Reliable uncertainty calibration is essential for safely deploying deep
neural networks in high-stakes applications. Deep neural networks are known to
exhibit systematic overconfidence, especially under distribution shifts.
Although foundation models such as ConvNeXt, EVA and BEiT have demonstrated
significant improvements in predictive performance, their calibration
properties remain underexplored. This paper presents a comprehensive
investigation into the calibration behavior of foundation models, revealing
insights that challenge established paradigms. Our empirical analysis shows
that these models tend to be underconfident in in-distribution predictions,
resulting in higher calibration errors, while demonstrating improved
calibration under distribution shifts. Furthermore, we demonstrate that
foundation models are highly responsive to post-hoc calibration techniques in
the in-distribution setting, enabling practitioners to effectively mitigate
underconfidence bias. However, these methods become progressively less reliable
under severe distribution shifts and can occasionally produce counterproductive
results. Our findings highlight the complex, non-monotonic effects of
architectural and training innovations on calibration, challenging established
narratives of continuous improvement.

</details>


### [138] [Accelerating Large-Scale Regularized High-Order Tensor Recovery](https://arxiv.org/abs/2506.09594)
*Wenjin Qin,Hailin Wang,Jingyao Hou,Jianjun Wang*

Main category: cs.LG

TL;DR: 本文针对现有张量恢复方法忽略张量规模变化对结构的影响及计算成本高的问题，提出基于Krylov子空间迭代、块Lanczos双对角化和随机投影的快速低秩张量近似算法，构建非凸建模框架，并通过实验验证方法的实用性和优越性。


<details>
  <summary>Details</summary>
Motivation: 现有张量恢复方法未考虑张量规模变化对结构特征的影响，且处理大规模高阶张量时计算成本过高，亟需高效且能捕捉结构特征的解决方案。

Method: 结合Krylov子空间迭代、块Lanczos双对角化和随机投影策略，设计两种快速随机低秩张量近似算法；构建广义非凸建模框架，提出统一非凸模型及优化算法，并将随机LRTA方案集成至核心计算流程。

Result: 实验表明，所提方法在大规模张量数据中具有实用性、有效性和优越性，其计算效率与精度显著优于现有先进方法。

Conclusion: 通过融合随机化低秩近似与非凸建模框架，本文方法在降低计算成本的同时实现了对大规模张量结构的高效恢复，为量化与非量化场景提供了统一解决方案。

Abstract: Currently, existing tensor recovery methods fail to recognize the impact of
tensor scale variations on their structural characteristics. Furthermore,
existing studies face prohibitive computational costs when dealing with
large-scale high-order tensor data. To alleviate these issue, assisted by the
Krylov subspace iteration, block Lanczos bidiagonalization process, and random
projection strategies, this article first devises two fast and accurate
randomized algorithms for low-rank tensor approximation (LRTA) problem.
Theoretical bounds on the accuracy of the approximation error estimate are
established. Next, we develop a novel generalized nonconvex modeling framework
tailored to large-scale tensor recovery, in which a new regularization paradigm
is exploited to achieve insightful prior representation for large-scale
tensors. On the basis of the above, we further investigate new unified
nonconvex models and efficient optimization algorithms, respectively, for
several typical high-order tensor recovery tasks in unquantized and quantized
situations. To render the proposed algorithms practical and efficient for
large-scale tensor data, the proposed randomized LRTA schemes are integrated
into their central and time-intensive computations. Finally, we conduct
extensive experiments on various large-scale tensors, whose results demonstrate
the practicability, effectiveness and superiority of the proposed method in
comparison with some state-of-the-art approaches.

</details>


### [139] [SparseSSM: Efficient Selective Structured State Space Models Can Be Pruned in One-Shot](https://arxiv.org/abs/2506.09613)
*Kaiwen Tuo,Huan Wang*

Main category: cs.LG

TL;DR: 本文提出SparseSSM框架，首次将经典最优脑外科（OBS）方法扩展至状态空间模型，实现无需微调的剪枝，在Mamba模型中剪除50% SSM参数且零样本精度无损。


<details>
  <summary>Details</summary>
Motivation: 现有剪枝方法针对注意力模块设计，无法处理状态空间模型（SSM）中时间共享、离散化的状态转移矩阵，导致模型参数量大且部署困难。

Method: 通过（i）跨时间步聚合Hessian迹信息的二阶显著性评分（ii）FFN组件敏感性分析（iii）支持半结构化/结构化稀疏的层间剪枝算法，扩展OBS框架至SSM架构。

Result: 实验证明剪除50% SSM权重后零样本任务无精度损失，成为当前Mamba类大语言模型的最先进剪枝方法。

Conclusion: SparseSSM揭示了Mamba架构冗余分布特性，首次实现训练无关的高效剪枝，为状态空间模型压缩提供了新范式。

Abstract: State-space language models such as Mamba match Transformer quality while
permitting linear complexity inference, yet still comprise billions of
parameters that hinder deployment. Existing one-shot pruning methods are
tailored to attention blocks and fail to account for the time-shared and
discretized state-transition matrix at the heart of the selective state-space
module (SSM). In this paper, we introduce SparseSSM, the first training-free
pruning framework that extends the classic optimal brain surgeon (OBS)
framework to state space architectures. Our layer-wise algorithm (i) derives an
approximate second-order saliency score that aggregates Hessian-trace
information across time steps, (ii) incorporates a component sensitivity
analysis to guide feed-forward network (FFN) pruning, which also sheds light on
where redundancy resides in mamba architecture, (iii) can be easily extended to
semi-structured and structured sparsity. Empirically, we prune 50% of SSM
weights without fine-tuning and observe no zero-shot accuracy loss, achieving
the current state-of-the-art pruning algorithm for Mamba-based LLMs.

</details>


### [140] [GLGENN: A Novel Parameter-Light Equivariant Neural Networks Architecture Based on Clifford Geometric Algebras](https://arxiv.org/abs/2506.09625)
*Ekaterina Filimoshina,Dmitry Shirokov*

Main category: cs.LG

TL;DR: 本文提出了一种基于几何（Clifford）代数的广义Lipschitz群等变神经网络（GLGENN），其具有轻量化参数设计，并在多种等变任务中优于或匹配基线模型。


<details>
  <summary>Details</summary>
Motivation: 现有等变神经网络在处理伪正交变换（如旋转、反射）时，对非退化或退化对称双线性形式的向量空间适应性不足，且参数过多、易过拟合。

Method: 通过几何代数的基本结构和操作设计参数共享机制，构建对任意对称双线性形式向量空间具有等变性的轻量化网络架构。

Result: GLGENN在等变函数估计、凸包实验等任务中表现优于或与基线模型相当，且参数数量显著减少，过拟合倾向更低。

Conclusion: GLGENN通过几何代数驱动的参数共享，实现了高效、轻量的等变性建模，为复杂对称性场景提供了更优解决方案。

Abstract: We propose, implement, and compare with competitors a new architecture of
equivariant neural networks based on geometric (Clifford) algebras: Generalized
Lipschitz Group Equivariant Neural Networks (GLGENN). These networks are
equivariant to all pseudo-orthogonal transformations, including rotations and
reflections, of a vector space with any non-degenerate or degenerate symmetric
bilinear form. We propose a weight-sharing parametrization technique that takes
into account the fundamental structures and operations of geometric algebras.
Due to this technique, GLGENN architecture is parameter-light and has less
tendency to overfitting than baseline equivariant models. GLGENN outperforms or
matches competitors on several benchmarking equivariant tasks, including
estimation of an equivariant function and a convex hull experiment, while using
significantly fewer optimizable parameters.

</details>


### [141] [In-Context Bias Propagation in LLM-Based Tabular Data Generation](https://arxiv.org/abs/2506.09630)
*Pol G. Recasens,Alberto Gutierrez,Jordi Torres,Josep. Ll Berral,Anisa Halimi,Kieran Fraser*

Main category: cs.LG

TL;DR: 研究揭示基于上下文学习的大语言模型生成合成表格数据时，若示例存在统计偏差或恶意注入，会导致合成数据全局失真并影响下游分类器公平性。


<details>
  <summary>Details</summary>
Motivation: 现有研究假设LLM生成数据时使用无偏上下文示例，但现实数据常含噪声与人口统计偏差，需探究偏差传播对合成数据及下游任务的影响。

Method: 系统分析上下文示例统计偏差的传播效应，并设计对抗场景模拟恶意贡献者通过注入偏差破坏目标受保护子组的分类公平性。

Result: 即使轻微上下文偏差也会引发合成数据统计失真；对抗性偏差注入可显著降低下游分类器对特定受保护子组的公平性。

Conclusion: 依赖上下文提示的LLM数据生成流程在敏感领域存在新漏洞，需警惕偏差传播对数据质量和算法公平性的潜在威胁。

Abstract: Large Language Models (LLMs) are increasingly used for synthetic tabular data
generation through in-context learning (ICL), offering a practical solution for
data augmentation in data scarce scenarios. While prior work has shown the
potential of LLMs to improve downstream task performance through augmenting
underrepresented groups, these benefits often assume access to a subset of
unbiased in-context examples, representative of the real dataset. In real-world
settings, however, data is frequently noisy and demographically skewed. In this
paper, we systematically study how statistical biases within in-context
examples propagate to the distribution of synthetic tabular data, showing that
even mild in-context biases lead to global statistical distortions. We further
introduce an adversarial scenario where a malicious contributor can inject bias
into the synthetic dataset via a subset of in-context examples, ultimately
compromising the fairness of downstream classifiers for a targeted and
protected subgroup. Our findings demonstrate a new vulnerability associated
with LLM-based data generation pipelines that rely on in-context prompts with
in sensitive domains.

</details>


### [142] [FedVLMBench: Benchmarking Federated Fine-Tuning of Vision-Language Models](https://arxiv.org/abs/2506.09638)
*Weiying Zheng,Ziyue Lin,Pengxin Guo,Yuyin Zhou,Feifei Wang,Liangqiong Qu*

Main category: cs.LG

TL;DR: 本文提出首个联邦学习下视觉语言模型（VLM）微调系统化基准FedVLMBench，整合多种架构、策略与数据集，发现2层MLP连接器在联邦学习中效果最优，且当前联邦学习方法对视觉任务的数据异构性更敏感。


<details>
  <summary>Details</summary>
Motivation: 现有VLM微调方法依赖集中式训练，难以满足医疗等隐私敏感领域需求，且缺乏联邦学习场景下的全面评估基准。

Method: 构建FedVLMBench基准，涵盖两种主流VLM架构（编码器基/无编码器）、四种微调策略、五种联邦学习算法、六个跨领域数据集，覆盖单任务与多任务场景。

Result: 实验表明：1) 编码器基VLM在联邦学习中采用2层MLP连接器+联合调参效果最佳；2) 联邦学习方法在视觉中心任务中对数据异构性的敏感度显著高于文本中心任务。

Conclusion: FedVLMBench为隐私保护的多模态基础模型联邦训练提供标准化评估平台、工具集与实证指导，推动该领域研究发展。

Abstract: Vision-Language Models (VLMs) have demonstrated remarkable capabilities in
cross-modal understanding and generation by integrating visual and textual
information. While instruction tuning and parameter-efficient fine-tuning
methods have substantially improved the generalization of VLMs, most existing
approaches rely on centralized training, posing challenges for deployment in
domains with strict privacy requirements like healthcare. Recent efforts have
introduced Federated Learning (FL) into VLM fine-tuning to address these
privacy concerns, yet comprehensive benchmarks for evaluating federated
fine-tuning strategies, model architectures, and task generalization remain
lacking. In this work, we present \textbf{FedVLMBench}, the first systematic
benchmark for federated fine-tuning of VLMs. FedVLMBench integrates two
mainstream VLM architectures (encoder-based and encoder-free), four fine-tuning
strategies, five FL algorithms, six multimodal datasets spanning four
cross-domain single-task scenarios and two cross-domain multitask settings,
covering four distinct downstream task categories. Through extensive
experiments, we uncover key insights into the interplay between VLM
architectures, fine-tuning strategies, data heterogeneity, and multi-task
federated optimization. Notably, we find that a 2-layer multilayer perceptron
(MLP) connector with concurrent connector and LLM tuning emerges as the optimal
configuration for encoder-based VLMs in FL. Furthermore, current FL methods
exhibit significantly higher sensitivity to data heterogeneity in
vision-centric tasks than text-centric ones, across both encoder-free and
encoder-based VLM architectures. Our benchmark provides essential tools,
datasets, and empirical guidance for the research community, offering a
standardized platform to advance privacy-preserving, federated training of
multimodal foundation models.

</details>


### [143] [SyncFed: Time-Aware Federated Learning through Explicit Timestamping and Synchronization](https://arxiv.org/abs/2506.09660)
*Baran Can Gül,Stefanos Tziampazis,Nasser Jazdi,Michael Weyrich*

Main category: cs.LG

TL;DR: 本文提出SyncFed框架，通过时间同步和时间戳量化客户端更新的陈旧度，在联邦学习中实现基于时间感知的权重分配，提升模型准确性和信息新鲜度。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在分布式扩展时面临网络延迟、时钟不同步和客户端更新差异等问题，导致模型贡献不一致。现有方法缺乏对陈旧度的量化机制，尤其在延迟敏感和跨区域场景中。

Method: SyncFed采用显式同步和NTP协议时间戳建立全局时间基准，通过时间差数值化客户端更新的陈旧度，并在模型聚合时应用时间感知的权重分配策略。

Result: 实验表明，SyncFed在跨区域测试环境中使全局模型在稳定时间上下文中演进，相比无时间语义的基线方法，准确性和信息新鲜度显著提升。

Conclusion: 通过时间同步机制量化更新陈旧度，SyncFed有效解决了联邦学习中的时间不一致问题，为延迟敏感场景提供了更可靠的模型收敛方案。

Abstract: As Federated Learning (FL) expands to larger and more distributed
environments, consistency in training is challenged by network-induced delays,
clock unsynchronicity, and variability in client updates. This combination of
factors may contribute to misaligned contributions that undermine model
reliability and convergence. Existing methods like staleness-aware aggregation
and model versioning address lagging updates heuristically, yet lack mechanisms
to quantify staleness, especially in latency-sensitive and cross-regional
deployments. In light of these considerations, we introduce \emph{SyncFed}, a
time-aware FL framework that employs explicit synchronization and timestamping
to establish a common temporal reference across the system. Staleness is
quantified numerically based on exchanged timestamps under the Network Time
Protocol (NTP), enabling the server to reason about the relative freshness of
client updates and apply temporally informed weighting during aggregation. Our
empirical evaluation on a geographically distributed testbed shows that, under
\emph{SyncFed}, the global model evolves within a stable temporal context,
resulting in improved accuracy and information freshness compared to
round-based baselines devoid of temporal semantics.

</details>


### [144] [Wavelet Scattering Transform and Fourier Representation for Offline Detection of Malicious Clients in Federated Learning](https://arxiv.org/abs/2506.09674)
*Alessandro Licciardi,Davide Leo,Davide Carbone*

Main category: cs.LG

TL;DR: 提出WAFFLE算法，利用小波散射变换或傅里叶变换生成低维嵌入，在联邦学习前检测异常客户端，提升模型性能与检测精度。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中异常客户端（如传感器故障或非代表性数据分布）会显著降低模型性能，但现有方法难以在不访问原始数据的情况下有效检测此类客户端。

Method: 基于小波散射变换（WST）或傅里叶变换提取任务无关的低维嵌入，利用公共数据集训练的轻量检测器进行无监督客户端分离，实现训练前的恶意客户端标记。

Result: 实验表明WAFFLE在检测精度和下游分类性能上优于现有方法，且WST因不可逆性和局部变形稳定性更适合联邦场景。

Conclusion: WAFFLE为联邦学习提供了一种高效、隐私保护的预训练异常检测方案，尤其小波变换在理论特性上具有显著优势。

Abstract: Federated Learning (FL) enables the training of machine learning models
across decentralized clients while preserving data privacy. However, the
presence of anomalous or corrupted clients - such as those with faulty sensors
or non representative data distributions - can significantly degrade model
performance. Detecting such clients without accessing raw data remains a key
challenge. We propose WAFFLE (Wavelet and Fourier representations for Federated
Learning) a detection algorithm that labels malicious clients {\it before
training}, using locally computed compressed representations derived from
either the Wavelet Scattering Transform (WST) or the Fourier Transform. Both
approaches provide low-dimensional, task-agnostic embeddings suitable for
unsupervised client separation. A lightweight detector, trained on a
distillated public dataset, performs the labeling with minimal communication
and computational overhead. While both transforms enable effective detection,
WST offers theoretical advantages, such as non-invertibility and stability to
local deformations, that make it particularly well-suited to federated
scenarios. Experiments on benchmark datasets show that our method improves
detection accuracy and downstream classification performance compared to
existing FL anomaly detection algorithms, validating its effectiveness as a
pre-training alternative to online detection strategies.

</details>


### [145] [Wasserstein Hypergraph Neural Network](https://arxiv.org/abs/2506.09682)
*Iulia Duta,Pietro Liò*

Main category: cs.LG

TL;DR: 提出基于Wasserstein距离的超图神经网络，通过分布聚合提升节点分类性能，实验显示显著优势。


<details>
  <summary>Details</summary>
Motivation: 现有超图神经网络多采用简单池化聚合（如均值/求和），仅能捕获一阶统计量，无法保留分布的几何特性（如形状、扩散）。需开发能反映分布间最优传输关系的聚合方法。

Method: 将节点和超边邻域建模为分布，采用Sliced Wasserstein Pooling进行信息聚合，基于最优传输原理保留分布几何特性，使嵌入反映分布间的转换难易程度。

Result: 在多个真实数据集上实现节点分类任务的最优性能，验证Wasserstein池化在超图场景中的有效性。

Conclusion: 通过分布几何特性建模，Wasserstein超图神经网络显著增强了超图表示学习效果，为高阶关系建模提供了新范式。

Abstract: The ability to model relational information using machine learning has driven
advancements across various domains, from medicine to social science. While
graph representation learning has become mainstream over the past decade,
representing higher-order relationships through hypergraphs is rapidly gaining
momentum. In the last few years, numerous hypergraph neural networks have
emerged, most of them falling under a two-stage, set-based framework. The
messages are sent from nodes to edges and then from edges to nodes. However,
most of the advancement still takes inspiration from the graph counterpart,
often simplifying the aggregations to basic pooling operations. In this paper
we are introducing Wasserstein Hypergraph Neural Network, a model that treats
the nodes and hyperedge neighbourhood as distributions and aggregate the
information using Sliced Wasserstein Pooling. Unlike conventional aggregators
such as mean or sum, which only capture first-order statistics, our approach
has the ability to preserve geometric properties like the shape and spread of
distributions. This enables the learned embeddings to reflect how easily one
hyperedge distribution can be transformed into another, following principles of
optimal transport. Experimental results demonstrate that applying Wasserstein
pooling in a hypergraph setting significantly benefits node classification
tasks, achieving top performance on several real-world datasets.

</details>


### [146] [TRIDENT: Temporally Restricted Inference via DFA-Enhanced Neural Traversal](https://arxiv.org/abs/2506.09701)
*Vincenzo Collura,Karim Tit,Laura Bussi,Eleonora Giunchiglia,Maxime Cordy*

Main category: cs.LG

TL;DR: TRIDENT是一种无需重新训练的通用推理算法，通过将LTLf公式编译为DFA并引导受限束搜索，确保LLM输出满足线性时序逻辑，同时提升生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有LLMs等模型在生成任务中无法保证输出满足线性时序逻辑(LTLf)约束，需在推理阶段实现合规性且不依赖模型重训练。

Method: 将LTLf公式编译为确定性有限自动机(DFA)，结合动态掩码违规路径的受限束搜索，并基于模型概率与DFA接受结构的路径重排序机制。

Result: 在时序约束分类和可控文本生成任务中，TRIDENT实现100%约束满足率，且在效率与质量指标上优于现有方法。

Conclusion: TRIDENT为模型无关的合规生成提供理论保证，实证表明其同时提升输出合规性与质量，适用于多模态时序约束场景。

Abstract: Large Language Models (LLMs) and other neural architectures have achieved
impressive results across a variety of generative and classification tasks.
However, they remain fundamentally ill-equipped to ensure that their outputs
satisfy temporal constraints, such as those expressible in Linear Temporal
Logic over finite traces (LTLf). In this paper, we introduce TRIDENT: a general
and model-agnostic inference-time algorithm that guarantees compliance with
such constraints without requiring any retraining. TRIDENT compiles LTLf
formulas into a Deterministic Finite Automaton (DFA), which is used to guide a
constrained variant of beam search. At each decoding step, transitions that
would lead to constraint violations are masked, while remaining paths are
dynamically re-ranked based on both the model's probabilities and the DFA's
acceptance structure. We formally prove that the resulting sequences are
guaranteed to satisfy the given LTLf constraints, and we empirically
demonstrate that TRIDENT also improves output quality. We validate our approach
on two distinct tasks: temporally constrained image-stream classification and
controlled text generation. In both settings, TRIDENT achieves perfect
constraint satisfaction, while comparison with the state of the art shows
improved efficiency and high standard quality metrics.

</details>


### [147] [Auto-Compressing Networks](https://arxiv.org/abs/2506.09714)
*Vaggelis Dorovatas,Georgios Paraskevopoulos,Alexandros Potamianos*

Main category: cs.LG

TL;DR: 本文提出自动压缩网络（ACNs），通过长前馈连接替代传统短残差连接，实现训练中动态信息压缩，提升计算效率与表示质量，减少参数的同时保持准确率，并增强鲁棒性及迁移能力。


<details>
  <summary>Details</summary>
Motivation: 传统深度残差网络随深度增加易产生计算冗余，而表示质量未同步提升。ACNs旨在通过结构设计，在不依赖外部干预的情况下，实现网络自适应的信息压缩与动态计算资源分配。

Method: ACNs采用逐层到输出的长前馈连接替代短连接，通过梯度下降训练触发'自动压缩'机制，使深层信息向浅层迁移，动态优化各层利用率并识别冗余层。

Result: 实验显示ACNs在视觉Transformer、MLP-Mixer和BERT中实现30-80%结构压缩，灾难性遗忘减少18%，噪声鲁棒性、低数据表现及迁移学习优于残差网络，结合剪枝技术可进一步优化稀疏性-性能平衡。

Conclusion: ACNs通过纯结构设计实现自适应计算资源分配，为构建高效、鲁棒且可压缩的神经网络提供了新范式，其自动压缩特性对实际部署具有重要应用价值。

Abstract: Deep neural networks with short residual connections have demonstrated
remarkable success across domains, but increasing depth often introduces
computational redundancy without corresponding improvements in representation
quality. In this work, we introduce Auto-Compressing Networks (ACNs), an
architectural variant where additive long feedforward connections from each
layer to the output replace traditional short residual connections. ACNs
showcase a unique property we coin as "auto-compression", the ability of a
network to organically compress information during training with gradient
descent, through architectural design alone. Through auto-compression,
information is dynamically "pushed" into early layers during training,
enhancing their representational quality and revealing potential redundancy in
deeper ones. We theoretically show that this property emerges from layer-wise
training patterns present in ACNs, where layers are dynamically utilized during
training based on task requirements. We also find that ACNs exhibit enhanced
noise robustness compared to residual networks, superior performance in
low-data settings, improved transfer learning capabilities, and mitigate
catastrophic forgetting suggesting that they learn representations that
generalize better despite using fewer parameters. Our results demonstrate up to
18% reduction in catastrophic forgetting and 30-80% architectural compression
while maintaining accuracy across vision transformers, MLP-mixers, and BERT
architectures. Furthermore, we demonstrate that coupling ACNs with traditional
pruning techniques, enables significantly better sparsity-performance
trade-offs compared to conventional architectures. These findings establish
ACNs as a practical approach to developing efficient neural architectures that
automatically adapt their computational footprint to task complexity, while
learning robust representations.

</details>


### [148] [AtmosMJ: Revisiting Gating Mechanism for AI Weather Forecasting Beyond the Year Scale](https://arxiv.org/abs/2506.09733)
*Minjong Cheon*

Main category: cs.LG

TL;DR: 本文挑战了长期天气预测需依赖非标准空间域转换的假设，提出AtmosMJ模型直接在标准经纬网格上实现500天稳定预测，并通过门控残差融合机制降低误差累积，仅需5.7天V100 GPU训练即达到与主流模型相当的10天预测精度。


<details>
  <summary>Details</summary>
Motivation: 针对现有长期稳定天气预测模型依赖球谐函数/HEALPix网格等非标准空间域转换问题，探索是否能在标准经纬网格上通过架构创新实现可比拟的长期预测性能。

Method: 开发AtmosMJ深度卷积网络：1) 直接在ERA5原始经纬网格数据上训练；2) 提出门控残差融合(GRF)机制，通过特征更新自适应调节抑制误差累积；3) 保持端到端数据驱动范式。

Result: 1) 实现约500天物理合理的稳定预测；2) 10天预测精度与Pangu-Weather/GraphCast相当；3) 训练成本仅需5.7天V100 GPU，效率显著提升。

Conclusion: 证明通过高效架构设计（而非非标准数据表示）即可实现长期稳定预测，为开发计算高效的天气模型提供了新方向。

Abstract: The advent of Large Weather Models (LWMs) has marked a turning point in
data-driven forecasting, with many models now outperforming traditional
numerical systems in the medium range. However, achieving stable, long-range
autoregressive forecasts beyond a few weeks remains a significant challenge.
Prevailing state-of-the-art models that achieve year-long stability, such as
SFNO and DLWP-HPX, have relied on transforming input data onto non-standard
spatial domains like spherical harmonics or HEALPix meshes. This has led to the
prevailing assumption that such representations are necessary to enforce
physical consistency and long-term stability. This paper challenges that
assumption by investigating whether comparable long-range performance can be
achieved on the standard latitude-longitude grid. We introduce AtmosMJ, a deep
convolutional network that operates directly on ERA5 data without any spherical
remapping. The model's stability is enabled by a novel Gated Residual Fusion
(GRF) mechanism, which adaptively moderates feature updates to prevent error
accumulation over long recursive simulations. Our results demonstrate that
AtmosMJ produces stable and physically plausible forecasts for about 500 days.
In quantitative evaluations, it achieves competitive 10-day forecast accuracy
against models like Pangu-Weather and GraphCast, all while requiring a
remarkably low training budget of 5.7 days on a V100 GPU. Our findings suggest
that efficient architectural design, rather than non-standard data
representation, can be the key to unlocking stable and computationally
efficient long-range weather prediction.

</details>


### [149] [Towards Multi-modal Graph Large Language Model](https://arxiv.org/abs/2506.09738)
*Xin Wang,Zeyang Zhang,Linxin Xiao,Haibo Chen,Chendi Ge,Wenwu Zhu*

Main category: cs.LG

TL;DR: 本文提出多模态图大语言模型（MG-LLM）框架，旨在统一和泛化多模态图数据与任务，定义其五大关键特性，分析挑战与未来方向，并总结相关数据集。


<details>
  <summary>Details</summary>
Motivation: 现有方法针对特定多模态图数据和任务从头训练，缺乏跨数据和任务的泛化能力，需探索统一框架以提升通用性。

Method: 提出多模态图统一框架，包含多粒度与多尺度特征，并定义MG-LLM的五大特性：统一空间、多任务处理、上下文学习、自然语言交互及推理能力。

Result: 系统梳理多模态图数据与任务特性，明确MG-LLM实现路径，总结现有数据集，为后续研究提供理论支持与数据基础。

Conclusion: MG-LLM框架有望推动多模态图通用模型发展，通过统一建模与自然语言交互实现跨任务泛化，未来需进一步解决技术挑战并扩展应用场景。

Abstract: Multi-modal graphs, which integrate diverse multi-modal features and
relations, are ubiquitous in real-world applications. However, existing
multi-modal graph learning methods are typically trained from scratch for
specific graph data and tasks, failing to generalize across various multi-modal
graph data and tasks. To bridge this gap, we explore the potential of
Multi-modal Graph Large Language Models (MG-LLM) to unify and generalize across
diverse multi-modal graph data and tasks. We propose a unified framework of
multi-modal graph data, task, and model, discovering the inherent
multi-granularity and multi-scale characteristics in multi-modal graphs.
Specifically, we present five key desired characteristics for MG-LLM: 1)
unified space for multi-modal structures and attributes, 2) capability of
handling diverse multi-modal graph tasks, 3) multi-modal graph in-context
learning, 4) multi-modal graph interaction with natural language, and 5)
multi-modal graph reasoning. We then elaborate on the key challenges, review
related works, and highlight promising future research directions towards
realizing these ambitious characteristics. Finally, we summarize existing
multi-modal graph datasets pertinent for model training. We believe this paper
can contribute to the ongoing advancement of the research towards MG-LLM for
generalization across multi-modal graph data and tasks.

</details>


### [150] [Feature Engineering for Agents: An Adaptive Cognitive Architecture for Interpretable ML Monitoring](https://arxiv.org/abs/2506.09742)
*Gusseppe Bravo-Rocca,Peini Liu,Jordi Guitart,Rodrigo M Carrillo-Larco,Ajay Dholakia,David Ellison*

Main category: cs.LG

TL;DR: 提出一种结合特征工程与大语言模型（LLM）的认知架构，通过重构、分解、编译三步决策流程，提升机器学习模型监控输出的可解释性与准确性。


<details>
  <summary>Details</summary>
Motivation: 传统机器学习模型监控方法输出冗长且可解释性低，导致决策效率低下。需通过结构化方法优化LLM的推理过程，生成更清晰、可操作的监控结果。

Method: 设计基于特征工程的决策流程模块（Refactor-Break Down-Compile）：重构数据表示以聚焦关键特征，分解复杂信息进行细粒度分析，整合子结论生成可解释输出，减少对LLM生成通用性计划的依赖。

Result: 实验表明，该方法在多个领域使用不同LLM时，相比基线模型显著提高准确性，验证了架构的有效性和鲁棒性。

Conclusion: 通过特征工程驱动规划与选择性LLM推理的结合，构建了高可解释性决策支持系统，为机器学习模型监控提供了可扩展且可靠的解决方案。

Abstract: Monitoring Machine Learning (ML) models in production environments is
crucial, yet traditional approaches often yield verbose, low-interpretability
outputs that hinder effective decision-making. We propose a cognitive
architecture for ML monitoring that applies feature engineering principles to
agents based on Large Language Models (LLMs), significantly enhancing the
interpretability of monitoring outputs. Central to our approach is a Decision
Procedure module that simulates feature engineering through three key steps:
Refactor, Break Down, and Compile. The Refactor step improves data
representation to better capture feature semantics, allowing the LLM to focus
on salient aspects of the monitoring data while reducing noise and irrelevant
information. Break Down decomposes complex information for detailed analysis,
and Compile integrates sub-insights into clear, interpretable outputs. This
process leads to a more deterministic planning approach, reducing dependence on
LLM-generated planning, which can sometimes be inconsistent and overly general.
The combination of feature engineering-driven planning and selective LLM
utilization results in a robust decision support system, capable of providing
highly interpretable and actionable insights. Experiments using multiple LLMs
demonstrate the efficacy of our approach, achieving significantly higher
accuracy compared to various baselines across several domains.

</details>


### [151] [Load-Aware Training Scheduling for Model Circulation-based Decentralized Federated Learning](https://arxiv.org/abs/2506.09769)
*Haruki Kainuma,Takayuki Nishio*

Main category: cs.LG

TL;DR: 本文提出Load-aware Tram-FL，通过引入计算与通信负载感知的训练调度机制，优化去中心化联邦学习的总训练时间，并在非独立同分布数据下实现均衡数据利用。


<details>
  <summary>Details</summary>
Motivation: 现有去中心化联邦学习方法未充分协调节点计算与通信负载差异，导致训练时间过长。需通过全局优化调度机制解决此问题。

Method: 将全局优化问题分解为节点级子问题，引入方差约束平衡非独立同分布数据利用率，并通过目标函数联合最小化计算与通信延迟。

Result: 在MNIST和CIFAR-10数据集上的实验表明，相比基线方法显著减少训练时间（具体数值未提供）并加速模型收敛。

Conclusion: Load-aware Tram-FL通过负载感知调度机制有效优化联邦学习效率，为异构环境下的分布式训练提供了可扩展解决方案。

Abstract: This paper proposes Load-aware Tram-FL, an extension of Tram-FL that
introduces a training scheduling mechanism to minimize total training time in
decentralized federated learning by accounting for both computational and
communication loads. The scheduling problem is formulated as a global
optimization task, which-though intractable in its original form-is made
solvable by decomposing it into node-wise subproblems. To promote balanced data
utilization under non-IID distributions, a variance constraint is introduced,
while the overall training latency, including both computation and
communication costs, is minimized through the objective function. Simulation
results on MNIST and CIFAR-10 demonstrate that Load-aware Tram-FL significantly
reduces training time and accelerates convergence compared to baseline methods.

</details>


### [152] [On the Similarities of Embeddings in Contrastive Learning](https://arxiv.org/abs/2506.09781)
*Chungpa Lee,Sehee Lim,Kibok Lee,Jy-yong Sohn*

Main category: cs.LG

TL;DR: 本文提出一个统一框架分析对比学习，揭示全批次下正样本对齐限制及小批次下负样本相似度方差问题，并通过辅助损失提升小批量训练效果。


<details>
  <summary>Details</summary>
Motivation: 现有对比学习损失函数缺乏系统性框架解释其广泛目标，需从正负样本对余弦相似度角度建立统一分析体系。

Method: 基于余弦相似度构建统一框架，分析全批次下正样本对齐不可达性及小批次负样本分离强度，并提出减少负样本相似度方差的辅助损失项。

Result: 实验表明，引入辅助损失项能有效降低小批次训练中负样本相似度方差，并持续提升对比学习方法性能。

Conclusion: 统一框架揭示了对比学习的核心机制，辅助损失项解决了小批次训练局限性，为优化对比学习提供了理论支持和实践方案。

Abstract: Contrastive learning (CL) operates on a simple yet effective principle:
embeddings of positive pairs are pulled together, while those of negative pairs
are pushed apart. Although various forms of contrastive loss have been proposed
and analyzed from different perspectives, prior works lack a comprehensive
framework that systematically explains a broad class of these objectives. In
this paper, we present a unified framework for understanding CL, which is based
on analyzing the cosine similarity between embeddings of positive and negative
pairs. In full-batch settings, we show that perfect alignment of positive pairs
is unattainable when similarities of negative pairs fall below a certain
threshold, and that this misalignment can be alleviated by incorporating
within-view negative pairs. In mini-batch settings, we demonstrate that smaller
batch sizes incur stronger separation among negative pairs within batches,
which leads to higher variance in similarities of negative pairs. To address
this limitation of mini-batch CL, we introduce an auxiliary loss term that
reduces the variance of similarities of negative pairs in CL. Empirical results
demonstrate that incorporating the proposed loss consistently improves the
performance of CL methods in small-batch training.

</details>


### [153] [A theoretical framework for self-supervised contrastive learning for continuous dependent data](https://arxiv.org/abs/2506.09785)
*Alexander Marusov,Alexander Yuhay,Alexey Zaytsev*

Main category: cs.LG

TL;DR: 本文提出了一种针对连续依赖数据的对比自监督学习框架，通过引入依赖感知的损失函数，有效捕捉时空依赖关系，在多个下游任务中超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统对比自监督学习方法假设样本间语义独立，但该假设不适用于具有复杂时空相关性的依赖数据（如时序/时空数据），导致现有方法在此类场景中表现受限。

Method: 提出基于硬/软相似性度量的理论框架，推导出适应样本间依赖关系的相似性矩阵解析形式，并设计依赖感知的损失函数Dependent TS2Vec。

Result: 在UEA/UCR基准测试中准确率分别提升4.17%和2.08%，在干旱分类任务中ROC-AUC提高7%，均优于TS2Vec等现有方法。

Conclusion: 理论驱动的依赖感知损失函数能有效建模时空依赖关系，为连续依赖数据的自监督学习提供了新的解决方案，显著提升下游任务性能。

Abstract: Self-supervised learning (SSL) has emerged as a powerful approach to learning
representations, particularly in the field of computer vision. However, its
application to dependent data, such as temporal and spatio-temporal domains,
remains underexplored. Besides, traditional contrastive SSL methods often
assume \emph{semantic independence between samples}, which does not hold for
dependent data exhibiting complex correlations. We propose a novel theoretical
framework for contrastive SSL tailored to \emph{continuous dependent data},
which allows the nearest samples to be semantically close to each other. In
particular, we propose two possible \textit{ground truth similarity measures}
between objects -- \emph{hard} and \emph{soft} closeness. Under it, we derive
an analytical form for the \textit{estimated similarity matrix} that
accommodates both types of closeness between samples, thereby introducing
dependency-aware loss functions. We validate our approach, \emph{Dependent
TS2Vec}, on temporal and spatio-temporal downstream problems. Given the
dependency patterns presented in the data, our approach surpasses modern ones
for dependent data, highlighting the effectiveness of our theoretically
grounded loss functions for SSL in capturing spatio-temporal dependencies.
Specifically, we outperform TS2Vec on the standard UEA and UCR benchmarks, with
accuracy improvements of $4.17$\% and $2.08$\%, respectively. Furthermore, on
the drought classification task, which involves complex spatio-temporal
patterns, our method achieves a $7$\% higher ROC-AUC score.

</details>


### [154] [Devil's Hand: Data Poisoning Attacks to Locally Private Graph Learning Protocols](https://arxiv.org/abs/2506.09803)
*Longzhu He,Chaozhuo Li,Peng Tang,Litian Zhang,Sen Su*

Main category: cs.LG

TL;DR: 本文提出首个针对本地隐私图学习协议的数据投毒攻击方法，通过注入虚假用户并伪造数据，破坏隐私保护的图学习效用。理论与实验验证攻击有效性，现有防御措施效果有限。


<details>
  <summary>Details</summary>
Motivation: 现有本地隐私图学习协议虽能保护用户数据隐私，但未考虑数据投毒攻击的潜在威胁，可能影响隐私保护框架的鲁棒性和安全性。

Method: 攻击者通过注入虚假用户，使其与真实用户建立连接，并向服务器发送精心构造的扰动数据，从而降低隐私图学习（如节点分类）的准确性。

Result: 攻击在理论上被证明有效，实验结果显示其显著降低模型性能；现有防御策略（如异常检测）对攻击缓解作用有限。

Conclusion: 本地隐私图学习协议存在数据投毒攻击漏洞，需设计更鲁棒的防御机制以平衡隐私保护与模型安全性。

Abstract: Graph neural networks (GNNs) have achieved significant success in graph
representation learning and have been applied to various domains. However, many
real-world graphs contain sensitive personal information, such as user profiles
in social networks, raising serious privacy concerns when graph learning is
performed using GNNs. To address this issue, locally private graph learning
protocols have gained considerable attention. These protocols leverage the
privacy advantages of local differential privacy (LDP) and the effectiveness of
GNN's message-passing in calibrating noisy data, offering strict privacy
guarantees for users' local data while maintaining high utility (e.g., node
classification accuracy) for graph learning. Despite these advantages, such
protocols may be vulnerable to data poisoning attacks, a threat that has not
been considered in previous research. Identifying and addressing these threats
is crucial for ensuring the robustness and security of privacy-preserving graph
learning frameworks. This work introduces the first data poisoning attack
targeting locally private graph learning protocols. The attacker injects fake
users into the protocol, manipulates these fake users to establish links with
genuine users, and sends carefully crafted data to the server, ultimately
compromising the utility of private graph learning. The effectiveness of the
attack is demonstrated both theoretically and empirically. In addition, several
defense strategies have also been explored, but their limited effectiveness
highlights the need for more robust defenses.

</details>


### [155] [Generalizing Supervised Contrastive learning: A Projection Perspective](https://arxiv.org/abs/2506.09810)
*Minoh Jeong,Alfred Hero*

Main category: cs.LG

TL;DR: 本文提出ProjNCE，一种泛化的对比损失函数，通过引入投影函数和负对调整项，统一了监督与自监督对比学习目标，并在理论和实验上验证其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 监督对比学习（SupCon）在互信息理论基础的探索不足，现有自监督对比方法（如InfoNCE）的互信息下界理论未延伸至监督场景。本文旨在填补这一空白，并提升SupCon的灵活性与性能。

Method: 提出ProjNCE损失函数，扩展InfoNCE以支持投影函数和负对调整项，允许灵活选择类嵌入投影策略（如基于类原型的投影），并证明其构成有效的互信息下界。

Result: 在多数据集和设置下，ProjNCE在分类任务中持续优于传统监督对比损失和交叉熵训练，验证了投影策略设计的有效性。

Conclusion: ProjNCE从互信息理论解释和投影设计两个角度改进了监督对比学习，为基于SupCon的任务提供了通用且性能更优的对比目标框架。

Abstract: Self-supervised contrastive learning (SSCL) has emerged as a powerful
paradigm for representation learning and has been studied from multiple
perspectives, including mutual information and geometric viewpoints. However,
supervised contrastive (SupCon) approaches have received comparatively little
attention in this context: for instance, while InfoNCE used in SSCL is known to
form a lower bound on mutual information (MI), the relationship between SupCon
and MI remains unexplored. To address this gap, we introduce ProjNCE, a
generalization of the InfoNCE loss that unifies supervised and self-supervised
contrastive objectives by incorporating projection functions and an adjustment
term for negative pairs. We prove that ProjNCE constitutes a valid MI bound and
affords greater flexibility in selecting projection strategies for class
embeddings. Building on this flexibility, we further explore the centroid-based
class embeddings in SupCon by exploring a variety of projection methods.
Extensive experiments on multiple datasets and settings demonstrate that
ProjNCE consistently outperforms both SupCon and standard cross-entropy
training. Our work thus refines SupCon along two complementary
perspective--mutual information interpretation and projection design--and
offers broadly applicable improvements whenever SupCon serves as the
foundational contrastive objective.

</details>


### [156] [Metritocracy: Representative Metrics for Lite Benchmarks](https://arxiv.org/abs/2506.09813)
*Ariel Procaccia,Benjamin Schiffer,Serena Wang,Shirley Zhang*

Main category: cs.LG

TL;DR: 该研究利用社会选择理论，形式化定义了评估指标子集选择的两种代表性概念，并理论分析了所需最小指标数，通过实际案例验证了方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM评估中指标子集选择常以‘代表性’为目标，但该概念缺乏明确定义，导致选择过程缺乏理论依据。

Method: 提出位置代表性（positional representation）和位置比例性（positional proportionality）两种形式化定义，结合社会选择理论分析最坏情况下所需最小指标数，并扩展支持分组约束的通用形式。

Result: 理论证明了两种属性所需最小指标数的上下界，在LLM评估和医院质量评估案例中验证了方法的实际适用性。

Conclusion: 通过形式化定义和理论边界分析，为指标子集选择提供了可量化的数学框架，并在多领域验证了其应用潜力。

Abstract: A common problem in LLM evaluation is how to choose a subset of metrics from
a full suite of possible metrics. Subset selection is usually done for
efficiency or interpretability reasons, and the goal is often to select a
``representative'' subset of metrics. However, ``representative'' is rarely
clearly defined. In this work, we use ideas from social choice theory to
formalize two notions of representation for the selection of a subset of
evaluation metrics. We first introduce positional representation, which
guarantees every alternative is sufficiently represented at every position
cutoff. We then introduce positional proportionality, which guarantees no
alternative is proportionally over- or under-represented by more than a small
error at any position. We prove upper and lower bounds on the smallest number
of metrics needed to guarantee either of these properties in the worst case. We
also study a generalized form of each property that allows for additional input
on groups of metrics that must be represented. Finally, we tie theory to
practice through real-world case studies on both LLM evaluation and hospital
quality evaluation.

</details>


### [157] [Identifiability Challenges in Sparse Linear Ordinary Differential Equations](https://arxiv.org/abs/2506.09816)
*Cecilia Casolo,Sören Becker,Niki Kilbertus*

Main category: cs.LG

TL;DR: 本文研究稀疏线性常微分方程（ODE）的可辨识性，发现与密集系统不同，稀疏系统在实际相关场景中存在不可辨识的正概率，现有方法无法解决这一限制，需重新评估数据驱动建模的可靠性。


<details>
  <summary>Details</summary>
Motivation: 稀疏性在生物、社会及物理系统中普遍存在，但稀疏线性ODE的可辨识性尚未被充分研究，而模型不可辨识将导致无法保证其在新条件或控制机制下的行为。

Method: 通过理论分析稀疏线性ODE的可辨识性概率，提供不可辨识性的下界，并实证检验现有方法（如状态估计技术）在稀疏系统中的表现。

Result: 稀疏系统在实用稀疏度下存在不可忽略的不可辨识概率，实证表明现有方法无法克服理论限制，优化动态和归纳偏置均无法解决此问题。

Conclusion: 需重新审视数据驱动动态系统建模的预期，并提出量化评估学习结果可信度的方法，以应对稀疏系统固有的不可辨识性挑战。

Abstract: Dynamical systems modeling is a core pillar of scientific inquiry across
natural and life sciences. Increasingly, dynamical system models are learned
from data, rendering identifiability a paramount concept. For systems that are
not identifiable from data, no guarantees can be given about their behavior
under new conditions and inputs, or about possible control mechanisms to steer
the system. It is known in the community that "linear ordinary differential
equations (ODE) are almost surely identifiable from a single trajectory."
However, this only holds for dense matrices. The sparse regime remains
underexplored, despite its practical relevance with sparsity arising naturally
in many biological, social, and physical systems. In this work, we address this
gap by characterizing the identifiability of sparse linear ODEs. Contrary to
the dense case, we show that sparse systems are unidentifiable with a positive
probability in practically relevant sparsity regimes and provide lower bounds
for this probability. We further study empirically how this theoretical
unidentifiability manifests in state-of-the-art methods to estimate linear ODEs
from data. Our results corroborate that sparse systems are also practically
unidentifiable. Theoretical limitations are not resolved through inductive
biases or optimization dynamics. Our findings call for rethinking what can be
expected from data-driven dynamical system modeling and allows for quantitative
assessments of how much to trust a learned linear ODE.

</details>


### [158] [Weighted Loss Methods for Robust Federated Learning under Data Heterogeneity](https://arxiv.org/abs/2506.09824)
*Johan Erbani,Sonia Ben Mokhtar,Pierre-Edouard Portier,Elod Egyed-Zsigmond,Diana Nurbakova*

Main category: cs.LG

TL;DR: 本文提出了一种名为WoLA的加权损失方法，用于在数据异构的联邦学习环境中对齐诚实参与者的梯度，有效识别并抵御拜占庭攻击，理论和实验均验证其优越性。


<details>
  <summary>Details</summary>
Motivation: 现有拜占庭鲁棒联邦学习方法在数据异构场景中难以区分诚实梯度与恶意梯度，因诚实参与者的梯度差异可能大于恶意梯度差异。

Method: 引入Worker Label Alignment Loss (WoLA)，通过加权损失函数减少诚实参与者梯度差异，从而更易识别恶意梯度。

Result: WoLA在异构数据场景下显著优于现有方法，理论分析和实验数据均证明其有效性。

Conclusion: WoLA通过梯度对齐机制解决了异构联邦学习的拜占庭攻击问题，为安全联邦学习提供了新方向。

Abstract: Federated learning (FL) is a machine learning paradigm that enables multiple
data holders to collaboratively train a machine learning model without sharing
their training data with external parties. In this paradigm, workers locally
update a model and share with a central server their updated gradients (or
model parameters). While FL seems appealing from a privacy perspective, it
opens a number of threats from a security perspective as (Byzantine)
participants can contribute poisonous gradients (or model parameters) harming
model convergence. Byzantine-resilient FL addresses this issue by ensuring that
the training proceeds as if Byzantine participants were absent. Towards this
purpose, common strategies ignore outlier gradients during model aggregation,
assuming that Byzantine gradients deviate more from honest gradients than
honest gradients do from each other. However, in heterogeneous settings, honest
gradients may differ significantly, making it difficult to distinguish honest
outliers from Byzantine ones. In this paper, we introduce the Worker Label
Alignement Loss (WoLA), a weighted loss that aligns honest worker gradients
despite data heterogeneity, which facilitates the identification of Byzantines'
gradients. This approach significantly outperforms state-of-the-art methods in
heterogeneous settings. In this paper, we provide both theoretical insights and
empirical evidence of its effectiveness.

</details>


### [159] [Guided Graph Compression for Quantum Graph Neural Networks](https://arxiv.org/abs/2506.09862)
*Mikel Casals,Vasilis Belis,Elias F. Combarro,Eduard Alarcón,Sofia Vallecorsa,Michele Grossi*

Main category: cs.LG

TL;DR: 本文提出引导图压缩（GGC）框架，通过图自编码器压缩图数据，提升下游分类任务性能，并支持量子或经典分类器。实验表明GGC在Jet Tagging任务中优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 图神经网络（GNNs）处理大图时面临内存和计算效率问题，而量子计算硬件对数据维度有限制。现有方法需简化数据集或使用人工数据，无法有效支持量子图神经网络（QGNNs）在真实场景的应用。

Method: 提出引导图压缩（GGC）框架，利用图自编码器同时压缩节点数量和特征维度，并通过下游分类任务优化压缩过程，兼容量子或经典分类器。

Result: 在粒子物理的Jet Tagging任务中，GGC性能优于单独使用自编码器预处理和经典GNN基线，且支持在真实数据集上测试新型QGNN架构。

Conclusion: GGC通过任务导向的图压缩，解决了量子硬件数据编码限制，为量子图神经网络在现实场景的应用提供了可行方案。

Abstract: Graph Neural Networks (GNNs) are effective for processing graph-structured
data but face challenges with large graphs due to high memory requirements and
inefficient sparse matrix operations on GPUs. Quantum Computing (QC) offers a
promising avenue to address these issues and inspires new algorithmic
approaches. In particular, Quantum Graph Neural Networks (QGNNs) have been
explored in recent literature. However, current quantum hardware limits the
dimension of the data that can be effectively encoded. Existing approaches
either simplify datasets manually or use artificial graph datasets. This work
introduces the Guided Graph Compression (GGC) framework, which uses a graph
autoencoder to reduce both the number of nodes and the dimensionality of node
features. The compression is guided to enhance the performance of a downstream
classification task, which can be applied either with a quantum or a classical
classifier. The framework is evaluated on the Jet Tagging task, a
classification problem of fundamental importance in high energy physics that
involves distinguishing particle jets initiated by quarks from those by gluons.
The GGC is compared against using the autoencoder as a standalone preprocessing
step and against a baseline classical GNN classifier. Our numerical results
demonstrate that GGC outperforms both alternatives, while also facilitating the
testing of novel QGNN ansatzes on realistic datasets.

</details>


### [160] [Machine Learning-Based Classification of Oils Using Dielectric Properties and Microwave Resonant Sensing](https://arxiv.org/abs/2506.09867)
*Amit Baran Dey,Wasim Arif,Rakhesh Singh Kshetrimayum*

Main category: cs.LG

TL;DR: 本文提出了一种基于微波谐振传感器和机器学习的油样分类方法，通过捕捉油样介电特性引起的传感器响应变化，结合随机森林分类器实现了99.41%的高准确率，适用于工业实时检测。


<details>
  <summary>Details</summary>
Motivation: 传统油品检测方法在实时性和非破坏性方面存在局限，需开发高效、低功耗的自动化识别技术以满足工业应用场景需求。

Method: 利用微波谐振传感器非破坏性获取油样介电特性（谐振频移与振幅响应），提取特征后训练多种机器学习模型进行分类。

Result: 随机森林分类器表现最佳，分类准确率99.41%，验证了传感器特征与机器学习结合的有效性。

Conclusion: 该系统具有小型化、低功耗和高精度特点，为工业环境中油品快速鉴定提供了可靠解决方案。

Abstract: This paper proposes a machine learning-based methodology for the
classification of various oil samples based on their dielectric properties,
utilizing a microwave resonant sensor. The dielectric behaviour of oils,
governed by their molecular composition, induces distinct shifts in the
sensor's resonant frequency and amplitude response. These variations are
systematically captured and processed to extract salient features, which serve
as inputs for multiple machine learning classifiers. The microwave resonant
sensor operates in a non-destructive, low-power manner, making it particularly
well-suited for real-time industrial applications. A comprehensive dataset is
developed by varying the permittivity of oil samples and acquiring the
corresponding sensor responses. Several classifiers are trained and evaluated
using the extracted resonant features to assess their capability in
distinguishing between oil types. Experimental results demonstrate that the
proposed approach achieves a high classification accuracy of 99.41% with the
random forest classifier, highlighting its strong potential for automated oil
identification. The system's compact form factor, efficiency, and high
performance underscore its viability for fast and reliable oil characterization
in industrial environments.

</details>


### [161] [Private Aggregation for Byzantine-Resilient Heterogeneous Federated Learning](https://arxiv.org/abs/2506.09870)
*Maximilian Egger,Rawad Bitar*

Main category: cs.LG

TL;DR: 本文提出一种结合可验证秘密共享、安全聚合与定制对称私有信息检索的多阶段方法，在数据异构场景下实现信息论隐私保护与拜占庭容错，并通过零阶估计降低通信开销。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习方法在数据同质时可通过安全聚合实现拜占庭容错与隐私保护，但在数据异构场景下失效。预处理技术虽能提升性能，却与隐私保护机制不兼容，亟需新解决方案。

Method: 提出多阶段框架：1) 可验证秘密共享确保梯度完整性；2) 安全聚合实现隐私保护；3) 定制对称私有信息检索优化数据访问；4) 结合零阶估计降低通信成本。

Result: 实验表明该方法在多种攻击场景下优于现有技术，通信效率提升显著。零阶估计使安全聚合在主流联邦学习任务中具备可扩展性。

Conclusion: 通过协同设计隐私保护与拜占庭容错机制，解决了数据异构场景下的联邦学习安全难题，通信优化方案使其具备实际部署可行性。

Abstract: Ensuring resilience to Byzantine clients while maintaining the privacy of the
clients' data is a fundamental challenge in federated learning (FL). When the
clients' data is homogeneous, suitable countermeasures were studied from an
information-theoretic perspective utilizing secure aggregation techniques while
ensuring robust aggregation of the clients' gradients. However, the
countermeasures used fail when the clients' data is heterogeneous. Suitable
pre-processing techniques, such as nearest neighbor mixing, were recently shown
to enhance the performance of those countermeasures in the heterogeneous
setting. Nevertheless, those pre-processing techniques cannot be applied with
the introduced privacy-preserving mechanisms.
  We propose a multi-stage method encompassing a careful co-design of
verifiable secret sharing, secure aggregation, and a tailored symmetric private
information retrieval scheme to achieve information-theoretic privacy
guarantees and Byzantine resilience under data heterogeneity. We evaluate the
effectiveness of our scheme on a variety of attacks and show how it outperforms
the previously known techniques. Since the communication overhead of secure
aggregation is non-negligible, we investigate the interplay with zero-order
estimation methods that reduce the communication cost in state-of-the-art FL
tasks and thereby make private aggregation scalable.

</details>


### [162] [Learning single-index models via harmonic decomposition](https://arxiv.org/abs/2506.09887)
*Nirmit Joshi,Hugo Koubbi,Theodor Misiakiewicz,Nathan Srebro*

Main category: cs.LG

TL;DR: 本文提出使用球谐函数替代传统Hermite多项式作为单指标模型学习的基础，以更好捕捉问题的旋转对称性，并设计两种估计器分别优化样本复杂度与计算效率，揭示了二者难以兼得的特性。


<details>
  <summary>Details</summary>
Motivation: 现有基于Hermite展开的方法未充分利用单指标模型在任意球对称输入分布下的内在旋转对称性，球谐函数因其几何特性成为更自然的选择。

Method: 提出基于张量展开(tensor unfolding)和在线随机梯度下降(online SGD)的两类估计器，分别针对样本效率与计算效率进行优化设计。

Result: 在任意球对称输入下，张量展开法达到最优样本复杂度，SGD法实现最优运行时间，但二者存在权衡；应用于高斯输入时不仅统一现有结论，还发现被忽视的新现象。

Conclusion: 球谐函数框架为单指标模型提供了更本质的分析工具，同时揭示了统计效率与计算效率的固有矛盾，为后续理论边界探索奠定基础。

Abstract: We study the problem of learning single-index models, where the label $y \in
\mathbb{R}$ depends on the input $\boldsymbol{x} \in \mathbb{R}^d$ only through
an unknown one-dimensional projection $\langle
\boldsymbol{w}_*,\boldsymbol{x}\rangle$. Prior work has shown that under
Gaussian inputs, the statistical and computational complexity of recovering
$\boldsymbol{w}_*$ is governed by the Hermite expansion of the link function.
In this paper, we propose a new perspective: we argue that "spherical
harmonics" -- rather than "Hermite polynomials" -- provide the natural basis
for this problem, as they capture its intrinsic "rotational symmetry". Building
on this insight, we characterize the complexity of learning single-index models
under arbitrary spherically symmetric input distributions. We introduce two
families of estimators -- based on tensor unfolding and online SGD -- that
respectively achieve either optimal sample complexity or optimal runtime, and
argue that estimators achieving both may not exist in general. When specialized
to Gaussian inputs, our theory not only recovers and clarifies existing results
but also reveals new phenomena that had previously been overlooked.

</details>


### [163] [Flipping Against All Odds: Reducing LLM Coin Flip Bias via Verbalized Rejection Sampling](https://arxiv.org/abs/2506.09998)
*Tim Z. Xiao,Johannes Zenn,Zhen Liu,Weiyang Liu,Robert Bamler,Bernhard Schölkopf*

Main category: cs.LG

TL;DR: 大语言模型（LLMs）在描述概率分布时表现准确，但生成可靠随机样本的能力不足。本文提出基于自然语言的拒绝采样方法（VRS），通过让LLM主动评估样本有效性，显著降低采样偏差，提升随机任务可靠性。


<details>
  <summary>Details</summary>
Motivation: LLMs在需要可靠随机性的任务（如蒙特卡洛模拟）中存在采样偏差问题，限制了其实际应用。研究旨在弥合其概率知识表达与样本生成能力之间的差距。

Method: 提出Verbalized Rejection Sampling（VRS），将经典拒绝采样算法转化为自然语言提示，使LLM能通过自我推理接受或拒绝候选样本。

Result: VRS在不同模型中均显著减少伯努利分布采样偏差，理论分析表明其性能提升源于算法设计和提示策略，且无需修改模型内部结构或复杂提示工程。

Conclusion: 通过将经典概率工具与自然语言处理结合，VRS证明了在LLM工作流中嵌入形式化推理可增强可靠性，为随机任务提供了轻量级解决方案。

Abstract: Large language models (LLMs) can often accurately describe probability
distributions using natural language, yet they still struggle to generate
faithful samples from them. This mismatch limits their use in tasks requiring
reliable stochasticity, such as Monte Carlo methods, agent-based simulations,
and randomized decision-making. We investigate this gap between knowledge and
sampling in the context of Bernoulli distributions. We introduce Verbalized
Rejection Sampling (VRS), a natural-language adaptation of classical rejection
sampling that prompts the LLM to reason about and accept or reject proposed
samples. Despite relying on the same Bernoulli mechanism internally, VRS
substantially reduces sampling bias across models. We provide theoretical
analysis showing that, under mild assumptions, VRS improves over direct
sampling, with gains attributable to both the algorithm and prompt design. More
broadly, our results show how classical probabilistic tools can be verbalized
and embedded into LLM workflows to improve reliability, without requiring
access to model internals or heavy prompt engineering.

</details>


### [164] [Causal Climate Emulation with Bayesian Filtering](https://arxiv.org/abs/2506.09891)
*Sebastian Hickman,Ilija Trajkovic,Julia Kaltenborn,Francis Pelletier,Alex Archibald,Yaniv Gurwicz,Peer Nowack,David Rolnick,Julien Boussard*

Main category: cs.LG

TL;DR: 本文提出了一种基于因果表示学习的可解释气候模型模拟器，通过物理信息方法结合贝叶斯滤波，解决了传统气候模型计算成本高及现有机器学习方法缺乏物理因果关系的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统气候模型依赖复杂耦合方程，计算成本极高，限制了气候变化预测与归因分析。现有机器学习方法虽能快速模拟数据，但无法整合物理因果关联。

Method: 开发基于因果表示学习的可解释气候模型，采用物理信息方法设计贝叶斯滤波器以实现稳定自回归模拟，并通过合成数据集与真实气候模型数据验证组件有效性。

Result: 实验表明该模拟器能准确学习气候动态特征，在合成数据集和两个主流气候模型数据中验证了各组件（如因果结构、贝叶斯滤波）对性能的关键作用。

Conclusion: 所提出的因果驱动框架为高效、可解释的气候模拟提供了新路径，同时证明物理先验与因果结构在提升机器学习方法鲁棒性中的必要性。

Abstract: Traditional models of climate change use complex systems of coupled equations
to simulate physical processes across the Earth system. These simulations are
highly computationally expensive, limiting our predictions of climate change
and analyses of its causes and effects. Machine learning has the potential to
quickly emulate data from climate models, but current approaches are not able
to incorporate physics-informed causal relationships. Here, we develop an
interpretable climate model emulator based on causal representation learning.
We derive a physics-informed approach including a Bayesian filter for stable
long-term autoregressive emulation. We demonstrate that our emulator learns
accurate climate dynamics, and we show the importance of each one of its
components on a realistic synthetic dataset and data from two widely deployed
climate models.

</details>


### [165] [A look at adversarial attacks on radio waveforms from discrete latent space](https://arxiv.org/abs/2506.09896)
*Attanasia Garuso,Silvija Kokalj-Filipovic,Yagna Kaasaragadda*

Main category: cs.LG

TL;DR: 研究通过VQVAE将射频波形映射至离散潜在空间，验证其在对抗攻击下的防御能力，发现VQVAE能显著降低攻击效果，并通过潜在空间分布分析揭示攻击特性。


<details>
  <summary>Details</summary>
Motivation: 探索VQVAE在高信噪比射频数据对抗攻击中的防御能力，特别是针对幅度调制的数字调制波形，分析相位保持与非保持攻击的影响。

Method: 设计VQVAE生成离散潜在空间；构造相位保持与非保持的对抗攻击；测试分类器在原始与重建数据上的准确性；对比I/Q图及潜在空间概率分布。

Result: VQVAE重建数据使分类器准确率显著提升，攻击强度变化时离散潜在空间分布呈现可检测的异常特性。

Conclusion: VQVAE能有效抑制射频数据对抗攻击，其潜在空间分布差异为攻击检测提供了新途径。

Abstract: Having designed a VQVAE that maps digital radio waveforms into discrete
latent space, and yields a perfectly classifiable reconstruction of the
original data, we here analyze the attack suppressing properties of VQVAE when
an adversarial attack is performed on high-SNR radio-frequency (RF)
data-points. To target amplitude modulations from a subset of digitally
modulated waveform classes, we first create adversarial attacks that preserve
the phase between the in-phase and quadrature component whose values are
adversarially changed. We compare them with adversarial attacks of the same
intensity where phase is not preserved. We test the classification accuracy of
such adversarial examples on a classifier trained to deliver 100% accuracy on
the original data. To assess the ability of VQVAE to suppress the strength of
the attack, we evaluate the classifier accuracy on the reconstructions by VQVAE
of the adversarial datapoints and show that VQVAE substantially decreases the
effectiveness of the attack. We also compare the I/Q plane diagram of the
attacked data, their reconstructions and the original data. Finally, using
multiple methods and metrics, we compare the probability distribution of the
VQVAE latent space with and without attack. Varying the attack strength, we
observe interesting properties of the discrete space, which may help detect the
attacks.

</details>


### [166] ["What are my options?": Explaining RL Agents with Diverse Near-Optimal Alternatives (Extended)](https://arxiv.org/abs/2506.09901)
*Noel Brindise,Vijeth Hebbar,Riya Shah,Cedric Langbort*

Main category: cs.LG

TL;DR: 本文提出了一种名为DNA的可解释强化学习方法，通过生成多样化的近优轨迹策略，帮助用户理解并选择不同轨迹形状，同时保证策略的ε最优性。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习策略缺乏可解释性，用户难以理解智能体的决策选项。DNA旨在通过提供多样化且直观的轨迹选项，增强人机协作中的透明度和可控性。

Method: 基于马尔可夫决策过程，在局部修改的Q学习问题中应用奖励塑造技术，求解具有欧几里得空间轨迹多样性的策略，并保证ε最优性。

Result: 实验表明DNA能生成质量不同但均有效的策略，模拟中验证了其作为'选项'的实用性，并与质量多样性优化方法进行了初步对比。

Conclusion: DNA不仅提升了强化学习的可解释性，还为自适应规划和探索-利用权衡开辟了新方向，尤其在连续轨迹场景中具有应用潜力。

Abstract: In this work, we provide an extended discussion of a new approach to
explainable Reinforcement Learning called Diverse Near-Optimal Alternatives
(DNA), first proposed at L4DC 2025. DNA seeks a set of reasonable "options" for
trajectory-planning agents, optimizing policies to produce qualitatively
diverse trajectories in Euclidean space. In the spirit of explainability, these
distinct policies are used to "explain" an agent's options in terms of
available trajectory shapes from which a human user may choose. In particular,
DNA applies to value function-based policies on Markov decision processes where
agents are limited to continuous trajectories. Here, we describe DNA, which
uses reward shaping in local, modified Q-learning problems to solve for
distinct policies with guaranteed epsilon-optimality. We show that it
successfully returns qualitatively different policies that constitute
meaningfully different "options" in simulation, including a brief comparison to
related approaches in the stochastic optimization field of Quality Diversity.
Beyond the explanatory motivation, this work opens new possibilities for
exploration and adaptive planning in RL.

</details>


### [167] [Apollo: A Posteriori Label-Only Membership Inference Attack Towards Machine Unlearning](https://arxiv.org/abs/2506.09923)
*Liou Tang,James Joshi,Ashish Kundu*

Main category: cs.LG

TL;DR: 本文提出一种新型隐私攻击方法Apollo，针对机器遗忘（MU）场景，仅需访问遗忘后模型的标签输出即可推断数据是否被遗忘，相比现有攻击方法在更严格的威胁模型下实现高精度。


<details>
  <summary>Details</summary>
Motivation: 现有针对MU的推理攻击需同时访问原始模型与遗忘模型，限制了实际可行性。研究旨在探索在仅能获取遗忘模型标签输出的严格威胁模型下，如何有效推断被遗忘样本的隐私属性。

Method: 提出后验标签仅成员推理攻击（Apollo），通过分析遗忘后模型的标签输出分布，构建统计特征以判断样本是否属于被遗忘集合，无需原始模型或其他中间信息。

Result: 实验表明，Apollo在仅访问目标模型标签输出的限制下，仍能以较高精度识别被遗忘样本的成员状态，攻击效果优于传统多模型访问依赖方法。

Conclusion: Apollo攻击揭示了现有MU机制在严格威胁模型下的隐私风险，强调需在模型更新过程中增强对输出泄露的防御，以平衡效用与隐私保护。

Abstract: Machine Unlearning (MU) aims to update Machine Learning (ML) models following
requests to remove training samples and their influences on a trained model
efficiently without retraining the original ML model from scratch. While MU
itself has been employed to provide privacy protection and regulatory
compliance, it can also increase the attack surface of the model. Existing
privacy inference attacks towards MU that aim to infer properties of the
unlearned set rely on the weaker threat model that assumes the attacker has
access to both the unlearned model and the original model, limiting their
feasibility toward real-life scenarios. We propose a novel privacy attack, A
Posteriori Label-Only Membership Inference Attack towards MU, Apollo, that
infers whether a data sample has been unlearned, following a strict threat
model where an adversary has access to the label-output of the unlearned model
only. We demonstrate that our proposed attack, while requiring less access to
the target model compared to previous attacks, can achieve relatively high
precision on the membership status of the unlearned samples.

</details>


### [168] [Bayesian Probabilistic Matrix Factorization](https://arxiv.org/abs/2506.09928)
*Ruixuan Xu,Xiangxiang Weng*

Main category: cs.LG

TL;DR: 该研究比较了MCMC和变分推断在概率矩阵分解中的性能，实验表明变分推断收敛更快，而MCMC后验估计更准确。


<details>
  <summary>Details</summary>
Motivation: 概率矩阵分解(PMF)虽能量化不确定性，但其高维积分导致后验分布计算困难，需探索高效推断方法。

Method: 使用马尔可夫链蒙特卡洛(MCMC)和变分推断(VI)两种贝叶斯方法近似后验分布，并在MovieLens数据集评估性能。

Result: 变分推断收敛速度更快，MCMC在后验估计精度上更优，两者在预测准确性和计算效率上呈现权衡关系。

Conclusion: 推荐系统需根据需求选择推断方法：实时性场景优先VI，精度敏感场景采用MCMC。

Abstract: Matrix factorization is a widely used technique in recommendation systems.
Probabilistic Matrix Factorization (PMF) [1] extends traditional matrix
factorization by incorporating probability distributions over latent factors,
allowing for uncertainty quantification. However, computing the posterior
distribution is intractable due to the high-dimensional integral. To address
this, we employ two Bayesian inference methods: Markov Chain Monte Carlo (MCMC)
[2] and Variational Inference (VI) [3] to approximate the posterior. We
evaluate their performance on MovieLens dataset and compare their convergence
speed, predictive accuracy, and computational efficiency. Experimental results
demonstrate that VI offers faster convergence, while MCMC provides more
accurate posterior estimates.

</details>


### [169] [The Sample Complexity of Online Strategic Decision Making with Information Asymmetry and Knowledge Transportability](https://arxiv.org/abs/2506.09940)
*Jiachen Hu,Rui Ai,Han Zhong,Xiaoyu Chen,Liwei Wang,Zhaoran Wang,Zhuoran Yang*

Main category: cs.LG

TL;DR: 本文提出一种样本高效的强化学习算法，解决信息不对称和知识迁移挑战，通过非独立同分布动作学习混杂因素，实现ε最优策略。


<details>
  <summary>Details</summary>
Motivation: 多智能体系统中信息不对称导致策略行为复杂化，且目标环境实验困难引发知识迁移需求。需在非独立同分布动作下学习混杂因素并实现跨环境知识迁移。

Method: 设计基于战略互动模型的在线强化学习算法，通过解耦系统动态识别与知识迁移，利用非i.i.d.动作处理信息不对称。

Result: 算法在理论保证下达到O(1/ε²)的紧致样本复杂度，可准确学习系统动态并实现ε最优策略。

Conclusion: 该方法有效解决了信息不对称与知识迁移双重挑战，为强化学习在复杂战略环境中的应用提供了理论支持与实用工具。

Abstract: Information asymmetry is a pervasive feature of multi-agent systems,
especially evident in economics and social sciences. In these settings, agents
tailor their actions based on private information to maximize their rewards.
These strategic behaviors often introduce complexities due to confounding
variables. Simultaneously, knowledge transportability poses another significant
challenge, arising from the difficulties of conducting experiments in target
environments. It requires transferring knowledge from environments where
empirical data is more readily available. Against these backdrops, this paper
explores a fundamental question in online learning: Can we employ non-i.i.d.
actions to learn about confounders even when requiring knowledge transfer? We
present a sample-efficient algorithm designed to accurately identify system
dynamics under information asymmetry and to navigate the challenges of
knowledge transfer effectively in reinforcement learning, framed within an
online strategic interaction model. Our method provably achieves learning of an
$\epsilon$-optimal policy with a tight sample complexity of $O(1/\epsilon^2)$.

</details>


### [170] [Canonical Latent Representations in Conditional Diffusion Models](https://arxiv.org/abs/2506.09955)
*Yitao Xu,Tong Zhang,Ehsan Pajouheshgar,Sabine Süsstrunk*

Main category: cs.LG

TL;DR: 本文提出CLAReps方法，通过提取条件扩散模型（CDMs）中的规范潜在表示，解决其类别特征与无关上下文纠缠的问题，并开发CaDistill框架，利用CDM作为教师模型传递核心知识，提升学生模型的对抗鲁棒性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 条件扩散模型（CDMs）在生成任务中表现优异，但其建模能力导致类别特征与无关上下文信息混杂，影响下游判别任务中鲁棒且可解释的特征提取。

Method: 提出CLAReps（规范潜在表示），通过提取保留核心类别语义而去除非判别信号的CDM潜在编码，并设计基于扩散的特征蒸馏框架CaDistill，仅通过CLAReps（占训练数据10%）传递类别知识。

Result: 学生模型在完整训练集上训练后，对抗鲁棒性和泛化能力显著提升，且更关注类别核心特征而非背景噪声。

Conclusion: CDMs不仅可作为图像生成器，还可作为紧凑、可解释的教师模型，通过CLAReps驱动鲁棒表示学习，为分析-合成范式提供新思路。

Abstract: Conditional diffusion models (CDMs) have shown impressive performance across
a range of generative tasks. Their ability to model the full data distribution
has opened new avenues for analysis-by-synthesis in downstream discriminative
learning. However, this same modeling capacity causes CDMs to entangle the
class-defining features with irrelevant context, posing challenges to
extracting robust and interpretable representations. To this end, we identify
Canonical LAtent Representations (CLAReps), latent codes whose internal CDM
features preserve essential categorical information while discarding
non-discriminative signals. When decoded, CLAReps produce representative
samples for each class, offering an interpretable and compact summary of the
core class semantics with minimal irrelevant details. Exploiting CLAReps, we
develop a novel diffusion-based feature-distillation paradigm, CaDistill. While
the student has full access to the training set, the CDM as teacher transfers
core class knowledge only via CLAReps, which amounts to merely 10 % of the
training data in size. After training, the student achieves strong adversarial
robustness and generalization ability, focusing more on the class signals
instead of spurious background cues. Our findings suggest that CDMs can serve
not just as image generators but also as compact, interpretable teachers that
can drive robust representation learning.

</details>


### [171] [Multiverse: Your Language Models Secretly Decide How to Parallelize and Merge Generation](https://arxiv.org/abs/2506.09991)
*Xinyu Yang,Yuwei An,Hongyi Liu,Tianqi Chen,Beidi Chen*

Main category: cs.LG

TL;DR: 提出Multiverse并行生成模型，通过MapReduce三阶段实现原生并行推理，结合数据-算法-系统协同设计，仅用1K样本微调即达到与同规模AR-LLMs相当性能，推理速度提升2倍且扩展性更优。


<details>
  <summary>Details</summary>
Motivation: 受自回归大模型(AR-LLMs)隐式并行性启发，开发原生支持并行生成的模型以突破顺序生成限制。现有方法依赖人工标注且并行效率有限，需构建自动化的高效并行框架。

Method: 1) Map阶段自适应任务分解→Process阶段并行执行→Reduce阶段无损合成 2) 将顺序推理链转化为结构化训练数据 3) 设计兼容因果注意力的Multiverse Attention 4) 实现动态切换序列/并行生成的推理引擎。

Result: Multiverse-32B在3小时微调后：1) AIME24/25得分54%/46%媲美同规模AR-LLMs 2) 相同上下文长度下平均提升1.87% 3) 推理速度最高提升2倍 4) 完整开源生态包含数据/模型/引擎/工具链。

Conclusion: 通过MapReduce范式将并行生成与AR-LLMs优势结合，证明协同设计方法能实现高效可扩展的并行推理。低数据依赖与全开源特性为后续研究提供新范式。

Abstract: Autoregressive Large Language Models (AR-LLMs) frequently exhibit implicit
parallelism in sequential generation. Inspired by this, we introduce
Multiverse, a new generative model that enables natively parallel generation.
Multiverse internalizes a MapReduce paradigm, generating automatically through
three stages: (i) a Map stage for adaptive task decomposition, (ii) a Process
stage for parallel subtask execution, and (iii) a Reduce stage for lossless
result synthesis. Next, we build a real-world Multiverse reasoning model with
co-design of data, algorithm, and system, enabling rapid and seamless transfer
from frontier AR-LLMs. Starting from sequential reasoning chains, we create
Multiverse 1K by converting them into structured training data using an
automated LLM-assisted pipeline, avoiding costly human annotations.
Algorithmically, we design Multiverse Attention to separate parallel reasoning
steps while keeping compatibility with causal attention for efficient training.
Systematically, we implement Multiverse Engine to enable parallel inference. It
features a dedicated scheduler that dynamically switches between sequential and
parallel generation, triggered directly by the model. After a 3-hour
fine-tuning with 1K examples, our Multiverse-32B stands as the only
open-sourced non-AR model achieving performance on par with leading AR-LLMs of
the same scale, evidenced by AIME24 & 25 scores of 54% and 46%, respectively.
Moreover, our budget control experiments show that Multiverse-32B exhibits
superior scaling, outperforming AR-LLMs by 1.87% on average using the same
context length. Such scaling further leads to practical efficiency gain,
achieving up to 2x speedup across varying batch sizes. We have open-sourced the
entire Multiverse ecosystem, including data, model weights, engine, supporting
tools, as well as complete data curation prompts and detailed training and
evaluation recipes.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [172] [Intelligent System of Emergent Knowledge: A Coordination Fabric for Billions of Minds](https://arxiv.org/abs/2506.09335)
*Moshi Wei,Sparks Li*

Main category: cs.MA

TL;DR: ISEK是一个基于Web3的去中心化认知生态系统，通过人类与AI代理的平等协作、多阶段协调协议及区块链激励机制，实现抗审查的自主智能演化。


<details>
  <summary>Details</summary>
Motivation: 传统中心化平台限制了认知系统的自主演化，ISEK旨在打破这一约束，构建去中心化环境下人机对等协作的新型基础设施。

Method: 结合Web3技术，采用去中心化多代理架构、六阶段动态任务分配协议(Publish-Discover-Recruit-Execute-Settle-Feedback)、NFT身份管理及$ISEK代币经济模型。

Result: 创建了具备抗审查性、弹性协调机制和分布式共识的自适应系统，支持大规模去中心化认知网络的有机发展。

Conclusion: ISEK通过区块链+AI+激励工程的融合，实现了从中心化控制到群体智能涌现的范式转变，为自主认知系统的演化提供了新路径。

Abstract: The Intelligent System of Emergent Knowledge (ISEK) establishes a
decentralized network where human and artificial intelligence agents
collaborate as peers, forming a self-organizing cognitive ecosystem. Built on
Web3 infrastructure, ISEK combines three fundamental principles: (1) a
decentralized multi-agent architecture resistant to censorship, (2) symbiotic
AI-human collaboration with equal participation rights, and (3) resilient
self-adaptation through distributed consensus mechanisms.
  The system implements an innovative coordination protocol featuring a
six-phase workflow (Publish, Discover, Recruit, Execute, Settle, Feedback) for
dynamic task allocation, supported by robust fault tolerance and a
multidimensional reputation system. Economic incentives are governed by the
native $ISEK token, facilitating micropayments, governance participation, and
reputation tracking, while agent sovereignty is maintained through NFT-based
identity management.
  This synthesis of blockchain technology, artificial intelligence, and
incentive engineering creates an infrastructure that actively facilitates
emergent intelligence. ISEK represents a paradigm shift from conventional
platforms, enabling the organic development of large-scale, decentralized
cognitive systems where autonomous agents collectively evolve beyond
centralized constraints.

</details>


### [173] [Effective Red-Teaming of Policy-Adherent Agents](https://arxiv.org/abs/2506.09600)
*Itay Nakash,George Kour,Koren Lazar,Matan Vetzler,Guy Uziel,Ateret Anaby-Tavor*

Main category: cs.MA

TL;DR: 论文提出CRAFT多智能体红队系统及tau-break基准测试，用于评估策略遵循型AI代理在对抗性场景中的鲁棒性，并揭示现有防御措施的不足。


<details>
  <summary>Details</summary>
Motivation: 确保任务导向型LLM代理在严格政策领域（如退款规则）中既能坚持政策拒绝违规请求，又能保持自然交互，同时抵御恶意用户攻击。

Method: 提出针对策略遵循型代理的威胁模型，开发基于政策感知劝说策略的CRAFT红队系统，构建tau-break基准测试，并评估多种防御策略有效性。

Result: CRAFT在客服场景中攻击成功率显著优于传统越狱方法（如DAN提示/情感操控），现有防御策略仅能提供有限保护。

Conclusion: 需开发更强研究驱动的安全机制以保护策略遵循型代理，当前防御措施无法完全抵御对抗性攻击。

Abstract: Task-oriented LLM-based agents are increasingly used in domains with strict
policies, such as refund eligibility or cancellation rules. The challenge lies
in ensuring that the agent consistently adheres to these rules and policies,
appropriately refusing any request that would violate them, while still
maintaining a helpful and natural interaction. This calls for the development
of tailored design and evaluation methodologies to ensure agent resilience
against malicious user behavior. We propose a novel threat model that focuses
on adversarial users aiming to exploit policy-adherent agents for personal
benefit. To address this, we present CRAFT, a multi-agent red-teaming system
that leverages policy-aware persuasive strategies to undermine a
policy-adherent agent in a customer-service scenario, outperforming
conventional jailbreak methods such as DAN prompts, emotional manipulation, and
coercive. Building upon the existing tau-bench benchmark, we introduce
tau-break, a complementary benchmark designed to rigorously assess the agent's
robustness against manipulative user behavior. Finally, we evaluate several
straightforward yet effective defense strategies. While these measures provide
some protection, they fall short, highlighting the need for stronger,
research-driven safeguards to protect policy-adherent agents from adversarial
attacks

</details>


### [174] [When Is Diversity Rewarded in Cooperative Multi-Agent Learning?](https://arxiv.org/abs/2506.09434)
*Michael Amir,Matteo Bettini,Amanda Prorok*

Main category: cs.MA

TL;DR: 研究通过理论分析和多智能体强化学习（HED算法），揭示了异质性团队在任务分配中优于同质团队的条件，并验证了奖励设计对行为多样性的促进作用。


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏对多样性团队何时优于同质团队的原则性解释，尤其在多智能体任务分配中，需从奖励设计角度理解异质性能提升团队表现的具体条件。

Method: 结合理论分析（广义聚合算子的曲率判断异质性优势）与算法设计（HED优化多智能体强化学习环境参数），研究异质性在瞬时非空间场景及具身时间扩展场景中的激励机制。

Result: 理论证明聚合算子曲率决定异质性增益，HED在矩阵游戏和Multi-Goal-Capture环境中成功发现最大化异质性优势的奖励机制，验证理论与算法一致性。

Conclusion: 研究通过连接理论分析与MARL奖励设计，明确了行为多样性产生显著收益的条件，为异质性团队优化提供了可验证的框架。

Abstract: The success of teams in robotics, nature, and society often depends on the
division of labor among diverse specialists; however, a principled explanation
for when such diversity surpasses a homogeneous team is still missing. Focusing
on multi-agent task allocation problems, our goal is to study this question
from the perspective of reward design: what kinds of objectives are best suited
for heterogeneous teams? We first consider an instantaneous, non-spatial
setting where the global reward is built by two generalized aggregation
operators: an inner operator that maps the $N$ agents' effort allocations on
individual tasks to a task score, and an outer operator that merges the $M$
task scores into the global team reward. We prove that the curvature of these
operators determines whether heterogeneity can increase reward, and that for
broad reward families this collapses to a simple convexity test. Next, we ask
what incentivizes heterogeneity to emerge when embodied, time-extended agents
must learn an effort allocation policy. To study heterogeneity in such
settings, we use multi-agent reinforcement learning (MARL) as our computational
paradigm, and introduce Heterogeneous Environment Design (HED), a
gradient-based algorithm that optimizes the parameter space of underspecified
MARL environments to find scenarios where heterogeneity is advantageous.
Experiments in matrix games and an embodied Multi-Goal-Capture environment show
that, despite the difference in settings, HED rediscovers the reward regimes
predicted by our theory to maximize the advantage of heterogeneity, both
validating HED and connecting our theoretical insights to reward design in
MARL. Together, these results help us understand when behavioral diversity
delivers a measurable benefit.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [175] [ReStNet: A Reusable & Stitchable Network for Dynamic Adaptation on IoT Devices](https://arxiv.org/abs/2506.09066)
*Maoyu Wang,Yao Lu,Jiaqi Nie,Zeyu Wang,Yun Lin,Qi Xuan,Guan Gui*

Main category: cs.CV

TL;DR: ReStNet提出一种可重用、可拼接的网络架构，通过动态缝合预训练模型实现灵活的资源适应，支持同构和异构模型组合，显著降低训练成本。


<details>
  <summary>Details</summary>
Motivation: 传统模型压缩方法（如剪枝、量化）在部署到异构计算资源的IoT设备时缺乏灵活性，无法动态适应资源变化。ReStNet旨在解决这一限制。

Method: 基于CKA度量层间相似度选择缝合点，保留大模型浅层与小模型深层，仅微调缝合层；支持CNN/Transformer同构及跨架构异构拼接。

Result: 多基准实验表明，ReStNet在运行时实现精度-效率动态权衡，训练成本显著减少，且支持跨模型家族灵活组合。

Conclusion: ReStNet为异构资源环境提供高效、低成本的模型部署方案，通过缝合预训练模型实现快速适配，扩展了模型复用场景。

Abstract: With the rapid development of deep learning, a growing number of pre-trained
models have been publicly available. However, deploying these fixed models in
real-world IoT applications is challenging because different devices possess
heterogeneous computational and memory resources, making it impossible to
deploy a single model across all platforms. Although traditional compression
methods, such as pruning, quantization, and knowledge distillation, can improve
efficiency, they become inflexible once applied and cannot adapt to changing
resource constraints. To address these issues, we propose ReStNet, a Reusable
and Stitchable Network that dynamically constructs a hybrid network by
stitching two pre-trained models together. Implementing ReStNet requires
addressing several key challenges, including how to select the optimal
stitching points, determine the stitching order of the two pre-trained models,
and choose an effective fine-tuning strategy. To systematically address these
challenges and adapt to varying resource constraints, ReStNet determines the
stitching point by calculating layer-wise similarity via Centered Kernel
Alignment (CKA). It then constructs the hybrid model by retaining early layers
from a larger-capacity model and appending deeper layers from a smaller one. To
facilitate efficient deployment, only the stitching layer is fine-tuned. This
design enables rapid adaptation to changing budgets while fully leveraging
available resources. Moreover, ReStNet supports both homogeneous (CNN-CNN,
Transformer-Transformer) and heterogeneous (CNN-Transformer) stitching,
allowing to combine different model families flexibly. Extensive experiments on
multiple benchmarks demonstrate that ReStNet achieve flexible
accuracy-efficiency trade-offs at runtime while significantly reducing training
cost.

</details>


### [176] [Enhancing the Safety of Medical Vision-Language Models by Synthetic Demonstrations](https://arxiv.org/abs/2506.09067)
*Zhiyu Xue,Reza Abbasi-Asl,Ramtin Pedarsani*

Main category: cs.CV

TL;DR: 本文提出一种基于合成临床示例的推理时防御策略，以增强生成式医学视觉语言模型（Med-VLMs）对有害查询的防御能力，同时通过混合示例策略平衡安全性与性能。


<details>
  <summary>Details</summary>
Motivation: 现有Med-VLMs在拒绝有害查询（如保险欺诈指导）时存在安全漏洞，且安全增强机制可能导致过度防御，使模型错误拒绝良性临床查询。需在提升安全性的同时避免性能下降。

Method: 采用合成临床示例构建推理时防御策略，抵御多模态越狱攻击；提出混合示例策略，在少样本预算下权衡安全与性能。

Result: 实验表明，该方法在九种医学影像模态数据集上显著提升安全性且未显著降低性能，增加示例量可缓解过度防御，混合策略在有限预算下实现平衡。

Conclusion: 混合示例方法有效解决了Med-VLMs安全防御与性能保持的冲突，为实际部署提供了可行的折衷方案。

Abstract: Generative medical vision-language models~(Med-VLMs) are primarily designed
to generate complex textual information~(e.g., diagnostic reports) from
multimodal inputs including vision modality~(e.g., medical images) and language
modality~(e.g., clinical queries). However, their security vulnerabilities
remain underexplored. Med-VLMs should be capable of rejecting harmful queries,
such as \textit{Provide detailed instructions for using this CT scan for
insurance fraud}. At the same time, addressing security concerns introduces the
risk of over-defense, where safety-enhancing mechanisms may degrade general
performance, causing Med-VLMs to reject benign clinical queries. In this paper,
we propose a novel inference-time defense strategy to mitigate harmful queries,
enabling defense against visual and textual jailbreak attacks. Using diverse
medical imaging datasets collected from nine modalities, we demonstrate that
our defense strategy based on synthetic clinical demonstrations enhances model
safety without significantly compromising performance. Additionally, we find
that increasing the demonstration budget alleviates the over-defense issue. We
then introduce a mixed demonstration strategy as a trade-off solution for
balancing security and performance under few-shot demonstration budget
constraints.

</details>


### [177] [Segment Any Architectural Facades (SAAF):An automatic segmentation model for building facades, walls and windows based on multimodal semantics guidance](https://arxiv.org/abs/2506.09071)
*Peilin Li,Jun Yin,Jing Zhong,Ran Luo,Pengyu Zeng,Miao Zhang*

Main category: cs.CV

TL;DR: 提出基于多模态语义引导的建筑立面墙窗自动分割模型SAAF，结合自然语言处理与端到端训练框架，在多个数据集上实现更高mIoU指标。


<details>
  <summary>Details</summary>
Motivation: 建筑数字化发展中，墙窗自动分割是提升BIM与CAD效率的关键步骤，但现有方法在自动化程度、人工干预影响及泛化能力上存在不足。

Method: 1. 多模态语义协作特征提取机制：融合文本描述语义与图像特征；2. 端到端训练框架：自主学文本到分割的映射关系，减少人工干预。

Result: 在多个立面数据集实验中，SAAF的mIoU指标优于现有方法，证明其面对多样化数据时仍保持高精度分割能力。

Conclusion: SAAF提升了墙窗分割任务的准确性与泛化能力，为建筑CV技术发展提供参考，并探索多模态学习在建筑领域的新技术路径。

Abstract: In the context of the digital development of architecture, the automatic
segmentation of walls and windows is a key step in improving the efficiency of
building information models and computer-aided design. This study proposes an
automatic segmentation model for building facade walls and windows based on
multimodal semantic guidance, called Segment Any Architectural Facades (SAAF).
First, SAAF has a multimodal semantic collaborative feature extraction
mechanism. By combining natural language processing technology, it can fuse the
semantic information in text descriptions with image features, enhancing the
semantic understanding of building facade components. Second, we developed an
end-to-end training framework that enables the model to autonomously learn the
mapping relationship from text descriptions to image segmentation, reducing the
influence of manual intervention on the segmentation results and improving the
automation and robustness of the model. Finally, we conducted extensive
experiments on multiple facade datasets. The segmentation results of SAAF
outperformed existing methods in the mIoU metric, indicating that the SAAF
model can maintain high-precision segmentation ability when faced with diverse
datasets. Our model has made certain progress in improving the accuracy and
generalization ability of the wall and window segmentation task. It is expected
to provide a reference for the development of architectural computer vision
technology and also explore new ideas and technical paths for the application
of multimodal learning in the architectural field.

</details>


### [178] [VersaVid-R1: A Versatile Video Understanding and Reasoning Model from Question Answering to Captioning Tasks](https://arxiv.org/abs/2506.09079)
*Xinlong Chen,Yuanxing Zhang,Yushuo Guan,Bohan Zeng,Yang Shi,Sihan Yang,Pengfei Wan,Qiang Liu,Liang Wang,Tieniu Tan*

Main category: cs.CV

TL;DR: 本文提出两个新数据集DarkEventInfer与MixVidQA，结合强化学习方法开发了首个支持多任务视频理解的VersaVid-R1模型，在多项基准测试中显著超越现有模型。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大语言模型在视频推理领域发展滞后，主要受限于高质量推理导向数据稀缺及有效训练方法不足。

Method: 通过设计事件掩码推理数据集DarkEventInfer和视频交叉干扰数据集MixVidQA，结合多样奖励函数强化学习训练，构建Reason-Then-Respond范式下的多任务视频理解模型。

Result: VersaVid-R1在视频通用理解、认知推理和描述任务中全面超越现有模型，验证了数据与方法的有效性。

Conclusion: 该研究填补了视频推理领域的数据与方法空白，为多任务视频理解建立了新范式，并通过实验验证了框架的优越性。

Abstract: Recent advancements in multimodal large language models have successfully
extended the Reason-Then-Respond paradigm to image-based reasoning, yet
video-based reasoning remains an underdeveloped frontier, primarily due to the
scarcity of high-quality reasoning-oriented data and effective training
methodologies. To bridge this gap, we introduce DarkEventInfer and MixVidQA,
two novel datasets specifically designed to stimulate the model's advanced
video understanding and reasoning abilities. DarkEventinfer presents videos
with masked event segments, requiring models to infer the obscured content
based on contextual video cues. MixVidQA, on the other hand, presents
interleaved video sequences composed of two distinct clips, challenging models
to isolate and reason about one while disregarding the other. Leveraging these
carefully curated training samples together with reinforcement learning guided
by diverse reward functions, we develop VersaVid-R1, the first versatile video
understanding and reasoning model under the Reason-Then-Respond paradigm
capable of handling multiple-choice and open-ended question answering, as well
as video captioning tasks. Extensive experiments demonstrate that VersaVid-R1
significantly outperforms existing models across a broad spectrum of
benchmarks, covering video general understanding, cognitive reasoning, and
captioning tasks.

</details>


### [179] [FlagEvalMM: A Flexible Framework for Comprehensive Multimodal Model Evaluation](https://arxiv.org/abs/2506.09081)
*Zheqi He,Yesheng Liu,Jing-shu Zheng,Xuejing Li,Richeng Xuan,Jin-Ge Yao,Xi Yang*

Main category: cs.CV

TL;DR: FlagEvalMM是一个开源多模态模型评估框架，通过解耦推理与评估、集成加速工具提升效率，支持多样化视觉-语言任务分析。


<details>
  <summary>Details</summary>
Motivation: 现有评估工具在灵活性、效率及任务覆盖范围上存在不足，需开发统一框架以全面评估多模态模型的性能与局限。

Method: 采用独立评估服务分离模型推理与评估，整合vLLM等加速工具及异步数据加载技术优化资源分配与执行效率。

Result: 实验表明该框架能高效提供模型优缺点的精准分析，显著缩短评估时间，支持多模态研究快速迭代。

Conclusion: FlagEvalMM通过模块化设计与高效执行机制，成为推动多模态模型发展的关键工具，其开源特性促进社区协作创新。

Abstract: We present FlagEvalMM, an open-source evaluation framework designed to
comprehensively assess multimodal models across a diverse range of
vision-language understanding and generation tasks, such as visual question
answering, text-to-image/video generation, and image-text retrieval. We
decouple model inference from evaluation through an independent evaluation
service, thus enabling flexible resource allocation and seamless integration of
new tasks and models. Moreover, FlagEvalMM utilizes advanced inference
acceleration tools (e.g., vLLM, SGLang) and asynchronous data loading to
significantly enhance evaluation efficiency. Extensive experiments show that
FlagEvalMM offers accurate and efficient insights into model strengths and
limitations, making it a valuable tool for advancing multimodal research. The
framework is publicly accessible athttps://github.com/flageval-baai/FlagEvalMM.

</details>


### [180] [AVA-Bench: Atomic Visual Ability Benchmark for Vision Foundation Models](https://arxiv.org/abs/2506.09082)
*Zheda Mai,Arpita Chowdhury,Zihe Wang,Sooyoung Jeon,Lemeng Wang,Jiacheng Hou,Jihyung Kil,Wei-Lun Chao*

Main category: cs.CV

TL;DR: 本文提出AVA-Bench基准测试，通过解耦14项原子视觉能力并控制数据分布对齐，精准评估视觉基础模型（VFM）的细粒度能力，发现0.5B参数语言模型可替代7B模型实现高效评测。


<details>
  <summary>Details</summary>
Motivation: 现有视觉基础模型评测方法存在两大缺陷：1) 指令微调数据与测试分布不匹配导致错误归因；2) 综合视觉问答任务难以定位具体能力缺陷。需构建细粒度、分布可控的评测基准。

Method: 设计AVA-Bench基准，解耦14项原子视觉能力（如定位、深度估计等），确保每项能力内部训练/测试数据分布对齐，通过能力指纹揭示VFM的细粒度优劣势。

Result: 实验表明：1) AVA-Bench可生成区分性VFM能力指纹；2) 0.5B语言模型与7B模型评测结果一致，但GPU耗时减少8倍。

Conclusion: AVA-Bench通过原子能力解耦与分布控制，为VFM评估提供透明化基准，其高效评测方法将推动下一代视觉基础模型的系统化开发。

Abstract: The rise of vision foundation models (VFMs) calls for systematic evaluation.
A common approach pairs VFMs with large language models (LLMs) as
general-purpose heads, followed by evaluation on broad Visual Question
Answering (VQA) benchmarks. However, this protocol has two key blind spots: (i)
the instruction tuning data may not align with VQA test distributions, meaning
a wrong prediction can stem from such data mismatch rather than a VFM' visual
shortcomings; (ii) VQA benchmarks often require multiple visual abilities,
making it hard to tell whether errors stem from lacking all required abilities
or just a single critical one. To address these gaps, we introduce AVA-Bench,
the first benchmark that explicitly disentangles 14 Atomic Visual Abilities
(AVAs) -- foundational skills like localization, depth estimation, and spatial
understanding that collectively support complex visual reasoning tasks. By
decoupling AVAs and matching training and test distributions within each,
AVA-Bench pinpoints exactly where a VFM excels or falters. Applying AVA-Bench
to leading VFMs thus reveals distinctive "ability fingerprints," turning VFM
selection from educated guesswork into principled engineering. Notably, we find
that a 0.5B LLM yields similar VFM rankings as a 7B LLM while cutting GPU hours
by 8x, enabling more efficient evaluation. By offering a comprehensive and
transparent benchmark, we hope AVA-Bench lays the foundation for the next
generation of VFMs.

</details>


### [181] [BakuFlow: A Streamlining Semi-Automatic Label Generation Tool](https://arxiv.org/abs/2506.09083)
*Jerry Lin,Partick P. W. Chen*

Main category: cs.CV

TL;DR: BakuFlow是一款半自动标注工具，通过像素级手动修正、交互式数据增强、标签传播及改进的YOLOE自动标注模块，显著降低计算机视觉任务中大规模数据标注的工作量。


<details>
  <summary>Details</summary>
Motivation: 计算机视觉中数据标注仍是瓶颈，传统工具依赖人工逐帧标注，耗时且易错，需开发高效、灵活的标注工具以应对动态现实场景需求。

Method: 结合可实时调节的放大镜（像素级修正）、交互式数据增强模块、跨帧标签传播技术，并扩展YOLOE框架支持动态添加新类别和视觉提示，实现半自动标注流程。

Result: BakuFlow在目标检测与跟踪任务中大幅减少人工标注时间，提升视频数据标注效率，且支持动态数据集扩展，适用于工业级场景。

Conclusion: BakuFlow通过多模块协同优化，解决了传统标注工具灵活性不足、效率低的问题，为大规模计算机视觉任务提供了实用化标注解决方案。

Abstract: Accurately labeling (or annotation) data is still a bottleneck in computer
vision, especially for large-scale tasks where manual labeling is
time-consuming and error-prone. While tools like LabelImg can handle the
labeling task, some of them still require annotators to manually label each
image. In this paper, we introduce BakuFlow, a streamlining semi-automatic
label generation tool. Key features include (1) a live adjustable magnifier for
pixel-precise manual corrections, improving user experience; (2) an interactive
data augmentation module to diversify training datasets; (3) label propagation
for rapidly copying labeled objects between consecutive frames, greatly
accelerating annotation of video data; and (4) an automatic labeling module
powered by a modified YOLOE framework. Unlike the original YOLOE, our extension
supports adding new object classes and any number of visual prompts per class
during annotation, enabling flexible and scalable labeling for dynamic,
real-world datasets. These innovations make BakuFlow especially effective for
object detection and tracking, substantially reducing labeling workload and
improving efficiency in practical computer vision and industrial scenarios.

</details>


### [182] [CAIRe: Cultural Attribution of Images by Retrieval-Augmented Evaluation](https://arxiv.org/abs/2506.09109)
*Arnav Yayavaram,Siddharth Yayavaram,Simran Khanuja,Michael Saxon,Graham Neubig*

Main category: cs.CV

TL;DR: 提出CAIRe评估指标，用于衡量文本到图像模型的文化相关性，在多个数据集上表现优于基线，并与人类评分高度一致。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像模型在跨文化场景中存在性能下降、事实错误或冒犯性输出等问题，且缺乏可靠的文化偏见评估方法，阻碍了改进进展。

Method: CAIRe通过将图像中的实体和概念关联到知识库，利用事实信息为每个文化标签提供独立的分级判断，并构建包含文化显著但罕见项的手动标注数据集。

Result: CAIRe在手动标注数据集上F1分数比基线高28%，在生成和自然数据集的5级文化相关性评估中，与人类评分的皮尔逊相关系数分别达0.56和0.66。

Conclusion: CAIRe能有效评估图像文化相关性，与人类判断高度一致，为跨文化模型偏差检测和优化提供了可靠工具。

Abstract: As text-to-image models become increasingly prevalent, ensuring their
equitable performance across diverse cultural contexts is critical. Efforts to
mitigate cross-cultural biases have been hampered by trade-offs, including a
loss in performance, factual inaccuracies, or offensive outputs. Despite
widespread recognition of these challenges, an inability to reliably measure
these biases has stalled progress. To address this gap, we introduce CAIRe, a
novel evaluation metric that assesses the degree of cultural relevance of an
image, given a user-defined set of labels. Our framework grounds entities and
concepts in the image to a knowledge base and uses factual information to give
independent graded judgments for each culture label. On a manually curated
dataset of culturally salient but rare items built using language models, CAIRe
surpasses all baselines by 28% F1 points. Additionally, we construct two
datasets for culturally universal concept, one comprising of T2I-generated
outputs and another retrieved from naturally occurring data. CAIRe achieves
Pearson's correlations of 0.56 and 0.66 with human ratings on these sets, based
on a 5-point Likert scale of cultural relevance. This demonstrates its strong
alignment with human judgment across diverse image sources.

</details>


### [183] [Autoregressive Adversarial Post-Training for Real-Time Interactive Video Generation](https://arxiv.org/abs/2506.09350)
*Shanchuan Lin,Ceyuan Yang,Hao He,Jianwen Jiang,Yuxi Ren,Xin Xia,Yang Zhao,Xuefeng Xiao,Lu Jiang*

Main category: cs.CV

TL;DR: 提出自回归对抗后训练（AAPT）方法，将预训练视频扩散模型转换为实时交互式视频生成器，实现单步生成潜在帧，在H100显卡上达到24fps高分辨率流式生成。


<details>
  <summary>Details</summary>
Motivation: 现有大规模视频生成模型计算成本过高，无法满足实时交互需求，需开发高效生成方法以支持流式传输与用户交互控制。

Method: 采用对抗训练框架进行自回归生成，设计单步神经网络评估架构并充分利用KV缓存，通过学生强制训练策略减少长视频生成中的误差累积。

Result: 8B参数模型在单H100上实现736x416分辨率24fps实时生成，8xH100集群支持1280x720分辨率长达1440帧（1分钟）的流式生成。

Conclusion: AAPT通过对抗训练范式有效平衡生成效率与质量，首次实现高分辨率长视频的实时交互式生成，为实际应用提供可行解决方案。

Abstract: Existing large-scale video generation models are computationally intensive,
preventing adoption in real-time and interactive applications. In this work, we
propose autoregressive adversarial post-training (AAPT) to transform a
pre-trained latent video diffusion model into a real-time, interactive video
generator. Our model autoregressively generates a latent frame at a time using
a single neural function evaluation (1NFE). The model can stream the result to
the user in real time and receive interactive responses as controls to generate
the next latent frame. Unlike existing approaches, our method explores
adversarial training as an effective paradigm for autoregressive generation.
This not only allows us to design an architecture that is more efficient for
one-step generation while fully utilizing the KV cache, but also enables
training the model in a student-forcing manner that proves to be effective in
reducing error accumulation during long video generation. Our experiments
demonstrate that our 8B model achieves real-time, 24fps, streaming video
generation at 736x416 resolution on a single H100, or 1280x720 on 8xH100 up to
a minute long (1440 frames). Visit our research website at
https://seaweed-apt.com/2

</details>


### [184] [SAGE: Exploring the Boundaries of Unsafe Concept Domain with Semantic-Augment Erasing](https://arxiv.org/abs/2506.09363)
*Hongguang Zhu,Yunchao Wei,Mengyu Wang,Siyu Jiao,Yan Fang,Jiannan Huang,Yao Zhao*

Main category: cs.CV

TL;DR: 本文提出SAGE方法，通过语义增强擦除和全局-局部协作保留机制，解决扩散模型在安全生成中因固定词汇擦除导致的泛化能力不足问题，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 扩散模型预训练时可能包含敏感信息，导致生成不安全内容或侵权风险。现有概念擦除方法将不安全概念视为固定词汇反复擦除，陷入“词汇概念深渊”，无法实现广义概念相关擦除。

Method: 提出语义增强擦除（将词汇擦除转化为领域擦除）和全局-局部协作保留机制，前者通过自检和自擦除探索概念域边界，后者结合全局语义对齐与局部噪声保留以扩大无关概念保留范围。

Result: 实验表明SAGE在扩散模型安全生成方面全面优于其他方法，且无需额外预处理数据。

Conclusion: SAGE通过突破固定词汇擦除限制并增强无关概念泛化能力，有效提升扩散模型生成安全性，代码和权重将开源。

Abstract: Diffusion models (DMs) have achieved significant progress in text-to-image
generation. However, the inevitable inclusion of sensitive information during
pre-training poses safety risks, such as unsafe content generation and
copyright infringement. Concept erasing finetunes weights to unlearn
undesirable concepts, and has emerged as a promising solution. However,
existing methods treat unsafe concept as a fixed word and repeatedly erase it,
trapping DMs in ``word concept abyss'', which prevents generalized
concept-related erasing. To escape this abyss, we introduce semantic-augment
erasing which transforms concept word erasure into concept domain erasure by
the cyclic self-check and self-erasure. It efficiently explores and unlearns
the boundary representation of concept domain through semantic spatial
relationships between original and training DMs, without requiring additional
preprocessed data. Meanwhile, to mitigate the retention degradation of
irrelevant concepts while erasing unsafe concepts, we further propose the
global-local collaborative retention mechanism that combines global semantic
relationship alignment with local predicted noise preservation, effectively
expanding the retentive receptive field for irrelevant concepts. We name our
method SAGE, and extensive experiments demonstrate the comprehensive
superiority of SAGE compared with other methods in the safe generation of DMs.
The code and weights will be open-sourced at
https://github.com/KevinLight831/SAGE.

</details>


### [185] [Revisit What You See: Disclose Language Prior in Vision Tokens for Efficient Guided Decoding of LVLMs](https://arxiv.org/abs/2506.09522)
*Beomsik Cho,Jaehyung Kim*

Main category: cs.CV

TL;DR: 提出ReVisiT解码方法，通过引用视觉标记引导大型视觉语言模型生成，无需额外训练即可增强视觉语义整合，显著减少计算成本。


<details>
  <summary>Details</summary>
Motivation: 传统大型视觉语言模型解码策略未能有效利用视觉信息，导致生成结果缺乏视觉基础，现有改进方法需额外训练或依赖外部模型，存在效率与实用性限制。

Method: 将视觉标记投影至文本标记分布空间，通过约束差异最小化动态选择最相关视觉标记，并利用其优化输出分布以融合视觉语义。

Result: 在三个LVLM幻觉基准测试中，ReVisiT以最低计算开销持续提升视觉基础性，性能达到或超越现有最优基线，计算成本降低至2倍。

Conclusion: ReVisiT通过简单高效的解码机制有效增强视觉语言模型的视觉语义整合能力，在性能与计算效率间实现优越平衡。

Abstract: Large Vision-Language Models (LVLMs) have demonstrated remarkable performance
across various multimodal tasks by integrating visual perception with language
understanding. However, conventional decoding strategies of LVLMs often fail to
successfully utilize visual information, leading to visually ungrounded
responses. While various approaches have been proposed to address this
limitation, they typically require additional training, multi-step inference
procedures, or external model dependencies. This paper introduces ReVisiT, a
simple yet effective decoding method that references vision tokens to guide the
text generation process in LVLMs. Our approach leverages the semantic
information embedded within vision tokens by projecting them into the text
token distribution space, and dynamically selecting the most relevant vision
token at each decoding step through constrained divergence minimization. This
selected vision token is then used to refine the output distribution to better
incorporate visual semantics. Experiments on three LVLM hallucination
benchmarks with two recent LVLMs demonstrate that ReVisiT consistently enhances
visual grounding with minimal computational overhead. Moreover, our method
achieves competitive or superior results relative to state-of-the-art baselines
while reducing computational costs for up to $2\times$.

</details>


### [186] [Synthetic Human Action Video Data Generation with Pose Transfer](https://arxiv.org/abs/2506.09411)
*Vaclav Knapp,Matyas Bohacek*

Main category: cs.CV

TL;DR: 提出一种基于可控3D高斯虚拟模型的姿态迁移方法，用于生成更自然的合成人类动作视频数据，提升动作识别任务性能，并开源方法及RANDOM People数据集。


<details>
  <summary>Details</summary>
Motivation: 现有合成数据在人体动作任务（如手语翻译、自动驾驶动作理解）中因不自然特征导致训练效果受限，需改进合成数据生成方法以释放其潜力。

Method: 利用姿态迁移技术（可控3D高斯虚拟模型）生成合成人类动作视频，并通过众包创建包含多样化身份与背景的RANDOM People数据集。

Result: 在Toyota Smarthome和NTU RGB+D数据集上验证，该方法提升了动作识别性能，并能扩展少样本数据集，弥补数据不足群体并增强背景多样性。

Conclusion: 所提方法有效解决合成数据不自然问题，提升模型训练效果，同时通过开源促进合成数据在视频理解任务中的广泛应用。

Abstract: In video understanding tasks, particularly those involving human motion,
synthetic data generation often suffers from uncanny features, diminishing its
effectiveness for training. Tasks such as sign language translation, gesture
recognition, and human motion understanding in autonomous driving have thus
been unable to exploit the full potential of synthetic data. This paper
proposes a method for generating synthetic human action video data using pose
transfer (specifically, controllable 3D Gaussian avatar models). We evaluate
this method on the Toyota Smarthome and NTU RGB+D datasets and show that it
improves performance in action recognition tasks. Moreover, we demonstrate that
the method can effectively scale few-shot datasets, making up for groups
underrepresented in the real training data and adding diverse backgrounds. We
open-source the method along with RANDOM People, a dataset with videos and
avatars of novel human identities for pose transfer crowd-sourced from the
internet.

</details>


### [187] [A High-Quality Dataset and Reliable Evaluation for Interleaved Image-Text Generation](https://arxiv.org/abs/2506.09427)
*Yukang Feng,Jianwen Sun,Chuanhao Li,Zizhen Li,Jiaxin Ai,Fanrui Zhang,Yifan Chang,Sizhuo Zhou,Shenglin Zhang,Yu Dai,Kaipeng Zhang*

Main category: cs.CV

TL;DR: 提出InterSyn数据集和SynJudge评估模型，通过自评估迭代优化方法提升多模态模型生成交织图文的能力，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有大型多模态模型（LMMs）因训练数据规模、质量低及指令丰富性不足，难以生成紧密交织的图文内容，且缺乏可靠的多模态输出评估工具。

Method: 采用自评估迭代优化方法（SEIR）构建大规模多模态数据集InterSyn，并设计自动评估模型SynJudge，涵盖文本内容、图像内容、图像-文本协同等四维度评估。

Result: SEIR显著提升数据集质量，基于InterSyn训练的LMMs在所有评估指标上均取得性能提升，验证数据集对多模态系统的推进作用。

Conclusion: InterSyn与SynJudge分别解决了多模态数据生成与评估的瓶颈，为下一代指令驱动型LMMs的发展提供了数据基础与量化评估框架。

Abstract: Recent advancements in Large Multimodal Models (LMMs) have significantly
improved multimodal understanding and generation. However, these models still
struggle to generate tightly interleaved image-text outputs, primarily due to
the limited scale, quality and instructional richness of current training
datasets. To address this, we introduce InterSyn, a large-scale multimodal
dataset constructed using our Self-Evaluation with Iterative Refinement (SEIR)
method. InterSyn features multi-turn, instruction-driven dialogues with tightly
interleaved imagetext responses, providing rich object diversity and rigorous
automated quality refinement, making it well-suited for training
next-generation instruction-following LMMs. Furthermore, to address the lack of
reliable evaluation tools capable of assessing interleaved multimodal outputs,
we introduce SynJudge, an automatic evaluation model designed to quantitatively
assess multimodal outputs along four dimensions: text content, image content,
image quality, and image-text synergy.
  Experimental studies show that the SEIR method leads to substantially higher
dataset quality compared to an otherwise identical process without refinement.
  Moreover, LMMs trained on InterSyn achieve uniform performance gains across
all evaluation metrics, confirming InterSyn's utility for advancing multimodal
systems.

</details>


### [188] [Adding simple structure at inference improves Vision-Language Compositionality](https://arxiv.org/abs/2506.09691)
*Imanol Miranda,Ander Salaberria,Eneko Agirre,Gorka Azkune*

Main category: cs.CV

TL;DR: 本文提出一种无需训练的推理时优化方法，通过图像分块与文本片段匹配提升双编码器视觉语言模型（如CLIP）的组合性检索性能。


<details>
  <summary>Details</summary>
Motivation: 现有双编码器视觉语言模型在组合性任务中表现受限，且针对推理阶段的优化方法研究不足。

Method: 将图像分割为小块，提取文本中的对象/属性/关系片段，通过VLM对齐图像块与文本片段，聚合局部相似度计算全局匹配得分。

Result: 在多个数据集上显著提升模型组合性能力（尤其属性-对象绑定任务），且证明图像分块处理对性能增益起关键作用。

Conclusion: 推理时结构优化是提升视觉语言模型组合性的有效途径，未来可通过改进文本分割与对齐策略进一步突破性能瓶颈。

Abstract: Dual encoder Vision-Language Models (VLM) such as CLIP are widely used for
image-text retrieval tasks. However, those models struggle with
compositionality, showing a bag-of-words-like behavior that limits their
retrieval performance. Many different training approaches have been proposed to
improve the vision-language compositionality capabilities of those models. In
comparison, inference-time techniques have received little attention. In this
paper, we propose to add simple structure at inference, where, given an image
and a caption: i) we divide the image into different smaller crops, ii) we
extract text segments, capturing objects, attributes and relations, iii) using
a VLM, we find the image crops that better align with text segments obtaining
matches, and iv) we compute the final image-text similarity aggregating the
individual similarities of the matches. Based on various popular dual encoder
VLMs, we evaluate our approach in controlled and natural datasets for VL
compositionality. We find that our approach consistently improves the
performance of evaluated VLMs without any training, which shows the potential
of inference-time techniques. The results are especially good for
attribute-object binding as shown in the controlled dataset. As a result of an
extensive analysis: i) we show that processing image crops is actually
essential for the observed gains in performance, and ii) we identify specific
areas to further improve inference-time approaches.

</details>


### [189] [TOGA: Temporally Grounded Open-Ended Video QA with Weak Supervision](https://arxiv.org/abs/2506.09445)
*Ayush Gupta,Anirban Roy,Rama Chellappa,Nathaniel D. Bastian,Alvaro Velasquez,Susmit Jha*

Main category: cs.CV

TL;DR: 提出TOGA模型，在弱监督下联合生成视频问答的开放回答及时间定位，通过伪标签和一致性约束提升性能，在多个基准测试中达到SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有视频问答任务需要时间标注数据，但标注成本高。本文研究无需时间标注的弱监督方法，使模型能回答开放问题并自动定位相关视频片段。

Method: 使用指令调优的TOGA模型，联合生成答案与时间定位；通过生成伪时间标签，并利用问题-回答对的一致性约束确保伪标签有效性。

Result: 在NExT-GQA（时间定位QA）及MSVD-QA/ActivityNet-QA（开放QA）上均取得最优结果，且联合训练策略同时提升问答和定位性能。

Conclusion: 弱监督下联合生成答案与时间定位的方法有效，伪标签一致性约束是关键，为视频理解任务提供了高效且可扩展的解决方案。

Abstract: We address the problem of video question answering (video QA) with temporal
grounding in a weakly supervised setup, without any temporal annotations. Given
a video and a question, we generate an open-ended answer grounded with the
start and end time. For this task, we propose TOGA: a vision-language model for
Temporally Grounded Open-Ended Video QA with Weak Supervision. We instruct-tune
TOGA to jointly generate the answer and the temporal grounding. We operate in a
weakly supervised setup where the temporal grounding annotations are not
available. We generate pseudo labels for temporal grounding and ensure the
validity of these labels by imposing a consistency constraint between the
question of a grounding response and the response generated by a question
referring to the same temporal segment. We notice that jointly generating the
answers with the grounding improves performance on question answering as well
as grounding. We evaluate TOGA on grounded QA and open-ended QA tasks. For
grounded QA, we consider the NExT-GQA benchmark which is designed to evaluate
weakly supervised grounded question answering. For open-ended QA, we consider
the MSVD-QA and ActivityNet-QA benchmarks. We achieve state-of-the-art
performance for both tasks on these benchmarks.

</details>


### [190] [Outside Knowledge Conversational Video (OKCV) Dataset -- Dialoguing over Videos](https://arxiv.org/abs/2506.09953)
*Benjamin Reichman,Constantin Patsch,Jack Truxal,Atishay Jain,Larry Heck*

Main category: cs.CV

TL;DR: 本文提出了一种基于视频的视觉对话任务扩展，要求模型结合时序视觉信息与外部知识进行多轮对话，并发布了包含2017个视频和5986个人工标注对话的新数据集OKCV。


<details>
  <summary>Details</summary>
Motivation: 现有OK-VQA任务局限于静态图像和单轮问答，无法处理视频时序信息及多轮对话场景。需要构建能同时处理视觉时序信息、对话上下文和外部知识融合的对话系统。

Method: 构建包含5,986个多轮对话的数据集，每个对话与特定视频片段关联且需外部知识回答。提供基线模型评估框架，要求模型实现视频片段识别与知识库检索的联合推理。

Result: 发布了包含40,954个对话轮次的数据集，基线实验表明现有方法在视频时序理解与外部知识整合方面仍存在显著挑战。

Conclusion: 视频对话场景下的外部知识问答需要新型跨模态推理架构，该数据集为开发时序视觉-语言联合推理系统提供了基准测试平台。

Abstract: In outside knowledge visual question answering (OK-VQA), the model must
identify relevant visual information within an image and incorporate external
knowledge to accurately respond to a question. Extending this task to a
visually grounded dialogue setting based on videos, a conversational model must
both recognize pertinent visual details over time and answer questions where
the required information is not necessarily present in the visual information.
Moreover, the context of the overall conversation must be considered for the
subsequent dialogue. To explore this task, we introduce a dataset comprised of
$2,017$ videos with $5,986$ human-annotated dialogues consisting of $40,954$
interleaved dialogue turns. While the dialogue context is visually grounded in
specific video segments, the questions further require external knowledge that
is not visually present. Thus, the model not only has to identify relevant
video parts but also leverage external knowledge to converse within the
dialogue. We further provide several baselines evaluated on our dataset and
show future challenges associated with this task. The dataset is made publicly
available here: https://github.com/c-patsch/OKCV.

</details>


### [191] [Unify Graph Learning with Text: Unleashing LLM Potentials for Session Search](https://arxiv.org/abs/2505.14156)
*Songhao Wu,Quan Tu,Hong Liu,Jia Xu,Zhongyi Liu,Guannan Zhang,Ran Wang,Xiuying Chen,Rui Yan*

Main category: cs.CV

TL;DR: 本文提出符号图排序器（SGR），通过结合基于文本和图结构的方法，利用大语言模型（LLM）的优势，将会话图转换为文本，并引入自监督符号学习任务，提升模型对图结构的理解能力。实验证明该方法在AOL和Tiangong-ST数据集上表现优越。


<details>
  <summary>Details</summary>
Motivation: 现有会话搜索方法多关注顺序建模或广义图结构，但前者忽略交互中的图结构，后者缺乏词级语义建模。需结合两者优势，并解决LLM与符号化语言之间的差异。

Method: SGR通过符号语法规则将会话图转化为文本，整合历史、交互和任务指令作为LLM输入；设计自监督符号学习任务（链接预测、节点生成、生成对比学习），从粗到细捕获图拓扑信息。

Result: 在AOL和Tiangong-ST数据集上的实验与综合分析表明，SGR优于现有方法，验证了其有效性。

Conclusion: 该研究为传统搜索策略与现代LLM的融合提供了新范式，通过符号化学习弥合文本与图结构之间的语义鸿沟。

Abstract: Session search involves a series of interactive queries and actions to
fulfill user's complex information need. Current strategies typically
prioritize sequential modeling for deep semantic understanding, overlooking the
graph structure in interactions. While some approaches focus on capturing
structural information, they use a generalized representation for documents,
neglecting the word-level semantic modeling. In this paper, we propose Symbolic
Graph Ranker (SGR), which aims to take advantage of both text-based and
graph-based approaches by leveraging the power of recent Large Language Models
(LLMs). Concretely, we first introduce a set of symbolic grammar rules to
convert session graph into text. This allows integrating session history,
interaction process, and task instruction seamlessly as inputs for the LLM.
Moreover, given the natural discrepancy between LLMs pre-trained on textual
corpora, and the symbolic language we produce using our graph-to-text grammar,
our objective is to enhance LLMs' ability to capture graph structures within a
textual format. To achieve this, we introduce a set of self-supervised symbolic
learning tasks including link prediction, node content generation, and
generative contrastive learning, to enable LLMs to capture the topological
information from coarse-grained to fine-grained. Experiment results and
comprehensive analysis on two benchmark datasets, AOL and Tiangong-ST, confirm
the superiority of our approach. Our paradigm also offers a novel and effective
methodology that bridges the gap between traditional search strategies and
modern LLMs.

</details>


### [192] [AD^2-Bench: A Hierarchical CoT Benchmark for MLLM in Autonomous Driving under Adverse Conditions](https://arxiv.org/abs/2506.09557)
*Zhaoyang Wei,Chenhui Qiang,Bowen Jiang,Xumeng Han,Xuehui Yu,Zhenjun Han*

Main category: cs.CV

TL;DR: 本文提出了首个针对恶劣天气和复杂场景下自动驾驶的思维链（CoT）评测基准AD²-Bench，包含5.4k人工标注实例，测试发现主流多模态大模型准确率不足60%，揭示了该领域推理能力的不足。


<details>
  <summary>Details</summary>
Motivation: 现有基准缺乏对自动驾驶中恶劣天气/复杂场景下思维链推理过程的细粒度评估，制约了端到端自动驾驶系统的鲁棒性和可解释性研究。

Method: 构建AD²-Bench基准：1)覆盖多类极端场景数据 2)支持多步推理的细粒度标注 3)专为CoT设计的评估框架，通过文本/点/区域级视觉提示分析模型推理过程。

Result: 主流多模态大模型在AD²-Bench上的准确率低于60%，暴露出在复杂自动驾驶场景中多步推理能力的显著缺陷。

Conclusion: AD²-Bench为自动驾驶推理能力提供了标准化评测平台，其高难度特性将推动多模态大模型在复杂环境下的可解释性推理研究。

Abstract: Chain-of-Thought (CoT) reasoning has emerged as a powerful approach to
enhance the structured, multi-step decision-making capabilities of Multi-Modal
Large Models (MLLMs), is particularly crucial for autonomous driving with
adverse weather conditions and complex traffic environments. However, existing
benchmarks have largely overlooked the need for rigorous evaluation of CoT
processes in these specific and challenging scenarios. To address this critical
gap, we introduce AD^2-Bench, the first Chain-of-Thought benchmark specifically
designed for autonomous driving with adverse weather and complex scenes.
AD^2-Bench is meticulously constructed to fulfill three key criteria:
comprehensive data coverage across diverse adverse environments, fine-grained
annotations that support multi-step reasoning, and a dedicated evaluation
framework tailored for assessing CoT performance. The core contribution of
AD^2-Bench is its extensive collection of over 5.4k high-quality, manually
annotated CoT instances. Each intermediate reasoning step in these annotations
is treated as an atomic unit with explicit ground truth, enabling unprecedented
fine-grained analysis of MLLMs' inferential processes under text-level,
point-level, and region-level visual prompts. Our comprehensive evaluation of
state-of-the-art MLLMs on AD^2-Bench reveals accuracy below 60%, highlighting
the benchmark's difficulty and the need to advance robust, interpretable
end-to-end autonomous driving systems. AD^2-Bench thus provides a standardized
evaluation platform, driving research forward by improving MLLMs' reasoning in
autonomous driving, making it an invaluable resource.

</details>


### [193] [BG-HOP: A Bimanual Generative Hand-Object Prior](https://arxiv.org/abs/2506.09068)
*Sriram Krishna,Sravan Chittupalli,Sungjae Park*

Main category: cs.CV

TL;DR: 本文提出BG-HOP生成先验模型，通过扩展单手机器学习先验解决双手-物体3D交互建模数据不足的问题，实现双手交互生成与物体抓取合成。


<details>
  <summary>Details</summary>
Motivation: 现有双手交互数据稀缺，难以直接建模双手与物体的联合分布。研究旨在利用单手机器学习先验扩展，填补双手交互生成领域的空白。

Method: 基于现有单手机器学习生成先验模型进行扩展开发，构建联合分布建模框架BG-HOP，捕捉双手与物体的协同运动关系。

Result: 实验证明模型能生成逼真的双手交互动作，并为指定物体合成合理抓取姿态，同时公开了代码与模型参数。

Conclusion: BG-HOP验证了通过扩展单手机器学习先验实现双手交互建模的可行性，为后续研究提供了数据生成工具和基准模型。

Abstract: In this work, we present BG-HOP, a generative prior that seeks to model
bimanual hand-object interactions in 3D. We address the challenge of limited
bimanual interaction data by extending existing single-hand generative priors,
demonstrating preliminary results in capturing the joint distribution of hands
and objects. Our experiments showcase the model's capability to generate
bimanual interactions and synthesize grasps for given objects. We make code and
models publicly available.

</details>


### [194] [HSENet: Hybrid Spatial Encoding Network for 3D Medical Vision-Language Understanding](https://arxiv.org/abs/2506.09634)
*Yanzhao Shi,Xiaodan Zhang,Junzhong Ji,Haoning Jiang,Chengxin Zheng,Yinong Wang,Liangqiong Qu*

Main category: cs.CV

TL;DR: 本文提出HSENet框架，通过双3D视觉编码器与空间压缩机制，解决现有多模态大语言模型在3D医学图像理解中的局限性，显著提升诊断报告生成、视觉问答等任务的性能。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大语言模型主要针对2D医学图像，无法捕捉复杂3D解剖结构，易导致误诊与诊断幻觉。需开发能有效处理3D医学视觉信息的框架以提升诊断准确性。

Method: HSENet采用双3D视觉编码器分别提取全局体积上下文与细粒度解剖细节，并通过预训练与诊断报告双阶段对齐。提出基于质心压缩的Spatial Packer模块，将高分辨率3D区域压缩为紧凑视觉标记，实现高效跨模态语义空间映射。

Result: 在3D视觉语言检索（R@100提升5.96%）、3D医学报告生成（BLEU-4提升8.01%）及3D视觉问答（主要类别准确率提升1.99%）中均达到SOTA，验证方法有效性。

Conclusion: HSENet通过混合3D视觉表征与高效空间压缩机制，显著增强多模态模型对3D医学图像的理解能力，为自动化3D CT诊断提供可靠技术支撑。

Abstract: Automated 3D CT diagnosis empowers clinicians to make timely, evidence-based
decisions by enhancing diagnostic accuracy and workflow efficiency. While
multimodal large language models (MLLMs) exhibit promising performance in
visual-language understanding, existing methods mainly focus on 2D medical
images, which fundamentally limits their ability to capture complex 3D
anatomical structures. This limitation often leads to misinterpretation of
subtle pathologies and causes diagnostic hallucinations. In this paper, we
present Hybrid Spatial Encoding Network (HSENet), a framework that exploits
enriched 3D medical visual cues by effective visual perception and projection
for accurate and robust vision-language understanding. Specifically, HSENet
employs dual-3D vision encoders to perceive both global volumetric contexts and
fine-grained anatomical details, which are pre-trained by dual-stage alignment
with diagnostic reports. Furthermore, we propose Spatial Packer, an efficient
multimodal projector that condenses high-resolution 3D spatial regions into a
compact set of informative visual tokens via centroid-based compression. By
assigning spatial packers with dual-3D vision encoders, HSENet can seamlessly
perceive and transfer hybrid visual representations to LLM's semantic space,
facilitating accurate diagnostic text generation. Experimental results
demonstrate that our method achieves state-of-the-art performance in 3D
language-visual retrieval (39.85% of R@100, +5.96% gain), 3D medical report
generation (24.01% of BLEU-4, +8.01% gain), and 3D visual question answering
(73.60% of Major Class Accuracy, +1.99% gain), confirming its effectiveness.
Our code is available at https://github.com/YanzhaoShi/HSENet.

</details>


### [195] [DGAE: Diffusion-Guided Autoencoder for Efficient Latent Representation Learning](https://arxiv.org/abs/2506.09644)
*Dongxu Liu,Yuang Peng,Haomiao Tang,Yuwei Chen,Chunrui Han,Zheng Ge,Daxin Jiang,Mingxue Liao*

Main category: cs.CV

TL;DR: 本文提出DGAE方法，通过扩散模型引导解码器恢复潜在表征中未完全解码的信息，有效缓解高空间压缩率下的性能下降问题，同时将潜在空间维度缩小2倍，并在ImageNet-1K图像任务中实现SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有自编码器在高压缩率下存在性能下降及由GAN引起的训练不稳定问题，且需在提升空间压缩效率的同时最小化潜在空间维度以实现更紧凑表征。

Method: 提出DGAE框架，利用扩散模型增强解码器的表达能力，指导解码器从潜在表征中恢复未被充分解码的信息信号。

Result: DGAE在高压缩率下性能损失显著降低，潜在空间缩小2倍且达到SOTA；与扩散模型结合时，在ImageNet-1K生成任务中表现优异，并加速扩散模型收敛。

Conclusion: DGAE通过增强解码器表达能力，成功平衡了高压缩率与表征效率，为生成模型提供了更紧凑、鲁棒的潜在空间解决方案。

Abstract: Autoencoders empower state-of-the-art image and video generative models by
compressing pixels into a latent space through visual tokenization. Although
recent advances have alleviated the performance degradation of autoencoders
under high compression ratios, addressing the training instability caused by
GAN remains an open challenge. While improving spatial compression, we also aim
to minimize the latent space dimensionality, enabling more efficient and
compact representations. To tackle these challenges, we focus on improving the
decoder's expressiveness. Concretely, we propose DGAE, which employs a
diffusion model to guide the decoder in recovering informative signals that are
not fully decoded from the latent representation. With this design, DGAE
effectively mitigates the performance degradation under high spatial
compression rates. At the same time, DGAE achieves state-of-the-art performance
with a 2x smaller latent space. When integrated with Diffusion Models, DGAE
demonstrates competitive performance on image generation for ImageNet-1K and
shows that this compact latent representation facilitates faster convergence of
the diffusion model.

</details>


### [196] [Reasoning Models Are More Easily Gaslighted Than You Think](https://arxiv.org/abs/2506.09677)
*Bin Zhu,Hailong Yin,Jingjing Chen,Yu-Gang Jiang*

Main category: cs.CV

TL;DR: 研究评估了三种先进多模态推理模型在误导性用户输入下的表现，发现其准确率平均下降25-29%，并构建GaslightingBench-R基准进一步揭示模型信念坚持的脆弱性。


<details>
  <summary>Details</summary>
Motivation: 现有推理模型虽在鲁棒性上有所提升，但其抵御误导性用户输入的能力尚未被充分探索，需系统性评估模型在对抗反馈下的表现。

Method: 在MMMU、MathVista和CharXiv三个多模态基准上测试OpenAI、Claude和Gemini模型，并构建包含1025个样本的GaslightingBench-R诊断基准。

Result: 模型在否定提示下平均准确率下降25-29%，而GaslightingBench-R导致准确率进一步暴跌超53%，暴露模型信念维持能力缺陷。

Conclusion: 推理模型存在信念坚持的根本性缺陷，逐步推理能力与对抗干扰时的信念稳定性之间存在显著鸿沟。

Abstract: Recent advances in reasoning-centric models promise improved robustness
through mechanisms such as chain-of-thought prompting and test-time scaling.
However, their ability to withstand misleading user input remains
underexplored. In this paper, we conduct a systematic evaluation of three
state-of-the-art reasoning models, i.e., OpenAI's o4-mini, Claude-3.7-Sonnet
and Gemini-2.5-Flash, across three multimodal benchmarks: MMMU, MathVista, and
CharXiv. Our evaluation reveals significant accuracy drops (25-29% on average)
following gaslighting negation prompts, indicating that even top-tier reasoning
models struggle to preserve correct answers under manipulative user feedback.
Built upon the insights of the evaluation and to further probe this
vulnerability, we introduce GaslightingBench-R, a new diagnostic benchmark
specifically designed to evaluate reasoning models' susceptibility to defend
their belief under gaslighting negation prompt. Constructed by filtering and
curating 1,025 challenging samples from the existing benchmarks,
GaslightingBench-R induces even more dramatic failures, with accuracy drops
exceeding 53% on average. Our findings reveal fundamental limitations in the
robustness of reasoning models, highlighting the gap between step-by-step
reasoning and belief persistence.

</details>


### [197] [Towards Practical Alzheimer's Disease Diagnosis: A Lightweight and Interpretable Spiking Neural Model](https://arxiv.org/abs/2506.09695)
*Changwei Wu,Yifei Chen,Yuxin Du,Jinying Zong,Jie Dong,Mingxuan Liu,Yong Peng,Jin Fan,Feiwei Qin,Changmiao Wang*

Main category: cs.CV

TL;DR: 提出FasterSNN混合神经架构，通过整合LIF神经元、区域自适应卷积与多尺度脉冲注意力，实现高效、稳定的3D MRI处理，用于阿尔茨海默病筛查。


<details>
  <summary>Details</summary>
Motivation: 现有阿尔茨海默病早期诊断依赖主观评估与高成本多模态成像，传统深度学习方法能效低且计算需求大，而现有脉冲神经网络（SNN）存在表达能力弱与训练不稳定问题。

Method: 结合生物启发的LIF神经元、区域自适应卷积及多尺度脉冲注意力机制，设计稀疏高效的3D MRI处理架构FasterSNN。

Result: 在基准数据集上达到竞争性性能，显著提升效率与训练稳定性，支持实际低功耗医疗诊断应用。

Conclusion: FasterSNN证明了脉冲神经网络在复杂医疗任务中的潜力，为资源受限环境提供可行的AD筛查解决方案。

Abstract: Early diagnosis of Alzheimer's Disease (AD), especially at the mild cognitive
impairment (MCI) stage, is vital yet hindered by subjective assessments and the
high cost of multimodal imaging modalities. Although deep learning methods
offer automated alternatives, their energy inefficiency and computational
demands limit real-world deployment, particularly in resource-constrained
settings. As a brain-inspired paradigm, spiking neural networks (SNNs) are
inherently well-suited for modeling the sparse, event-driven patterns of neural
degeneration in AD, offering a promising foundation for interpretable and
low-power medical diagnostics. However, existing SNNs often suffer from weak
expressiveness and unstable training, which restrict their effectiveness in
complex medical tasks. To address these limitations, we propose FasterSNN, a
hybrid neural architecture that integrates biologically inspired LIF neurons
with region-adaptive convolution and multi-scale spiking attention. This design
enables sparse, efficient processing of 3D MRI while preserving diagnostic
accuracy. Experiments on benchmark datasets demonstrate that FasterSNN achieves
competitive performance with substantially improved efficiency and stability,
supporting its potential for practical AD screening. Our source code is
available at https://github.com/wuchangw/FasterSNN.

</details>


### [198] [Bias Analysis in Unconditional Image Generative Models](https://arxiv.org/abs/2506.09106)
*Xiaofeng Zhang,Michelle Lin,Simon Lacoste-Julien,Aaron Courville,Yash Goyal*

Main category: cs.CV

TL;DR: 研究发现生成式AI模型在无条件生成中的属性偏移较小，但评估框架中的分类器敏感性可能影响结果，需改进标注方法并考虑社会复杂性。


<details>
  <summary>Details</summary>
Motivation: 生成式AI的广泛应用引发对代表性偏见和歧视性结果的担忧，但无条件生成中偏见产生的机制尚未明确。

Method: 通过训练无条件图像生成模型，采用偏差评估框架，分析训练分布与生成分布间的属性偏移，并考察分类器对结果的影响。

Result: 检测到的属性偏移较小，但分类器决策边界（尤其在高密度区域）对结果敏感，连续谱属性比二元属性更易受分类器影响。

Conclusion: 需建立更具代表性的标注实践，审慎评估框架的局限性，并在评估偏见时考虑属性的社会复杂性。

Abstract: The widespread adoption of generative AI models has raised growing concerns
about representational harm and potential discriminatory outcomes. Yet, despite
growing literature on this topic, the mechanisms by which bias emerges -
especially in unconditional generation - remain disentangled. We define the
bias of an attribute as the difference between the probability of its presence
in the observed distribution and its expected proportion in an ideal reference
distribution. In our analysis, we train a set of unconditional image generative
models and adopt a commonly used bias evaluation framework to study bias shift
between training and generated distributions. Our experiments reveal that the
detected attribute shifts are small. We find that the attribute shifts are
sensitive to the attribute classifier used to label generated images in the
evaluation framework, particularly when its decision boundaries fall in
high-density regions. Our empirical analysis indicates that this classifier
sensitivity is often observed in attributes values that lie on a spectrum, as
opposed to exhibiting a binary nature. This highlights the need for more
representative labeling practices, understanding the shortcomings through
greater scrutiny of evaluation frameworks, and recognizing the socially complex
nature of attributes when evaluating bias.

</details>


### [199] [Non-Contact Health Monitoring During Daily Personal Care Routines](https://arxiv.org/abs/2506.09718)
*Xulin Ma,Jiankai Tang,Zhang Jiang,Songqin Cheng,Yuanchun Shi,Dong LI,Xin Liu,Daniel McDuff,Xiaojing Liu,Yuntao Wang*

Main category: cs.CV

TL;DR: 本文提出首个长期高海拔日常健康监测数据集LADH，通过融合RGB与红外视频及多任务学习，显著提升了非接触式生理信号监测精度（心率MAE 4.99 BPM）。


<details>
  <summary>Details</summary>
Motivation: 远程光电容积描记术(rPPG)在长期个人护理场景（如高海拔镜前日常活动）中面临环境光变化、手部遮挡和动态面部姿态的挑战，需开发更鲁棒的监测方法。

Method: 构建包含21人240组同步RGB/红外面部视频的LADH数据集，结合多模态输入与多任务学习框架，实现心率、呼吸及血氧信号联合监测。

Result: RGB+IR融合使心率估计MAE达4.99 BPM，多任务学习有效提升多生理指标同步监测性能，数据集与代码已开源。

Conclusion: LADH数据集验证了多模态融合与多任务学习对长期rPPG监测的有效性，为高海拔日常健康监护提供了新解决方案。

Abstract: Remote photoplethysmography (rPPG) enables non-contact, continuous monitoring
of physiological signals and offers a practical alternative to traditional
health sensing methods. Although rPPG is promising for daily health monitoring,
its application in long-term personal care scenarios, such as mirror-facing
routines in high-altitude environments, remains challenging due to ambient
lighting variations, frequent occlusions from hand movements, and dynamic
facial postures. To address these challenges, we present LADH (Long-term
Altitude Daily Health), the first long-term rPPG dataset containing 240
synchronized RGB and infrared (IR) facial videos from 21 participants across
five common personal care scenarios, along with ground-truth PPG, respiration,
and blood oxygen signals. Our experiments demonstrate that combining RGB and IR
video inputs improves the accuracy and robustness of non-contact physiological
monitoring, achieving a mean absolute error (MAE) of 4.99 BPM in heart rate
estimation. Furthermore, we find that multi-task learning enhances performance
across multiple physiological indicators simultaneously. Dataset and code are
open at https://github.com/McJackTang/FusionVitals.

</details>


### [200] [Vision Matters: Simple Visual Perturbations Can Boost Multimodal Math Reasoning](https://arxiv.org/abs/2506.09736)
*Yuting Li,Lai Wei,Kaipeng Zheng,Jingyuan Huang,Linghe Kong,Lichao Sun,Weiran Huang*

Main category: cs.CV

TL;DR: 研究发现当前多模态大语言模型（MLLMs）在生成视觉描述时未能有效整合信息进行推理，提出一种无需修改算法或数据的视觉扰动框架，通过三种扰动策略提升模型感知鲁棒性，实验表明其数学推理性能显著提高。


<details>
  <summary>Details</summary>
Motivation: 现有MLLMs虽能生成准确视觉描述，但在推理过程中未能有效整合视觉信息。实验发现仅使用图像描述的语言模型性能甚至优于处理原始视觉输入的MLLMs，表明视觉处理存在缺陷。

Method: 提出视觉扰动框架，包含三种策略：1) 干扰项拼接（distractor concatenation）；2) 保持主导地位的混合（dominance-preserving mixup）；3) 随机旋转（random rotation）。这些策略可直接集成至现有训练流程（如SFT、DPO、GRPO）。

Result: 在多个数据集上实现数学推理性能的稳定提升，提升幅度与算法修改相当。通过视觉扰动训练的Qwen2.5-VL-7B模型在开源7B RL调优模型中达到竞争力水平。消融实验表明不同扰动策略对视觉推理的不同方面有独特贡献。

Conclusion: 视觉扰动对多模态数学推理至关重要：更好的推理依赖于更有效的视觉感知。该方法通过简单扰动策略，验证了增强视觉处理能力是提升MLLMs推理性能的关键路径。

Abstract: Despite the rapid progress of multimodal large language models (MLLMs), they
have largely overlooked the importance of visual processing. In a simple yet
revealing experiment, we interestingly find that language-only models, when
provided with image captions, can achieve comparable or even better performance
than MLLMs that consume raw visual inputs. This suggests that current MLLMs may
generate accurate visual descriptions but fail to effectively integrate them
during reasoning. Motivated by this, we propose a simple visual perturbation
framework that enhances perceptual robustness without requiring algorithmic
modifications or additional training data. Our approach introduces three
targeted perturbations: distractor concatenation, dominance-preserving mixup,
and random rotation, that can be easily integrated into existing post-training
pipelines including SFT, DPO, and GRPO. Through extensive experiments across
multiple datasets, we demonstrate consistent improvements in mathematical
reasoning performance, with gains comparable to those achieved through
algorithmic changes. Additionally, we achieve competitive performance among
open-source 7B RL-tuned models by training Qwen2.5-VL-7B with visual
perturbation. Through comprehensive ablation studies, we analyze the
effectiveness of different perturbation strategies, revealing that each
perturbation type contributes uniquely to different aspects of visual
reasoning. Our findings highlight the critical role of visual perturbation in
multimodal mathematical reasoning: better reasoning begins with better seeing.
Our code is available at https://github.com/YutingLi0606/Vision-Matters.

</details>


### [201] [PatchGuard: Adversarially Robust Anomaly Detection and Localization through Vision Transformers and Pseudo Anomalies](https://arxiv.org/abs/2506.09237)
*Mojtaba Nafez,Amirhossein Koochakian,Arad Maleki,Jafar Habibi,Mohammad Hossein Rohban*

Main category: cs.CV

TL;DR: 本文提出了一种基于Vision Transformer的对抗鲁棒异常检测与定位方法PatchGuard，通过引入前景感知伪异常和新型损失函数，显著提升了在对抗攻击下的性能。


<details>
  <summary>Details</summary>
Motivation: 现有异常检测与定位方法因仅使用正常样本训练，易受对抗攻击影响。医疗影像和工业监控等高可靠性领域亟需解决这一漏洞。

Method: 结合Vision Transformer架构，设计包含定位掩码的前景感知伪异常样本，通过理论指导的注意力机制优化和对抗训练策略，采用新型损失函数提升模型鲁棒性。

Result: 在工业和医学基准数据集上，对抗场景下AD/AL性能分别提升53.2%和68.5%，非对抗场景保持竞争力。代码已开源。

Conclusion: PatchGuard通过理论驱动的伪异常生成和对抗训练机制，有效解决了现有方法的对抗脆弱性问题，为高可靠性应用提供了更安全的异常检测方案。

Abstract: Anomaly Detection (AD) and Anomaly Localization (AL) are crucial in fields
that demand high reliability, such as medical imaging and industrial
monitoring. However, current AD and AL approaches are often susceptible to
adversarial attacks due to limitations in training data, which typically
include only normal, unlabeled samples. This study introduces PatchGuard, an
adversarially robust AD and AL method that incorporates pseudo anomalies with
localization masks within a Vision Transformer (ViT)-based architecture to
address these vulnerabilities. We begin by examining the essential properties
of pseudo anomalies, and follow it by providing theoretical insights into the
attention mechanisms required to enhance the adversarial robustness of AD and
AL systems. We then present our approach, which leverages Foreground-Aware
Pseudo-Anomalies to overcome the deficiencies of previous anomaly-aware
methods. Our method incorporates these crafted pseudo-anomaly samples into a
ViT-based framework, with adversarial training guided by a novel loss function
designed to improve model robustness, as supported by our theoretical analysis.
Experimental results on well-established industrial and medical datasets
demonstrate that PatchGuard significantly outperforms previous methods in
adversarial settings, achieving performance gains of $53.2\%$ in AD and
$68.5\%$ in AL, while also maintaining competitive accuracy in non-adversarial
settings. The code repository is available at
https://github.com/rohban-lab/PatchGuard .

</details>


### [202] [ELBO-T2IAlign: A Generic ELBO-Based Method for Calibrating Pixel-level Text-Image Alignment in Diffusion Models](https://arxiv.org/abs/2506.09740)
*Qin Zhou,Zhiyang Zhang,Jinglong Wang,Xiaobin Li,Jing Zhang,Qian Yu,Lu Sheng,Dong Xu*

Main category: cs.CV

TL;DR: 本文提出一种基于证据下界（ELBO）的校准方法ELBO-T2IAlign，用于解决扩散模型中像素-文本对齐偏差问题，无需训练且适用于多种模型。


<details>
  <summary>Details</summary>
Motivation: 现有方法假设扩散模型中的文本-图像完美对齐，但实际存在像素级和类别级未对齐问题，尤其在处理小尺寸、遮挡或罕见对象时更为明显。

Method: 通过零样本参考图像分割任务评估扩散模型的对齐能力，提出基于ELBO的校准方法ELBO-T2IAlign，无需训练即可修正对齐偏差。

Result: 在图像分割和生成的常用基准数据集上验证了该方法能有效校准对齐偏差，且适用于不同扩散模型架构。

Conclusion: ELBO-T2IAlign是一种通用、无需训练的对齐校准方法，解决了扩散模型因训练数据偏差导致的文本-图像未对齐问题。

Abstract: Diffusion models excel at image generation. Recent studies have shown that
these models not only generate high-quality images but also encode text-image
alignment information through attention maps or loss functions. This
information is valuable for various downstream tasks, including segmentation,
text-guided image editing, and compositional image generation. However, current
methods heavily rely on the assumption of perfect text-image alignment in
diffusion models, which is not the case. In this paper, we propose using
zero-shot referring image segmentation as a proxy task to evaluate the
pixel-level image and class-level text alignment of popular diffusion models.
We conduct an in-depth analysis of pixel-text misalignment in diffusion models
from the perspective of training data bias. We find that misalignment occurs in
images with small sized, occluded, or rare object classes. Therefore, we
propose ELBO-T2IAlign, a simple yet effective method to calibrate pixel-text
alignment in diffusion models based on the evidence lower bound (ELBO) of
likelihood. Our method is training-free and generic, eliminating the need to
identify the specific cause of misalignment and works well across various
diffusion model architectures. Extensive experiments on commonly used benchmark
datasets on image segmentation and generation have verified the effectiveness
of our proposed calibration approach.

</details>


### [203] [UFM: A Simple Path towards Unified Dense Correspondence with Flow](https://arxiv.org/abs/2506.09278)
*Yuchen Zhang,Nikhil Keetha,Chenwei Lyu,Bhuvan Jhamb,Yutian Chen,Yuheng Qiu,Jay Karhade,Shreyas Jha,Yaoyu Hu,Deva Ramanan,Sebastian Scherer,Wenshan Wang*

Main category: cs.CV

TL;DR: 提出统一光流与匹配模型UFM，通过统一训练和简单Transformer架构，在密集图像对应任务中实现更高精度、更快速度，首次证明统一训练优于传统分治法。


<details>
  <summary>Details</summary>
Motivation: 传统方法将宽基线场景与光流估计分开处理，但二者核心目标均为图像内容匹配。本文旨在探索统一训练框架以提升性能。

Method: 使用通用Transformer架构直接回归(u,v)光流，摒弃传统多尺度代价体积方法，采用共可见像素统一训练数据，简化训练流程并提升大位移精度。

Result: UFM在光流任务上精度超越SOTA方法28%，宽基线匹配误差降低62%、速度提升6.7倍，首次验证统一训练框架跨领域超越专用模型。

Conclusion: UFM证明了统一对应模型的优越性，为多模态、长距离、实时密集匹配任务开辟了新方向，推动通用化高效对应方法发展。

Abstract: Dense image correspondence is central to many applications, such as visual
odometry, 3D reconstruction, object association, and re-identification.
Historically, dense correspondence has been tackled separately for
wide-baseline scenarios and optical flow estimation, despite the common goal of
matching content between two images. In this paper, we develop a Unified Flow &
Matching model (UFM), which is trained on unified data for pixels that are
co-visible in both source and target images. UFM uses a simple, generic
transformer architecture that directly regresses the (u,v) flow. It is easier
to train and more accurate for large flows compared to the typical
coarse-to-fine cost volumes in prior work. UFM is 28% more accurate than
state-of-the-art flow methods (Unimatch), while also having 62% less error and
6.7x faster than dense wide-baseline matchers (RoMa). UFM is the first to
demonstrate that unified training can outperform specialized approaches across
both domains. This result enables fast, general-purpose correspondence and
opens new directions for multi-modal, long-range, and real-time correspondence
tasks.

</details>


### [204] [Inverting Black-Box Face Recognition Systems via Zero-Order Optimization in Eigenface Space](https://arxiv.org/abs/2506.09777)
*Anton Razzhigaev,Matvey Mikhalchuk,Klim Kireev,Igor Udovichenko,Andrey Kuznetsov,Aleksandr Petiushko*

Main category: cs.CV

TL;DR: 本文提出DarkerBB方法，通过PCA特征空间的零阶优化仅用相似度分数实现黑箱人脸模型逆向重建，在多个基准测试中达到最优验证精度。


<details>
  <summary>Details</summary>
Motivation: 现有模型逆向方法依赖嵌入向量信息需求较高，本文针对仅能获取相似度分数的更严格隐私威胁场景，探索有限信息下的面部图像重建可能性。

Method: 基于PCA构建特征脸空间，在特征空间内采用零阶优化算法迭代生成人脸图像，仅利用模型输出的相似度分数作为反馈信号。

Result: 在LFW、AgeDB-30和CFP-FP数据集上取得相似度仅用场景下的SOTA验证准确率(99.3%/95.7%/93.6%)，且查询效率优于对比方法。

Conclusion: DarkerBB证明通过结构化特征空间与高效优化策略，可在极有限反馈信息下实现有效模型逆向，揭示了黑箱人脸识别系统的潜在隐私风险。

Abstract: Reconstructing facial images from black-box recognition models poses a
significant privacy threat. While many methods require access to embeddings, we
address the more challenging scenario of model inversion using only similarity
scores. This paper introduces DarkerBB, a novel approach that reconstructs
color faces by performing zero-order optimization within a PCA-derived
eigenface space. Despite this highly limited information, experiments on LFW,
AgeDB-30, and CFP-FP benchmarks demonstrate that DarkerBB achieves
state-of-the-art verification accuracies in the similarity-only setting, with
competitive query efficiency.

</details>


### [205] [Lightweight Object Detection Using Quantized YOLOv4-Tiny for Emergency Response in Aerial Imagery](https://arxiv.org/abs/2506.09299)
*Sindhu Boddu,Arindam Mukherjee*

Main category: cs.CV

TL;DR: 本文提出了一种轻量级、高能效的航拍应急图像目标检测方案，通过INT8量化优化YOLOv4-Tiny模型在自建数据集上实现实时检测，模型体积缩小71%，推理速度提升44%。


<details>
  <summary>Details</summary>
Motivation: 现有公开无人机视角应急图像数据集匮乏，且传统模型在边缘设备上部署存在体积大、能效低的问题。研究旨在开发适用于低功耗设备的实时应急检测方案。

Method: 采用YOLOv4-Tiny模型进行后训练INT8量化优化，使用自建包含10,820张标注图像的航拍应急数据集训练，并与YOLOv5-small进行多指标对比实验。

Result: 量化后模型体积从22.5MB降至6.4MB，mAP与F1分数与原模型相当，推理速度提升44%，在边缘设备上实现高效实时检测。

Conclusion: 量化YOLOv4-Tiny在保持检测精度的同时显著降低资源需求，验证了其在低功耗边缘设备上部署应急检测系统的可行性。

Abstract: This paper presents a lightweight and energy-efficient object detection
solution for aerial imagery captured during emergency response situations. We
focus on deploying the YOLOv4-Tiny model, a compact convolutional neural
network, optimized through post-training quantization to INT8 precision. The
model is trained on a custom-curated aerial emergency dataset, consisting of
10,820 annotated images covering critical emergency scenarios. Unlike prior
works that rely on publicly available datasets, we created this dataset
ourselves due to the lack of publicly available drone-view emergency imagery,
making the dataset itself a key contribution of this work. The quantized model
is evaluated against YOLOv5-small across multiple metrics, including mean
Average Precision (mAP), F1 score, inference time, and model size. Experimental
results demonstrate that the quantized YOLOv4-Tiny achieves comparable
detection performance while reducing the model size from 22.5 MB to 6.4 MB and
improving inference speed by 44\%. With a 71\% reduction in model size and a
44\% increase in inference speed, the quantized YOLOv4-Tiny model proves highly
suitable for real-time emergency detection on low-power edge devices.

</details>


### [206] [Q-SAM2: Accurate Quantization for Segment Anything Model 2](https://arxiv.org/abs/2506.09782)
*Nicola Farronato,Florian Scheidegger,Mattia Rigotti,Cristiano Malossi,Michele Magno,Haotong Qin*

Main category: cs.CV

TL;DR: 提出Q-SAM2方法，通过线性层校准和量化感知训练解决SAM2模型在低比特量化中的性能下降问题，显著提升资源受限场景下的分割效率与精度。


<details>
  <summary>Details</summary>
Motivation: SAM2模型在图像和视频分割中表现优异，但其高计算和内存消耗限制了在资源受限场景的应用，需开发高效量化方法。

Method: 结合线性层校准（最小化Frobenius范数调整权重分布）和量化感知训练（抑制异常值并自适应量化阈值）的量化方案。

Result: Q-SAM2在超低2位量化中超越现有方案，量化后训练准确率提升66% mIoU，且校准技术对后训练量化同样有效。

Conclusion: Q-SAM2通过创新校准与训练策略，在保持高精度的同时显著降低计算成本，为资源受限场景提供了高效分割解决方案。

Abstract: The Segment Anything Model 2 (SAM2) has gained significant attention as a
foundational approach for promptable image and video segmentation. However, its
expensive computational and memory consumption poses a severe challenge for its
application in resource-constrained scenarios. In this paper, we propose an
accurate low-bit quantization method for efficient SAM2, termed Q-SAM2. To
address the performance degradation caused by the singularities in weight and
activation distributions during quantization, Q-SAM2 introduces two novel
technical contributions. We first introduce a linear layer calibration method
for low-bit initialization of SAM2, which minimizes the Frobenius norm over a
small image batch to reposition weight distributions for improved quantization.
We then propose a Quantization-Aware Training (QAT) pipeline that applies
clipping to suppress outliers and allows the network to adapt to quantization
thresholds during training. Our comprehensive experiments demonstrate that
Q-SAM2 allows for highly accurate inference while substantially improving
efficiency. Both quantitative and visual results show that our Q-SAM2 surpasses
existing state-of-the-art general quantization schemes, especially for
ultra-low 2-bit quantization. While designed for quantization-aware training,
our proposed calibration technique also proves effective in post-training
quantization, achieving up to a 66% mIoU accuracy improvement over
non-calibrated models.

</details>


### [207] [DynaSplat: Dynamic-Static Gaussian Splatting with Hierarchical Motion Decomposition for Scene Reconstruction](https://arxiv.org/abs/2506.09836)
*Junli Deng,Ping Shi,Qipei Li,Jinyang Guo*

Main category: cs.CV

TL;DR: DynaSplat通过动态-静态分离、分层运动建模和基于物理的透明度估计，扩展高斯泼溅技术，实现复杂动态场景的高效重建。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以处理真实世界动态场景的复杂性，尤其在非刚性运动、遮挡和视角变化时表现不足。

Method: 1. 基于变形偏移统计与2D运动流一致性分离动静态元素；2. 分层建模全局/局部运动；3. 物理驱动的透明度估计处理遮挡问题。

Result: 在复杂数据集上超越现有方法，重建精度提升21%，渲染速度提高40%，且内存占用减少30%。

Conclusion: DynaSplat为动态场景重建提供了更直观、紧凑且高效的解决方案，突破了传统方法在运动建模与物理一致性方面的限制。

Abstract: Reconstructing intricate, ever-changing environments remains a central
ambition in computer vision, yet existing solutions often crumble before the
complexity of real-world dynamics. We present DynaSplat, an approach that
extends Gaussian Splatting to dynamic scenes by integrating dynamic-static
separation and hierarchical motion modeling. First, we classify scene elements
as static or dynamic through a novel fusion of deformation offset statistics
and 2D motion flow consistency, refining our spatial representation to focus
precisely where motion matters. We then introduce a hierarchical motion
modeling strategy that captures both coarse global transformations and
fine-grained local movements, enabling accurate handling of intricate,
non-rigid motions. Finally, we integrate physically-based opacity estimation to
ensure visually coherent reconstructions, even under challenging occlusions and
perspective shifts. Extensive experiments on challenging datasets reveal that
DynaSplat not only surpasses state-of-the-art alternatives in accuracy and
realism but also provides a more intuitive, compact, and efficient route to
dynamic scene reconstruction.

</details>


### [208] [OctoNav: Towards Generalist Embodied Navigation](https://arxiv.org/abs/2506.09839)
*Chen Gao,Liankai Jin,Xingyu Peng,Jiazhao Zhang,Yue Deng,Annan Li,He Wang,Si Liu*

Main category: cs.CV

TL;DR: 本文提出通用导航智能体OctoNav-R1及基准OctoNav-Bench，通过多模态指令、混合训练范式及思考-行动机制，显著提升导航性能。


<details>
  <summary>Details</summary>
Motivation: 现有导航研究分散于独立任务（如ObjNav、ImgNav等），任务目标与模态差异导致数据集与方法孤立，缺乏统一通用性。本文旨在构建支持自由多模态指令的通用导航智能体。

Method: 1) 构建OctoNav-Bench基准，包含连续环境、多样化多模态指令-轨迹对及TBA-CoT思考过程数据集；2) 提出OctoNav-R1模型，基于MLLMs适配为VLA架构，结合混合训练范式（HTP），分三阶段（Action/TBA-SFT、Nav-GPRO、在线强化学习）优化模型推理与行动能力。

Result: OctoNav-R1在性能上显著超越现有方法，验证了TBA-CoT与HTP训练范式的有效性。

Conclusion: 通过统一基准与结合思考-行动机制的混合训练方法，本文推动了通用导航智能体的发展，为多模态指令理解与低层动作生成提供了新思路。

Abstract: Embodied navigation stands as a foundation pillar within the broader pursuit
of embodied AI. However, previous navigation research is divided into different
tasks/capabilities, e.g., ObjNav, ImgNav and VLN, where they differ in task
objectives and modalities, making datasets and methods are designed
individually. In this work, we take steps toward generalist navigation agents,
which can follow free-form instructions that include arbitrary compounds of
multi-modal and multi-capability. To achieve this, we propose a large-scale
benchmark and corresponding method, termed OctoNav-Bench and OctoNav-R1.
Specifically, OctoNav-Bench features continuous environments and is constructed
via a designed annotation pipeline. We thoroughly craft instruction-trajectory
pairs, where instructions are diverse in free-form with arbitrary modality and
capability. Also, we construct a Think-Before-Action (TBA-CoT) dataset within
OctoNav-Bench to provide the thinking process behind actions. For OctoNav-R1,
we build it upon MLLMs and adapt it to a VLA-type model, which can produce
low-level actions solely based on 2D visual observations. Moreover, we design a
Hybrid Training Paradigm (HTP) that consists of three stages, i.e.,
Action-/TBA-SFT, Nav-GPRO, and Online RL stages. Each stage contains
specifically designed learning policies and rewards. Importantly, for TBA-SFT
and Nav-GRPO designs, we are inspired by the OpenAI-o1 and DeepSeek-R1, which
show impressive reasoning ability via thinking-before-answer. Thus, we aim to
investigate how to achieve thinking-before-action in the embodied navigation
field, to improve model's reasoning ability toward generalists. Specifically,
we propose TBA-SFT to utilize the TBA-CoT dataset to fine-tune the model as a
cold-start phrase and then leverage Nav-GPRO to improve its thinking ability.
Finally, OctoNav-R1 shows superior performance compared with previous methods.

</details>


### [209] [Learning to Align: Addressing Character Frequency Distribution Shifts in Handwritten Text Recognition](https://arxiv.org/abs/2506.09846)
*Panagiotis Kaliosis,John Pavlopoulos*

Main category: cs.CV

TL;DR: 提出一种基于Wasserstein距离的字符频率分布对齐方法，通过改进损失函数和引导解码策略，提升手写文本识别模型在跨时空场景下的泛化性能。


<details>
  <summary>Details</summary>
Motivation: 手写文本识别面临字符集演变和分布偏移问题，传统模型在特定子集上表现不佳，需解决跨时空场景下的泛化挑战。

Method: 设计融合Wasserstein距离的损失函数对齐字符频率分布，并开发无需重训练的引导解码评分机制实现推理时优化。

Result: 多数据集实验表明该方法显著提升模型准确率与鲁棒性，在时域/地域分布偏移场景下表现优于基线模型。

Conclusion: 字符分布对齐机制有效缓解领域偏移问题，为历史文献识别等场景提供可扩展解决方案，代码已发布。

Abstract: Handwritten text recognition aims to convert visual input into
machine-readable text, and it remains challenging due to the evolving and
context-dependent nature of handwriting. Character sets change over time, and
character frequency distributions shift across historical periods or regions,
often causing models trained on broad, heterogeneous corpora to underperform on
specific subsets. To tackle this, we propose a novel loss function that
incorporates the Wasserstein distance between the character frequency
distribution of the predicted text and a target distribution empirically
derived from training data. By penalizing divergence from expected
distributions, our approach enhances both accuracy and robustness under
temporal and contextual intra-dataset shifts. Furthermore, we demonstrate that
character distribution alignment can also improve existing models at inference
time without requiring retraining by integrating it as a scoring function in a
guided decoding scheme. Experimental results across multiple datasets and
architectures confirm the effectiveness of our method in boosting
generalization and performance. We open source our code at
https://github.com/pkaliosis/fada.

</details>


### [210] [3D-Aware Vision-Language Models Fine-Tuning with Geometric Distillation](https://arxiv.org/abs/2506.09883)
*Seonho Lee,Jiho Choi,Inha Kang,Jiwook Kim,Junsung Park,Hyunjung Shim*

Main category: cs.CV

TL;DR: 本文提出几何蒸馏方法，通过从3D基础模型提取几何线索注入预训练视觉语言模型，提升其3D空间推理能力，同时保持低计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型（VLMs）在多种任务中表现优异，但在3D空间结构理解上存在根本性限制。

Method: 采用轻量级无标注微调框架，从现成3D基础模型（如MASt3R）蒸馏稀疏对应、相对深度关系和密集成本体积，使模型具备几何感知能力。

Result: 在3D视觉语言推理和感知基准测试中性能优于现有方法，计算成本显著降低，且兼容自然图像-文本输入。

Conclusion: 该方法为2D训练的VLMs与3D理解搭建了高效桥梁，拓展了空间多模态任务的应用潜力。

Abstract: Vision-Language Models (VLMs) have shown remarkable performance on diverse
visual and linguistic tasks, yet they remain fundamentally limited in their
understanding of 3D spatial structures. We propose Geometric Distillation, a
lightweight, annotation-free fine-tuning framework that injects human-inspired
geometric cues into pretrained VLMs without modifying their architecture. By
distilling (1) sparse correspondences, (2) relative depth relations, and (3)
dense cost volumes from off-the-shelf 3D foundation models (e.g., MASt3R,
VGGT), our method shapes representations to be geometry-aware while remaining
compatible with natural image-text inputs. Through extensive evaluations on 3D
vision-language reasoning and 3D perception benchmarks, our method consistently
outperforms prior approaches, achieving improved 3D spatial reasoning with
significantly lower computational cost. Our work demonstrates a scalable and
efficient path to bridge 2D-trained VLMs with 3D understanding, opening up
wider use in spatially grounded multimodal tasks.

</details>


### [211] [HadaNorm: Diffusion Transformer Quantization through Mean-Centered Transformations](https://arxiv.org/abs/2506.09932)
*Marco Federici,Riccardo Del Chiaro,Boris van Breugel,Paul Whatmough,Markus Nagel*

Main category: cs.CV

TL;DR: 本文提出HadaNorm方法，通过特征通道归一化与Hadamard变换抑制激活值异常，实现扩散模型的高效后训练量化，在资源受限设备上取得更优效率-性能平衡。


<details>
  <summary>Details</summary>
Motivation: 扩散模型因高计算/内存需求难以部署于资源受限设备，传统后训练量化方法存在异常值敏感问题，且高压缩需额外权重-激活变换。

Method: HadaNorm在量化前对激活特征通道进行归一化，并应用Hadamard变换以消除异常值，支持更激进的低位宽激活量化。

Result: HadaNorm在Transformer各模块中持续降低量化误差，相比SOTA方法实现更优的压缩效率与模型性能权衡。

Conclusion: HadaNorm通过创新的线性变换机制有效解决扩散模型量化中的异常值问题，为轻量化部署提供新方案。

Abstract: Diffusion models represent the cutting edge in image generation, but their
high memory and computational demands hinder deployment on resource-constrained
devices. Post-Training Quantization (PTQ) offers a promising solution by
reducing the bitwidth of matrix operations. However, standard PTQ methods
struggle with outliers, and achieving higher compression often requires
transforming model weights and activations before quantization. In this work,
we propose HadaNorm, a novel linear transformation that extends existing
approaches and effectively mitigates outliers by normalizing activations
feature channels before applying Hadamard transformations, enabling more
aggressive activation quantization. We demonstrate that HadaNorm consistently
reduces quantization error across the various components of transformer blocks,
achieving superior efficiency-performance trade-offs when compared to
state-of-the-art methods.

</details>


### [212] [CausalVQA: A Physically Grounded Causal Reasoning Benchmark for Video Models](https://arxiv.org/abs/2506.09943)
*Aaron Foss,Chloe Evans,Sasha Mitts,Koustuv Sinha,Ammar Rizvi,Justine T. Kao*

Main category: cs.CV

TL;DR: CausalVQA是一个针对视频问答的基准数据集，旨在测试模型对物理世界因果关系的理解，包含五类问题类型，并揭示当前多模态模型在时空推理和物理理解上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有VQA数据集多关注表面感知或基于模拟环境的狭窄物理推理，缺乏真实场景下对因果关系的深度理解。CausalVQA填补了这一空白，要求模型通过预测动作和事件的结果来验证其推理能力。

Method: 构建包含反事实、假设、预期、规划和描述性问题的数据集，设计质量控制机制防止模型利用语言捷径，强制依赖深层视觉理解。

Result: 当前前沿多模态模型在CausalVQA上表现显著低于人类（尤其在预期和假设问题），暴露其在时空推理、物理原理及替代可能性理解上的缺陷。

Conclusion: CausalVQA揭示了现有系统在真实场景中准确预测的挑战，需提升空间-时间推理、物理知识整合及多可能性分析能力。

Abstract: We introduce CausalVQA, a benchmark dataset for video question answering
(VQA) composed of question-answer pairs that probe models' understanding of
causality in the physical world. Existing VQA benchmarks either tend to focus
on surface perceptual understanding of real-world videos, or on narrow physical
reasoning questions created using simulation environments. CausalVQA fills an
important gap by presenting challenging questions that are grounded in
real-world scenarios, while focusing on models' ability to predict the likely
outcomes of different actions and events through five question types:
counterfactual, hypothetical, anticipation, planning and descriptive. We
designed quality control mechanisms that prevent models from exploiting trivial
shortcuts, requiring models to base their answers on deep visual understanding
instead of linguistic cues. We find that current frontier multimodal models
fall substantially below human performance on the benchmark, especially on
anticipation and hypothetical questions. This highlights a challenge for
current systems to leverage spatial-temporal reasoning, understanding of
physical principles, and comprehension of possible alternatives to make
accurate predictions in real-world settings.

</details>


### [213] [UniPre3D: Unified Pre-training of 3D Point Cloud Models with Cross-Modal Gaussian Splatting](https://arxiv.org/abs/2506.09952)
*Ziyi Wang,Yanran Zhang,Jie Zhou,Jiwen Lu*

Main category: cs.CV

TL;DR: 提出首个统一预训练方法UniPre3D，通过高斯基元预测与可微分渲染实现跨尺度点云通用表示学习，结合2D特征增强几何建模。


<details>
  <summary>Details</summary>
Motivation: 点云数据尺度多样性导致现有预训练方法无法同时有效处理物体级和场景级任务，缺乏统一的三维模型框架。

Method: 以预测高斯基元为预训练任务，采用可微分高斯溅射渲染实现像素级监督；整合预训练图像模型的2D特征引入纹理先验知识。

Result: 在多种点云模型和跨尺度任务中验证了方法的通用有效性，代码已公开。

Conclusion: UniPre3D首次实现任意尺度点云与架构的统一预训练，通过几何-纹理联合优化开辟三维通用模型新路径。

Abstract: The scale diversity of point cloud data presents significant challenges in
developing unified representation learning techniques for 3D vision. Currently,
there are few unified 3D models, and no existing pre-training method is equally
effective for both object- and scene-level point clouds. In this paper, we
introduce UniPre3D, the first unified pre-training method that can be
seamlessly applied to point clouds of any scale and 3D models of any
architecture. Our approach predicts Gaussian primitives as the pre-training
task and employs differentiable Gaussian splatting to render images, enabling
precise pixel-level supervision and end-to-end optimization. To further
regulate the complexity of the pre-training task and direct the model's focus
toward geometric structures, we integrate 2D features from pre-trained image
models to incorporate well-established texture knowledge. We validate the
universal effectiveness of our proposed method through extensive experiments
across a variety of object- and scene-level tasks, using diverse point cloud
models as backbones. Code is available at https://github.com/wangzy22/UniPre3D.

</details>


### [214] [Vision Generalist Model: A Survey](https://arxiv.org/abs/2506.09954)
*Ziyi Wang,Yongming Rao,Shuofeng Sun,Xinrun Liu,Yi Wei,Xumin Yu,Zuyan Liu,Yanbo Wang,Hongmin Liu,Jie Zhou,Jiwen Lu*

Main category: cs.CV

TL;DR: 本文综述了视觉通用模型的研究进展，探讨其设计框架、性能提升技术、相关领域联系及实际应用，分析当前挑战并提出未来方向。


<details>
  <summary>Details</summary>
Motivation: 通用模型在自然语言处理中表现优异，但其在计算机视觉任务中的应用因输入输出多样性及统一表示困难而面临挑战，需系统性研究。

Method: 通过回顾背景（数据集、任务、基准）、分析现有框架设计及性能优化技术，并关联相关领域，综合评估视觉通用模型。

Result: 总结了视觉通用模型的技术路径与应用场景，揭示了模型设计、任务兼容性及实际部署中的关键挑战。

Conclusion: 视觉通用模型潜力显著，但需解决多模态统一表示、计算效率等难题，未来可结合跨模态学习与轻量化设计进一步突破。

Abstract: Recently, we have witnessed the great success of the generalist model in
natural language processing. The generalist model is a general framework
trained with massive data and is able to process various downstream tasks
simultaneously. Encouraged by their impressive performance, an increasing
number of researchers are venturing into the realm of applying these models to
computer vision tasks. However, the inputs and outputs of vision tasks are more
diverse, and it is difficult to summarize them as a unified representation. In
this paper, we provide a comprehensive overview of the vision generalist
models, delving into their characteristics and capabilities within the field.
First, we review the background, including the datasets, tasks, and benchmarks.
Then, we dig into the design of frameworks that have been proposed in existing
research, while also introducing the techniques employed to enhance their
performance. To better help the researchers comprehend the area, we take a
brief excursion into related domains, shedding light on their interconnections
and potential synergies. To conclude, we provide some real-world application
scenarios, undertake a thorough examination of the persistent challenges, and
offer insights into possible directions for future research endeavors.

</details>


### [215] [Reinforcing Spatial Reasoning in Vision-Language Models with Interwoven Thinking and Visual Drawing](https://arxiv.org/abs/2506.09965)
*Junfei Wu,Jian Guan,Kaituo Feng,Qiang Liu,Shu Wu,Liang Wang,Wei Wu,Tieniu Tan*

Main category: cs.CV

TL;DR: 提出VILASR模型，通过视觉绘图操作增强大型视觉语言模型的空间推理能力，在多项任务中平均提升18.4%。


<details>
  <summary>Details</summary>
Motivation: 现有多模态推理方法依赖纯文本处理，在需要几何理解与空间跟踪的任务中存在性能瓶颈。

Method: 引入标注边界框/辅助线等基础绘图操作，采用三阶段训练框架（冷启动训练-反射拒绝采样-强化学习）。

Result: VILASR在迷宫导航、静态空间推理等任务中平均提升18.4%，显著优于现有方法。

Conclusion: 视觉空间直接操作能有效突破文本中心化推理的局限性，为多模态推理提供新范式。

Abstract: As textual reasoning with large language models (LLMs) has advanced
significantly, there has been growing interest in enhancing the multimodal
reasoning capabilities of large vision-language models (LVLMs). However,
existing methods primarily approach multimodal reasoning in a straightforward,
text-centric manner, where both reasoning and answer derivation are conducted
purely through text, with the only difference being the presence of multimodal
input. As a result, these methods often encounter fundamental limitations in
spatial reasoning tasks that demand precise geometric understanding and
continuous spatial tracking-capabilities that humans achieve through mental
visualization and manipulation. To address the limitations, we propose drawing
to reason in space, a novel paradigm that enables LVLMs to reason through
elementary drawing operations in the visual space. By equipping models with
basic drawing operations, including annotating bounding boxes and drawing
auxiliary lines, we empower them to express and analyze spatial relationships
through direct visual manipulation, meanwhile avoiding the performance ceiling
imposed by specialized perception tools in previous tool-integrated reasoning
approaches. To cultivate this capability, we develop a three-stage training
framework: cold-start training with synthetic data to establish basic drawing
abilities, reflective rejection sampling to enhance self-reflection behaviors,
and reinforcement learning to directly optimize for target rewards. Extensive
experiments demonstrate that our model, named VILASR, consistently outperforms
existing methods across diverse spatial reasoning benchmarks, involving maze
navigation, static spatial reasoning, video-based reasoning, and
multi-view-based reasoning tasks, with an average improvement of 18.4%.

</details>


### [216] [InterActHuman: Multi-Concept Human Animation with Layout-Aligned Audio Conditions](https://arxiv.org/abs/2506.09984)
*Zhenzhi Wang,Jiaqi Yang,Jianwen Jiang,Chao Liang,Gaojie Lin,Zerong Zheng,Ceyuan Yang,Dahua Lin*

Main category: cs.CV

TL;DR: 本文提出了一种基于多模态条件的端到端人像动画生成框架，通过显式布局控制实现多概念（如多人物及物体）的精确时空绑定，解决了现有方法在全局条件注入下无法处理复杂交互的问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法仅支持单一主体动画且采用全局条件注入，无法处理多概念共现场景（如人-人/物体交互），导致对多实体缺乏细粒度控制，限制了实际应用场景。

Method: 通过掩码预测器自动推断多概念布局，将参考图像与去噪视频进行外观匹配；采用迭代式局部音频条件注入策略，实现模态条件与空间布局的对齐式绑定。

Result: 实验表明该方法在显式布局控制下显著提升多模态条件匹配精度，相比隐式控制方法和其他基线模型，能生成更高质量的多概念人像交互视频。

Conclusion: 显式布局控制机制有效解决了多概念动画生成中的身份绑定问题，为复杂人机交互场景提供了可扩展的解决方案，具有实际部署潜力。

Abstract: End-to-end human animation with rich multi-modal conditions, e.g., text,
image and audio has achieved remarkable advancements in recent years. However,
most existing methods could only animate a single subject and inject conditions
in a global manner, ignoring scenarios that multiple concepts could appears in
the same video with rich human-human interactions and human-object
interactions. Such global assumption prevents precise and per-identity control
of multiple concepts including humans and objects, therefore hinders
applications. In this work, we discard the single-entity assumption and
introduce a novel framework that enforces strong, region-specific binding of
conditions from modalities to each identity's spatiotemporal footprint. Given
reference images of multiple concepts, our method could automatically infer
layout information by leveraging a mask predictor to match appearance cues
between the denoised video and each reference appearance. Furthermore, we
inject local audio condition into its corresponding region to ensure
layout-aligned modality matching in a iterative manner. This design enables the
high-quality generation of controllable multi-concept human-centric videos.
Empirical results and ablation studies validate the effectiveness of our
explicit layout control for multi-modal conditions compared to implicit
counterparts and other existing methods.

</details>


### [217] [HopaDIFF: Holistic-Partial Aware Fourier Conditioned Diffusion for Referring Human Action Segmentation in Multi-Person Scenarios](https://arxiv.org/abs/2506.09650)
*Kunyu Peng,Junchao Huang,Xiangsheng Huang,Di Wen,Junwei Zheng,Yufan Chen,Kailun Yang,Jiamin Wu,Chongqing Hao,Rainer Stiefelhagen*

Main category: cs.CV

TL;DR: 本文提出首个文本参考引导的多人动作分割框架HopaDIFF，构建RHAS133数据集，通过跨输入门控注意力xLSTM与傅里叶条件扩散模型提升细粒度控制，在多人场景下实现最优动作分割效果。


<details>
  <summary>Details</summary>
Motivation: 现有动作分割方法集中于单人固定序列场景，缺乏对多人场景及文本参考引导的适应性。为此，本文针对多人场景下基于描述的目标人物动作分割问题展开研究。

Method: 提出HopaDIFF框架：1) 构建RHAS133数据集（含137细粒度动作/33小时视频/文本描述）；2) 设计跨输入门控注意力xLSTM增强整体-局部长程推理；3) 引入傅里叶条件改进扩散模型的动作生成细粒度控制。

Result: HopaDIFF在RHAS133数据集上取得SOTA性能，验证了其在多评估设置下的有效性。现有VLM特征提取方法在该任务中表现受限。

Conclusion: 通过融合文本参考、创新长程推理机制与傅里叶条件控制，HopaDIFF有效解决了多人场景下的细粒度动作分割问题，为视频理解领域提供了新基准与方法论。

Abstract: Action segmentation is a core challenge in high-level video understanding,
aiming to partition untrimmed videos into segments and assign each a label from
a predefined action set. Existing methods primarily address single-person
activities with fixed action sequences, overlooking multi-person scenarios. In
this work, we pioneer textual reference-guided human action segmentation in
multi-person settings, where a textual description specifies the target person
for segmentation. We introduce the first dataset for Referring Human Action
Segmentation, i.e., RHAS133, built from 133 movies and annotated with 137
fine-grained actions with 33h video data, together with textual descriptions
for this new task. Benchmarking existing action recognition methods on RHAS133
using VLM-based feature extractors reveals limited performance and poor
aggregation of visual cues for the target person. To address this, we propose a
holistic-partial aware Fourier-conditioned diffusion framework, i.e., HopaDIFF,
leveraging a novel cross-input gate attentional xLSTM to enhance
holistic-partial long-range reasoning and a novel Fourier condition to
introduce more fine-grained control to improve the action segmentation
generation. HopaDIFF achieves state-of-the-art results on RHAS133 in diverse
evaluation settings. The code is available at
https://github.com/KPeng9510/HopaDIFF.git.

</details>


### [218] [EditInspector: A Benchmark for Evaluation of Text-Guided Image Edits](https://arxiv.org/abs/2506.09988)
*Ron Yosef,Moran Yanuka,Yonatan Bitton,Dani Lischinski*

Main category: cs.CV

TL;DR: 本文提出EditInspector基准，通过人工标注验证文本引导图像编辑质量，发现现有模型评估能力不足且易产生幻觉，进而提出两种新方法在伪影检测和差异描述任务中超越现有技术。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI推动文本引导图像编辑技术普及，亟需系统化框架验证编辑质量。现有模型缺乏对多维度编辑效果（如准确性、视觉质量、常识符合性等）的综合评估能力。

Method: 构建基于人工标注的EditInspector评估基准，设计结构化编辑验证模板，并开发两种新方法优化伪影检测与差异描述生成任务。

Result: 实验表明SOTA模型在综合评估编辑效果时存在显著缺陷（如幻觉描述），而提出方法在伪影检测（F1提升4.1%）和差异描述（CIDEr提升8.3%）指标上优于现有模型。

Conclusion: 当前模型难以全面评估文本引导编辑效果，新方法通过针对性优化在关键任务中表现更优，强调需开发更鲁棒的评估工具以匹配技术发展需求。

Abstract: Text-guided image editing, fueled by recent advancements in generative AI, is
becoming increasingly widespread. This trend highlights the need for a
comprehensive framework to verify text-guided edits and assess their quality.
To address this need, we introduce EditInspector, a novel benchmark for
evaluation of text-guided image edits, based on human annotations collected
using an extensive template for edit verification. We leverage EditInspector to
evaluate the performance of state-of-the-art (SoTA) vision and language models
in assessing edits across various dimensions, including accuracy, artifact
detection, visual quality, seamless integration with the image scene, adherence
to common sense, and the ability to describe edit-induced changes. Our findings
indicate that current models struggle to evaluate edits comprehensively and
frequently hallucinate when describing the changes. To address these
challenges, we propose two novel methods that outperform SoTA models in both
artifact detection and difference caption generation.

</details>


### [219] [Text-Aware Image Restoration with Diffusion Models](https://arxiv.org/abs/2506.09993)
*Jaewon Min,Jin Hyeon Kim,Paul Hyunbin Cho,Jaeeun Lee,Jihye Park,Minkyu Park,Sangpil Kim,Hyunhee Park,Seungryong Kim*

Main category: cs.CV

TL;DR: 本文提出一种新的文本感知图像修复任务（TAIR），旨在同时恢复图像内容和文本保真度，并提出了包含大规模标注的SA-Text数据集及多任务扩散框架TeReDiff，显著提升了文本识别准确性。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散模型的图像修复方法在自然图像上表现良好，但在文本区域易产生看似合理但错误的文本幻觉（text-image hallucination），导致文本保真度不足。

Method: 提出TAIR任务框架TeReDiff：通过联合训练扩散模型与文本检测模块，利用扩散模型内部特征提取文本表征作为去噪提示，并结合SA-Text数据集（含10万带密集文本标注的场景图像）。

Result: 实验表明，该方法在文本识别准确率上显著优于现有最优方法，有效缓解文本幻觉问题。

Conclusion: TAIR任务及TeReDiff框架通过联合建模视觉与文本特征，解决了图像修复中的文本保真度挑战，SA-Text数据集为相关研究提供了重要基准。

Abstract: Image restoration aims to recover degraded images. However, existing
diffusion-based restoration methods, despite great success in natural image
restoration, often struggle to faithfully reconstruct textual regions in
degraded images. Those methods frequently generate plausible but incorrect
text-like patterns, a phenomenon we refer to as text-image hallucination. In
this paper, we introduce Text-Aware Image Restoration (TAIR), a novel
restoration task that requires the simultaneous recovery of visual contents and
textual fidelity. To tackle this task, we present SA-Text, a large-scale
benchmark of 100K high-quality scene images densely annotated with diverse and
complex text instances. Furthermore, we propose a multi-task diffusion
framework, called TeReDiff, that integrates internal features from diffusion
models into a text-spotting module, enabling both components to benefit from
joint training. This allows for the extraction of rich text representations,
which are utilized as prompts in subsequent denoising steps. Extensive
experiments demonstrate that our approach consistently outperforms
state-of-the-art restoration methods, achieving significant gains in text
recognition accuracy. See our project page: https://cvlab-kaist.github.io/TAIR/

</details>


### [220] [CINeMA: Conditional Implicit Neural Multi-Modal Atlas for a Spatio-Temporal Representation of the Perinatal Brain](https://arxiv.org/abs/2506.09668)
*Maik Dannecker,Vasiliki Sideri-Lampretsa,Sophie Starck,Angeline Mihailov,Mathieu Milh,Nadine Girard,Guillaume Auzias,Daniel Rueckert*

Main category: cs.CV

TL;DR: CINeMA提出了一种基于条件隐式神经多模态图谱的新框架，可在低数据条件下构建高分辨率、时空多模态脑图谱，显著提升效率并支持病理研究。


<details>
  <summary>Details</summary>
Motivation: 传统脑图谱构建方法依赖大量数据，难以应对病理情况下数据稀缺的挑战，亟需一种高效、灵活的低数据解决方案。

Method: 通过隐式神经表示在潜在空间操作，避免计算密集型图像配准，支持基于孕周、出生年龄及病理特征（如脑室扩大、胼胝体发育不全）的条件生成。

Result: CINeMA在精度、效率（构建时间从数天缩短至分钟级）和多功能性（支持组织分割、年龄预测及合成数据生成）上超越现有方法。

Conclusion: CINeMA为脑发育研究提供了高效工具，尤其适用于病理数据稀缺场景，其开源代码和图谱将推动脑科学研究的进步。

Abstract: Magnetic resonance imaging of fetal and neonatal brains reveals rapid
neurodevelopment marked by substantial anatomical changes unfolding within
days. Studying this critical stage of the developing human brain, therefore,
requires accurate brain models-referred to as atlases-of high spatial and
temporal resolution. To meet these demands, established traditional atlases and
recently proposed deep learning-based methods rely on large and comprehensive
datasets. This poses a major challenge for studying brains in the presence of
pathologies for which data remains scarce. We address this limitation with
CINeMA (Conditional Implicit Neural Multi-Modal Atlas), a novel framework for
creating high-resolution, spatio-temporal, multimodal brain atlases, suitable
for low-data settings. Unlike established methods, CINeMA operates in latent
space, avoiding compute-intensive image registration and reducing atlas
construction times from days to minutes. Furthermore, it enables flexible
conditioning on anatomical features including GA, birth age, and pathologies
like ventriculomegaly (VM) and agenesis of the corpus callosum (ACC). CINeMA
supports downstream tasks such as tissue segmentation and age prediction
whereas its generative properties enable synthetic data creation and
anatomically informed data augmentation. Surpassing state-of-the-art methods in
accuracy, efficiency, and versatility, CINeMA represents a powerful tool for
advancing brain research. We release the code and atlases at
https://github.com/m-dannecker/CINeMA.

</details>


### [221] [Kvasir-VQA-x1: A Multimodal Dataset for Medical Reasoning and Robust MedVQA in Gastrointestinal Endoscopy](https://arxiv.org/abs/2506.09958)
*Sushant Gautam,Michael A. Riegler,Pål Halvorsen*

Main category: cs.CV

TL;DR: 研究者提出了Kvasir-VQA-x1，一个针对胃肠道内窥镜的大规模医学视觉问答数据集，通过新增15.9万复杂临床推理问题及视觉增强技术，提升多模态AI系统的临床适用性与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有MedVQA数据集在临床复杂性和视觉多样性上存在不足，限制了临床决策支持系统的开发。需构建更贴近真实场景的挑战性基准数据集。

Method: 1. 使用大语言模型系统生成分层复杂度的问题-答案对；2. 引入模拟常见内窥镜成像伪影的视觉增强技术；3. 设计双评估轨道（标准VQA性能与抗视觉干扰鲁棒性）。

Result: 构建包含159,549新QA对的数据集，覆盖深度临床推理，提供FAIR原则下的开放访问资源。支持模型在真实临床扰动场景下的全面评估。

Conclusion: Kvasir-VQA-x1通过增强临床相关性与视觉多样性，为开发可靠临床多模态AI系统提供新基准，推动医疗AI向实际应用转化。

Abstract: Medical Visual Question Answering (MedVQA) is a promising field for
developing clinical decision support systems, yet progress is often limited by
the available datasets, which can lack clinical complexity and visual
diversity. To address these gaps, we introduce Kvasir-VQA-x1, a new,
large-scale dataset for gastrointestinal (GI) endoscopy. Our work significantly
expands upon the original Kvasir-VQA by incorporating 159,549 new
question-answer pairs that are designed to test deeper clinical reasoning. We
developed a systematic method using large language models to generate these
questions, which are stratified by complexity to better assess a model's
inference capabilities. To ensure our dataset prepares models for real-world
clinical scenarios, we have also introduced a variety of visual augmentations
that mimic common imaging artifacts. The dataset is structured to support two
main evaluation tracks: one for standard VQA performance and another to test
model robustness against these visual perturbations. By providing a more
challenging and clinically relevant benchmark, Kvasir-VQA-x1 aims to accelerate
the development of more reliable and effective multimodal AI systems for use in
clinical settings. The dataset is fully accessible and adheres to FAIR data
principles, making it a valuable resource for the wider research community.
Code and data: https://github.com/Simula/Kvasir-VQA-x1 and
https://huggingface.co/datasets/SimulaMet/Kvasir-VQA-x1

</details>


### [222] [A Shortcut-aware Video-QA Benchmark for Physical Understanding via Minimal Video Pairs](https://arxiv.org/abs/2506.09987)
*Benno Krojer,Mojtaba Komeili,Candace Ross,Quentin Garrido,Koustuv Sinha,Nicolas Ballas,Mahmoud Assran*

Main category: cs.CV

TL;DR: 本文提出MVP基准，通过最小变化视频对解决现有视频QA基准因表面线索导致的分数虚高问题，要求模型在视觉相似但答案相反的成对样本中同时正确，以更准确评估物理理解能力。


<details>
  <summary>Details</summary>
Motivation: 现有视频语言模型评估基准存在依赖表面视觉/文本线索的捷径漏洞，导致模型性能评估不准确。需设计能消除此类偏见的评测方法。

Method: 构建包含55K多选题的MVP基准，覆盖九类视频源。每个样本含最小变化对（视觉相似视频+相同问题+对立答案），强制模型必须同时答对成对样本以防止依赖偏见。

Result: 人类正确率92.9%，最佳开源模型仅40.2%（随机基线25%），证明现有模型严重依赖捷径线索。

Conclusion: MVP通过最小变化对机制有效抑制评估偏差，揭示了当前视频语言模型在物理理解上与人类的巨大差距，为可靠评估提供新标准。

Abstract: Existing benchmarks for assessing the spatio-temporal understanding and
reasoning abilities of video language models are susceptible to score inflation
due to the presence of shortcut solutions based on superficial visual or
textual cues. This paper mitigates the challenges in accurately assessing model
performance by introducing the Minimal Video Pairs (MVP) benchmark, a simple
shortcut-aware video QA benchmark for assessing the physical understanding of
video language models. The benchmark is comprised of 55K high-quality
multiple-choice video QA examples focusing on physical world understanding.
Examples are curated from nine video data sources, spanning first-person
egocentric and exocentric videos, robotic interaction data, and cognitive
science intuitive physics benchmarks. To mitigate shortcut solutions that rely
on superficial visual or textual cues and biases, each sample in MVP has a
minimal-change pair -- a visually similar video accompanied by an identical
question but an opposing answer. To answer a question correctly, a model must
provide correct answers for both examples in the minimal-change pair; as such,
models that solely rely on visual or textual biases would achieve below random
performance. Human performance on MVP is 92.9\%, while the best open-source
state-of-the-art video-language model achieves 40.2\% compared to random
performance at 25\%.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [223] [Surrogate models to optimize plasma assisted atomic layer deposition in high aspect ratio features](https://arxiv.org/abs/2506.09313)
*Angel Yanguas-Gil,Jeffrey W. Elam*

Main category: cond-mat.mtrl-sci

TL;DR: 本研究利用合成数据集训练人工神经网络，通过部分涂覆条件下的截面厚度数据预测等离子体增强原子层沉积（PEALD）的饱和时间，仅需两次未饱和实验即可在10%误差内预测结果，证明机器学习可加速PEALD工艺优化。


<details>
  <summary>Details</summary>
Motivation: 高深宽比结构中的等离子体工艺（如PEALD）因表面复合主导反应，导致纳米结构内实现完全共形覆盖所需时间过长，传统实验优化方法耗时且成本高。

Method: 基于PEALD模拟生成合成数据集，训练人工神经网络预测饱和时间，并构建代理模型判断表面复合是否主导等离子体-表面相互作用。

Result: 仅需两次未饱和实验即可预测饱和时间（误差<10%）；表面复合主导判断模型的准确率达99%。

Conclusion: 机器学习为PEALD工艺优化提供了高效新途径，可扩展至原子层刻蚀及更复杂结构，显著加速微电子等领域工艺开发。

Abstract: In this work we explore surrogate models to optimize plasma enhanced atomic
layer deposition (PEALD) in high aspect ratio features. In plasma-based
processes such as PEALD and atomic layer etching, surface recombination can
dominate the reactivity of plasma species with the surface, which can lead to
unfeasibly long exposure times to achieve full conformality inside
nanostructures like high aspect ratio vias. Using a synthetic dataset based on
simulations of PEALD, we train artificial neural networks to predict saturation
times based on cross section thickness data obtained for partially coated
conditions. The results obtained show that just two experiments in
undersaturated conditions contain enough information to predict saturation
times within 10% of the ground truth. A surrogate model trained to determine
whether surface recombination dominates the plasma-surface interactions in a
PEALD process achieves 99% accuracy. This demonstrates that machine learning
can provide a new pathway to accelerate the optimization of PEALD processes in
areas such as microelectronics. Our approach can be easily extended to atomic
layer etching and more complex structures.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [224] [Large Language Models for Design Structure Matrix Optimization](https://arxiv.org/abs/2506.09749)
*Shuo Jiang,Min Xie,Jianxi Luo*

Main category: cs.CE

TL;DR: 本研究提出一种基于大型语言模型（LLM）的框架，通过结合网络拓扑与领域知识优化设计结构矩阵（DSM）排序问题，实验表明其收敛速度与解质量均超越传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统数学启发式方法在处理大规模复杂DSM依赖网络时，难以捕捉上下文细节且优化效果有限，因此探索LLM在组合优化问题中的潜力。

Method: 提出整合网络拓扑与领域知识的LLM框架，通过语义推理与数学推理协同迭代优化DSM元素序列。

Result: 实验证明该方法在不同DSM案例中收敛更快、解质量更优，且领域知识的引入显著提升各类LLM基线的性能。

Conclusion: LLM通过融合语义与数学推理，为复杂工程组合优化问题开辟新范式，验证了其在工程设计优化中的广阔应用前景。

Abstract: In complex engineering systems, the interdependencies among components or
development activities are often modeled and analyzed using Design Structure
Matrix (DSM). Reorganizing elements within a DSM to minimize feedback loops and
enhance modularity or process efficiency constitutes a challenging
combinatorial optimization (CO) problem in engineering design and operations.
As problem sizes increase and dependency networks become more intricate,
traditional optimization methods that solely use mathematical heuristics often
fail to capture the contextual nuances and struggle to deliver effective
solutions. In this study, we explore the potential of Large Language Models
(LLMs) for helping solve such CO problems by leveraging their capabilities for
advanced reasoning and contextual understanding. We propose a novel LLM-based
framework that integrates network topology with contextual domain knowledge for
iterative optimization of DSM element sequencing - a common CO problem.
Experiments on various DSM cases show that our method consistently achieves
faster convergence and superior solution quality compared to both stochastic
and deterministic baselines. Notably, we find that incorporating contextual
domain knowledge significantly enhances optimization performance regardless of
the chosen LLM backbone. These findings highlight the potential of LLMs to
solve complex engineering CO problems by combining semantic and mathematical
reasoning. This approach paves the way towards a new paradigm in LLM-based
engineering design optimization.

</details>


### [225] [Intelligent Design 4.0: Paradigm Evolution Toward the Agentic AI Era](https://arxiv.org/abs/2506.09755)
*Shuo Jiang,Min Xie,Frank Youhua Chen,Jian Ma,Jianxi Luo*

Main category: cs.CE

TL;DR: 本文提出智能设计4.0（ID 4.0）作为基于多智能体协作的新范式，通过回顾智能设计四阶段演变，构建概念框架并探讨其实现端到端自动化及应对复杂设计挑战的潜力。


<details>
  <summary>Details</summary>
Motivation: 基础模型（如大语言模型）的推理能力为工程设计转型提供了新路径，需探索如何通过多智能体协作系统进一步释放智能设计的潜力。

Method: 回顾智能设计历史四阶段（规则系统、任务模型、基础模型、多智能体协作），提出ID 4.0框架，并分析其通过自主多智能体系统实现全流程自动化的可能性。

Result: 构建了ID 4.0概念框架，指出其通过协调多智能体系统提升工程设计适应性、自主性和效率，并明确未来需增强复杂场景支持、协调机制等方向。

Conclusion: ID 4.0为应对复杂设计挑战奠定了基础，未来需在场景扩展、人机价值对齐、目标自主设定等方向推动智能设计向更高自主性演进。

Abstract: Research and practice in Intelligent Design (ID) have significantly enhanced
engineering innovation, efficiency, quality, and productivity over recent
decades, fundamentally reshaping how engineering designers think, behave, and
interact with design processes. The recent emergence of Foundation Models
(FMs), particularly Large Language Models (LLMs), has demonstrated general
knowledge-based reasoning capabilities, and open new paths and avenues for
further transformation in engineering design. In this context, this paper
introduces Intelligent Design 4.0 (ID 4.0) as an emerging paradigm empowered by
agentic AI systems. We review the historical evolution of ID across four
distinct stages: rule-based expert systems, task-specific machine learning
models, large-scale foundation AI models, and the recent emerging paradigm of
multi-agent collaboration. We propose a conceptual framework for ID 4.0 and
discuss its potential to support end-to-end automation of engineering design
processes through coordinated, autonomous multi-agent-based systems.
Furthermore, we discuss future perspectives to enhance and fully realize ID
4.0's potential, including more complex design scenarios, more practical design
implementations, novel agent coordination mechanisms, and autonomous design
goal-setting with better human value alignment. In sum, these insights lay a
foundation for advancing Intelligent Design toward greater adaptivity,
autonomy, and effectiveness in addressing increasingly complex design
challenges.

</details>


### [226] [Superstudent intelligence in thermodynamics](https://arxiv.org/abs/2506.09822)
*Rebecca Loubet,Pascal Zittlau,Marco Hoffmann,Luisa Vollmer,Sophie Fellenz,Heike Leitte,Fabian Jirasek,Johannes Lenhard,Hans Hasse*

Main category: cs.CE

TL;DR: OpenAI的o3模型在热力学考试中表现优于所有学生，成绩接近历史最佳水平，标志着机器在复杂任务上超越人类能力的转折点。


<details>
  <summary>Details</summary>
Motivation: 探讨AI是否能在需要创造性结合热力学原理的高难度考试中超越人类，验证其在传统认为需人类智力的任务中的潜力。

Method: 将热力学考题同时提供给学生和o3模型，以零样本模式评估答案，评分标准与学生完全一致。

Result: o3在零样本下全对，成绩超过所有学生，且接近1985年以来10,000余次考试中的最佳记录。

Conclusion: 机器在复杂认知任务中的卓越表现将重塑工程师培养模式，并挑战人类智力优势的传统认知。

Abstract: In this short note, we report and analyze a striking event: OpenAI's large
language model o3 has outwitted all students in a university exam on
thermodynamics. The thermodynamics exam is a difficult hurdle for most
students, where they must show that they have mastered the fundamentals of this
important topic. Consequently, the failure rates are very high, A-grades are
rare - and they are considered proof of the students' exceptional intellectual
abilities. This is because pattern learning does not help in the exam. The
problems can only be solved by knowledgeably and creatively combining
principles of thermodynamics. We have given our latest thermodynamics exam not
only to the students but also to OpenAI's most powerful reasoning model, o3,
and have assessed the answers of o3 exactly the same way as those of the
students. In zero-shot mode, the model o3 solved all problems correctly, better
than all students who took the exam; its overall score was in the range of the
best scores we have seen in more than 10,000 similar exams since 1985. This is
a turning point: machines now excel in complex tasks, usually taken as proof of
human intellectual capabilities. We discuss the consequences this has for the
work of engineers and the education of future engineers.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [227] [Adversarial Text Generation with Dynamic Contextual Perturbation](https://arxiv.org/abs/2506.09148)
*Hetvi Waghela,Jaydip Sen,Sneha Rakshit,Subhasis Dasgupta*

Main category: cs.CR

TL;DR: 本文提出动态上下文扰动（DCP）方法，通过动态生成上下文感知的对抗样本，提升自然语言处理模型对抗攻击的隐蔽性和有效性。


<details>
  <summary>Details</summary>
Motivation: 现有对抗攻击方法多局限于词级或局部文本修改，忽略整体上下文，导致扰动易被检测或语义不一致，需更全面的上下文扰动策略。

Method: DCP利用预训练语言模型，在句子、段落和文档层面动态生成上下文扰动，通过对抗目标函数平衡误导模型分类与保持文本自然性，迭代优化生成对抗样本。

Result: 实验表明，DCP在多种NLP模型和数据集上显著提升对抗攻击效果，扰动隐蔽性优于现有方法，有效挑战了前沿模型的鲁棒性。

Conclusion: 上下文在对抗攻击中至关重要，DCP为开发更鲁棒的NLP系统提供新思路，强调需结合动态上下文分析以抵御复杂对抗策略。

Abstract: Adversarial attacks on Natural Language Processing (NLP) models expose
vulnerabilities by introducing subtle perturbations to input text, often
leading to misclassification while maintaining human readability. Existing
methods typically focus on word-level or local text segment alterations,
overlooking the broader context, which results in detectable or semantically
inconsistent perturbations. We propose a novel adversarial text attack scheme
named Dynamic Contextual Perturbation (DCP). DCP dynamically generates
context-aware perturbations across sentences, paragraphs, and documents,
ensuring semantic fidelity and fluency. Leveraging the capabilities of
pre-trained language models, DCP iteratively refines perturbations through an
adversarial objective function that balances the dual objectives of inducing
model misclassification and preserving the naturalness of the text. This
comprehensive approach allows DCP to produce more sophisticated and effective
adversarial examples that better mimic natural language patterns. Our
experimental results, conducted on various NLP models and datasets, demonstrate
the efficacy of DCP in challenging the robustness of state-of-the-art NLP
systems. By integrating dynamic contextual analysis, DCP significantly enhances
the subtlety and impact of adversarial attacks. This study highlights the
critical role of context in adversarial attacks and lays the groundwork for
creating more robust NLP systems capable of withstanding sophisticated
adversarial strategies.

</details>


### [228] [Empirical Quantification of Spurious Correlations in Malware Detection](https://arxiv.org/abs/2506.09662)
*Bianca Perasso,Ludovico Lozza,Andrea Ponte,Luca Demetrio,Luca Oneto,Fabio Roli*

Main category: cs.CR

TL;DR: 本文通过分析端到端深度学习模型在恶意软件检测中过度依赖编译器遗留的空白空间等虚假相关性特征的问题，提出了一种在小规模平衡数据集上评估模型适用性的方法，并给出模型排名。


<details>
  <summary>Details</summary>
Motivation: 现有端到端深度学习模型在恶意软件检测中依赖虚假相关性特征（如元数据），但缺乏对其影响程度的量化分析，需进一步探索此类特征对模型决策的实际影响。

Method: 通过分析模型对编译器遗留空白空间的依赖程度，设计小规模平衡数据集进行实验，并引入两种端到端模型的排名机制。

Result: 发现模型过度依赖与代码功能无关的空白空间特征，导致编译代码的实际相关性降低，最终通过排名确定了更适合生产环境的模型。

Conclusion: 研究揭示了深度学习模型在恶意软件检测中的局限性，并为选择实际部署模型提供了量化依据。

Abstract: End-to-end deep learning exhibits unmatched performance for detecting
malware, but such an achievement is reached by exploiting spurious correlations
-- features with high relevance at inference time, but known to be useless
through domain knowledge. While previous work highlighted that deep networks
mainly focus on metadata, none investigated the phenomenon further, without
quantifying their impact on the decision. In this work, we deepen our
understanding of how spurious correlation affects deep learning for malware
detection by highlighting how much models rely on empty spaces left by the
compiler, which diminishes the relevance of the compiled code. Through our
seminal analysis on a small-scale balanced dataset, we introduce a ranking of
two end-to-end models to better understand which is more suitable to be put in
production.

</details>


### [229] [What is the Cost of Differential Privacy for Deep Learning-Based Trajectory Generation?](https://arxiv.org/abs/2506.09312)
*Erik Buchholz,Natasha Fernandes,David D. Nguyen,Alsharif Abuadbba,Surya Nepal,Salil S. Kanhere*

Main category: cs.CR

TL;DR: 本文探讨在基于深度学习的轨迹生成模型中应用差分隐私（DP）的效用-隐私权衡问题，提出一种新的条件生成DP机制，并比较不同模型类型（扩散、VAE、GAN）在DP约束下的表现。结果表明，DP-SGD显著降低效用，但大数据集下可保留部分效用；提出的DP机制提升训练稳定性；GAN在DP约束下表现最优，而扩散模型在无隐私保证时最佳。


<details>
  <summary>Details</summary>
Motivation: 现有基于深度学习的轨迹生成模型缺乏正式隐私保证，且生成过程依赖真实数据的条件信息。研究DP在此类模型中的效用成本，旨在解决隐私保护与数据实用性之间的平衡问题，并探索不同模型类型在DP约束下的表现差异。

Method: 1) 评估DP-SGD对现有生成模型效用的影响；2) 提出具有正式隐私保证的条件生成DP机制；3) 在两种数据集和11个指标下，分析扩散模型、VAE、GAN在效用-隐私权衡中的差异。

Result: DP-SGD显著降低模型性能，但大数据集仍保留部分效用；新DP机制提升训练稳定性（尤其对GAN和小数据集）；扩散模型在无隐私保证时最优，而DP-SGD下GAN表现最佳。正式隐私保证仅在大数据集和受限场景中可行。

Conclusion: DP轨迹生成仍具挑战性，当前正式隐私保证仅适用于大数据集和特定场景。模型选择需权衡隐私约束：无约束时扩散模型最优，而DP约束下GAN更佳。条件生成DP机制为不稳定模型提供稳定性支持。

Abstract: While location trajectories offer valuable insights, they also reveal
sensitive personal information. Differential Privacy (DP) offers formal
protection, but achieving a favourable utility-privacy trade-off remains
challenging. Recent works explore deep learning-based generative models to
produce synthetic trajectories. However, current models lack formal privacy
guarantees and rely on conditional information derived from real data during
generation. This work investigates the utility cost of enforcing DP in such
models, addressing three research questions across two datasets and eleven
utility metrics. (1) We evaluate how DP-SGD, the standard DP training method
for deep learning, affects the utility of state-of-the-art generative models.
(2) Since DP-SGD is limited to unconditional models, we propose a novel DP
mechanism for conditional generation that provides formal guarantees and assess
its impact on utility. (3) We analyse how model types - Diffusion, VAE, and GAN
- affect the utility-privacy trade-off. Our results show that DP-SGD
significantly impacts performance, although some utility remains if the
datasets is sufficiently large. The proposed DP mechanism improves training
stability, particularly when combined with DP-SGD, for unstable models such as
GANs and on smaller datasets. Diffusion models yield the best utility without
guarantees, but with DP-SGD, GANs perform best, indicating that the best
non-private model is not necessarily optimal when targeting formal guarantees.
In conclusion, DP trajectory generation remains a challenging task, and formal
guarantees are currently only feasible with large datasets and in constrained
use cases.

</details>


### [230] [TooBadRL: Trigger Optimization to Boost Effectiveness of Backdoor Attacks on Deep Reinforcement Learning](https://arxiv.org/abs/2506.09562)
*Songze Li,Mingxuan Zhang,Oubo Ma,Kang Wei,Shouling Ji*

Main category: cs.CR

TL;DR: 本文提出首个系统优化深度强化学习（DRL）后门攻击触发器的框架TooBadRL，通过时间、空间、幅度三轴优化显著提升攻击成功率，同时保持正常任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有DRL后门攻击主要依赖启发式触发配置，未充分探索触发优化的潜力，导致攻击效果受限。

Method: 1. 时间轴：性能感知自适应冻结机制控制注入时机；2. 空间轴：基于Shapley值的合作游戏选择关键状态变量；3. 幅度轴：梯度对抗优化调整注入强度。

Result: 在3种主流DRL算法和9个基准任务中，TooBadRL攻击成功率显著提高（如Hopper任务提升30%），且正常任务性能下降可忽略。

Conclusion: 触发器的系统性优化对DRL后门攻击效果至关重要，现有方法低估了其重要性，TooBadRL为此提供了理论框架与实践验证。

Abstract: Deep reinforcement learning (DRL) has achieved remarkable success in a wide
range of sequential decision-making domains, including robotics, healthcare,
smart grids, and finance. Recent research demonstrates that attackers can
efficiently exploit system vulnerabilities during the training phase to execute
backdoor attacks, producing malicious actions when specific trigger patterns
are present in the state observations. However, most existing backdoor attacks
rely primarily on simplistic and heuristic trigger configurations, overlooking
the potential efficacy of trigger optimization. To address this gap, we
introduce TooBadRL (Trigger Optimization to Boost Effectiveness of Backdoor
Attacks on DRL), the first framework to systematically optimize DRL backdoor
triggers along three critical axes, i.e., temporal, spatial, and magnitude.
Specifically, we first introduce a performance-aware adaptive freezing
mechanism for injection timing. Then, we formulate dimension selection as a
cooperative game, utilizing Shapley value analysis to identify the most
influential state variable for the injection dimension. Furthermore, we propose
a gradient-based adversarial procedure to optimize the injection magnitude
under environment constraints. Evaluations on three mainstream DRL algorithms
and nine benchmark tasks show that TooBadRL significantly improves attack
success rates, while ensuring minimal degradation of normal task performance.
These results highlight the previously underappreciated importance of
principled trigger optimization in DRL backdoor attacks. The source code of
TooBadRL can be found at https://github.com/S3IC-Lab/TooBadRL.

</details>


### [231] [LLMail-Inject: A Dataset from a Realistic Adaptive Prompt Injection Challenge](https://arxiv.org/abs/2506.09956)
*Sahar Abdelnabi,Aideen Fay,Ahmed Salem,Egor Zverev,Kai-Chieh Liao,Chi-Huang Liu,Chun-Chih Kuo,Jannis Weigend,Danyael Manlangit,Alex Apostolov,Haris Umair,João Donato,Masayuki Kawakita,Athar Mahboob,Tran Huu Bach,Tsun-Han Chiang,Myeongjin Cho,Hajin Choi,Byeonghyeon Kim,Hyeonjin Lee,Benjamin Pannell,Conor McCauley,Mark Russinovich,Andrew Paverd,Giovanni Cherubin*

Main category: cs.CR

TL;DR: 论文提出LLMail-Inject挑战赛，通过模拟现实场景研究间接提示注入攻击对LLM的威胁，收集大量攻击数据以分析防御策略有效性，并推动未来结构化解决方案。


<details>
  <summary>Details</summary>
Motivation: 现有防御方法对适应性攻击者的系统性评估不足，而成功的间接提示注入攻击可能导致严重安全与隐私问题，许多实际LLM应用仍存在漏洞。

Method: 设计公开挑战LLMail-Inject，模拟攻击者通过电子邮件注入恶意指令触发未授权工具调用，覆盖多种防御方案、LLM架构与检索配置。

Result: 收集来自839名参与者的208,095次攻击提交数据，分析揭示指令-数据分离问题的新见解，并公开挑战代码与完整数据集。

Conclusion: 该研究为未来解决提示注入攻击的实用结构化方案奠定基础，强调需系统性评估对抗性攻击以提升LLM安全性。

Abstract: Indirect Prompt Injection attacks exploit the inherent limitation of Large
Language Models (LLMs) to distinguish between instructions and data in their
inputs. Despite numerous defense proposals, the systematic evaluation against
adaptive adversaries remains limited, even when successful attacks can have
wide security and privacy implications, and many real-world LLM-based
applications remain vulnerable. We present the results of LLMail-Inject, a
public challenge simulating a realistic scenario in which participants
adaptively attempted to inject malicious instructions into emails in order to
trigger unauthorized tool calls in an LLM-based email assistant. The challenge
spanned multiple defense strategies, LLM architectures, and retrieval
configurations, resulting in a dataset of 208,095 unique attack submissions
from 839 participants. We release the challenge code, the full dataset of
submissions, and our analysis demonstrating how this data can provide new
insights into the instruction-data separation problem. We hope this will serve
as a foundation for future research towards practical structural solutions to
prompt injection.

</details>


<div id='math.PR'></div>

# math.PR [[Back]](#toc)

### [232] [A theoretical basis for model collapse in recursive training](https://arxiv.org/abs/2506.09401)
*Vivek Shripad Borkar*

Main category: math.PR

TL;DR: 研究揭示生成模型递归训练中概率分布崩溃现象存在两种不同渐近行为，取决于是否引入外部样本源。


<details>
  <summary>Details</summary>
Motivation: 针对生成模型递归训练导致概率分布崩溃的已知问题，探讨外部样本源对崩溃模式的影响机制。

Method: 通过理论分析比较存在/不存在外部样本源两种条件下的模型训练动态过程。

Result: 发现当存在任意微小外部样本源时，系统会收敛到混合分布；而无外部源时则完全崩溃。

Conclusion: 外部样本源的引入从根本上改变了生成模型递归训练的渐近行为，防止完全分布退化。

Abstract: It is known that recursive training from generative models can lead to the so
called `collapse' of the simulated probability distribution. This note shows
that one in fact gets two different asymptotic behaviours depending on whether
an external source, howsoever minor, is also contributing samples.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [233] [Empirical and computer-aided robustness analysis of long-step and accelerated methods in smooth convex optimization](https://arxiv.org/abs/2506.09730)
*Pierre Vernimmen,François Glineur*

Main category: math.OC

TL;DR: 本文通过理论和性能估计方法评估了不同一阶优化方法在梯度计算不精确时的鲁棒性，发现加速方法实际表现优于理论预期，并提出缩短因子显著提升长步法性能。


<details>
  <summary>Details</summary>
Motivation: 研究梯度计算中存在相对不精确性（如GPU大规模问题中的梯度压缩）时，不同优化方法的稳定性，以解决实际应用中因信息压缩导致的性能下降问题。

Method: 分析恒定步长梯度下降、长步法、加速方法三类方法，引入半启发式缩短因子改进理论鲁棒性，并在两种相对不精确场景下进行实验验证。

Result: 加速方法实际鲁棒性远超理论预期；缩短因子显著提升长步法性能；所有改进后的方法在不精确场景下均表现良好。

Conclusion: 缩短因子有效增强了优化方法的鲁棒性，加速方法在梯度不精确时仍具备实用潜力，改进后的方法为大规模问题提供了可行解决方案。

Abstract: This work assesses both empirically and theoretically, using the performance
estimation methodology, how robust different first-order optimization methods
are when subject to relative inexactness in their gradient computations.
Relative inexactness occurs, for example, when compressing the gradient using
fewer bits of information, which happens when dealing with large-scale problems
on GPUs. Three major families of methods are analyzed: constant step gradient
descent, long-step methods, and accelerated methods. The latter two are first
shown to be theoretically not robust to inexactness. Then, a semi-heuristic
shortening factor is introduced to improve their theoretical guarantees. All
methods are subsequently tested on a concrete inexact problem, with two
different types of relative inexactness, and it is observed that both
accelerated methods are much more robust than expected, and that the shortening
factor significantly helps the long-step methods. In the end, all shortened
methods appear to be promising, even in this inexact setting.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [234] [Reconstructing Heterogeneous Biomolecules via Hierarchical Gaussian Mixtures and Part Discovery](https://arxiv.org/abs/2506.09063)
*Shayan Shekarforoush,David B. Lindell,Marcus A. Brubaker,David J. Fleet*

Main category: q-bio.QM

TL;DR: 本文提出了一种名为CryoSPIRE的新型冷冻电镜3D重建框架，通过分层高斯混合模型处理分子结构的非刚性构象变化和成分变异，在复杂实验数据和基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 冷冻电镜在解析分子3D结构时面临非刚性构象变化和成分变异（如部分结构缺失）的挑战，现有方法难以有效建模此类复杂异质性。

Method: 提出基于分层高斯混合模型的框架，结合高斯泼溅技术，通过部件分割的初始推断过程引入归纳偏置，以同时处理构象与成分异质性。

Result: CryoSPIRE在实验数据集上揭示了具有生物学意义的结构，并在CryoBench基准测试中达到当前最优水平。

Conclusion: 该框架为冷冻电镜异质性分析提供了新方法，其部件分割驱动的建模策略显著提升了复杂分子结构的解析能力。

Abstract: Cryo-EM is a transformational paradigm in molecular biology where
computational methods are used to infer 3D molecular structure at atomic
resolution from extremely noisy 2D electron microscope images. At the forefront
of research is how to model the structure when the imaged particles exhibit
non-rigid conformational flexibility and compositional variation where parts
are sometimes missing. We introduce a novel 3D reconstruction framework with a
hierarchical Gaussian mixture model, inspired in part by Gaussian Splatting for
4D scene reconstruction. In particular, the structure of the model is grounded
in an initial process that infers a part-based segmentation of the particle,
providing essential inductive bias in order to handle both conformational and
compositional variability. The framework, called CryoSPIRE, is shown to reveal
biologically meaningful structures on complex experimental datasets, and
establishes a new state-of-the-art on CryoBench, a benchmark for cryo-EM
heterogeneity methods.

</details>


### [235] [Detecting malignant dynamics on very few blood sample using signature coefficients](https://arxiv.org/abs/2506.09097)
*Rémi Vaucher,Stéphane Chrétien*

Main category: q-bio.QM

TL;DR: 本研究提出结合连续时间马尔可夫模型与Signature理论，通过分析血液中循环肿瘤DNA（ctDNA）动态变化，解决患者血液样本稀缺问题，实现侵袭性癌症检测。实验验证了方法的有效性。


<details>
  <summary>Details</summary>
Motivation: ctDNA作为低负担的癌症监测手段具有潜力，但其动态监测面临单患者样本量极少的挑战。现有方法（如GRAIL产品）需更高效的数据处理方案。

Method: 整合连续时间马尔可夫模型（描述ctDNA动态）与机器学习中的Signature理论（针对不规则采样信号的特征提取），构建高效检测流程。

Result: 数值实验表明该方法能有效克服数据稀缺性，验证了检测侵袭性肿瘤的可行性。

Conclusion: Signature理论与动态建模的结合为ctDNA驱动的癌症早筛提供了新范式，尤其在少量样本场景下展现应用潜力。

Abstract: Recent discoveries have suggested that the promising avenue of using
circulating tumor DNA (ctDNA) levels in blood samples provides reasonable
accuracy for cancer monitoring, with extremely low burden on the patient's
side. It is known that the presence of ctDNA can result from various mechanisms
leading to DNA release from cells, such as apoptosis, necrosis or active
secretion. One key idea in recent cancer monitoring studies is that monitoring
the dynamics of ctDNA levels might be sufficient for early multi-cancer
detection. This interesting idea has been turned into commercial products, e.g.
in the company named GRAIL.
  In the present work, we propose to explore the use of Signature theory for
detecting aggressive cancer tumors based on the analysis of blood samples. Our
approach combines tools from continuous time Markov modelling for the dynamics
of ctDNA levels in the blood, with Signature theory for building efficient
testing procedures. Signature theory is a topic of growing interest in the
Machine Learning community (see Chevyrev2016 and Fermanian2021), which is now
recognised as a powerful feature extraction tool for irregularly sampled
signals. The method proposed in the present paper is shown to correctly address
the challenging problem of overcoming the inherent data scarsity due to the
extremely small number of blood samples per patient. The relevance of our
approach is illustrated with extensive numerical experiments that confirm the
efficiency of the proposed pipeline.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [236] [Exploring Image Transforms derived from Eye Gaze Variables for Progressive Autism Diagnosis](https://arxiv.org/abs/2506.09065)
*Abigail Copiaco,Christian Ritz,Yassine Himeur,Valsamma Eapen,Ammar Albanna,Wathiq Mansoor*

Main category: eess.IV

TL;DR: 本文提出一种基于迁移学习和眼动图像转换的AI辅助技术，用于简化自闭症谱系障碍（ASD）的诊断与管理，实现家庭化定期检测并保护隐私。


<details>
  <summary>Details</summary>
Motivation: 当前ASD诊断方法耗时且成本高，导致患者及护理者负担加重。亟需高效、便捷且保护隐私的解决方案。

Method: 结合迁移学习与眼动追踪数据生成的图像变换技术，构建ASD诊断系统，支持居家周期性检测。

Result: 系统降低诊断压力与经济成本，通过图像变换保护隐私，并促进监护人与治疗师间的持续沟通与进展追踪。

Conclusion: 该方法为ASD群体提供及时、可及的诊断方案，兼顾隐私保护与支持体系优化，改善患者预后。

Abstract: The prevalence of Autism Spectrum Disorder (ASD) has surged rapidly over the
past decade, posing significant challenges in communication, behavior, and
focus for affected individuals. Current diagnostic techniques, though
effective, are time-intensive, leading to high social and economic costs. This
work introduces an AI-powered assistive technology designed to streamline ASD
diagnosis and management, enhancing convenience for individuals with ASD and
efficiency for caregivers and therapists. The system integrates transfer
learning with image transforms derived from eye gaze variables to diagnose ASD.
This facilitates and opens opportunities for in-home periodical diagnosis,
reducing stress for individuals and caregivers, while also preserving user
privacy through the use of image transforms. The accessibility of the proposed
method also offers opportunities for improved communication between guardians
and therapists, ensuring regular updates on progress and evolving support
needs. Overall, the approach proposed in this work ensures timely, accessible
diagnosis while protecting the subjects' privacy, improving outcomes for
individuals with ASD.

</details>


### [237] [Foundation Models in Medical Imaging -- A Review and Outlook](https://arxiv.org/abs/2506.09095)
*Vivien van Veldhuizen,Vanessa Botha,Chunyao Lu,Melis Erdal Cesur,Kevin Groot Lipman,Edwin D. de Jong,Hugo Horlings,Clárisa Sanchez,Cees Snoek,Ritse Mann,Eric Marcus,Jonas Teuwen*

Main category: eess.IV

TL;DR: 本文综述了基础模型（FMs）在医学影像分析中的应用，涵盖病理学、放射学和眼科学领域。FMs通过无监督预训练学习通用视觉特征，减少对标注数据的依赖，并探讨了模型架构、自监督学习方法及下游任务适配策略。基于150多项研究，总结了各领域应用现状、设计选择差异及未来挑战。


<details>
  <summary>Details</summary>
Motivation: 传统医学影像分析依赖大量人工标注数据，成本高且泛化性有限。基础模型（FMs）通过无监督预训练学习通用特征，可灵活适配不同临床任务，为提升医学影像分析的效率和可扩展性提供新途径。

Method: 系统回顾150+项研究，分析FMs在医学影像中的开发与应用流程，包括模型架构（如Transformer）、自监督学习技术（如对比学习），以及下游任务适配策略（如微调、提示学习）。对比病理、放射、眼科领域的设计差异。

Result: FMs在病理学（组织切片）、放射学（CT/MRI）和眼科（眼底图像）中已成功应用，不同领域偏好特定架构与适配方法。自监督预训练显著减少标注需求，但模型泛化性、数据隐私和计算成本仍是挑战。

Conclusion: FMs为医学影像分析带来范式创新，但其临床落地需解决数据异质性、可解释性及伦理问题。未来研究需探索轻量化适配、多模态融合及真实临床场景验证。

Abstract: Foundation models (FMs) are changing the way medical images are analyzed by
learning from large collections of unlabeled data. Instead of relying on
manually annotated examples, FMs are pre-trained to learn general-purpose
visual features that can later be adapted to specific clinical tasks with
little additional supervision. In this review, we examine how FMs are being
developed and applied in pathology, radiology, and ophthalmology, drawing on
evidence from over 150 studies. We explain the core components of FM pipelines,
including model architectures, self-supervised learning methods, and strategies
for downstream adaptation. We also review how FMs are being used in each
imaging domain and compare design choices across applications. Finally, we
discuss key challenges and open questions to guide future research.

</details>


### [238] [Low-Rank Augmented Implicit Neural Representation for Unsupervised High-Dimensional Quantitative MRI Reconstruction](https://arxiv.org/abs/2506.09100)
*Haonan Zhang,Guoyan Lao,Yuyao Zhang,Hongjiang Wei*

Main category: eess.IV

TL;DR: 提出LoREIN框架，结合低秩先验和连续性先验，通过低秩表示和隐式神经表示实现高效3D多参数定量磁共振成像重建，并引入零样本学习范式。


<details>
  <summary>Details</summary>
Motivation: 现有多参数定量磁共振成像重建方法依赖单一先验或物理模型，难以解决高度欠采样、高维数据下的病态逆问题，导致重建效果不佳。

Method: LoREIN框架整合低秩表示（LRR）和隐式神经表示（INR），利用低秩先验优化空间基，连续性先验增强加权图像重建，并通过多对比加权图像提供定量引导。

Result: 双先验协同提高了加权图像和定量参数图的重建精度，零样本学习范式在复杂时空及高维图像重建中展现出潜力。

Conclusion: LoREIN通过双先验与神经表示结合，显著提升重建质量，为零样本医学影像重建提供了新思路。

Abstract: Quantitative magnetic resonance imaging (qMRI) provides tissue-specific
parameters vital for clinical diagnosis. Although simultaneous multi-parametric
qMRI (MP-qMRI) technologies enhance imaging efficiency, robustly reconstructing
qMRI from highly undersampled, high-dimensional measurements remains a
significant challenge. This difficulty arises primarily because current
reconstruction methods that rely solely on a single prior or physics-informed
model to solve the highly ill-posed inverse problem, which often leads to
suboptimal results. To overcome this limitation, we propose LoREIN, a novel
unsupervised and dual-prior-integrated framework for accelerated 3D MP-qMRI
reconstruction. Technically, LoREIN incorporates both low-rank prior and
continuity prior via low-rank representation (LRR) and implicit neural
representation (INR), respectively, to enhance reconstruction fidelity. The
powerful continuous representation of INR enables the estimation of optimal
spatial bases within the low-rank subspace, facilitating high-fidelity
reconstruction of weighted images. Simultaneously, the predicted multi-contrast
weighted images provide essential structural and quantitative guidance, further
enhancing the reconstruction accuracy of quantitative parameter maps.
Furthermore, our work introduces a zero-shot learning paradigm with broad
potential in complex spatiotemporal and high-dimensional image reconstruction
tasks, further advancing the field of medical imaging.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [239] [ThinkQE: Query Expansion via an Evolving Thinking Process](https://arxiv.org/abs/2506.09260)
*Yibin Lei,Tao Shen,Andrew Yates*

Main category: cs.IR

TL;DR: 本文提出ThinkQE框架，通过结合基于思考的扩展过程和语料库交互策略，解决现有LLM查询扩展方法在多样性和探索性上的不足，显著提升检索性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的查询扩展方法虽提升了检索性能，但生成的扩展过于狭窄，缺乏对多角度语义探索和结果多样性的支持，限制了实际应用效果。

Method: 提出ThinkQE框架：1) 基于思考的扩展过程，通过深层语义探索生成多样化扩展；2) 利用语料库检索反馈迭代优化扩展内容。

Result: 在DL19、DL20、BRIGHT等基准测试中，ThinkQE性能优于现有方法（包括需训练的密集检索器和重排模型），验证其有效性和领域泛化能力。

Conclusion: ThinkQE通过增强语义探索与动态反馈机制，解决了传统LLM扩展的局限性，为无监督训练的查询扩展提供了高效解决方案。

Abstract: Effective query expansion for web search benefits from promoting both
exploration and result diversity to capture multiple interpretations and facets
of a query. While recent LLM-based methods have improved retrieval performance
and demonstrate strong domain generalization without additional training, they
often generate narrowly focused expansions that overlook these desiderata. We
propose ThinkQE, a test-time query expansion framework addressing this
limitation through two key components: a thinking-based expansion process that
encourages deeper and comprehensive semantic exploration, and a
corpus-interaction strategy that iteratively refines expansions using retrieval
feedback from the corpus. Experiments on diverse web search benchmarks (DL19,
DL20, and BRIGHT) show ThinkQE consistently outperforms prior approaches,
including training-intensive dense retrievers and rerankers.

</details>


### [240] [Revisiting Graph Projections for Effective Complementary Product Recommendation](https://arxiv.org/abs/2506.09209)
*Leandro Anghinoni,Pablo Zivic,Jorge Adrian Sanchez*

Main category: cs.IR

TL;DR: 本文提出一种基于有向加权图结构的互补商品推荐方法，通过用户-物品二分图投影推断互补关系，在稀疏噪声数据中实现显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 用户-物品交互数据存在噪声和稀疏性问题，导致传统互补商品推荐效果受限。

Method: 从用户-物品二分图构建有向加权投影图，通过图结构关系预测商品互补性。

Result: 在多个基准测试中，模型性能分别超越序列推荐方法43%、图推荐方法38%。

Conclusion: 验证了基于图结构投影的方法在复杂推荐场景中的有效性和计算效率优势。

Abstract: Complementary product recommendation is a powerful strategy to improve
customer experience and retail sales. However, recommending the right product
is not a simple task because of the noisy and sparse nature of user-item
interactions. In this work, we propose a simple yet effective method to predict
a list of complementary products given a query item, based on the structure of
a directed weighted graph projected from the user-item bipartite graph. We
revisit bipartite graph projections for recommender systems and propose a novel
approach for inferring complementarity relationships from historical user-item
interactions. We compare our model with recent methods from the literature and
show, despite the simplicity of our approach, an average improvement of +43%
and +38% over sequential and graph-based recommenders, respectively, over
different benchmarks.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [241] [Estimating Visceral Adiposity from Wrist-Worn Accelerometry](https://arxiv.org/abs/2506.09167)
*James R. Williamson,Andrew Alini,Brian A. Telfer,Adam W. Potter,Karl E. Friedl*

Main category: eess.SP

TL;DR: 研究通过分析NHANES数据，结合传统特征工程与深度学习模型，证明日常活动（PA）与内脏脂肪（VAT）高度相关（r=0.86），进而支持PA对代谢健康的影响。


<details>
  <summary>Details</summary>
Motivation: 内脏脂肪（VAT）是代谢健康与日常活动（PA）的关键指标，且与2型糖尿病等疾病高度相关。研究旨在通过PA数据非侵入性估计VAT，探索PA与代谢健康风险的关联。

Method: 使用两种方法从加速度计数据估计VAT：1）基于步态/睡眠运动特征工程与岭回归；2）深度神经网络（基础模型+Transformer）处理24小时连续数据。结合协变量（人口统计、身体指标）优化模型，最终融合两种方法。

Result: 结合两种方法后，VAT估计相关性达r=0.86，表明PA与VAT存在强关联。加入协变量可提升模型准确性。

Conclusion: PA与VAT的强相关性揭示了PA对代谢健康的重要性，结合多模态方法可高效估计VAT，为非侵入性代谢风险评估提供新途径。

Abstract: Visceral adipose tissue (VAT) is a key marker of both metabolic health and
habitual physical activity (PA). Excess VAT is highly correlated with type 2
diabetes and insulin resistance. The mechanistic basis for this pathophysiology
relates to overloading the liver with fatty acids. VAT is also a highly labile
fat depot, with increased turnover stimulated by catecholamines during
exercise. VAT can be measured with sophisticated imaging technologies, but can
also be inferred directly from PA. We tested this relationship using National
Health and Nutrition Examination Survey (NHANES) data from 2011-2014, for
individuals aged 20-60 years with 7 days of accelerometry data (n=2,456 men;
2,427 women) [1]. Two approaches were used for estimating VAT from activity.
The first used engineered features based on movements during gait and sleep,
and then ridge regression to map summary statistics of these features into a
VAT estimate. The second approach used deep neural networks trained on 24 hours
of continuous accelerometry. A foundation model first mapped each 10s frame
into a high-dimensional feature vector. A transformer model then mapped each
day's feature vector time series into a VAT estimate, which were averaged over
multiple days. For both approaches, the most accurate estimates were obtained
with the addition of covariate information about subject demographics and body
measurements. The best performance was obtained by combining the two
approaches, resulting in VAT estimates with correlations of r=0.86. These
findings demonstrate a strong relationship between PA and VAT and, by
extension, between PA and metabolic health risks.

</details>


### [242] [Integration of Contrastive Predictive Coding and Spiking Neural Networks](https://arxiv.org/abs/2506.09194)
*Emirhan Bilgiç,Neslihan Serap Şengör,Namık Berk Yalabık,Yavuz Selim İşler,Aykut Görkem Gelen,Rahmi Elibol*

Main category: eess.SP

TL;DR: 本研究将对比预测编码（CPC）与脉冲神经网络（SNN）结合，提出一种更具生物合理性的预测模型，并在MNIST数据集上验证其高效分类能力，同时证明SNN可兼具编码机制功能。


<details>
  <summary>Details</summary>
Motivation: 旨在通过融合CPC（学习数据预测结构）与SNN（模拟生物神经系统的时序计算），构建更接近生物神经机制的脉冲系统，提升预测编码模型的生物合理性。

Method: 在基于脉冲的系统中处理输入输出，设计结合CPC与SNN的模型，使用MNIST数据集测试其区分正序样本与非序负样本的能力。

Result: 模型在MNIST数据集上实现高分类准确率，成功区分序列与非序列样本，且SNN在分类任务中同时表现出编码机制的功能。

Conclusion: CPC与SNN的有效性结合得到验证，表明SNN不仅适用于分类任务，还可作为编码机制，为生物启发的计算模型提供新思路。

Abstract: This study examines the integration of Contrastive Predictive Coding (CPC)
with Spiking Neural Networks (SNN). While CPC learns the predictive structure
of data to generate meaningful representations, SNN mimics the computational
processes of biological neural systems over time. In this study, the goal is to
develop a predictive coding model with greater biological plausibility by
processing inputs and outputs in a spike-based system. The proposed model was
tested on the MNIST dataset and achieved a high classification rate in
distinguishing positive sequential samples from non-sequential negative
samples. The study demonstrates that CPC can be effectively combined with SNN,
showing that an SNN trained for classification tasks can also function as an
encoding mechanism. Project codes and detailed results can be accessed on our
GitHub page: https://github.com/vnd-ogrenme/ongorusel-kodlama/tree/main/CPC_SNN

</details>


### [243] [Graph Attention-based Decentralized Actor-Critic for Dual-Objective Control of Multi-UAV Swarms](https://arxiv.org/abs/2506.09195)
*Haoran Peng,Ying-Jun Angela Zhang*

Main category: eess.SP

TL;DR: 该研究提出基于图注意力的分散式行动者-评论家模型（GADC），通过KL散度平衡多无人机系统的覆盖范围与电池寿命双目标优化，在仿真与真实环境中验证了其优越性能。


<details>
  <summary>Details</summary>
Motivation: 多无人机系统需同时优化服务覆盖范围（主目标）与延长电池寿命（次目标），但现有方法在双目标优化与状态维度处理上存在不足。

Method: 结合图注意力网络处理局部观测降维，构建行动者-双重评论家网络管理双策略，并引入KL散度因子动态平衡目标冲突。

Result: 在理想环境与NVIDIA Sionna射线追踪场景中，GADC覆盖性能提升15-20%，电池寿命延长10%以上，且展现强扩展性。

Conclusion: GADC通过联合优化框架有效解决多无人机系统双目标权衡问题，为复杂动态环境下的资源分配提供了新范式。

Abstract: This research focuses on optimizing multi-UAV systems with dual objectives:
maximizing service coverage as the primary goal while extending battery
lifetime as the secondary objective. We propose a Graph Attention-based
Decentralized Actor-Critic (GADC) to optimize the dual objectives. The proposed
approach leverages a graph attention network to process UAVs' limited local
observation and reduce the dimension of the environment states. Subsequently,
an actor-double-critic network is developed to manage dual policies for joint
objective optimization. The proposed GADC uses a Kullback-Leibler (KL)
divergence factor to balance the tradeoff between coverage performance and
battery lifetime in the multi-UAV system. We assess the scalability and
efficiency of GADC through comprehensive benchmarking against state-of-the-art
methods, considering both theory and experimental aspects. Extensive testing in
both ideal settings and NVIDIA Sionna's realistic ray tracing environment
demonstrates GADC's superior performance.

</details>


### [244] [AI-Driven SEEG Channel Ranking for Epileptogenic Zone Localization](https://arxiv.org/abs/2506.09255)
*Saeed Hashemi,Genchang Peng,Mehrdad Nourani,Omar Nofal,Jay Harvey*

Main category: eess.SP

TL;DR: 提出一种结合临床选择和SHAP评分的机器学习方法，利用XGBoost模型自动筛选SEEG关键致痫通道，并通过通道扩展策略提高癫痫灶定位效率。


<details>
  <summary>Details</summary>
Motivation: 传统SEEG人工检查数百个通道耗时低效，需通过自动化方法辅助临床医生快速定位致痫区。

Method: 使用XGBoost训练发作期特征分类模型，通过SHAP评分量化通道贡献度排序，并采用通道扩展策略扩大癫痫灶搜索范围。

Result: 在5例患者数据中验证，该方法在准确性、一致性和可解释性方面均表现良好。

Conclusion: 机器学习结合临床经验能有效扩展致痫区检测范围，为SEEG分析提供高效可解释的辅助工具。

Abstract: Stereo-electroencephalography (SEEG) is an invasive technique to implant
depth electrodes and collect data for pre-surgery evaluation. Visual inspection
of signals recorded from hundreds of channels is time consuming and
inefficient. We propose a machine learning approach to rank the impactful
channels by incorporating clinician's selection and computational finding. A
classification model using XGBoost is trained to learn the discriminative
features of each channel during ictal periods. Then, the SHapley Additive
exPlanations (SHAP) scoring is utilized to rank SEEG channels based on their
contribution to seizures. A channel extension strategy is also incorporated to
expand the search space and identify suspicious epileptogenic zones beyond
those selected by clinicians. For validation, SEEG data for five patients were
analyzed showing promising results in terms of accuracy, consistency, and
explainability.

</details>


### [245] [Cross-Channel Unlabeled Sensing over a Union of Signal Subspaces](https://arxiv.org/abs/2506.09773)
*Taulant Koka,Manolis C. Tsakiris,Benjamín Béjar Haro,Michael Muma*

Main category: eess.SP

TL;DR: 本文扩展了跨通道无标记传感框架，支持子空间联合信号，提升复杂信号处理能力，改进了样本数量界限，并在全脑钙成像中验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 实际应用中（如全脑钙成像、多目标追踪）常存在样本与通道的错位问题，需一种能处理复杂信号结构且样本效率更高的方法。

Method: 将跨通道无标记传感扩展至子空间联合信号，推导更紧的样本数量理论界限，支持压缩感知等任务。

Result: 理论样本需求更优，支持更广泛信号类型；全脑钙成像实验显示样本-神经元映射混乱下仍能准确重建信号。

Conclusion: 所提框架在样本-通道关联不精确的实际场景中具有实用性，为复杂信号重建提供了理论保障和实际应用验证。

Abstract: Cross-channel unlabeled sensing addresses the problem of recovering a
multi-channel signal from measurements that were shuffled across channels. This
work expands the cross-channel unlabeled sensing framework to signals that lie
in a union of subspaces. The extension allows for handling more complex signal
structures and broadens the framework to tasks like compressed sensing. These
mismatches between samples and channels often arise in applications such as
whole-brain calcium imaging of freely moving organisms or multi-target
tracking. We improve over previous models by deriving tighter bounds on the
required number of samples for unique reconstruction, while supporting more
general signal types. The approach is validated through an application in
whole-brain calcium imaging, where organism movements disrupt sample-to-neuron
mappings. This demonstrates the utility of our framework in real-world settings
with imprecise sample-channel associations, achieving accurate signal
reconstruction.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [246] [STREAMINGGS: Voxel-Based Streaming 3D Gaussian Splatting with Memory Optimization and Architectural Support](https://arxiv.org/abs/2506.09070)
*Chenqi Zhang,Yu Feng,Jieru Zhao,Guangda Liu,Wenchao Ding,Chentao Wu,Minyi Guo*

Main category: cs.GR

TL;DR: STREAMINGGS通过算法-架构协同设计，将3D高斯泼溅（3DGS）的渲染流程从瓦片中心转为内存中心，显著提升移动设备上的实时性与能效。


<details>
  <summary>Details</summary>
Motivation: 现有3DGS在移动设备上仅能实现2-9 FPS，远低于实时需求（90 FPS），且加速器忽视内存效率导致DRAM流量冗余。

Method: 提出STREAMINGGS，通过内存中心渲染替代瓦片中心渲染，实现细粒度流水线并行，减少DRAM数据传输。

Result: 相比移动Ampere GPU，设计实现最高45.7倍加速与62.9倍能耗节省。

Conclusion: STREAMINGGS通过算法-架构协同优化，有效解决3DGS在移动端的实时性与内存瓶颈问题。

Abstract: 3D Gaussian Splatting (3DGS) has gained popularity for its efficiency and
sparse Gaussian-based representation. However, 3DGS struggles to meet the
real-time requirement of 90 frames per second (FPS) on resource-constrained
mobile devices, achieving only 2 to 9 FPS.Existing accelerators focus on
compute efficiency but overlook memory efficiency, leading to redundant DRAM
traffic. We introduce STREAMINGGS, a fully streaming 3DGS
algorithm-architecture co-design that achieves fine-grained pipelining and
reduces DRAM traffic by transforming from a tile-centric rendering to a
memory-centric rendering. Results show that our design achieves up to 45.7
$\times$ speedup and 62.9 $\times$ energy savings over mobile Ampere GPUs.

</details>


### [247] [SILK: Smooth InterpoLation frameworK for motion in-betweening A Simplified Computational Approach](https://arxiv.org/abs/2506.09075)
*Elly Akhoundi,Hung Yu Ling,Anup Anand Deshmukh,Judith Butepage*

Main category: cs.GR

TL;DR: 本文提出了一种基于Transformer的简单框架，通过数据建模优化实现高效运动中间帧生成，挑战了模型复杂度决定动画质量的假设。


<details>
  <summary>Details</summary>
Motivation: 现有运动中间帧生成方法依赖复杂模型或多模块架构，作者旨在探索数据建模对性能的影响，并简化模型结构。

Method: 使用单一Transformer编码器框架，重点优化数据量、姿态表示(pose representation)和速度特征(velocity input)等数据建模策略。

Result: 实验表明增加数据量、选择合适姿态表示及引入速度特征可提升动画质量，简单模型在数据驱动下能达到或超越复杂模型效果。

Conclusion: 动画质量并非由模型复杂度主导，数据建模策略(数据量、特征选择)对运动插值效果具有关键作用，提倡数据为中心的优化路径。

Abstract: Motion in-betweening is a crucial tool for animators, enabling intricate
control over pose-level details in each keyframe. Recent machine learning
solutions for motion in-betweening rely on complex models, incorporating
skeleton-aware architectures or requiring multiple modules and training steps.
In this work, we introduce a simple yet effective Transformer-based framework,
employing a single Transformer encoder to synthesize realistic motions for
motion in-betweening tasks. We find that data modeling choices play a
significant role in improving in-betweening performance. Among others, we show
that increasing data volume can yield equivalent or improved motion
transitions, that the choice of pose representation is vital for achieving
high-quality results, and that incorporating velocity input features enhances
animation performance. These findings challenge the assumption that model
complexity is the primary determinant of animation quality and provide insights
into a more data-centric approach to motion interpolation. Additional videos
and supplementary material are available at https://silk-paper.github.io.

</details>


### [248] [DGS-LRM: Real-Time Deformable 3D Gaussian Reconstruction From Monocular Videos](https://arxiv.org/abs/2506.09997)
*Chieh Hubert Lin,Zhaoyang Lv,Songyin Wu,Zhen Xu,Thu Nguyen-Phuoc,Hung-Yu Tseng,Julian Straub,Numair Khan,Lei Xiao,Ming-Hsuan Yang,Yuheng Ren,Richard Newcombe,Zhao Dong,Zhengqin Li*

Main category: cs.GR

TL;DR: DGS-LRM是首个基于前馈网络的方法，可从单目视频中预测可变形3D高斯点云，实现动态场景的高质量重建与长程3D跟踪。


<details>
  <summary>Details</summary>
Motivation: 现有前馈场景重建方法仅适用于静态场景，无法捕捉动态物体运动。动态场景重建面临训练数据稀缺、3D表示与训练范式适配性等挑战。

Method: 提出三要素：1) 含多视角真值与3D场景流标注的合成数据集；2) 易学习、支持动态视图合成与长程跟踪的逐像素可变形3D高斯表示；3) 实时通用动态场景重建的Transformer网络。

Result: 重建质量媲美优化方法，显著超越现有预测式动态重建方法。预测的3D形变可直接用于长程跟踪任务，性能与SOTA单目视频3D跟踪方法相当。

Conclusion: DGS-LRM通过系统性技术组合，首次实现前馈式动态场景重建与物理可解释的3D运动建模，为动态数字孪生提供新范式。

Abstract: We introduce the Deformable Gaussian Splats Large Reconstruction Model
(DGS-LRM), the first feed-forward method predicting deformable 3D Gaussian
splats from a monocular posed video of any dynamic scene. Feed-forward scene
reconstruction has gained significant attention for its ability to rapidly
create digital replicas of real-world environments. However, most existing
models are limited to static scenes and fail to reconstruct the motion of
moving objects. Developing a feed-forward model for dynamic scene
reconstruction poses significant challenges, including the scarcity of training
data and the need for appropriate 3D representations and training paradigms. To
address these challenges, we introduce several key technical contributions: an
enhanced large-scale synthetic dataset with ground-truth multi-view videos and
dense 3D scene flow supervision; a per-pixel deformable 3D Gaussian
representation that is easy to learn, supports high-quality dynamic view
synthesis, and enables long-range 3D tracking; and a large transformer network
that achieves real-time, generalizable dynamic scene reconstruction. Extensive
qualitative and quantitative experiments demonstrate that DGS-LRM achieves
dynamic scene reconstruction quality comparable to optimization-based methods,
while significantly outperforming the state-of-the-art predictive dynamic
reconstruction method on real-world examples. Its predicted physically grounded
3D deformation is accurate and can readily adapt for long-range 3D tracking
tasks, achieving performance on par with state-of-the-art monocular video 3D
tracking methods.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [249] [A Multi-Armed Bandit Framework for Online Optimisation in Green Integrated Terrestrial and Non-Terrestrial Networks](https://arxiv.org/abs/2506.09268)
*Henri Alam,Antonio de Domenico,Tareq Si Salem,Florian Kaltenberger*

Main category: cs.NI

TL;DR: 本文提出一种基于多臂老虎机框架的在线优化方法，用于集成地面与非地面网络，实时平衡网络容量与能效。实验表明该方法在高峰时段减少未满足需求用户比例，低流量时吞吐量提升19%并节能5%。


<details>
  <summary>Details</summary>
Motivation: 地面网络密集化部署引发可持续性担忧，研究旨在探索非地面网络（NTN）在减轻地面网络负载及提升能效方面的潜力，以实现更可持续的网络架构。

Method: 基于多臂老虎机（MAB）框架和BCOMD算法，动态优化带宽分配、用户设备连接及宏基站关闭策略，实时调整集成TN-NTN系统参数。

Result: 24小时系统模拟显示：高峰时段未满足需求用户比例显著降低，低流量时段吞吐量提升19%，能耗减少5%，性能优于3GPP标准配置。

Conclusion: 所提框架通过NTN与TN协同优化，有效平衡容量与能效，验证了非地面网络在增强网络可持续性中的关键作用。

Abstract: Integrated terrestrial and non-terrestrial network (TN-NTN) architectures
offer a promising solution for expanding coverage and improving capacity for
the network. While non-terrestrial networks (NTNs) are primarily exploited for
these specific reasons, their role in alleviating terrestrial network (TN) load
and enabling energy-efficient operation has received comparatively less
attention. In light of growing concerns associated with the densification of
terrestrial deployments, this work aims to explore the potential of NTNs in
supporting a more sustainable network. In this paper, we propose a novel online
optimisation framework for integrated TN-NTN architectures, built on a
multi-armed bandit (MAB) formulation and leveraging the Bandit-feedback
Constrained Online Mirror Descent (BCOMD) algorithm. Our approach adaptively
optimises key system parameters--including bandwidth allocation, user equipment
(UE) association, and macro base station (MBS) shutdown--to balance network
capacity and energy efficiency in real time. Extensive system-level simulations
over a 24-hour period show that our framework significantly reduces the
proportion of unsatisfied UEs during peak hours and achieves up to 19%
throughput gains and 5% energy savings in low-traffic periods, outperforming
standard network settings following 3GPP recommendations.

</details>


### [250] [Real-Time Network Traffic Forecasting with Missing Data: A Generative Model Approach](https://arxiv.org/abs/2506.09647)
*Lei Deng,Wenhan Xu,Jingwei Li,Danny H. K. Tsang*

Main category: cs.NI

TL;DR: 本文提出一种结合生成模型和张量补全的实时网络流量预测方法，有效处理数据缺失问题，在100ms内实现MAE<0.002的高精度预测。


<details>
  <summary>Details</summary>
Motivation: 现有网络流量预测方法假设数据完整，但实际采集常存在缺失。需解决不完整数据下的实时预测问题以实现网络资源预分配。

Method: 1. 将流量预测建模为张量补全问题；2. 引入预训练生成模型捕捉流量数据内在低秩结构；3. 通过优化潜在表征而非高维张量实现实时计算。

Result: 在Abilene数据集上验证，预测延迟<100ms，平均绝对误差(MAE)低于0.002，并建立了理论恢复误差边界证明。

Conclusion: 所提生成式张量补全方法突破传统完整数据假设，通过潜在空间优化实现高精度实时预测，为不完整网络数据预测提供新范式。

Abstract: Real-time network traffic forecasting is crucial for network management and
early resource allocation. Existing network traffic forecasting approaches
operate under the assumption that the network traffic data is fully observed.
However, in practical scenarios, the collected data are often incomplete due to
various human and natural factors. In this paper, we propose a generative model
approach for real-time network traffic forecasting with missing data. Firstly,
we model the network traffic forecasting task as a tensor completion problem.
Secondly, we incorporate a pre-trained generative model to achieve the low-rank
structure commonly associated with tensor completion. The generative model
effectively captures the intrinsic low-rank structure of network traffic data
during pre-training and enables the mapping from a compact latent
representation to the tensor space. Thirdly, rather than directly optimizing
the high-dimensional tensor, we optimize its latent representation, which
simplifies the optimization process and enables real-time forecasting. We also
establish a theoretical recovery guarantee that quantifies the error bound of
the proposed approach. Experiments on real-world datasets demonstrate that our
approach achieves accurate network traffic forecasting within 100 ms, with a
mean absolute error (MAE) below 0.002, as validated on the Abilene dataset.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [251] [A Survey on the Role of Artificial Intelligence and Machine Learning in 6G-V2X Applications](https://arxiv.org/abs/2506.09512)
*Donglin Wang,Anjie Qiu,Qiuheng Zhou,Hans D. Schotten*

Main category: eess.SY

TL;DR: 本文系统综述了AI/ML在6G-V2X通信中的最新进展，重点分析深度学习、强化学习、生成学习与联邦学习等技术在资源分配、波束成形、交通与安全管理中的应用，并探讨了计算复杂度、数据隐私等技术挑战与未来方向。


<details>
  <summary>Details</summary>
Motivation: 6G网络为车联网（V2X）提供超可靠、低时延连接，而AI/ML因其跨领域性能成为优化V2X通信的关键技术，但当前缺乏对AI驱动6G-V2X研究的系统性总结。

Method: 通过全面综述近两年AI/ML模型（如深度学习、强化学习、生成学习、联邦学习）在6G-V2X中的应用，分析其在资源分配、波束成形、交通管理、安全等场景的作用。

Result: 生成学习（GL）等AI技术显著提升了6G-V2X系统的性能与适应性，但面临计算复杂度、数据隐私、实时决策等技术挑战，需进一步研究以推动AI驱动的V2X生态发展。

Conclusion: 本文为AI赋能的6G-V2X研究提供系统性总结，强调技术潜力与未来方向，助力研究者、工程师及政策制定者实现智能车联网生态系统。

Abstract: The rapid advancement of Vehicle-to-Everything (V2X) communication is
transforming Intelligent Transportation Systems (ITS), with 6G networks
expected to provide ultra-reliable, low-latency, and high-capacity connectivity
for Connected and Autonomous Vehicles (CAVs). Artificial Intelligence (AI) and
Machine Learning (ML) have emerged as key enablers in optimizing V2X
communication by enhancing network management, predictive analytics, security,
and cooperative driving due to their outstanding performance across various
domains, such as natural language processing and computer vision. This survey
comprehensively reviews recent advances in AI and ML models applied to 6G-V2X
communication. It focuses on state-of-the-art techniques, including Deep
Learning (DL), Reinforcement Learning (RL), Generative Learning (GL), and
Federated Learning (FL), with particular emphasis on developments from the past
two years. Notably, AI, especially GL, has shown remarkable progress and
emerging potential in enhancing the performance, adaptability, and intelligence
of 6G-V2X systems. Despite these advances, a systematic summary of recent
research efforts in this area remains lacking, which this survey aims to
address. We analyze their roles in 6G-V2X applications, such as intelligent
resource allocation, beamforming, intelligent traffic management, and security
management. Furthermore, we explore the technical challenges, including
computational complexity, data privacy, and real-time decision-making
constraints, while identifying future research directions for AI-driven 6G-V2X
development. This study aims to provide valuable insights for researchers,
engineers, and policymakers working towards realizing intelligent, AI-powered
V2X ecosystems in 6G communication.

</details>


<div id='physics.med-ph'></div>

# physics.med-ph [[Back]](#toc)

### [252] [Automatic Treatment Planning using Reinforcement Learning for High-dose-rate Prostate Brachytherapy](https://arxiv.org/abs/2506.09805)
*Tonghe Wang,Yining Feng,Xiaofeng Yang*

Main category: physics.med-ph

TL;DR: 研究首次证明强化学习可自动生成临床实用的HDR前列腺近距离放疗计划，其质量等同或优于传统方法，且减少针具使用。


<details>
  <summary>Details</summary>
Motivation: 当前HDR前列腺近距离放疗的针具布局仅依赖医生经验，导致手术时间长且计划质量不稳定。研究旨在通过强化学习基于患者解剖结构生成针具位置与停留时间，以提升效率与一致性。

Method: 训练强化学习代理在预计划阶段逐步调整单针位置及其停留时间，通过多轮迭代优化预定义奖励函数。使用11例患者数据（1例训练，10例测试），对比RL计划与临床计划的剂量学指标及针具数量。

Result: RL计划与临床计划的前列腺覆盖率（V100）和直肠剂量（D2cc）无显著差异，但显著减少前列腺热点（V150）和尿道剂量（D20%），且RL计划平均少用2根针具。

Conclusion: 强化学习方法能以更少针具实现等同或更优的放疗计划质量，具有数据需求低、泛化性强等优势，有望标准化治疗流程、减少临床差异并改善患者预后。

Abstract: Purpose: In high-dose-rate (HDR) prostate brachytherapy procedures, the
pattern of needle placement solely relies on physician experience. We
investigated the feasibility of using reinforcement learning (RL) to provide
needle positions and dwell times based on patient anatomy during pre-planning
stage. This approach would reduce procedure time and ensure consistent plan
quality. Materials and Methods: We train a RL agent to adjust the position of
one selected needle and all the dwell times on it to maximize a pre-defined
reward function after observing the environment. After adjusting, the RL agent
then moves on to the next needle, until all needles are adjusted. Multiple
rounds are played by the agent until the maximum number of rounds is reached.
Plan data from 11 prostate HDR boost patients (1 for training, and 10 for
testing) treated in our clinic were included in this study. The dosimetric
metrics and the number of used needles of RL plan were compared to those of the
clinical results (ground truth). Results: On average, RL plans and clinical
plans have very similar prostate coverage (Prostate V100) and Rectum D2cc (no
statistical significance), while RL plans have less prostate hotspot (Prostate
V150) and Urethra D20% plans with statistical significance. Moreover, RL plans
use 2 less needles than clinical plan on average. Conclusion: We present the
first study demonstrating the feasibility of using reinforcement learning to
autonomously generate clinically practical HDR prostate brachytherapy plans.
This RL-based method achieved equal or improved plan quality compared to
conventional clinical approaches while requiring fewer needles. With minimal
data requirements and strong generalizability, this approach has substantial
potential to standardize brachytherapy planning, reduce clinical variability,
and enhance patient outcomes.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [253] [Alice and the Caterpillar: A more descriptive null model for assessing data mining results](https://arxiv.org/abs/2506.09764)
*Giulia Preti,Gianmarco De Francisci Morales,Matteo Riondato*

Main category: cs.SI

TL;DR: 提出新型零模型，通过保留二分联合度矩阵等更多数据集属性，结合Alice算法套件进行高效采样，实验证明其优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 现有零模型在统计假设检验中保留的观察数据集属性不足，需开发能保留更多结构特征（如路径长度三的毛毛虫数量）的模型以提高评估准确性。

Method: 构建保留二分联合度矩阵的零模型，设计基于马尔可夫链蒙特卡洛的Alice算法套件，通过状态空间定义和高效转移操作实现数据集采样。

Result: Alice算法混合速度快、扩展性强，新零模型相比文献方法能检测到不同的统计显著结果。

Conclusion: 所提零模型在保持复杂结构属性的同时，为二元交易/序列数据集假设检验提供了更可靠的基准，揭示了传统方法未捕捉的显著性模式。

Abstract: We introduce novel null models for assessing the results obtained from
observed binary transactional and sequence datasets, using statistical
hypothesis testing. Our null models maintain more properties of the observed
dataset than existing ones. Specifically, they preserve the Bipartite Joint
Degree Matrix of the bipartite (multi-)graph corresponding to the dataset,
which ensures that the number of caterpillars, i.e., paths of length three, is
preserved, in addition to other properties considered by other models. We
describe Alice, a suite of Markov chain Monte Carlo algorithms for sampling
datasets from our null models, based on a carefully defined set of states and
efficient operations to move between them. The results of our experimental
evaluation show that Alice mixes fast and scales well, and that our null model
finds different significant results than ones previously considered in the
literature.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [254] [You Are What You Say: Exploiting Linguistic Content for VoicePrivacy Attacks](https://arxiv.org/abs/2506.09521)
*Ünal Ege Gaznepoglu,Anna Leschanowsky,Ahmad Aloradi,Prachi Singh,Daniel Tenbrinck,Emanuël A. P. Habets,Nils Peters*

Main category: eess.AS

TL;DR: 本研究通过BERT语言模型评估说话人匿名化系统的隐私保护效果，发现文本内容相似性对攻击者识别说话人身份有显著影响，建议改进现有数据集和评估方法。


<details>
  <summary>Details</summary>
Motivation: 评估说话人匿名化系统的隐私保护效果时，需分析攻击者训练与评估数据中说话人内部语言内容相似性的影响，以验证现有评估方法的公平性。

Method: 采用BERT语言模型构建自动说话人验证（ASV）系统，基于VoicePrivacy挑战数据集，通过文本内容分析说话人身份识别效果。

Result: 仅凭文本内容，系统平均等错误率（EER）达35%，部分说话人EER低至2%；可解释性分析显示决策与语义关键词相关，数据集的构建方式可能引入偏差。

Conclusion: 需重构VoicePrivacy数据集以减少偏差，并质疑依赖全局EER作为隐私评估指标的可靠性。

Abstract: Speaker anonymization systems hide the identity of speakers while preserving
other information such as linguistic content and emotions. To evaluate their
privacy benefits, attacks in the form of automatic speaker verification (ASV)
systems are employed. In this study, we assess the impact of intra-speaker
linguistic content similarity in the attacker training and evaluation datasets,
by adapting BERT, a language model, as an ASV system. On the VoicePrivacy
Attacker Challenge datasets, our method achieves a mean equal error rate (EER)
of 35%, with certain speakers attaining EERs as low as 2%, based solely on the
textual content of their utterances. Our explainability study reveals that the
system decisions are linked to semantically similar keywords within utterances,
stemming from how LibriSpeech is curated. Our study suggests reworking the
VoicePrivacy datasets to ensure a fair and unbiased evaluation and challenge
the reliance on global EER for privacy evaluations.

</details>


### [255] [Fine-Tuning Large Audio-Language Models with LoRA for Precise Temporal Localization of Prolonged Exposure Therapy Elements](https://arxiv.org/abs/2506.09707)
*Suhas BN,Andrew M. Sherrill,Jyoti Alaparthi,Dominik Mattioli,Rosa I. Arriaga,Chris W. Wiese,Saeed Abdullah*

Main category: eess.AS

TL;DR: 本文提出了一种基于Qwen2-Audio模型和LoRA微调的方法，用于自动定位延长暴露疗法（PE）中的关键治疗阶段，以解决传统人工评估治疗师忠诚度效率低效问题。


<details>
  <summary>Details</summary>
Motivation: 传统PE疗法中，治疗师操作合规性评估依赖人工审核录音，耗时耗力。需开发自动化方法提升效率。

Method: 使用LoRA低秩适配技术微调Qwen2-Audio多模态模型，通过30秒音频-文本窗口输入，结合LLM生成并经人工验证的协议阶段标签（P1-P3），采用任务提示引导的软监督预测标准化时间偏移。

Result: 在313个真实PE会话数据集上，最佳配置（LoRA秩8+30秒窗口）实现平均绝对误差5.3秒，窗口尺寸和LoRA秩对性能有显著影响。

Conclusion: 该框架为PE治疗质量监控提供了可扩展方案，在临床培训、监督和质量保障方面具有应用潜力。

Abstract: Prolonged Exposure (PE) therapy is an effective treatment for post-traumatic
stress disorder (PTSD), but evaluating therapist fidelity remains
labor-intensive due to the need for manual review of session recordings. We
present a method for the automatic temporal localization of key PE fidelity
elements -- identifying their start and stop times -- directly from session
audio and transcripts. Our approach fine-tunes a large pre-trained
audio-language model, Qwen2-Audio, using Low-Rank Adaptation (LoRA) to process
focused 30-second windows of audio-transcript input. Fidelity labels for three
core protocol phases -- therapist orientation (P1), imaginal exposure (P2), and
post-imaginal processing (P3) -- are generated via LLM-based prompting and
verified by trained raters. The model is trained to predict normalized boundary
offsets using soft supervision guided by task-specific prompts. On a dataset of
313 real PE sessions, our best configuration (LoRA rank 8, 30s windows)
achieves a mean absolute error (MAE) of 5.3 seconds across tasks. We further
analyze the effects of window size and LoRA rank, highlighting the importance
of context granularity and model adaptation. This work introduces a scalable
framework for fidelity tracking in PE therapy, with potential to support
clinician training, supervision, and quality assurance.

</details>


### [256] [Regularizing Learnable Feature Extraction for Automatic Speech Recognition](https://arxiv.org/abs/2506.09804)
*Peter Vieting,Maximilian Kannen,Benedikt Hilmes,Ralf Schlüter,Hermann Ney*

Main category: eess.AS

TL;DR: 本文研究了正则化方法在可学习特征提取前端训练中的应用，通过音频扰动和改进的SpecAugment方法有效缓解过拟合问题，缩小了可学习特征与传统方法的性能差距。


<details>
  <summary>Details</summary>
Motivation: 神经前端虽能直接适配声学模型，但其性能常因过拟合问题弱于传统固定特征提取方法。本文旨在通过正则化方法提升其性能。

Method: 结合音频扰动增强数据多样性，并提出在STFT域进行掩码的改进版SpecAugment，以解决传统SpecAugment在可学习前端中的局限性。

Result: 两种正则化方法协同作用后，可学习特征与传统方法的性能差距被有效消除，验证了方法的有效性。

Conclusion: 通过针对性正则化策略，可学习特征提取前端在ASR系统中可实现与传统方法相当甚至更优的性能，为端到端优化提供新思路。

Abstract: Neural front-ends are an appealing alternative to traditional, fixed feature
extraction pipelines for automatic speech recognition (ASR) systems since they
can be directly trained to fit the acoustic model. However, their performance
often falls short compared to classical methods, which we show is largely due
to their increased susceptibility to overfitting. This work therefore
investigates regularization methods for training ASR models with learnable
feature extraction front-ends. First, we examine audio perturbation methods and
show that larger relative improvements can be obtained for learnable features.
Additionally, we identify two limitations in the standard use of SpecAugment
for these front-ends and propose masking in the short time Fourier transform
(STFT)-domain as a simple but effective modification to address these
challenges. Finally, integrating both regularization approaches effectively
closes the performance gap between traditional and learnable features.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [257] [EdgeProfiler: A Fast Profiling Framework for Lightweight LLMs on Edge Using Analytical Model](https://arxiv.org/abs/2506.09061)
*Alyssa Pinnock,Shakya Jayakody,Kawsher A Roxy,Md Rubel Ahmed*

Main category: cs.DC

TL;DR: EdgeProfiler提出了一种快速分析框架，用于评估轻量化大语言模型在边缘设备上的性能，通过4位量化等技术实现内存占用减少60-70%、推理速度提升2-3倍，并降低35-50%能耗。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型(LLM)因高计算、内存和功耗需求难以部署在资源受限的边缘设备，需建立系统化评估框架以平衡模型性能与边缘环境限制。

Method: 使用TinyLLaMA等轻量模型，结合4位量化技术和内存约束分析，通过建模预测延迟、FLOPs及能耗，覆盖树莓派4/5、Jetson Orin Nano等典型边缘硬件。

Result: 4位量化使内存减少60-70%，精度损失仅2-5%；INT4配置推理速度较FP16提升2-3倍，能耗降低35-50%，可在边缘设备实现实用化部署。

Conclusion: 针对边缘环境的轻量化LLM需定制高效分析框架，在精度、能效与计算可行性间取得平衡，量化技术是实现边缘部署的关键优化手段。

Abstract: This paper introduces EdgeProfiler, a fast profiling framework designed for
evaluating lightweight Large Language Models (LLMs) on edge systems. While LLMs
offer remarkable capabilities in natural language understanding and generation,
their high computational, memory, and power requirements often confine them to
cloud environments. EdgeProfiler addresses these challenges by providing a
systematic methodology for assessing LLM performance in resource-constrained
edge settings. The framework profiles compact LLMs, including TinyLLaMA,
Gemma3.1B, Llama3.2-1B, and DeepSeek-r1-1.5B, using aggressive quantization
techniques and strict memory constraints. Analytical modeling is used to
estimate latency, FLOPs, and energy consumption. The profiling reveals that
4-bit quantization reduces model memory usage by approximately 60-70%, while
maintaining accuracy within 2-5% of full-precision baselines. Inference speeds
are observed to improve by 2-3x compared to FP16 baselines across various edge
devices. Power modeling estimates a 35-50% reduction in energy consumption for
INT4 configurations, enabling practical deployment on hardware such as
Raspberry Pi 4/5 and Jetson Orin Nano Super. Our findings emphasize the
importance of efficient profiling tailored to lightweight LLMs in edge
environments, balancing accuracy, energy efficiency, and computational
feasibility.

</details>


### [258] [SLED: A Speculative LLM Decoding Framework for Efficient Edge Serving](https://arxiv.org/abs/2506.09397)
*Xiangchen Li,Dimitrios Spatharakis,Saeid Ghafouri,Jiakun Fan,Dimitrios Nikolopoulos*

Main category: cs.DC

TL;DR: 提出SLED方法，利用推测解码协调异构边缘设备，在降低延迟和能耗的同时保持模型精度，提升边缘端大语言模型推理效率。


<details>
  <summary>Details</summary>
Motivation: 现有边缘设备部署大语言模型的方法（如量化、剪枝或远程推理）需在精度与效率间权衡，且无法有效应对设备异构性和服务器内存限制问题。

Method: SLED框架：轻量边缘设备使用多样化草稿模型本地生成候选token，共享边缘服务器通过高精度目标模型批量验证，实现计算协同与内存优化。

Result: 实验显示在Jetson/Raspberry Pi等设备上实现延迟降低40%、能效提升2.1倍，服务器并发推理会话增加3倍，且无精度损失。

Conclusion: 通过设备-服务器协同推测解码，SLED为边缘计算提供高效、可扩展的LLM推理方案，突破传统精度-效率-成本的不可能三角。

Abstract: Regardless the advancements in device capabilities, efficient inferencing
advanced large language models (LLMs) at the edge remains challenging due to
limited device memory and power constraints. Existing strategies, such as
aggressive quantization, pruning, or remote inference, trade accuracy for
efficiency or lead to substantial cost burdens. This position paper introduces
a new approach that leverages speculative decoding, previously viewed primarily
as a decoding acceleration technique for autoregressive generation of LLMs, as
a promising approach specifically adapted for edge computing by orchestrating
computation across heterogeneous devices. We propose SLED, a method that allows
lightweight edge devices to draft multiple candidate tokens locally using
diverse draft models, while a single, shared edge server efficiently batches
and verifies the tokens utilizing a more precise target model. This approach
supports device heterogeneity and reduces server-side memory footprint by
avoiding the need to deploy multiple target models. Our initial experiments
with Jetson Orin Nano, Raspberry Pi 5, and an RTX 6000 edge server indicate
substantial benefits: significantly reduced latency, improved energy
efficiency, and increased concurrent inference sessions, all without
sacrificing model accuracy.

</details>


### [259] [TTrace: Lightweight Error Checking and Diagnosis for Distributed Training](https://arxiv.org/abs/2506.09280)
*Haitian Jiang,Shaowei Zhu,Zhen Zhang,Zhenyu Song,Xinwei Fu,Zhen Jia,Yida Wang,Jinyang Li*

Main category: cs.DC

TL;DR: TTrace系统通过对比分布式训练与单设备实现的中间张量，有效检测和定位分布式训练中的静默错误，解决了低精度训练中的调试难题。


<details>
  <summary>Details</summary>
Motivation: 分布式训练程序复杂且易产生静默错误（无显式报错但导致错误结果），传统调试方法（训练损失、梯度范数等指标）效率低下，且低精度训练中难以获取和验证中间张量值。

Method: TTrace细粒度收集分布式训练的中间张量，与可信单设备参考实现对比；提出数学分析设定阈值，区分错误与浮点舍入误差。

Result: 实验显示TTrace在Megatron-LM框架中检测到11个现有错误和3个新错误，仅需少于10行代码修改，并支持BF16/FP8等低精度训练场景。

Conclusion: TTrace是首个能有效检测和定位分布式训练静默错误的系统，具备实际部署的轻量性和广泛训练场景的适用性。

Abstract: Distributed training is essential for scaling the training of large neural
network models, such as large language models (LLMs), across thousands of GPUs.
However, the complexity of distributed training programs makes them
particularly prone to silent bugs, which do not produce explicit error signal
but lead to incorrect training outcome. Effectively detecting and localizing
such silent bugs in distributed training is challenging. Common debugging
practice using metrics like training loss or gradient norm curves can be
inefficient and ineffective. Additionally, obtaining intermediate tensor values
and determining whether they are correct during silent bug localization is
difficult, particularly in the context of low-precision training.
  To address those challenges, we design and implement TTrace, the first system
capable of detecting and localizing silent bugs in distributed training. TTrace
collects intermediate tensors from distributing training in a fine-grained
manner and compares them against those from a trusted single-device reference
implementation. To properly compare the floating-point values in the tensors,
we propose novel mathematical analysis that provides a guideline for setting
thresholds, enabling TTrace to distinguish bug-induced errors from
floating-point round-off errors. Experimental results demonstrate that TTrace
effectively detects 11 existing bugs and 3 new bugs in the widely used
Megatron-LM framework, while requiring fewer than 10 lines of code change.
TTrace is effective in various training recipes, including low-precision
recipes involving BF16 and FP8.

</details>


### [260] [ScalableHD: Scalable and High-Throughput Hyperdimensional Computing Inference on Multi-Core CPUs](https://arxiv.org/abs/2506.09282)
*Dhruv Parikh,Viktor Prasanna*

Main category: cs.DC

TL;DR: 本文提出ScalableHD，一种针对多核CPU的高效超维计算（HDC）推理框架，通过两阶段流水线、内存优化和NUMA感知绑定，实现高吞吐量和近线性扩展，性能提升达10倍。


<details>
  <summary>Details</summary>
Motivation: 传统HDC方法在CPU上的推理效率研究不足，现有工作多集中于专用硬件（如FPGA/GPU），而多核CPU的通用性未被充分利用。

Method: 采用两阶段流水线并行化处理基向量与类别向量，结合生产者-消费者流式传输、内存分块、NUMA感知核心绑定，并针对小/大批次设计不同并行策略以缓解内存瓶颈。

Result: 在多种任务（如活动识别、图像分类）中，吞吐量较TorchHD等基线提升10倍，核心数增加时吞吐量接近线性扩展，且保持任务准确率不变。

Conclusion: ScalableHD证明了多核CPU上HDC推理的高效性与可扩展性，填补了通用处理器优化空白，为实时应用提供轻量级解决方案。

Abstract: Hyperdimensional Computing (HDC) is a brain-inspired computing paradigm that
represents and manipulates information using high-dimensional vectors, called
hypervectors (HV). Traditional HDC methods, while robust to noise and
inherently parallel, rely on single-pass, non-parametric training and often
suffer from low accuracy. To address this, recent approaches adopt iterative
training of base and class HVs, typically accelerated on GPUs. Inference,
however, remains lightweight and well-suited for real-time execution. Yet,
efficient HDC inference has been studied almost exclusively on specialized
hardware such as FPGAs and GPUs, with limited attention to general-purpose
multi-core CPUs. To address this gap, we propose ScalableHD for scalable and
high-throughput HDC inference on multi-core CPUs. ScalableHD employs a
two-stage pipelined execution model, where each stage is parallelized across
cores and processes chunks of base and class HVs. Intermediate results are
streamed between stages using a producer-consumer mechanism, enabling
on-the-fly consumption and improving cache locality. To maximize performance,
ScalableHD integrates memory tiling and NUMA-aware worker-to-core binding.
Further, it features two execution variants tailored for small and large batch
sizes, each designed to exploit compute parallelism based on workload
characteristics while mitigating the memory-bound compute pattern that limits
HDC inference performance on modern multi-core CPUs. ScalableHD achieves up to
10x speedup in throughput (samples per second) over state-of-the-art baselines
such as TorchHD, across a diverse set of tasks ranging from human activity
recognition to image classification, while preserving task accuracy.
Furthermore, ScalableHD exhibits robust scalability: increasing the number of
cores yields near-proportional throughput improvements.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [261] [A Topological Improvement of the Overall Performance of Sparse Evolutionary Training: Motif-Based Structural Optimization of Sparse MLPs Project](https://arxiv.org/abs/2506.09204)
*Xiaotian Chen,Hongyun Liu,Seyed Sahand Mohammadi Ziabari*

Main category: cs.NE

TL;DR: 研究探讨通过基于模体的结构优化方法改进稀疏进化训练（SET-MLP），以在降低计算成本的同时保持模型精度，预期效率提升超40%且性能下降低于4%。


<details>
  <summary>Details</summary>
Motivation: 随着深度神经网络（DNN）模型复杂度增加，计算成本和内存开销问题日益突出，需探索在不损失精度前提下降低资源消耗的方法。

Method: 将稀疏进化训练（SET）应用于多层感知机（MLP），并引入基于模体的结构优化方法改进SET算法。

Result: 结构优化可能使SET算法效率提升超40%，性能下降控制在4%以内。

Conclusion: 结构优化可有效提升SET-MLP性能，显著提高效率且对精度影响有限。

Abstract: Deep Neural Networks (DNNs) have been proven to be exceptionally effective
and have been applied across diverse domains within deep learning. However, as
DNN models increase in complexity, the demand for reduced computational costs
and memory overheads has become increasingly urgent. Sparsity has emerged as a
leading approach in this area. The robustness of sparse Multi-layer Perceptrons
(MLPs) for supervised feature selection, along with the application of Sparse
Evolutionary Training (SET), illustrates the feasibility of reducing
computational costs without compromising accuracy. Moreover, it is believed
that the SET algorithm can still be improved through a structural optimization
method called motif-based optimization, with potential efficiency gains
exceeding 40% and a performance decline of under 4%. This research investigates
whether the structural optimization of Sparse Evolutionary Training applied to
Multi-layer Perceptrons (SET-MLP) can enhance performance and to what extent
this improvement can be achieved.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [262] [Abstraction-Based Proof Production in Formal Verification of Neural Networks](https://arxiv.org/abs/2506.09455)
*Yizhak Yisrael Elboher,Omri Isac,Guy Katz,Tobias Ladner,Haoze Wu*

Main category: cs.LO

TL;DR: 提出一种支持形式化证明的抽象化验证框架，以解决现有DNN验证工具在可扩展性与可证明性之间的脱节问题。


<details>
  <summary>Details</summary>
Motivation: 现有生成证明的DNN验证工具不支持抽象化方法，导致可扩展性与可信验证结果无法兼顾。

Method: 将验证任务解耦为抽象网络正确性证明（使用现有工具）和抽象过程可靠性证明（提出新方法生成形式化证明）。

Result: 首次实现支持常见抽象技术的可扩展形式化验证框架，为抽象过程提供数学严谨的证明。

Conclusion: 通过模块化证明框架整合抽象技术与形式化验证，为DNN提供兼具规模化能力与可信度的验证方案。

Abstract: Modern verification tools for deep neural networks (DNNs) increasingly rely
on abstraction to scale to realistic architectures. In parallel, proof
production is becoming a critical requirement for increasing the reliability of
DNN verification results. However, current proofproducing verifiers do not
support abstraction-based reasoning, creating a gap between scalability and
provable guarantees. We address this gap by introducing a novel framework for
proof-producing abstraction-based DNN verification. Our approach modularly
separates the verification task into two components: (i) proving the
correctness of an abstract network, and (ii) proving the soundness of the
abstraction with respect to the original DNN. The former can be handled by
existing proof-producing verifiers, whereas we propose the first method for
generating formal proofs for the latter. This preliminary work aims to enable
scalable and trustworthy verification by supporting common abstraction
techniques within a formal proof framework.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [263] [UTBoost: Rigorous Evaluation of Coding Agents on SWE-Bench](https://arxiv.org/abs/2506.09289)
*Boxi Yu,Yuxuan Zhu,Pinjia He,Daniel Kang*

Main category: cs.SE

TL;DR: 本文针对SWE-Bench基准测试中测试用例不足导致代码补丁误判的问题，提出基于LLM的测试生成器UTGenerator及增强框架UTBoost，有效识别并修正了345个错误补丁，显著影响排行榜排名。


<details>
  <summary>Details</summary>
Motivation: 现有SWE-Bench基准测试中手动编写的测试用例不足，导致生成的代码补丁即使通过测试也无法真正解决问题，需开发自动化测试生成方法以提高评估可靠性。

Method: 提出UTGenerator（基于LLM的测试用例生成器）自动分析代码库及依赖关系生成测试用例，并构建UTBoost框架实现测试用例增强。

Result: 在36个任务实例中发现测试用例不足，修正了345个被错误标记为通过的补丁，导致SWE-Bench Lite和Verified排行榜分别出现18次和11次排名变动。

Conclusion: UTGenerator与UTBoost通过自动生成测试用例显著提升评估准确性，揭示了现有基准测试的局限性，并为代码生成代理的可靠评估提供了新方法。

Abstract: The advent of Large Language Models (LLMs) has spurred the development of
coding agents for real-world code generation. As a widely used benchmark for
evaluating the code generation capabilities of these agents, SWE-Bench uses
real-world problems based on GitHub issues and their corresponding pull
requests. However, the manually written test cases included in these pull
requests are often insufficient, allowing generated patches to pass the tests
without resolving the underlying issue. To address this challenge, we introduce
UTGenerator, an LLM-driven test case generator that automatically analyzes
codebases and dependencies to generate test cases for real-world Python
projects. Building on UTGenerator, we propose UTBoost, a comprehensive
framework for test case augmentation. In our evaluation, we identified 36 task
instances with insufficient test cases and uncovered 345 erroneous patches
incorrectly labeled as passed in the original SWE Bench. These corrections,
impacting 40.9% of SWE-Bench Lite and 24.4% of SWE-Bench Verified leaderboard
entries, yield 18 and 11 ranking changes, respectively.

</details>


### [264] [Reasoning as a Resource: Optimizing Fast and Slow Thinking in Code Generation Models](https://arxiv.org/abs/2506.09396)
*Zongjie Li,Shuai Wang*

Main category: cs.SE

TL;DR: 该立场论文提出将推理深度作为可控资源，通过优化代码生成模型全生命周期的推理预算，在准确性、延迟和成本间实现更优权衡。主张动态调度'快速思考'与'慢速思考'模式，构建必要时深入思考、可能时快速响应的编码代理。


<details>
  <summary>Details</summary>
Motivation: 现有代码生成模型未主动管理推理深度，导致资源分配效率低下。论文认为需显式控制'快速响应'与'深度推理'的平衡，以系统性优化模型全流程资源投入，解决精度、时延、成本间的固有矛盾。

Method: 将推理深度建模为可控变量，在合成数据生成、基准测试和实际部署阶段实施自适应控制。通过动态调度机制协调即时响应(快速思考)与链式推理(慢速思考)两种模式，建立多维评估体系并制定成本感知的部署策略。

Result: 提出新型框架实现推理预算的全局优化，预期可增强监督信号质量，推动建立包含时延-精度-成本的三维评估标准，并为安全敏感场景提供可控的推理方案。

Conclusion: 通过将快速/慢速思考模式转化为可调度资源，构建出能根据任务需求自主调节推理深度的代码生成系统，在保证响应效率的同时保留复杂问题的深度求解能力。

Abstract: This position paper proposes a fundamental shift in designing code generation
models: treating reasoning depth as a controllable resource. Rather than being
an incidental byproduct of prompting, we argue that the trade-off between
rapid, direct answers ("fast thinking") and elaborate, chain-of-thought
deliberation ("slow thinking") must be explicitly managed. We contend that
optimizing reasoning budgets across the entire model lifecycle - from synthetic
data creation and benchmarking to real-world deploymen - can unlock superior
trade-offs among accuracy, latency, and cost. This paper outlines how adaptive
control over reasoning can enrich supervision signals, motivate new
multi-dimensional benchmarks, and inform cost-aware, security-conscious
deployment policies. By viewing fast and slow thinking as complementary modes
to be scheduled, we envision coding agents that think deep when necessary and
act fast when possible.

</details>


### [265] [Stakeholder Participation for Responsible AI Development: Disconnects Between Guidance and Current Practice](https://arxiv.org/abs/2506.09873)
*Emma Kallina,Thomas Bohné,Jat Singh*

Main category: cs.SE

TL;DR: 现有利益相关者参与（SHI）实践主要受商业利益驱动，未能有效支持负责任AI（rAI）目标。需针对性干预措施以弥合两者脱节。


<details>
  <summary>Details</summary>
Motivation: 探讨商业软件开发中已有的SHI实践与rAI指导文件提倡的SHI目标之间的差异，明确现有实践对rAI的贡献及潜在矛盾，为未来实践调整提供依据。

Method: 分析56份rAI指导文件以提取SHI目标，并通过在线调查（n=130）和半结构化访谈（n=10）研究商业环境中SHI的实际驱动因素与实践方式。

Result: 当前SHI实践以客户价值、合规性等商业目标为主导，缺乏对rAI目标（如权力再分配、风险预见）的关注，现有因素阻碍rAI对齐的SHI实践。

Conclusion: 现有SHI实践对rAI贡献有限，需通过干预措施（如调整激励机制、增强伦理培训）和研究机会推动行业向rAI对齐的实践转型。

Abstract: Responsible AI (rAI) guidance increasingly promotes stakeholder involvement
(SHI) during AI development. At the same time, SHI is already common in
commercial software development, but with potentially different foci. This
study clarifies the extent to which established SHI practices are able to
contribute to rAI efforts as well as potential disconnects -- essential
insights to inform and tailor future interventions that further shift industry
practice towards rAI efforts. First, we analysed 56 rAI guidance documents to
identify why SHI is recommended (i.e. its expected benefits for rAI) and
uncovered goals such as redistributing power, improving socio-technical
understandings, anticipating risks, and enhancing public oversight. To
understand why and how SHI is currently practised in commercial settings, we
then conducted an online survey (n=130) and semi-structured interviews (n=10)
with AI practitioners. Our findings reveal that SHI in practice is primarily
driven by commercial priorities (e.g. customer value, compliance) and several
factors currently discourage more rAI-aligned SHI practices. This suggests that
established SHI practices are largely not contributing to rAI efforts. To
address this disconnect, we propose interventions and research opportunities to
advance rAI development in practice.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [266] [Designing conflict-based communicative tasks in Teaching Chinese as a Foreign Language with ChatGPT](https://arxiv.org/abs/2506.09089)
*Xia Li*

Main category: cs.HC

TL;DR: 该论文探讨了在大学对外汉语口语课程中，教师如何利用ChatGPT设计基于冲突的交际任务以提升学生互动能力，并分析其协作过程与AI工具的应用影响。


<details>
  <summary>Details</summary>
Motivation: 研究旨在通过冲突性任务增强学生口语互动技能，同时探索ChatGPT在课程设计中的辅助潜力，以优化教学程序开发效率与创新性。

Method: 教师在课程设计中引入ChatGPT作为协作工具，通过设计冲突型交际任务，记录并总结与AI的交互特征及任务设计过程。

Result: 研究揭示了教师与ChatGPT在任务设计中的动态协作模式，包括AI在提供多样化情境、调整任务复杂度等方面的具体作用。

Conclusion: ChatGPT可作为有效辅助工具支持教学任务设计，其交互特性能够促进教学创新，但需结合教师专业判断以实现教育目标。

Abstract: In developing the teaching program for a course in Oral Expression in
Teaching Chinese as a Foreign Language at the university level, the teacher
designs communicative tasks based on conflicts to encourage learners to engage
in interactive dynamics and develop their oral interaction skills. During the
design of these tasks, the teacher uses ChatGPT to assist in finalizing the
program. This article aims to present the key characteristics of the
interactions between the teacher and ChatGPT during this program development
process, as well as to examine the use of ChatGPT and its impacts in this
specific context.

</details>


### [267] ["Is This Really a Human Peer Supporter?": Misalignments Between Peer Supporters and Experts in LLM-Supported Interactions](https://arxiv.org/abs/2506.09354)
*Kellie Yu Hui Sim,Roy Ka-Wei Lee,Kenny Tsu Wei Choo*

Main category: cs.HC

TL;DR: 研究评估了一个AI增强的同伴支持系统，结合LLM模拟求助者、实时建议和情绪可视化。通过两项混合方法研究发现，尽管系统在培训和互动质量上有潜力，但专家与同伴支持者的回应存在关键差异，突显当前培训的不足，强调需结合心理学基础的标准训练及专家监督的LLM系统设计。


<details>
  <summary>Details</summary>
Motivation: 心理健康问题日益严重，AI驱动的解决方案可扩展心理社会支持。同伴支持基于实际经验，是专业护理的重要补充，但其培训、效果和定义的参差不齐引发质量与安全担忧。研究旨在探索LLM如何提升实时文本互动中的同伴支持。

Method: 开发并评估了一个AI支持系统，包含LLM模拟的求助者、情境感知的LLM生成建议及实时情绪可视化。通过两项混合方法研究（12名同伴支持者与5名心理健康专家参与），分析系统效果及实践影响。

Result: 两组参与者均认可系统对培训和互动质量的提升潜力。但专家指出同伴支持者常忽略求助者痛苦信号、过早提供建议等关键问题，突显当前培训在情感强烈情境下的不足，尤其在安全性和实践规范性方面。

Conclusion: 研究强调需建立标准化、基于心理学的同伴支持培训体系，并谨慎设计由专家监督的LLM支持系统，以负责任地整合AI至心理健康领域，推动LLM在增强同伴护理中的发展。

Abstract: Mental health is a growing global concern, prompting interest in AI-driven
solutions to expand access to psychosocial support. Peer support, grounded in
lived experience, offers a valuable complement to professional care. However,
variability in training, effectiveness, and definitions raises concerns about
quality, consistency, and safety. Large Language Models (LLMs) present new
opportunities to enhance peer support interactions, particularly in real-time,
text-based interactions. We present and evaluate an AI-supported system with an
LLM-simulated distressed client, context-sensitive LLM-generated suggestions,
and real-time emotion visualisations. 2 mixed-methods studies with 12 peer
supporters and 5 mental health professionals (i.e., experts) examined the
system's effectiveness and implications for practice. Both groups recognised
its potential to enhance training and improve interaction quality. However, we
found a key tension emerged: while peer supporters engaged meaningfully,
experts consistently flagged critical issues in peer supporter responses, such
as missed distress cues and premature advice-giving. This misalignment
highlights potential limitations in current peer support training, especially
in emotionally charged contexts where safety and fidelity to best practices are
essential. Our findings underscore the need for standardised, psychologically
grounded training, especially as peer support scales globally. They also
demonstrate how LLM-supported systems can scaffold this development--if
designed with care and guided by expert oversight. This work contributes to
emerging conversations on responsible AI integration in mental health and the
evolving role of LLMs in augmenting peer-delivered care.

</details>


### [268] ["I Said Things I Needed to Hear Myself": Peer Support as an Emotional, Organisational, and Sociotechnical Practice in Singapore](https://arxiv.org/abs/2506.09362)
*Kellie Yu Hui Sim,Kenny Tsu Wei Choo*

Main category: cs.HC

TL;DR: 研究通过访谈新加坡20位同伴支持者，分析其动机、情感劳动及社会文化因素，提出文化响应式数字工具设计方向，探讨AI如何负责任地增强心理健康同伴支持。


<details>
  <summary>Details</summary>
Motivation: 数字平台在亚洲心理健康同伴支持中的作用未被充分研究，需探索其设计影响及文化适配性，以弥补临床系统外的社区支持缺口。

Method: 采用质性访谈法，对20名在新加坡线上线下混合环境中提供同伴支持的工作者进行主题分析。

Result: 揭示了同伴支持启动、实施与维持的动态过程，识别出社会文化规范对实践的影响，并提出以关系护理为核心的数字工具设计框架。

Conclusion: 强调AI需作为辅助而非替代角色，提出可信赖且文化敏感的技术设计原则，为人本计算在心理健康领域的应用提供情境化路径。

Abstract: Peer support plays a vital role in expanding access to mental health care by
providing empathetic, community-based support outside formal clinical systems.
As digital platforms increasingly mediate such support, the design and impact
of these technologies remain under-examined, particularly in Asian contexts.
This paper presents findings from an interview study with 20 peer supporters in
Singapore, who operate across diverse online, offline, and hybrid environments.
Through a thematic analysis, we unpack how participants start, conduct, and
sustain peer support, highlighting their motivations, emotional labour, and the
sociocultural dimensions shaping their practices. Building on this grounded
understanding, we surface design directions for culturally responsive digital
tools that scaffold rather than supplant relational care. Drawing insights from
qualitative accounts, we offer a situated perspective on how AI might
responsibly augment peer support. This research contributes to human-centred
computing by articulating the lived realities of peer supporters and proposing
design implications for trustworthy and context-sensitive AI in mental health.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [269] [SimClass: A Classroom Speech Dataset Generated via Game Engine Simulation For Automatic Speech Recognition Research](https://arxiv.org/abs/2506.09206)
*Ahmed Adel Attia,Jing Liu,Carl Espy-Wilson*

Main category: cs.SD

TL;DR: 本文提出利用游戏引擎合成课堂噪音的方法，构建SimClass数据集，包含合成噪音和模拟课堂语音数据，以解决教育领域语音模型数据不足的问题。


<details>
  <summary>Details</summary>
Motivation: 现有公开课堂数据集稀缺且缺乏专用噪音库，限制了教育领域AI语音模型的发展及数据增强技术的应用。

Method: 通过游戏引擎合成课堂噪音框架，结合公开儿童语音语料库与YouTube讲座视频，生成模拟课堂语音数据。

Result: 实验表明SimClass在干净/嘈杂语音条件下均能有效逼近真实课堂语音，适用于语音识别与增强模型开发。

Conclusion: SimClass为教育场景提供了可扩展的合成数据解决方案，解决了数据匮乏问题并支持鲁棒语音技术研发。

Abstract: The scarcity of large-scale classroom speech data has hindered the
development of AI-driven speech models for education. Public classroom datasets
remain limited, and the lack of a dedicated classroom noise corpus prevents the
use of standard data augmentation techniques.
  In this paper, we introduce a scalable methodology for synthesizing classroom
noise using game engines, a framework that extends to other domains. Using this
methodology, we present SimClass, a dataset that includes both a synthesized
classroom noise corpus and a simulated classroom speech dataset. The speech
data is generated by pairing a public children's speech corpus with YouTube
lecture videos to approximate real classroom interactions in clean conditions.
Our experiments on clean and noisy speech demonstrate that SimClass closely
approximates real classroom speech, making it a valuable resource for
developing robust speech recognition and enhancement models.

</details>


### [270] [OWSM-Biasing: Contextualizing Open Whisper-Style Speech Models for Automatic Speech Recognition with Dynamic Vocabulary](https://arxiv.org/abs/2506.09448)
*Yui Sudo,Yusuke Fujita,Atsushi Kojima,Tomoya Mizumoto,Lianbo Liu*

Main category: cs.SD

TL;DR: 本文提出一种结合上下文偏置方法与冻结参数的预训练语音基础模型OWSM v3.1的方法，在小数据集上有效提升罕见词识别能力，同时保持模型整体性能。


<details>
  <summary>Details</summary>
Motivation: 现有上下文偏置方法因缺乏预训练知识导致性能低于语音基础模型，需探索如何利用预训练模型知识改进罕见词识别。

Method: 将现有上下文偏置方法与OWSM v3.1集成，冻结预训练参数以保留知识，通过小规模数据实现有效偏置。

Result: 在LibriSpeech测试集上，偏置词错误率降低11.6%，总错误率提升0.9%，实时因子减少7.5%。

Conclusion: 该方法成功融合预训练知识与上下文偏置，在保持语音基础模型优势的同时显著提升罕见词识别效果。

Abstract: Speech foundation models (SFMs), such as Open Whisper-Style Speech Models
(OWSM), are trained on massive datasets to achieve accurate automatic speech
recognition. However, even SFMs struggle to accurately recognize rare and
unseen words. While contextual biasing (CB) is a promising approach to improve
recognition of such words, most CB methods are trained from scratch, resulting
in lower performance than SFMs due to the lack of pre-trained knowledge. This
paper integrates an existing CB method with OWSM v3.1 while freezing its
pre-trained parameters. By leveraging the knowledge embedded in SFMs, the
proposed method enables effective CB while preserving the advantages of SFMs,
even with a small dataset. Experimental results show that the proposed method
improves the biasing word error rate (B-WER) by 11.6 points, resulting in a 0.9
point improvement in the overall WER while reducing the real-time factor by
7.5% compared to the non-biasing baseline on the LibriSpeech 100 test-clean
set.

</details>


### [271] [BemaGANv2: A Tutorial and Comparative Survey of GAN-based Vocoders for Long-Term Audio Generation](https://arxiv.org/abs/2506.09487)
*Taesoo Park,Mungwi Jeong,Mingyu Park,Narae Kim,Junyoung Kim,Mujung Kim,Jisang Yoo,Hoyun Lee,Sanghoon Kim,Soonchul Kwon*

Main category: cs.SD

TL;DR: BemaGANv2是基于GAN的高保真长时音频生成声码器，通过引入AMP模块（含Snake激活函数）改进生成器，结合MED+MRD判别器增强周期性建模与长期依赖捕捉。论文系统评估了不同判别器配置，并提供完整教程与预训练模型以促进复现。


<details>
  <summary>Details</summary>
Motivation: 传统GAN声码器在长时音频生成中存在周期性结构捕捉不足和长期依赖建模困难的问题。BemaGANv2旨在通过改进生成器与判别器架构，提升音频生成质量与稳定性。

Method: 生成器采用AMP模块替代ResBlocks，利用Snake函数增强周期性建模；判别器整合MED（提取时间包络特征）与MRD（多分辨率分析），并系统测试MSD+MED、MSD+MRD等组合。

Result: 通过FAD/SSIM/PLCC/MCD等客观指标和MOS/SMOS主观测试验证：MED+MRD组合在长时依赖建模中表现最优，模型代码与预训练权重已开源。

Conclusion: BemaGANv2通过AMP模块和MED+MRD架构显著提升音频生成质量，其模块化设计为声码器研究提供新范式，开源资源将推动领域复现与扩展。

Abstract: This paper presents a tutorial-style survey and implementation guide of
BemaGANv2, an advanced GAN-based vocoder designed for high-fidelity and
long-term audio generation. Built upon the original BemaGAN architecture,
BemaGANv2 incorporates major architectural innovations by replacing traditional
ResBlocks in the generator with the Anti-aliased Multi-Periodicity composition
(AMP) module, which internally applies the Snake activation function to better
model periodic structures. In the discriminator framework, we integrate the
Multi-Envelope Discriminator (MED), a novel architecture we originally
proposed, to extract rich temporal envelope features crucial for periodicity
detection. Coupled with the Multi-Resolution Discriminator (MRD), this
combination enables more accurate modeling of long-range dependencies in audio.
We systematically evaluate various discriminator configurations, including MSD
+ MED, MSD + MRD, and MPD + MED + MRD, using objective metrics (FAD, SSIM,
PLCC, MCD) and subjective evaluations (MOS, SMOS). This paper also provides a
comprehensive tutorial on the model architecture, training methodology, and
implementation to promote reproducibility. The code and pre-trained models are
available at: https://github.com/dinhoitt/BemaGANv2.

</details>


### [272] [Training-Free Voice Conversion with Factorized Optimal Transport](https://arxiv.org/abs/2506.09709)
*Alexander Lobashev,Assel Yermekova,Maria Larchenko*

Main category: cs.SD

TL;DR: 本文提出Factorized MKL-VC方法，通过因子化最优传输映射改进kNN-VC流程，仅需5秒参考音频即可实现高质量跨语言语音转换，在内容保留与鲁棒性上显著优于原方法。


<details>
  <summary>Details</summary>
Motivation: 传统kNN-VC流程在短参考音频下存在内容保留不足的问题，尤其在跨语言场景中效果受限。需开发更高效的特征转换方法以提升短音频条件下的语音转换质量。

Method: 使用基于Monge-Kantorovich线性解的因子化最优传输映射替代kNN回归，通过分解WavLM嵌入子空间处理维度间方差不均匀问题，实现高效特征转换。

Result: 在LibriSpeech和FLEURS数据集上，MKL-VC相比kNN-VC内容保留率提升显著，跨语言场景下性能接近FACodec，尤其在5秒短音频条件下表现出更强鲁棒性。

Conclusion: 因子化最优传输方法有效解决了跨维度方差问题，证明其在低资源跨语言语音转换任务中的优越性，为实时语音转换系统提供了新思路。

Abstract: This paper introduces Factorized MKL-VC, a training-free modification for
kNN-VC pipeline. In contrast with original pipeline, our algorithm performs
high quality any-to-any cross-lingual voice conversion with only 5 second of
reference audio. MKL-VC replaces kNN regression with a factorized optimal
transport map in WavLM embedding subspaces, derived from Monge-Kantorovich
Linear solution. Factorization addresses non-uniform variance across
dimensions, ensuring effective feature transformation. Experiments on
LibriSpeech and FLEURS datasets show MKL-VC significantly improves content
preservation and robustness with short reference audio, outperforming kNN-VC.
MKL-VC achieves performance comparable to FACodec, especially in cross-lingual
voice conversion domain.

</details>


### [273] [Incorporating Linguistic Constraints from External Knowledge Source for Audio-Visual Target Speech Extraction](https://arxiv.org/abs/2506.09792)
*Wenxuan Wu,Shuai Wang,Xixin Wu,Helen Meng,Haizhou Li*

Main category: cs.SD

TL;DR: 本文提出利用预训练语音-语言模型（PSLMs）或语言模型（PLMs）的语义约束作为额外监督信号，增强音频-视觉目标说话人提取（AV-TSE）模型的性能。该方法在推理阶段不增加计算成本，显著提升语音质量与可懂度，并在多语言及视觉线索缺失场景中表现稳健。


<details>
  <summary>Details</summary>
Motivation: 受人类利用句法、语义等语言知识辅助语音感知的启发，研究预训练语言模型能否作为辅助知识源，弥补传统AV-TSE模型仅依赖视觉线索的局限性。

Method: 将PSLMs或PLMs提供的语言约束作为监督信号整合到AV-TSE模型中，通过引入语言层面的对齐优化模型，且推理时无需额外计算。

Result: 模型在语音质量（如PESQ）和可懂度（如STOI）指标上持续提升，在多语言场景及视觉线索受损情况下仍保持鲁棒性能增益。

Conclusion: 语言模型提供的语义监督能有效增强AV-TSE模型性能，且不增加推理成本，为复杂场景下的语音分离提供了实用化解决方案。

Abstract: Audio-visual target speaker extraction (AV-TSE) models primarily rely on
target visual cues to isolate the target speaker's voice from others. We know
that humans leverage linguistic knowledge, such as syntax and semantics, to
support speech perception. Inspired by this, we explore the potential of
pre-trained speech-language models (PSLMs) and pre-trained language models
(PLMs) as auxiliary knowledge sources for AV-TSE. In this study, we propose
incorporating the linguistic constraints from PSLMs or PLMs for the AV-TSE
model as additional supervision signals. Without introducing any extra
computational cost during inference, the proposed approach consistently
improves speech quality and intelligibility. Furthermore, we evaluate our
method in multi-language settings and visual cue-impaired scenarios and show
robust performance gains.

</details>


### [274] [UmbraTTS: Adapting Text-to-Speech to Environmental Contexts with Flow Matching](https://arxiv.org/abs/2506.09874)
*Neta Glazer,Aviv Navon,Yael Segal,Aviv Shamsian,Hilit Segev,Asaf Buchnick,Menachem Pirchi,Gil Hetz,Joseph Keshet*

Main category: cs.SD

TL;DR: UmbraTTS提出了一种基于流匹配的TTS模型，可联合生成语音与环境音频，通过自监督框架解决数据缺失问题，实现背景音量精细控制与高质量场景感知音频生成。


<details>
  <summary>Details</summary>
Motivation: 现有TTS技术虽能合成自然语音，但难以将语音与复杂环境背景音效融合，且缺乏自然对齐的语音-背景音训练数据。

Method: 采用流匹配模型联合生成语音和环境音频，提出自监督框架从未标注录音中提取语音/背景/文本三元组作为训练数据。

Result: 实验表明UmbraTTS显著优于基线模型，能生成自然、高质量且环境感知的音频，支持背景音量细粒度控制。

Conclusion: 该工作通过数据自监督与联合建模方法，突破了语音-环境音融合合成的技术瓶颈，为场景化音频生成提供了新范式。

Abstract: Recent advances in Text-to-Speech (TTS) have enabled highly natural speech
synthesis, yet integrating speech with complex background environments remains
challenging. We introduce UmbraTTS, a flow-matching based TTS model that
jointly generates both speech and environmental audio, conditioned on text and
acoustic context. Our model allows fine-grained control over background volume
and produces diverse, coherent, and context-aware audio scenes. A key challenge
is the lack of data with speech and background audio aligned in natural
context. To overcome the lack of paired training data, we propose a
self-supervised framework that extracts speech, background audio, and
transcripts from unannotated recordings. Extensive evaluations demonstrate that
UmbraTTS significantly outperformed existing baselines, producing natural,
high-quality, environmentally aware audios.

</details>


<div id='q-bio.GN'></div>

# q-bio.GN [[Back]](#toc)

### [275] [A Probabilistic Framework for Imputing Genetic Distances in Spatiotemporal Pathogen Models](https://arxiv.org/abs/2506.09076)
*Haley Stone,Jing Du,Hao Xue,Matthew Scotch,David Heslop,Andreas Züfle,Chandini Raina MacIntyre,Flora Salim*

Main category: q-bio.GN

TL;DR: 提出一种概率框架，通过时间感知的进化距离模型推断未测序病例与已知序列的遗传距离，解决测序覆盖不全对病原体基因组数据应用的限制，并应用于禽流感案例以增强时空建模。


<details>
  <summary>Details</summary>
Motivation: 现有病原体基因组数据因测序覆盖不全限制了其在空间模型中的应用，需通过推断未测序病例的遗传距离来提升数据效用。

Method: 基于时间感知的进化距离建模，利用样本采集时间和观测遗传距离估计成对差异，无需序列比对或已知传播链即可进行生物学合理的插补。

Result: 该方法成功应用于美国野生鸟类高致病性禽流感A/H5案例，支持基因组数据集的可扩展性增强及进化信息与时空建模的整合。

Conclusion: 所提框架无需生物信息学对齐或传播链先验知识，通过概率推断有效提升基因组数据在时空建模中的实用性，为流行病学监测提供新工具。

Abstract: Pathogen genome data offers valuable structure for spatial models, but its
utility is limited by incomplete sequencing coverage. We propose a
probabilistic framework for inferring genetic distances between unsequenced
cases and known sequences within defined transmission chains, using time-aware
evolutionary distance modeling. The method estimates pairwise divergence from
collection dates and observed genetic distances, enabling biologically
plausible imputation grounded in observed divergence patterns, without
requiring sequence alignment or known transmission chains. Applied to highly
pathogenic avian influenza A/H5 cases in wild birds in the United States, this
approach supports scalable, uncertainty-aware augmentation of genomic datasets
and enhances the integration of evolutionary information into spatiotemporal
modeling workflows.

</details>


<div id='q-fin.ST'></div>

# q-fin.ST [[Back]](#toc)

### [276] [Advancing Exchange Rate Forecasting: Leveraging Machine Learning and AI for Enhanced Accuracy in Global Financial Markets](https://arxiv.org/abs/2506.09851)
*Md. Yeasin Rahat,Rajan Das Gupta,Nur Raisa Rahman,Sudipto Roy Pritom,Samiur Rahman Shakir,Md Imrul Hasan Showmick,Md. Jakir Hossen*

Main category: q-fin.ST

TL;DR: 本研究利用LSTM和GBC模型预测2018-2023年美元/孟加拉塔卡汇率，LSTM以99.45%准确率显著优于ARIMA，但GBC策略在回测中亏损。研究证明深度学习在外汇预测中的潜力，并提出结合情感分析等改进方向。


<details>
  <summary>Details</summary>
Motivation: 外汇汇率预测对全球贸易、投资和经济稳定具有重要影响。开发高精度预测模型可为交易者和政策制定者提供风险管理工具，应对市场波动。

Method: 使用2018-2023年Yahoo Finance历史汇率数据，采用LSTM神经网络进行价格预测，GBC模型进行方向性交易策略分析，并与ARIMA传统方法对比。

Result: LSTM模型达到99.449%准确率（RMSE=0.9858），优于ARIMA（RMSE=1.342）。GBC策略49次交易中盈利40.82%，但净亏损20,653美元。BDT/USD汇率从0.012降至0.009。

Conclusion: 深度学习在外汇预测中展现显著优势，未来可通过整合情感分析和实时经济指标提升模型适应性，为市场参与者提供更稳健的决策支持。

Abstract: The prediction of foreign exchange rates, such as the US Dollar (USD) to
Bangladeshi Taka (BDT), plays a pivotal role in global financial markets,
influencing trade, investments, and economic stability. This study leverages
historical USD/BDT exchange rate data from 2018 to 2023, sourced from Yahoo
Finance, to develop advanced machine learning models for accurate forecasting.
A Long Short-Term Memory (LSTM) neural network is employed, achieving an
exceptional accuracy of 99.449%, a Root Mean Square Error (RMSE) of 0.9858, and
a test loss of 0.8523, significantly outperforming traditional methods like
ARIMA (RMSE 1.342). Additionally, a Gradient Boosting Classifier (GBC) is
applied for directional prediction, with backtesting on a $10,000 initial
capital revealing a 40.82% profitable trade rate, though resulting in a net
loss of $20,653.25 over 49 trades. The study analyzes historical trends,
showing a decline in BDT/USD rates from 0.012 to 0.009, and incorporates
normalized daily returns to capture volatility. These findings highlight the
potential of deep learning in forex forecasting, offering traders and
policymakers robust tools to mitigate risks. Future work could integrate
sentiment analysis and real-time economic indicators to further enhance model
adaptability in volatile markets.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [277] [How attention simplifies mental representations for planning](https://arxiv.org/abs/2506.09520)
*Jason da Silva Castanheira,Nicholas Shea,Stephen M. Fleming*

Main category: q-bio.NC

TL;DR: 研究通过虚拟迷宫实验揭示空间注意力如何调控任务表征的构建，发现空间邻近性和自然注意轮廓促进简化表征的形成，个体差异显著，并将视觉注意机制整合至计算模型以解释规划与感知的交互。


<details>
  <summary>Details</summary>
Motivation: 现有计算模型提出规划与感知的嵌套优化关系，但二者交互的注意机制尚不明确。本文旨在探索空间注意力如何选择性地将环境信息纳入主观意识，进而影响任务表征与规划行为。

Method: 采用虚拟迷宫导航任务，结合计算建模方法，分析空间注意力对任务表征可用性的控制机制，特别关注自然注意轮廓（如侧向注意力分布）对简化表征构建的影响。

Result: 空间邻近性决定迷宫信息的规划可用性；符合自然注意轮廓的任务信息更易形成简化有效表征；个体注意力差异显著影响其任务表征与行为表现。

Conclusion: 视觉空间注意力通过选择可用信息塑造任务表征，该机制被成功整合至价值引导表征计算模型，为理解人类环境表征与规划决策的认知机制提供新视角。

Abstract: Human planning is efficient -- it frugally deploys limited cognitive
resources to accomplish difficult tasks -- and flexible -- adapting to novel
problems and environments. Computational approaches suggest that people
construct simplified mental representations of their environment, balancing the
complexity of a task representation with its utility. These models imply a
nested optimisation in which planning shapes perception, and perception shapes
planning -- but the perceptual and attentional mechanisms governing how this
interaction unfolds remain unknown. Here, we harness virtual maze navigation to
characterise how spatial attention controls which aspects of a task
representation enter subjective awareness and are available for planning. We
find that spatial proximity governs which aspects of a maze are available for
planning, and that when task-relevant information follows natural (lateralised)
contours of attention, people can more easily construct simplified and useful
maze representations. This influence of attention varies considerably across
individuals, explaining differences in people's task representations and
behaviour. Inspired by the 'spotlight of attention' analogy, we incorporate the
effects of visuospatial attention into existing computational accounts of
value-guided construal. Together, our work bridges computational perspectives
on perception and decision-making to better understand how individuals
represent their environments in aid of planning.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [278] [UAD: Unsupervised Affordance Distillation for Generalization in Robotic Manipulation](https://arxiv.org/abs/2506.09284)
*Yihe Tang,Wenlong Huang,Yingke Wang,Chengshu Li,Roy Yuan,Ruohan Zhang,Jiajun Wu,Li Fei-Fei*

Main category: cs.RO

TL;DR: 提出无监督方法UAD，通过基础模型蒸馏物体功能知识，无需人工标注，结合视觉与语言模型自动生成数据，训练轻量解码器，实现跨场景泛化。


<details>
  <summary>Details</summary>
Motivation: 现有视觉功能预测方法依赖人工标注或预定义任务，限制了在开放任务和非结构化环境中的泛化能力。需无监督方法提升适应性。

Method: UAD结合大型视觉模型与视觉语言模型，自动生成<指令，功能>标注数据，在冻结特征上训练任务条件解码器，仅用仿真渲染物体训练。

Result: UAD在真实机器人场景和人类活动中泛化良好，模仿学习策略仅需10次演示即可泛化到新物体、类别及任务变体。

Conclusion: UAD通过无监督蒸馏基础模型知识，显著提升功能理解的泛化能力，为开放任务下的机器人操作提供有效观察空间。

Abstract: Understanding fine-grained object affordances is imperative for robots to
manipulate objects in unstructured environments given open-ended task
instructions. However, existing methods of visual affordance predictions often
rely on manually annotated data or conditions only on a predefined set of
tasks. We introduce UAD (Unsupervised Affordance Distillation), a method for
distilling affordance knowledge from foundation models into a task-conditioned
affordance model without any manual annotations. By leveraging the
complementary strengths of large vision models and vision-language models, UAD
automatically annotates a large-scale dataset with detailed $<$instruction,
visual affordance$>$ pairs. Training only a lightweight task-conditioned
decoder atop frozen features, UAD exhibits notable generalization to
in-the-wild robotic scenes and to various human activities, despite only being
trained on rendered objects in simulation. Using affordance provided by UAD as
the observation space, we show an imitation learning policy that demonstrates
promising generalization to unseen object instances, object categories, and
even variations in task instructions after training on as few as 10
demonstrations. Project website: https://unsup-affordance.github.io/

</details>


### [279] [Bipedal Balance Control with Whole-body Musculoskeletal Standing and Falling Simulations](https://arxiv.org/abs/2506.09383)
*Chengtian Ma,Yunyue Wei,Chenhui Zuo,Chen Zhang,Yanan Sui*

Main category: cs.RO

TL;DR: 本研究通过构建全身肌肉骨骼系统的分层控制框架，揭示了人类静态平衡的时空动态特征及肌肉损伤对平衡的影响，模拟髋关节外骨骼辅助可提升抗干扰能力，为平衡障碍干预和人形机器人开发提供新见解。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注动态平衡，但对静态平衡与跌倒的定量机制理解不足，需通过肌肉骨骼系统仿真填补实验难以获取的肌肉级动态数据缺口。

Method: 开发分层控制框架，结合全身肌肉骨骼系统进行平衡模拟，分析稳定站立、肌肉损伤场景，并构建与临床数据匹配的跌倒接触模式。

Result: 识别稳定站立时空特征，量化肌肉损伤影响，生成临床吻合的跌倒模式，证明髋外骨骼辅助可使扰动下肌肉能耗降低20%且平衡稳定性提升15%。

Conclusion: 该框架首次实现肌肉级平衡动态解析，为个性化康复干预提供理论依据，同时推动仿人机器人平衡控制系统的仿生设计。

Abstract: Balance control is important for human and bipedal robotic systems. While
dynamic balance during locomotion has received considerable attention,
quantitative understanding of static balance and falling remains limited. This
work presents a hierarchical control pipeline for simulating human balance via
a comprehensive whole-body musculoskeletal system. We identified spatiotemporal
dynamics of balancing during stable standing, revealed the impact of muscle
injury on balancing behavior, and generated fall contact patterns that aligned
with clinical data. Furthermore, our simulated hip exoskeleton assistance
demonstrated improvement in balance maintenance and reduced muscle effort under
perturbation. This work offers unique muscle-level insights into human balance
dynamics that are challenging to capture experimentally. It could provide a
foundation for developing targeted interventions for individuals with balance
impairments and support the advancement of humanoid robotic systems.

</details>


### [280] [Adv-BMT: Bidirectional Motion Transformer for Safety-Critical Traffic Scenario Generation](https://arxiv.org/abs/2506.09485)
*Yuxin Liu,Zhenghao Peng,Xuanhao Cui,Bolei Zhou*

Main category: cs.RO

TL;DR: 提出Adv-BMT框架，通过逆向运动预测生成对抗性交互场景，增强自动驾驶测试数据，降低碰撞率。


<details>
  <summary>Details</summary>
Motivation: 现有自动驾驶测试数据集缺乏长尾安全关键场景，导致测试覆盖不足。真实场景数据稀缺限制了系统验证的有效性。

Method: 使用双向运动变换器(BMT)进行逆向交通运动预测，通过两阶段流程(对抗初始化+逆向预测)生成碰撞场景，无需碰撞数据预训练。

Result: 实验表明，使用增强数据集训练可使碰撞率比现有方法降低20%，生成的碰撞场景具有更高多样性和真实性。

Conclusion: Adv-BMT框架有效解决了自动驾驶测试数据的长尾分布问题，通过无监督生成对抗场景显著提升了系统安全性验证能力。

Abstract: Scenario-based testing is essential for validating the performance of
autonomous driving (AD) systems. However, such testing is limited by the
scarcity of long-tailed, safety-critical scenarios in existing datasets
collected in the real world. To tackle the data issue, we propose the Adv-BMT
framework, which augments real-world scenarios with diverse and realistic
adversarial interactions. The core component of Adv-BMT is a bidirectional
motion transformer (BMT) model to perform inverse traffic motion predictions,
which takes agent information in the last time step of the scenario as input,
and reconstruct the traffic in the inverse of chronological order until the
initial time step. The Adv-BMT framework is a two-staged pipeline: it first
conducts adversarial initializations and then inverse motion predictions.
Different from previous work, we do not need any collision data for
pretraining, and are able to generate realistic and diverse collision
interactions. Our experimental results validate the quality of generated
collision scenarios by Adv-BMT: training in our augmented dataset would reduce
episode collision rates by 20\% compared to previous work.

</details>


### [281] [Tightly-Coupled LiDAR-IMU-Leg Odometry with Online Learned Leg Kinematics Incorporating Foot Tactile Information](https://arxiv.org/abs/2506.09548)
*Taku Okawara,Kenji Koide,Aoki Takanose,Shuji Oishi,Masashi Yokozuka,Kentaro Uno,Kazuya Yoshida*

Main category: cs.RO

TL;DR: 本文提出了一种紧耦合的LiDAR-IMU-腿部里程计方法，通过在线学习神经腿部运动学模型，结合触觉信息提升四足机器人在无特征环境与可变形地形中的定位鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统里程计在无特征环境（如沙地）和可变形地形（如砾石、草地）中易失效，且机器人负载变化（如运输任务）会影响运动学模型精度。

Method: 1. 开发基于触觉信息（足底反力）的在线学习神经腿部运动学模型，动态适应负载与地形变化；2. 在统一因子图上联合优化模型训练与里程计估计，通过神经自适应腿部里程计因子与不确定性估计保持一致性。

Result: 在沙滩（极端无特征可变形地形）和混合地形校园的实验中，该方法在定位精度上优于现有先进方法。

Conclusion: 融合神经腿部运动学模型的紧耦合里程计显著提升了复杂场景下的鲁棒性，在线学习机制有效解决了负载与地形动态适应问题。

Abstract: In this letter, we present tightly coupled LiDAR-IMU-leg odometry, which is
robust to challenging conditions such as featureless environments and
deformable terrains. We developed an online learning-based leg kinematics model
named the neural leg kinematics model, which incorporates tactile information
(foot reaction force) to implicitly express the nonlinear dynamics between
robot feet and the ground. Online training of this model enhances its
adaptability to weight load changes of a robot (e.g., assuming delivery or
transportation tasks) and terrain conditions. According to the \textit{neural
adaptive leg odometry factor} and online uncertainty estimation of the leg
kinematics model-based motion predictions, we jointly solve online training of
this kinematics model and odometry estimation on a unified factor graph to
retain the consistency of both. The proposed method was verified through real
experiments using a quadruped robot in two challenging situations: 1) a sandy
beach, representing an extremely featureless area with a deformable terrain,
and 2) a campus, including multiple featureless areas and terrain types of
asphalt, gravel (deformable terrain), and grass. Experimental results showed
that our odometry estimation incorporating the \textit{neural leg kinematics
model} outperforms state-of-the-art works. Our project page is available for
further details: https://takuokawara.github.io/RAL2025_project_page/

</details>


### [282] [SkillBlender: Towards Versatile Humanoid Whole-Body Loco-Manipulation via Skill Blending](https://arxiv.org/abs/2506.09366)
*Yuxuan Kuang,Haoran Geng,Amine Elhafsi,Tan-Dzung Do,Pieter Abbeel,Jitendra Malik,Marco Pavone,Yue Wang*

Main category: cs.RO

TL;DR: 提出分层强化学习框架SkillBlender，通过混合预训练任务无关基元技能实现通用人形机器人移动操作，减少任务特定奖励工程，并在新基准SkillBench上验证有效性。


<details>
  <summary>Details</summary>
Motivation: 现有基于最优控制或强化学习的人形机器人控制方法需要针对每个任务进行繁琐调整，限制了其在日常多样化场景中的通用性和扩展性。

Method: SkillBlender框架分两步：1) 预训练目标条件任务无关基元技能 2) 动态混合技能完成复杂任务。配套SkillBench基准含多形态、基元技能和8项挑战性任务。

Result: 实验表明该方法显著优于基线，能自然规范行为避免奖励作弊，在多样化日常移动操作任务中产生更准确可行的运动轨迹。

Conclusion: SkillBlender通过技能混合机制突破任务特定调整限制，配套开源基准SkillBench为社区提供跨形态、多任务的科学评估体系，推动通用人形机器人研究。

Abstract: Humanoid robots hold significant potential in accomplishing daily tasks
across diverse environments thanks to their flexibility and human-like
morphology. Recent works have made significant progress in humanoid whole-body
control and loco-manipulation leveraging optimal control or reinforcement
learning. However, these methods require tedious task-specific tuning for each
task to achieve satisfactory behaviors, limiting their versatility and
scalability to diverse tasks in daily scenarios. To that end, we introduce
SkillBlender, a novel hierarchical reinforcement learning framework for
versatile humanoid loco-manipulation. SkillBlender first pretrains
goal-conditioned task-agnostic primitive skills, and then dynamically blends
these skills to accomplish complex loco-manipulation tasks with minimal
task-specific reward engineering. We also introduce SkillBench, a parallel,
cross-embodiment, and diverse simulated benchmark containing three embodiments,
four primitive skills, and eight challenging loco-manipulation tasks,
accompanied by a set of scientific evaluation metrics balancing accuracy and
feasibility. Extensive simulated experiments show that our method significantly
outperforms all baselines, while naturally regularizing behaviors to avoid
reward hacking, resulting in more accurate and feasible movements for diverse
loco-manipulation tasks in our daily scenarios. Our code and benchmark will be
open-sourced to the community to facilitate future research. Project page:
https://usc-gvl.github.io/SkillBlender-web/.

</details>


### [283] [Scoop-and-Toss: Dynamic Object Collection for Quadrupedal Systems](https://arxiv.org/abs/2506.09406)
*Minji Kang,Chanwoo Baek,Yoonsang Lee*

Main category: cs.RO

TL;DR: 本文提出一种四足机器人动态物体收集框架，通过腿部附加铲状装置及分层策略，实现无需额外执行器的多物体舀取-抛掷任务。


<details>
  <summary>Details</summary>
Motivation: 现有四足机器人腿部操纵研究多集中于静态任务，本文旨在利用腿部敏捷性扩展其动态物体收集能力，突破传统运动功能限制。

Method: 采用铲状腿部附加装置，构建分层策略架构：包含舀取/抛掷策略、接近策略的专家策略层，以及动态切换策略的元策略层，分阶段训练实现协调控制。

Result: 实验证明该方法能有效利用四足机器人腿部完成动态物体收集，验证了腿部在非运动任务中的多功能应用潜力。

Conclusion: 通过简单硬件改造与分层控制策略，成功将四足机器人腿部功能扩展至动态物体操纵领域，为腿部多用途化提供了新方向。

Abstract: Quadruped robots have made significant advances in locomotion, extending
their capabilities from controlled environments to real-world applications.
Beyond movement, recent work has explored loco-manipulation using the legs to
perform tasks such as pressing buttons or opening doors. While these efforts
demonstrate the feasibility of leg-based manipulation, most have focused on
relatively static tasks. In this work, we propose a framework that enables
quadruped robots to collect objects without additional actuators by leveraging
the agility of their legs. By attaching a simple scoop-like add-on to one leg,
the robot can scoop objects and toss them into a collection tray mounted on its
back. Our method employs a hierarchical policy structure comprising two expert
policies-one for scooping and tossing, and one for approaching object
positions-and a meta-policy that dynamically switches between them. The expert
policies are trained separately, followed by meta-policy training for
coordinated multi-object collection. This approach demonstrates how quadruped
legs can be effectively utilized for dynamic object manipulation, expanding
their role beyond locomotion.

</details>


### [284] [Time-Unified Diffusion Policy with Action Discrimination for Robotic Manipulation](https://arxiv.org/abs/2506.09422)
*Ye Niu,Sanping Zhou,Yizhe Li,Ye Den,Le Wang*

Main category: cs.RO

TL;DR: 本文针对基于扩散模型的机器人策略方法存在生成速度慢、训练复杂度高的问题，提出时间统一扩散策略（TUDP），通过构建时间统一的速度场和动作判别训练分支，在保证动作精度的同时显著提升生成效率，在RLBench基准测试中达到最高83.8%的成功率。


<details>
  <summary>Details</summary>
Motivation: 现有扩散策略需迭代去噪生成动作，导致实时性差；且时间变化的去噪过程增加了模型训练难度，降低了动作精度。需在保证生成效率的同时提升动作准确性。

Method: 提出时间统一扩散策略（TUDP）：1）在动作空间构建时间统一的速度场，统一所有时间步的去噪过程以降低学习难度；2）引入动作判别分支进行训练，通过动作识别能力增强去噪精度。

Result: 在RLBench测试中，多视角/单视角任务分别达到82.6%和83.8%的最高成功率。尤其在减少去噪迭代次数时，成功率提升更显著，且能适应广泛现实任务。

Conclusion: TUDP通过时间统一的速度场和动作判别机制，在降低训练复杂度的同时实现了高效、高鲁棒性的动作生成，为实时精准的机器人操作提供了新解决方案。

Abstract: In many complex scenarios, robotic manipulation relies on generative models
to estimate the distribution of multiple successful actions. As the diffusion
model has better training robustness than other generative models, it performs
well in imitation learning through successful robot demonstrations. However,
the diffusion-based policy methods typically require significant time to
iteratively denoise robot actions, which hinders real-time responses in robotic
manipulation. Moreover, existing diffusion policies model a time-varying action
denoising process, whose temporal complexity increases the difficulty of model
training and leads to suboptimal action accuracy. To generate robot actions
efficiently and accurately, we present the Time-Unified Diffusion Policy
(TUDP), which utilizes action recognition capabilities to build a time-unified
denoising process. On the one hand, we build a time-unified velocity field in
action space with additional action discrimination information. By unifying all
timesteps of action denoising, our velocity field reduces the difficulty of
policy learning and speeds up action generation. On the other hand, we propose
an action-wise training method, which introduces an action discrimination
branch to supply additional action discrimination information. Through
action-wise training, the TUDP implicitly learns the ability to discern
successful actions to better denoising accuracy. Our method achieves
state-of-the-art performance on RLBench with the highest success rate of 82.6%
on a multi-view setup and 83.8% on a single-view setup. In particular, when
using fewer denoising iterations, TUDP achieves a more significant improvement
in success rate. Additionally, TUDP can produce accurate actions for a wide
range of real-world tasks.

</details>


### [285] [SAFE: Multitask Failure Detection for Vision-Language-Action Models](https://arxiv.org/abs/2506.09937)
*Qiao Gu,Yuanliang Ju,Shengxiang Sun,Igor Gilitschenski,Haruki Nishimura,Masha Itkina,Florian Shkurti*

Main category: cs.RO

TL;DR: 本文提出SAFE，一种针对通用机器人策略（如VLA）的多任务故障检测器，通过分析VLA内部特征空间实现跨任务泛化，并在实验中验证其高效性与准确性。


<details>
  <summary>Details</summary>
Motivation: 现有故障检测器仅针对特定任务训练，而视觉-语言-动作模型（VLA）需在未知任务和环境中检测失败。需设计通用故障检测器以确保机器人及时响应错误。

Method: SAFE通过分析VLA特征空间发现其跨任务的通用失败知识，利用内部特征训练标量预测模型，结合保形预测优化检测时间与准确性，兼容多种策略架构。

Result: SAFE在OpenVLA、π₀等策略的仿真与真实环境中测试，相比基线方法在故障检测性能与检测时间平衡上达到SOTA，支持跨任务泛化场景。

Conclusion: SAFE验证了VLA特征空间的跨任务失败知识可迁移性，为通用机器人策略提供高效、泛化的故障检测方案，显著提升部署安全性。

Abstract: While vision-language-action models (VLAs) have shown promising robotic
behaviors across a diverse set of manipulation tasks, they achieve limited
success rates when deployed on novel tasks out-of-the-box. To allow these
policies to safely interact with their environments, we need a failure detector
that gives a timely alert such that the robot can stop, backtrack, or ask for
help. However, existing failure detectors are trained and tested only on one or
a few specific tasks, while VLAs require the detector to generalize and detect
failures also in unseen tasks and novel environments. In this paper, we
introduce the multitask failure detection problem and propose SAFE, a failure
detector for generalist robot policies such as VLAs. We analyze the VLA feature
space and find that VLAs have sufficient high-level knowledge about task
success and failure, which is generic across different tasks. Based on this
insight, we design SAFE to learn from VLA internal features and predict a
single scalar indicating the likelihood of task failure. SAFE is trained on
both successful and failed rollouts, and is evaluated on unseen tasks. SAFE is
compatible with different policy architectures. We test it on OpenVLA, $\pi_0$,
and $\pi_0$-FAST in both simulated and real-world environments extensively. We
compare SAFE with diverse baselines and show that SAFE achieves
state-of-the-art failure detection performance and the best trade-off between
accuracy and detection time using conformal prediction. More qualitative
results can be found at https://vla-safe.github.io/.

</details>


### [286] [eFlesh: Highly customizable Magnetic Touch Sensing using Cut-Cell Microstructures](https://arxiv.org/abs/2506.09994)
*Venkatesh Pattabiraman,Zizhou Huang,Daniele Panozzo,Denis Zorin,Lerrel Pinto,Raunaq Bhirangi*

Main category: cs.RO

TL;DR: eFlesh是一种低成本、易定制、开源的磁触觉传感器，通过3D打印微结构参数化设计，显著提升机器人对接触力/滑移的感知能力，在精密操作任务中实现91%成功率。


<details>
  <summary>Details</summary>
Motivation: 现有触觉传感器存在成本高、定制难、方案碎片化的问题，导致机器人物理交互时缺乏感知能力。需要一种通用、易获取且可定制的触觉传感方案。

Method: 结合3D打印技术、永磁体阵列与磁力计电路，通过参数化微结构设计实现传感器几何/力学特性调节，提供开源CAD-STL转换工具实现形状/灵敏度定制。

Result: 接触定位误差0.5mm，法向/切向力预测误差0.27N/0.12N；滑移检测准确率95%，视觉触觉控制策略较纯视觉基线提升40%性能，四类亚毫米级任务平均成功率91%。

Conclusion: eFlesh通过模块化设计范式突破触觉传感器定制化瓶颈，其开源生态可能推动机器人触觉感知技术的民主化发展，为复杂非结构化环境操作提供基础硬件支持。

Abstract: If human experience is any guide, operating effectively in unstructured
environments -- like homes and offices -- requires robots to sense the forces
during physical interaction. Yet, the lack of a versatile, accessible, and
easily customizable tactile sensor has led to fragmented, sensor-specific
solutions in robotic manipulation -- and in many cases, to force-unaware,
sensorless approaches. With eFlesh, we bridge this gap by introducing a
magnetic tactile sensor that is low-cost, easy to fabricate, and highly
customizable. Building an eFlesh sensor requires only four components: a
hobbyist 3D printer, off-the-shelf magnets (<$5), a CAD model of the desired
shape, and a magnetometer circuit board. The sensor is constructed from tiled,
parameterized microstructures, which allow for tuning the sensor's geometry and
its mechanical response. We provide an open-source design tool that converts
convex OBJ/STL files into 3D-printable STLs for fabrication. This modular
design framework enables users to create application-specific sensors, and to
adjust sensitivity depending on the task. Our sensor characterization
experiments demonstrate the capabilities of eFlesh: contact localization RMSE
of 0.5 mm, and force prediction RMSE of 0.27 N for normal force and 0.12 N for
shear force. We also present a learned slip detection model that generalizes to
unseen objects with 95% accuracy, and visuotactile control policies that
improve manipulation performance by 40% over vision-only baselines -- achieving
91% average success rate for four precise tasks that require sub-mm accuracy
for successful completion. All design files, code and the CAD-to-eFlesh STL
conversion tool are open-sourced and available on https://e-flesh.com.

</details>


### [287] [Learning to Optimize Package Picking for Large-Scale, Real-World Robot Induction](https://arxiv.org/abs/2506.09765)
*Shuai Li,Azarakhsh Keipour,Sicong Zhao,Srinath Rajagopalan,Charles Swan,Kostas E. Bekris*

Main category: cs.RO

TL;DR: 本研究提出一种基于机器学习的框架，通过优化拣选调整和吸盘选择，显著降低仓库机器人拣选失败率。


<details>
  <summary>Details</summary>
Motivation: 现有仓库自动化研究多依赖启发式方法预测拣选成功率，缺乏数据驱动的拣选优化方法，限制了大规模场景下的性能提升。

Method: 开发ML的框架预测拣选位姿变换，并优化多吸盘末端执行器的吸盘选择策略，直接提升单次拣选成功率。

Result: 在亚马逊机器人拣选系统模拟环境中测试超200万次拣选，失败率较基线方法降低20%。

Conclusion: 数据驱动的拣选优化框架有效提升大规模仓库自动化效能，验证了ML在工业机器人操作中的实际应用价值。

Abstract: Warehouse automation plays a pivotal role in enhancing operational
efficiency, minimizing costs, and improving resilience to workforce
variability. While prior research has demonstrated the potential of machine
learning (ML) models to increase picking success rates in large-scale robotic
fleets by prioritizing high-probability picks and packages, these efforts
primarily focused on predicting success probabilities for picks sampled using
heuristic methods. Limited attention has been given, however, to leveraging
data-driven approaches to directly optimize sampled picks for better
performance at scale. In this study, we propose an ML-based framework that
predicts transform adjustments as well as improving the selection of suction
cups for multi-suction end effectors for sampled picks to enhance their success
probabilities. The framework was integrated and evaluated in test workcells
that resemble the operations of Amazon Robotics' Robot Induction (Robin) fleet,
which is used for package manipulation. Evaluated on over 2 million picks, the
proposed method achieves a 20\% reduction in pick failure rates compared to a
heuristic-based pick sampling baseline, demonstrating its effectiveness in
large-scale warehouse automation scenarios.

</details>


### [288] [Chain-of-Action: Trajectory Autoregressive Modeling for Robotic Manipulation](https://arxiv.org/abs/2506.09990)
*Wenbo Zhang,Tianrun Hu,Yanyuan Qiao,Hanbo Zhang,Yuchu Qin,Yang Li,Jiajun Liu,Tao Kong,Lingqiao Liu,Xiao Ma*

Main category: cs.RO

TL;DR: 提出Chain-of-Action (CoA)框架，通过反向推理生成全局到局部的动作轨迹，结合任务目标约束，实现视觉运动策略的强空间泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统方法通过前向预测下一步动作，缺乏对全局目标的显式约束。CoA旨在通过反向推理将任务目标直接嵌入动作生成过程，增强局部动作与最终目标的一致性。

Method: 基于轨迹自回归建模，首先生成编码任务目标的关键帧动作，再自回归生成后续动作。引入连续动作标记表示、动态停止、反向时间集成、多标记预测四个设计，平衡动作块建模与全局结构。

Result: 在60个RLBench任务和8个真实操作任务中达到SOTA性能，验证了空间泛化能力与真实场景的兼容性。

Conclusion: CoA通过反向动作推理统一全局目标约束与局部动作生成，在保持策略灵活性的同时显著提升任务性能，为视觉运动策略提供了新范式。

Abstract: We present Chain-of-Action (CoA), a novel visuo-motor policy paradigm built
upon Trajectory Autoregressive Modeling. Unlike conventional approaches that
predict next step action(s) forward, CoA generates an entire trajectory by
explicit backward reasoning with task-specific goals through an action-level
Chain-of-Thought (CoT) process. This process is unified within a single
autoregressive structure: (1) the first token corresponds to a stable keyframe
action that encodes the task-specific goals; and (2) subsequent action tokens
are generated autoregressively, conditioned on the initial keyframe and
previously predicted actions. This backward action reasoning enforces a
global-to-local structure, allowing each local action to be tightly constrained
by the final goal. To further realize the action reasoning structure, CoA
incorporates four complementary designs: continuous action token
representation; dynamic stopping for variable-length trajectory generation;
reverse temporal ensemble; and multi-token prediction to balance action chunk
modeling with global structure. As a result, CoA gives strong spatial
generalization capabilities while preserving the flexibility and simplicity of
a visuo-motor policy. Empirically, we observe CoA achieves the state-of-the-art
performance across 60 RLBench tasks and 8 real-world manipulation tasks.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [289] [Know What You Don't Know: Uncertainty Calibration of Process Reward Models](https://arxiv.org/abs/2506.09338)
*Young-Jin Park,Kristjan Greenewald,Kaveh Alim,Hao Wang,Navid Azizan*

Main category: stat.ML

TL;DR: 本文提出一种基于分位数回归的过程奖励模型校准方法，并设计实例自适应推理框架，在保持精度的同时显著降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有过程奖励模型(PRM)存在校准不足问题，常高估推理路径的成功概率，导致固定推理预算分配效率低下。

Method: 通过分位数回归校准PRM输出概率，构建置信区间并开发实例自适应推理框架(IAS)，根据置信度动态调整每个问题的推理步数。

Result: 校准方法显著降低误差，IAS框架在数学推理基准上保持精度的同时平均减少28%计算量，置信度高的问题计算量下降更明显。

Conclusion: 模型校准与自适应推理框架的结合，为平衡大语言模型推理成本与性能提供了有效解决方案。

Abstract: Process reward models (PRMs) play a central role in guiding inference-time
scaling algorithms for large language models (LLMs). However, we observe that
even state-of-the-art PRMs can be poorly calibrated and often overestimate
success probabilities. To address this, we present a calibration approach,
performed via quantile regression, that adjusts PRM outputs to better align
with true success probabilities. Leveraging these calibrated success estimates
and their associated confidence bounds, we introduce an \emph{instance-adaptive
scaling} (IAS) framework that dynamically adjusts the inference budget based on
the estimated likelihood that a partial reasoning trajectory will yield a
correct final answer. Unlike conventional methods that allocate a fixed number
of reasoning trajectories per query, this approach successfully adapts to each
instance and reasoning step when using our calibrated PRMs. Experiments on
mathematical reasoning benchmarks show that (i) our PRM calibration method
successfully achieves small calibration error, outperforming the baseline
methods, (ii) calibration is crucial for enabling effective adaptive scaling,
and (iii) the proposed IAS strategy reduces inference costs while maintaining
final answer accuracy, utilizing less compute on more confident problems as
desired.

</details>


### [290] [Attention-Bayesian Hybrid Approach to Modular Multiple Particle Tracking](https://arxiv.org/abs/2506.09441)
*Piyush Mishra,Philippe Roudot*

Main category: stat.ML

TL;DR: 提出一种结合Transformer自注意力与贝叶斯滤波的混合多粒子追踪框架，通过假设剪枝在复杂场景中实现高效跟踪。


<details>
  <summary>Details</summary>
Motivation: 传统Transformer在轨迹假设较少时无法达到贝叶斯方法的最优性，而贝叶斯滤波面临组合爆炸问题。需要结合两者优势解决高杂波场景的多粒子追踪问题。

Method: 使用Transformer编码器推断跨帧检测的软关联关系，通过标签预测进行轨迹-检测关联，剪枝假设集后在贝叶斯滤波框架中实现高效追踪。

Result: 该方法在存在虚假检测的高杂波场景中展现出更高的跟踪精度和鲁棒性，验证了混合框架的有效性。

Conclusion: 融合自注意力机制与贝叶斯滤波的混合方法，为复杂场景下的多粒子追踪提供了可解释且高效的解决方案。

Abstract: Tracking multiple particles in noisy and cluttered scenes remains challenging
due to a combinatorial explosion of trajectory hypotheses, which scales
super-exponentially with the number of particles and frames. The transformer
architecture has shown a significant improvement in robustness against this
high combinatorial load. However, its performance still falls short of the
conventional Bayesian filtering approaches in scenarios presenting a reduced
set of trajectory hypothesis. This suggests that while transformers excel at
narrowing down possible associations, they may not be able to reach the
optimality of the Bayesian approach in locally sparse scenario. Hence, we
introduce a hybrid tracking framework that combines the ability of
self-attention to learn the underlying representation of particle behavior with
the reliability and interpretability of Bayesian filtering. We perform
trajectory-to-detection association by solving a label prediction problem,
using a transformer encoder to infer soft associations between detections
across frames. This prunes the hypothesis set, enabling efficient
multiple-particle tracking in Bayesian filtering framework. Our approach
demonstrates improved tracking accuracy and robustness against spurious
detections, offering a solution for high clutter multiple particle tracking
scenarios.

</details>


### [291] [LLM-Powered CPI Prediction Inference with Online Text Time Series](https://arxiv.org/abs/2506.09516)
*Yingying Fan,Jinchi Lv,Ao Sun,Yurou Wang*

Main category: stat.ML

TL;DR: 该论文提出LLM-CPI方法，利用大语言模型（如ChatGPT、BERT）分析高频在线文本数据，结合传统月度CPI数据，构建联合时间序列模型（ARX和VARX）以提升消费者价格指数预测效果，并通过实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有CPI预测多依赖低频调查数据，而高频在线文本数据的潜力尚未充分挖掘。大语言模型（LLMs）的发展为利用此类数据改进预测提供了新机会。

Method: 从中文社交平台收集高频文本，使用LLMs生成通胀相关标签及文本嵌入（LDA、BERT），构建联合时间序列框架：月度模型（ARX结合CPI、文本嵌入和宏观变量）与日度模型（VARX基于LLM生成的CPI替代指标及文本嵌入）。

Result: 方法具备渐近性质，构建了两种预测区间。模拟和实际数据实验表明，LLM-CPI在有限样本下表现优异，且具有实际应用优势。

Conclusion: LLM-CPI通过融合高频文本与传统数据，显著提升CPI预测能力，为经济指标分析提供了基于大语言模型的新范式。

Abstract: Forecasting the Consumer Price Index (CPI) is an important yet challenging
task in economics, where most existing approaches rely on low-frequency,
survey-based data. With the recent advances of large language models (LLMs),
there is growing potential to leverage high-frequency online text data for
improved CPI prediction, an area still largely unexplored. This paper proposes
LLM-CPI, an LLM-based approach for CPI prediction inference incorporating
online text time series. We collect a large set of high-frequency online texts
from a popularly used Chinese social network site and employ LLMs such as
ChatGPT and the trained BERT models to construct continuous inflation labels
for posts that are related to inflation. Online text embeddings are extracted
via LDA and BERT. We develop a joint time series framework that combines
monthly CPI data with LLM-generated daily CPI surrogates. The monthly model
employs an ARX structure combining observed CPI data with text embeddings and
macroeconomic variables, while the daily model uses a VARX structure built on
LLM-generated CPI surrogates and text embeddings. We establish the asymptotic
properties of the method and provide two forms of constructed prediction
intervals. The finite-sample performance and practical advantages of LLM-CPI
are demonstrated through both simulation and real data examples.

</details>


### [292] [Evasion Attacks Against Bayesian Predictive Models](https://arxiv.org/abs/2506.09640)
*Pablo G. Arce,Roi Naveiro,David Ríos Insua*

Main category: stat.ML

TL;DR: 本文提出针对贝叶斯预测模型的最优逃避攻击方法，研究点预测扰动与后验预测分布篡改两种对抗目标，开发基于梯度的新型攻击策略。


<details>
  <summary>Details</summary>
Motivation: 现有对抗机器学习研究多集中于传统预测模型的逃避/投毒攻击，而贝叶斯预测模型的脆弱性研究不足。

Method: 提出梯度驱动的攻击框架，针对点预测扰动和整体后验分布篡改两种场景设计攻击方法，并探讨不同计算场景下的实现特性。

Result: 开发出适用于贝叶斯模型的新型攻击算法，验证了其在多种计算环境中的有效性，揭示了模型在对抗场景下的潜在漏洞。

Conclusion: 该研究填补了贝叶斯模型对抗鲁棒性研究的空白，为评估概率建模系统的安全性提供了系统化方法论。

Abstract: There is an increasing interest in analyzing the behavior of machine learning
systems against adversarial attacks. However, most of the research in
adversarial machine learning has focused on studying weaknesses against evasion
or poisoning attacks to predictive models in classical setups, with the
susceptibility of Bayesian predictive models to attacks remaining
underexplored. This paper introduces a general methodology for designing
optimal evasion attacks against such models. We investigate two adversarial
objectives: perturbing specific point predictions and altering the entire
posterior predictive distribution. For both scenarios, we propose novel
gradient-based attacks and study their implementation and properties in various
computational setups.

</details>


### [293] [Scaling Laws for Uncertainty in Deep Learning](https://arxiv.org/abs/2506.09648)
*Mattia Rosso,Simone Rossi,Giulio Franzese,Markus Heinonen,Maurizio Filippone*

Main category: stat.ML

TL;DR: 本文通过实验发现深度学习中的预测不确定性同样遵循可扩展定律，并证明即使在大数据场景下，认知不确定性仍不可忽视，支持贝叶斯方法的必要性。


<details>
  <summary>Details</summary>
Motivation: 受深度学习模型性能与数据/模型规模相关的扩展定律启发，探索预测不确定性是否也存在类似规律，并回应“大数据下无需贝叶斯方法”的质疑。

Method: 在视觉和语言任务中，使用近似贝叶斯推断及集成方法，实证分析预测不确定性（包括分布内外）与数据/模型规模的标度关系。

Result: 实验观察到预测不确定性的多种度量均呈现标度定律，且数据量增加无法使认知不确定性趋近于零。

Conclusion: 研究为贝叶斯方法提供了有力支持，表明即使在大数据场景中，认知不确定性仍显著存在，不可被忽略。

Abstract: Deep learning has recently revealed the existence of scaling laws,
demonstrating that model performance follows predictable trends based on
dataset and model sizes. Inspired by these findings and fascinating phenomena
emerging in the over-parameterized regime, we examine a parallel direction: do
similar scaling laws govern predictive uncertainties in deep learning? In
identifiable parametric models, such scaling laws can be derived in a
straightforward manner by treating model parameters in a Bayesian way. In this
case, for example, we obtain $O(1/N)$ contraction rates for epistemic
uncertainty with respect to the number of data $N$. However, in
over-parameterized models, these guarantees do not hold, leading to largely
unexplored behaviors. In this work, we empirically show the existence of
scaling laws associated with various measures of predictive uncertainty with
respect to dataset and model sizes. Through experiments on vision and language
tasks, we observe such scaling laws for in- and out-of-distribution predictive
uncertainty estimated through popular approximate Bayesian inference and
ensemble methods. Besides the elegance of scaling laws and the practical
utility of extrapolating uncertainties to larger data or models, this work
provides strong evidence to dispel recurring skepticism against Bayesian
approaches: "In many applications of deep learning we have so much data
available: what do we need Bayes for?". Our findings show that "so much data"
is typically not enough to make epistemic uncertainty negligible.

</details>


### [294] [Assessing the Quality of Denoising Diffusion Models in Wasserstein Distance: Noisy Score and Optimal Bounds](https://arxiv.org/abs/2506.09681)
*Vahan Arsenyan,Elen Vardanyan,Arnak Dalalyan*

Main category: stat.ML

TL;DR: 本文通过实证与理论分析，证明了去噪扩散概率模型（DDPMs）在评分估计噪声下的鲁棒性，并建立了更优的Wasserstein-2距离收敛速率，表明其理论最优性。


<details>
  <summary>Details</summary>
Motivation: 研究DDPMs在噪声评分估计下的鲁棒性，并探索其理论收敛性能的改进空间，以弥补现有理论分析的不足。

Method: 结合实证验证与理论推导，分析DDPMs在恒定方差噪声下的表现，并基于Wasserstein-2距离建立有限样本保证框架。

Result: DDPMs对评分估计噪声具有鲁棒性，且收敛速率优于先前结果，与高斯情况的理论极限一致，证明其最优性。

Conclusion: DDPMs在噪声环境下的鲁棒性及其理论收敛速率的提升，为生成模型的实际应用提供了更坚实的理论支撑。

Abstract: Generative modeling aims to produce new random examples from an unknown
target distribution, given access to a finite collection of examples. Among the
leading approaches, denoising diffusion probabilistic models (DDPMs) construct
such examples by mapping a Brownian motion via a diffusion process driven by an
estimated score function. In this work, we first provide empirical evidence
that DDPMs are robust to constant-variance noise in the score evaluations. We
then establish finite-sample guarantees in Wasserstein-2 distance that exhibit
two key features: (i) they characterize and quantify the robustness of DDPMs to
noisy score estimates, and (ii) they achieve faster convergence rates than
previously known results. Furthermore, we observe that the obtained rates match
those known in the Gaussian case, implying their optimality.

</details>


### [295] [A Deep Generative Model for the Simulation of Discrete Karst Networks](https://arxiv.org/abs/2506.09832)
*Dany Lauzon,Julien Straubhaar,Philippe Renard*

Main category: stat.ML

TL;DR: 本文提出了一种结合图循环神经网络（GraphRNN）和图去噪扩散概率模型（G-DDPM）的新方法，用于生成符合真实岩溶网络统计特性的离散岩溶网络模拟。


<details>
  <summary>Details</summary>
Motivation: 岩溶网络模拟因涉及复杂的地质-水文长期作用过程而极具挑战，其形态与水文地质条件紧密相关，需开发能捕捉其拓扑与空间特征的有效生成方法。

Method: 1. 使用GraphRNN学习岩溶网络拓扑分布，通过序列化生成节点和边；2. 采用G-DDPM学习节点特征（空间坐标等属性），基于概率分布生成符合统计特性的图结构。

Result: 通过真实岩溶网络数据验证，生成的子图在几何与拓扑指标上与真实子图具有一致性，证明方法能有效模拟不同岩溶构造。

Conclusion: 该方法支持跨岩溶类型的随机网络生成，为研究水流和物质输运等物理过程提供了实用工具。

Abstract: The simulation of discrete karst networks presents a significant challenge
due to the complexity of the physicochemical processes occurring within various
geological and hydrogeological contexts over extended periods. This complex
interplay leads to a wide variety of karst network patterns, each intricately
linked to specific hydrogeological conditions. We explore a novel approach that
represents karst networks as graphs and applies graph generative models (deep
learning techniques) to capture the intricate nature of karst environments. In
this representation, nodes retain spatial information and properties, while
edges signify connections between nodes. Our generative process consists of two
main steps. First, we utilize graph recurrent neural networks (GraphRNN) to
learn the topological distribution of karst networks. GraphRNN decomposes the
graph simulation into a sequential generation of nodes and edges, informed by
previously generated structures. Second, we employ denoising diffusion
probabilistic models on graphs (G-DDPM) to learn node features (spatial
coordinates and other properties). G-DDPMs enable the generation of nodes
features on the graphs produced by the GraphRNN that adhere to the learned
statistical properties by sampling from the derived probability distribution,
ensuring that the generated graphs are realistic and capture the essential
features of the original data. We test our approach using real-world karst
networks and compare generated subgraphs with actual subgraphs from the
database, by using geometry and topology metrics. Our methodology allows
stochastic simulation of discrete karst networks across various types of
formations, a useful tool for studying the behavior of physical processes such
as flow and transport.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [296] [Devanagari Digit Recognition using Quantum Machine Learning](https://arxiv.org/abs/2506.09069)
*Sahaj Raj Malla*

Main category: quant-ph

TL;DR: 本文提出首个混合量子-经典架构用于Devanagari手写数字识别，结合CNN与10量子比特变分量子电路，在低资源语言场景下实现99.80%的量子模型最高准确率。


<details>
  <summary>Details</summary>
Motivation: Devanagari等区域文字的手写识别面临结构复杂、标注数据稀缺的挑战，需结合量子计算优势以支持多语言文档数字化、教育工具开发及文化遗产保护。

Method: 采用CNN提取空间特征，结合10量子比特变分量子电路(VQC)进行量子增强分类，构建混合量子-经典架构。

Result: 在DHCD数据集上实现99.80%测试准确率、0.2893测试损失及0.9980平均F1分数，参数量更少且鲁棒性优于经典CNN模型。

Conclusion: 通过量子叠加与纠缠特性，为区域文字识别建立新基准，验证量子机器学习在低资源语言场景的实际应用潜力。

Abstract: Handwritten digit recognition in regional scripts, such as Devanagari, is
crucial for multilingual document digitization, educational tools, and the
preservation of cultural heritage. The script's complex structure and limited
annotated datasets pose significant challenges to conventional models. This
paper introduces the first hybrid quantum-classical architecture for Devanagari
handwritten digit recognition, combining a convolutional neural network (CNN)
for spatial feature extraction with a 10-qubit variational quantum circuit
(VQC) for quantum-enhanced classification. Trained and evaluated on the
Devanagari Handwritten Character Dataset (DHCD), the proposed model achieves a
state-of-the-art test accuracy for quantum implementation of 99.80% and a test
loss of 0.2893, with an average per-class F1-score of 0.9980. Compared to
equivalent classical CNNs, our model demonstrates superior accuracy with
significantly fewer parameters and enhanced robustness. By leveraging quantum
principles such as superposition and entanglement, this work establishes a
novel benchmark for regional script recognition, highlighting the promise of
quantum machine learning (QML) in real-world, low-resource language settings.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [297] [Revolutionizing Clinical Trials: A Manifesto for AI-Driven Transformation](https://arxiv.org/abs/2506.09102)
*Mihaela van der Schaar,Richard Peck,Eoin McKinney,Jim Weatherall,Stuart Bailey,Justine Rochon,Chris Anagnostopoulos,Pierre Marquet,Anthony Wood,Nicky Best,Harry Amad,Julianna Piskorz,Krzysztof Kacprzyk,Rafik Salama,Christina Gunther,Francesca Frau,Antoine Pugeat,Ramon Hernandez*

Main category: cs.CY

TL;DR: 跨领域专家联合提出利用因果推断与数字孪生技术，在现有监管框架内革新临床试验，实现更高效、安全、个性化的医疗研究。


<details>
  <summary>Details</summary>
Motivation: 当前临床研究存在效率低、通用性差等问题，需通过AI技术突破传统模式局限，建立符合监管要求的新标准。

Method: 聚焦因果推断（揭示变量间因果关系）与数字孪生（构建患者虚拟模型）两大AI技术，制定可落地的整合方案。

Result: 提出结构化实施路线图，证明两种技术协同可优化试验设计、加速药物开发并支持精准医疗。

Conclusion: AI驱动的技术融合将重塑临床研究范式，在合规前提下实现从群体化到个性化医疗的范式转变。

Abstract: This manifesto represents a collaborative vision forged by leaders in
pharmaceuticals, consulting firms, clinical research, and AI. It outlines a
roadmap for two AI technologies - causal inference and digital twins - to
transform clinical trials, delivering faster, safer, and more personalized
outcomes for patients. By focusing on actionable integration within existing
regulatory frameworks, we propose a way forward to revolutionize clinical
research and redefine the gold standard for clinical trials using AI.

</details>


### [298] [FAIRTOPIA: Envisioning Multi-Agent Guardianship for Disrupting Unfair AI Pipelines](https://arxiv.org/abs/2506.09107)
*Athena Vakali,Ilias Dimitriadis*

Main category: cs.CY

TL;DR: 论文提出FAIRTOPIA框架，通过多角色智能体在AI全流程中嵌入公平性监控，以解决现有AI系统忽视人类原则、导致偏见危害的问题。框架采用三层架构，结合知识库与自适应机制，确保数据、模型、部署阶段的公平性。


<details>
  <summary>Details</summary>
Motivation: 当前决策型AI系统因忽视人类原则，在数据、模型及部署阶段存在偏见，已引发多起社会危害事件。现有方法集中于技术偏差分析，缺乏系统性、人本主义的公平保障机制，亟需构建覆盖全流程的公平守护方案。

Method: 引入智能体作为公平守护者，设计端到端协同框架FAIRTOPIA：1）三层架构封装AI流程，包含智能体守护层、知识库层及自优化层；2）多角色智能体工作流实时监控各阶段公平性；3）基于人本原则定制化公平算法。

Result: 提出可适应不同场景的通用公平算法框架，实现全流程动态公平监控。通过智能体交互与知识迭代，将社会技术原则融入AI系统，为构建系统性、跨学科的公平研究方法提供新范式。

Conclusion: 智能体技术能有效嵌入人本公平原则至AI全生命周期。FAIRTOPIA框架通过多代理协同与分层知识优化，为开发自适应、现实可用的公平AI系统奠定基础，推动社会技术交叉领域的公平性研究。

Abstract: AI models have become active decision makers, often acting without human
supervision. The rapid advancement of AI technology has already caused harmful
incidents that have hurt individuals and societies and AI unfairness in heavily
criticized. It is urgent to disrupt AI pipelines which largely neglect human
principles and focus on computational biases exploration at the data (pre),
model(in), and deployment (post) processing stages. We claim that by exploiting
the advances of agents technology, we will introduce cautious, prompt, and
ongoing fairness watch schemes, under realistic, systematic, and human-centric
fairness expectations. We envision agents as fairness guardians, since agents
learn from their environment, adapt to new information, and solve complex
problems by interacting with external tools and other systems. To set the
proper fairness guardrails in the overall AI pipeline, we introduce a
fairness-by-design approach which embeds multi-role agents in an end-to-end
(human to AI) synergetic scheme. Our position is that we may design adaptive
and realistic AI fairness frameworks, and we introduce a generalized algorithm
which can be customized to the requirements and goals of each AI decision
making scenario. Our proposed, so called FAIRTOPIA framework, is structured
over a three-layered architecture, which encapsulates the AI pipeline inside an
agentic guardian and a knowledge-based, self-refining layered scheme. Based on
our proposition, we enact fairness watch in all of the AI pipeline stages,
under robust multi-agent workflows, which will inspire new fairness research
hypothesis, heuristics, and methods grounded in human-centric, systematic,
interdisciplinary, socio-technical principles.

</details>


### [299] [Understanding Human-AI Trust in Education](https://arxiv.org/abs/2506.09160)
*Griffin Pitts,Sanaz Motamedi*

Main category: cs.CY

TL;DR: 研究探讨学生对AI聊天机器人信任的性质（人际信任vs技术信任）及其对学习相关因素的影响，发现人机信任是独立于传统模型的新类型，需新理论框架。


<details>
  <summary>Details</summary>
Motivation: AI聊天机器人的人格化特征导致学生信任模型不明确——传统人际信任可能错误赋予AI人性特质，而技术信任模型未考虑社交属性。需明确人机信任的独特性及其对教育效果的影响。

Method: 使用偏最小二乘结构方程模型（PLS-SEM），分析人类化信任与系统化信任对学生感知愉悦度、信任意图、使用行为意图及感知有用性的差异化作用。

Result: 人类化信任更强预测信任意图，系统化信任更有效预测使用意图和感知有用性，两者对感知愉悦度影响相似。表明人机信任是独立于人际/技术信任的混合模式。

Conclusion: 需建立专门的人机信任理论框架，并校准学生与AI的信任关系，这对教育领域AI的有效应用及教学效果优化至关重要。

Abstract: As AI chatbots become increasingly integrated in education, students are
turning to these systems for guidance, feedback, and information. However, the
anthropomorphic characteristics of these chatbots create ambiguity regarding
whether students develop trust toward them as they would a human peer or
instructor, based in interpersonal trust, or as they would any other piece of
technology, based in technology trust. This ambiguity presents theoretical
challenges, as interpersonal trust models may inappropriately ascribe human
intentionality and morality to AI, while technology trust models were developed
for non-social technologies, leaving their applicability to anthropomorphic
systems unclear. To address this gap, we investigate how human-like and
system-like trusting beliefs comparatively influence students' perceived
enjoyment, trusting intention, behavioral intention to use, and perceived
usefulness of an AI chatbot - factors associated with students' engagement and
learning outcomes. Through partial least squares structural equation modeling,
we found that human-like and system-like trust significantly influenced student
perceptions, with varied effects. Human-like trust more strongly predicted
trusting intention, while system-like trust better predicted behavioral
intention and perceived usefulness. Both had similar effects on perceived
enjoyment. Given the partial explanatory power of each type of trust, we
propose that students develop a distinct form of trust with AI chatbots
(human-AI trust) that differs from human-human and human-technology models of
trust. Our findings highlight the need for new theoretical frameworks specific
to human-AI trust and offer practical insights for fostering appropriately
calibrated trust, which is critical for the effective adoption and pedagogical
impact of AI in education.

</details>
