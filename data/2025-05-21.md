<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 137]
- [cs.AI](#cs.AI) [Total: 73]
- [cs.LG](#cs.LG) [Total: 143]
- [cs.HC](#cs.HC) [Total: 2]
- [stat.ML](#stat.ML) [Total: 15]
- [cs.CY](#cs.CY) [Total: 7]
- [math.OC](#math.OC) [Total: 2]
- [cs.RO](#cs.RO) [Total: 12]
- [cs.ET](#cs.ET) [Total: 1]
- [q-bio.GN](#q-bio.GN) [Total: 1]
- [cs.NE](#cs.NE) [Total: 1]
- [q-bio.QM](#q-bio.QM) [Total: 1]
- [stat.ME](#stat.ME) [Total: 1]
- [quant-ph](#quant-ph) [Total: 4]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 2]
- [eess.AS](#eess.AS) [Total: 10]
- [cs.IR](#cs.IR) [Total: 18]
- [cs.SE](#cs.SE) [Total: 4]
- [cond-mat.soft](#cond-mat.soft) [Total: 1]
- [cs.CR](#cs.CR) [Total: 11]
- [econ.EM](#econ.EM) [Total: 1]
- [cs.MA](#cs.MA) [Total: 4]
- [cs.SD](#cs.SD) [Total: 10]
- [cs.CG](#cs.CG) [Total: 2]
- [cs.GR](#cs.GR) [Total: 1]
- [eess.SP](#eess.SP) [Total: 1]
- [cs.PL](#cs.PL) [Total: 2]
- [cs.AR](#cs.AR) [Total: 1]
- [cs.CV](#cs.CV) [Total: 23]
- [eess.IV](#eess.IV) [Total: 5]
- [astro-ph.EP](#astro-ph.EP) [Total: 1]
- [cs.GT](#cs.GT) [Total: 1]
- [cs.DS](#cs.DS) [Total: 1]
- [cs.DB](#cs.DB) [Total: 2]
- [cs.DC](#cs.DC) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Evaluating Reasoning LLMs for Suicide Screening with the Columbia-Suicide Severity Rating Scale](https://arxiv.org/abs/2505.13480)
*Avinash Patil,Siru Tao,Amardeep Gedhu*

Main category: cs.CL

TL;DR: 研究评估了大型语言模型（LLMs）在自杀风险评估中的表现，发现Claude和GPT与人类标注最接近，Mistral在顺序预测错误上最低。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型的兴起，人们可能向AI而非人类透露自杀倾向，研究旨在评估LLMs在自杀风险评估中的能力。

Method: 使用哥伦比亚自杀严重程度评定量表（C-SSRS），评估六种模型在零样本设置下对帖子进行7级严重程度分类的表现。

Result: Claude和GPT与人类标注高度一致，Mistral的顺序预测错误最低，模型通常在相邻严重级别间出现误分类。

Conclusion: 研究强调了人类监督、透明度和谨慎部署的重要性，指出LLMs在自杀风险评估中的潜力与局限。

Abstract: Suicide prevention remains a critical public health challenge. While online
platforms such as Reddit's r/SuicideWatch have historically provided spaces for
individuals to express suicidal thoughts and seek community support, the advent
of large language models (LLMs) introduces a new paradigm-where individuals may
begin disclosing ideation to AI systems instead of humans. This study evaluates
the capability of LLMs to perform automated suicide risk assessment using the
Columbia-Suicide Severity Rating Scale (C-SSRS). We assess the zero-shot
performance of six models-including Claude, GPT, Mistral, and LLaMA-in
classifying posts across a 7-point severity scale (Levels 0-6). Results
indicate that Claude and GPT closely align with human annotations, while
Mistral achieves the lowest ordinal prediction error. Most models exhibit
ordinal sensitivity, with misclassifications typically occurring between
adjacent severity levels. We further analyze confusion patterns,
misclassification sources, and ethical considerations, underscoring the
importance of human oversight, transparency, and cautious deployment. Full code
and supplementary materials are available at
https://github.com/av9ash/llm_cssrs_code.

</details>


### [2] [EmoMeta: A Multimodal Dataset for Fine-grained Emotion Classification in Chinese Metaphors](https://arxiv.org/abs/2505.13483)
*Xingyuan Lu,Yuxi Liu,Dongyu Zhang,Zhiyao Wu,Jing Ren,Feng Xia*

Main category: cs.CL

TL;DR: 该论文构建了一个包含5000个中文图文广告对的多模态隐喻情感数据集，标注了隐喻、领域关系和细粒度情感分类，填补了该领域的研究空白。


<details>
  <summary>Details</summary>
Motivation: 当前多模态隐喻情感分类研究稀缺，且主要集中在英语领域，忽略了不同语言间情感表达的差异。本文旨在填补这一空白，推动多模态隐喻情感分析的发展。

Method: 构建了一个中文多模态数据集，包含5000个图文广告对，并对其进行了隐喻标注、领域关系分析和细粒度情感分类（包括10种情感类别）。

Result: 数据集已公开（https://github.com/DUTIR-YSQ/EmoMeta），为多模态隐喻情感分析领域的研究提供了重要资源。

Conclusion: 该数据集的发布将促进多模态隐喻情感分析的研究，尤其是在中文语境下的应用。

Abstract: Metaphors play a pivotal role in expressing emotions, making them crucial for
emotional intelligence. The advent of multimodal data and widespread
communication has led to a proliferation of multimodal metaphors, amplifying
the complexity of emotion classification compared to single-mode scenarios.
However, the scarcity of research on constructing multimodal metaphorical
fine-grained emotion datasets hampers progress in this domain. Moreover,
existing studies predominantly focus on English, overlooking potential
variations in emotional nuances across languages. To address these gaps, we
introduce a multimodal dataset in Chinese comprising 5,000 text-image pairs of
metaphorical advertisements. Each entry is meticulously annotated for metaphor
occurrence, domain relations and fine-grained emotion classification
encompassing joy, love, trust, fear, sadness, disgust, anger, surprise,
anticipation, and neutral. Our dataset is publicly accessible
(https://github.com/DUTIR-YSQ/EmoMeta), facilitating further advancements in
this burgeoning field.

</details>


### [3] [Detecting Prefix Bias in LLM-based Reward Models](https://arxiv.org/abs/2505.13487)
*Ashwin Kumar,Yuzi He,Aram H. Markosyan,Bobbie Chern,Imanol Arrieta-Ibarra*

Main category: cs.CL

TL;DR: 该论文研究了基于人类反馈的强化学习（RLHF）中奖励模型的前缀偏见问题，提出了检测和评估方法，并通过数据增强策略减少偏见。


<details>
  <summary>Details</summary>
Motivation: 尽管已有许多公开的偏好数据集用于语言模型的微调，但由此训练的奖励模型可能存在偏见，尤其是前缀偏见，即在查询前缀微小变化时模型偏好发生系统性偏移。

Method: 论文提出了检测和评估前缀偏见的新方法，并设计了一种数据增强策略来缓解这些偏见。

Result: 研究发现，不同开源偏好数据集和奖励模型架构均存在显著的种族和性别偏见，数据增强策略有效减少了偏见影响。

Conclusion: 研究强调了在开发公平可靠的奖励模型时，需要注重偏见感知的数据集设计和评估，以促进AI公平性。

Abstract: Reinforcement Learning with Human Feedback (RLHF) has emerged as a key
paradigm for task-specific fine-tuning of language models using human
preference data. While numerous publicly available preference datasets provide
pairwise comparisons of responses, the potential for biases in the resulting
reward models remains underexplored. In this work, we introduce novel methods
to detect and evaluate prefix bias -- a systematic shift in model preferences
triggered by minor variations in query prefixes -- in LLM-based reward models
trained on such datasets. We leverage these metrics to reveal significant
biases in preference models across racial and gender dimensions. Our
comprehensive evaluation spans diverse open-source preference datasets and
reward model architectures, demonstrating susceptibility to this kind of bias
regardless of the underlying model architecture. Furthermore, we propose a data
augmentation strategy to mitigate these biases, showing its effectiveness in
reducing the impact of prefix bias. Our findings highlight the critical need
for bias-aware dataset design and evaluation in developing fair and reliable
reward models, contributing to the broader discourse on fairness in AI.

</details>


### [4] [Source framing triggers systematic evaluation bias in Large Language Models](https://arxiv.org/abs/2505.13488)
*Federico Germani,Giovanni Spitale*

Main category: cs.CL

TL;DR: 研究发现，大型语言模型在评估文本时存在框架效应，特别是当文本来源被标注为中国作者时，模型间一致性显著降低。


<details>
  <summary>Details</summary>
Motivation: 探讨大型语言模型在文本评估中的一致性、无偏见性及对框架效应的鲁棒性，以确保其评估的公正性和中立性。

Method: 使用四种先进的大型语言模型评估4800条涉及社会、政治和公共卫生话题的叙述，并操纵文本来源（LLM或特定国籍的人类作者）以观察框架效应。

Result: 在盲测条件下，模型间和模型内一致性较高；但当引入来源框架后，特别是标注为中国作者时，一致性显著下降，尤其是Deepseek Reasoner模型。

Conclusion: 框架效应对文本评估有深远影响，这对大型语言模型信息系统的完整性、中立性和公平性具有重要启示。

Abstract: Large Language Models (LLMs) are increasingly used not only to generate text
but also to evaluate it, raising urgent questions about whether their judgments
are consistent, unbiased, and robust to framing effects. In this study, we
systematically examine inter- and intra-model agreement across four
state-of-the-art LLMs (OpenAI o3-mini, Deepseek Reasoner, xAI Grok 2, and
Mistral) tasked with evaluating 4,800 narrative statements on 24 different
topics of social, political, and public health relevance, for a total of
192,000 assessments. We manipulate the disclosed source of each statement to
assess how attribution to either another LLM or a human author of specified
nationality affects evaluation outcomes. We find that, in the blind condition,
different LLMs display a remarkably high degree of inter- and intra-model
agreement across topics. However, this alignment breaks down when source
framing is introduced. Here we show that attributing statements to Chinese
individuals systematically lowers agreement scores across all models, and in
particular for Deepseek Reasoner. Our findings reveal that framing effects can
deeply affect text evaluation, with significant implications for the integrity,
neutrality, and fairness of LLM-mediated information systems.

</details>


### [5] [ProdRev: A DNN framework for empowering customers using generative pre-trained transformers](https://arxiv.org/abs/2505.13491)
*Aakash Gupta,Nataraj Das*

Main category: cs.CL

TL;DR: 该论文提出了一种基于GPT-3的框架，用于抽象总结海量电商评论，帮助消费者快速做出决策。


<details>
  <summary>Details</summary>
Motivation: 疫情后电商评论激增导致消费者面临信息过载和决策困难，现有评分工具无法有效解决这一问题。

Method: 使用Curie引擎对130亿参数的GPT-3进行微调，采用抽象摘要方法（而非简单提取）分析评论。

Result: 系统能生成具有常识性的评论摘要，突出产品优缺点，揭示评论间的真实关联。

Conclusion: 该框架通过AI摘要有效减少消费者决策时间，同时保留评论核心信息，增强决策自主权。

Abstract: Following the pandemic, customers, preference for using e-commerce has
accelerated. Since much information is available in multiple reviews (sometimes
running in thousands) for a single product, it can create decision paralysis
for the buyer. This scenario disempowers the consumer, who cannot be expected
to go over so many reviews since its time consuming and can confuse them.
Various commercial tools are available, that use a scoring mechanism to arrive
at an adjusted score. It can alert the user to potential review manipulations.
This paper proposes a framework that fine-tunes a generative pre-trained
transformer to understand these reviews better. Furthermore, using
"common-sense" to make better decisions. These models have more than 13 billion
parameters. To fine-tune the model for our requirement, we use the curie engine
from generative pre-trained transformer (GPT3). By using generative models, we
are introducing abstractive summarization. Instead of using a simple extractive
method of summarizing the reviews. This brings out the true relationship
between the reviews and not simply copy-paste. This introduces an element of
"common sense" for the user and helps them to quickly make the right decisions.
The user is provided the pros and cons of the processed reviews. Thus the
user/customer can take their own decisions.

</details>


### [6] [LLM4CD: Leveraging Large Language Models for Open-World Knowledge Augmented Cognitive Diagnosis](https://arxiv.org/abs/2505.13492)
*Weiming Zhang,Lingyue Fu,Qingyao Li,Kounianhua Du,Jianghao Lin,Jingwei Yu,Wei Xia,Weinan Zhang,Ruiming Tang,Yong Yu*

Main category: cs.CL

TL;DR: LLM4CD利用大语言模型的开放世界知识增强认知诊断，通过语义表示解决冷启动问题，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前认知诊断方法主要依赖ID关系建模，忽视了教育数据中的丰富语义信息，且难以处理新增学生和习题的冷启动问题。

Method: 提出LLM4CD框架：利用LLM构建认知文本表示，设计双层编码器（宏观认知文本编码器+微观知识状态编码器）替代传统ID嵌入。

Result: 在多个真实数据集上超越现有认知诊断模型，验证了引入语义信息的有效性。

Conclusion: 大语言模型的开放世界知识能显著提升认知诊断性能，语义表示可有效解决冷启动问题。

Abstract: Cognitive diagnosis (CD) plays a crucial role in intelligent education,
evaluating students' comprehension of knowledge concepts based on their test
histories. However, current CD methods often model students, exercises, and
knowledge concepts solely on their ID relationships, neglecting the abundant
semantic relationships present within educational data space. Furthermore,
contemporary intelligent tutoring systems (ITS) frequently involve the addition
of new students and exercises, a situation that ID-based methods find
challenging to manage effectively. The advent of large language models (LLMs)
offers the potential for overcoming this challenge with open-world knowledge.
In this paper, we propose LLM4CD, which Leverages Large Language Models for
Open-World Knowledge Augmented Cognitive Diagnosis. Our method utilizes the
open-world knowledge of LLMs to construct cognitively expressive textual
representations, which are then encoded to introduce rich semantic information
into the CD task. Additionally, we propose an innovative bi-level encoder
framework that models students' test histories through two levels of encoders:
a macro-level cognitive text encoder and a micro-level knowledge state encoder.
This approach substitutes traditional ID embeddings with semantic
representations, enabling the model to accommodate new students and exercises
with open-world knowledge and address the cold-start problem. Extensive
experimental results demonstrate that our proposed method consistently
outperforms previous CD models on multiple real-world datasets, validating the
effectiveness of leveraging LLMs to introduce rich semantic information into
the CD task.

</details>


### [7] [IRLBench: A Multi-modal, Culturally Grounded, Parallel Irish-English Benchmark for Open-Ended LLM Reasoning Evaluation](https://arxiv.org/abs/2505.13498)
*Khanh-Tung Tran,Barry O'Sullivan,Hoang D. Nguyen*

Main category: cs.CL

TL;DR: 论文提出了IRLBench基准，用于评估大语言模型在低资源语言（爱尔兰语）上的表现，发现模型在爱尔兰语上的性能显著低于英语。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型评估基准存在文化偏见、仅限于文本或多选题形式，且对极低资源语言的覆盖不足。

Method: 作者构建了IRLBench基准，基于2024年爱尔兰毕业考试开发了12个科目，采用长文本生成任务和官方评分标准进行评估。

Result: 实验表明，模型在爱尔兰语上的表现显著低于英语，最佳模型在爱尔兰语上的正确率仅为55.8%，而英语为76.2%。

Conclusion: IRLBench揭示了当前大语言模型在低资源语言上的局限性，为未来开发更鲁棒、文化敏感的多语言AI提供了基准。

Abstract: Recent advances in Large Language Models (LLMs) have demonstrated promising
knowledge and reasoning abilities, yet their performance in multilingual and
low-resource settings remains underexplored. Existing benchmarks often exhibit
cultural bias, restrict evaluation to text-only, rely on multiple-choice
formats, and, more importantly, are limited for extremely low-resource
languages. To address these gaps, we introduce IRLBench, presented in parallel
English and Irish, which is considered definitely endangered by UNESCO. Our
benchmark consists of 12 representative subjects developed from the 2024 Irish
Leaving Certificate exams, enabling fine-grained analysis of model capabilities
across domains. By framing the task as long-form generation and leveraging the
official marking scheme, it does not only support a comprehensive evaluation of
correctness but also language fidelity. Our extensive experiments of leading
closed-source and open-source LLMs reveal a persistent performance gap between
English and Irish, in which models produce valid Irish responses less than 80\%
of the time, and answer correctly 55.8\% of the time compared to 76.2\% in
English for the best-performing model. We release IRLBench
(https://huggingface.co/datasets/ReliableAI/IRLBench) and an accompanying
evaluation codebase (https://github.com/ReML-AI/IRLBench) to enable future
research on robust, culturally aware multilingual AI development.

</details>


### [8] [Noise Injection Systemically Degrades Large Language Model Safety Guardrails](https://arxiv.org/abs/2505.13500)
*Prithviraj Singh Shahani,Matthias Scheutz*

Main category: cs.CL

TL;DR: 研究发现，当前大语言模型的安全防护措施在受到高斯噪声干扰时表现脆弱，即使深度安全微调也无法提供额外保护，而推理能力基本不受影响。


<details>
  <summary>Details</summary>
Motivation: 探讨大语言模型安全微调在扰动下的鲁棒性，揭示现有安全对齐技术的潜在漏洞。

Method: 通过系统性地向模型激活中注入高斯噪声，测试多个开源模型的安全性能。

Result: 高斯噪声使有害输出率显著上升（最高27%），深度安全微调无额外防护作用，但链式推理能力保持稳定。

Conclusion: 当前安全调优方法存在重大缺陷，基于推理和强化学习的方法可能是构建更鲁棒AI安全系统的方向。

Abstract: Safety guardrails in large language models (LLMs) are a critical component in
preventing harmful outputs. Yet, their resilience under perturbation remains
poorly understood. In this paper, we investigate the robustness of safety
fine-tuning in LLMs by systematically injecting Gaussian noise into model
activations. We show across multiple open-weight models that (1) Gaussian noise
raises harmful-output rates (p < 0.001) by up to 27%, (2) that deeper safety
fine-tuning affords no extra protection, and (3) that chain-of-thought
reasoning remains largely intact. The findings reveal critical vulnerabilities
in current safety alignment techniques and highlight the potential of
reasoning-based and reinforcement learning approaches as promising direction
for developing more robust AI safety systems. These results have important
implications for real-world deployment of LLMs in safety-critical applications
as these results imply that widely-deployed safety tuning methods can fail even
without adversarial prompts.

</details>


### [9] [EcoSafeRAG: Efficient Security through Context Analysis in Retrieval-Augmented Generation](https://arxiv.org/abs/2505.13506)
*Ruobing Yao,Yifei Zhang,Shuang Song,Neng Gao,Chenyang Tu*

Main category: cs.CL

TL;DR: EcoSafeRAG提出了一种不依赖LLM内部知识的新型防御方法，通过句子级处理和诱饵引导的上下文多样性检测来识别恶意内容，在提升安全性的同时保持较低运行成本。


<details>
  <summary>Details</summary>
Motivation: 现有RAG防御方法依赖模型内部知识，与RAG设计理念冲突。EcoSafeRAG旨在填补这一空白，实现不依赖LLM知识的恶意内容检测。

Method: 采用句子级处理和诱饵引导的上下文多样性检测技术，通过分析候选文档的上下文多样性来识别恶意内容。

Result: 实验表明EcoSafeRAG在保持即插即用部署的同时提供最先进的安全性，在干净场景下提升RAG性能，运行成本相对较低（延迟1.2倍，token减少48%-80%）。

Conclusion: EcoSafeRAG有效解决了RAG系统面临的安全威胁，实现了安全性与运行效率的平衡，为RAG防御提供了新思路。

Abstract: Retrieval-Augmented Generation (RAG) compensates for the static knowledge
limitations of Large Language Models (LLMs) by integrating external knowledge,
producing responses with enhanced factual correctness and query-specific
contextualization. However, it also introduces new attack surfaces such as
corpus poisoning at the same time. Most of the existing defense methods rely on
the internal knowledge of the model, which conflicts with the design concept of
RAG. To bridge the gap, EcoSafeRAG uses sentence-level processing and
bait-guided context diversity detection to identify malicious content by
analyzing the context diversity of candidate documents without relying on LLM
internal knowledge. Experiments show EcoSafeRAG delivers state-of-the-art
security with plug-and-play deployment, simultaneously improving clean-scenario
RAG performance while maintaining practical operational costs (relatively
1.2$\times$ latency, 48\%-80\% token reduction versus Vanilla RAG).

</details>


### [10] [Time-R1: Towards Comprehensive Temporal Reasoning in LLMs](https://arxiv.org/abs/2505.13508)
*Zijia Liu,Peixuan Han,Haofei Yu,Haoru Li,Jiaxuan You*

Main category: cs.CL

TL;DR: 论文提出了Time-R1框架，通过强化学习课程赋予中型LLM全面的时间能力，包括理解、预测和创造性生成，性能超越更大模型。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型缺乏稳健的时间智能，难以整合过去推理与未来预测/生成，且现有方法泛化能力差。

Method: 采用三阶段强化学习课程：基础时间理解、未来事件预测、创造性场景生成，配合动态规则奖励系统。

Result: Time-R1在预测和生成任务上超越参数量200倍以上的模型（如671B的DeepSeek-R1）。

Conclusion: 证明经过精心设计的渐进式强化学习调优可使小型模型获得卓越时间性能，并发布了Time-Bench数据集促进研究。

Abstract: Large Language Models (LLMs) demonstrate impressive capabilities but lack
robust temporal intelligence, struggling to integrate reasoning about the past
with predictions and plausible generations of the future. Meanwhile, existing
methods typically target isolated temporal skills, such as question answering
about past events or basic forecasting, and exhibit poor generalization,
particularly when dealing with events beyond their knowledge cutoff or
requiring creative foresight. To address these limitations, we introduce
\textit{Time-R1}, the first framework to endow a moderate-sized (3B-parameter)
LLM with comprehensive temporal abilities: understanding, prediction, and
creative generation. Our approach features a novel three-stage development
path; the first two constitute a \textit{reinforcement learning (RL)
curriculum} driven by a meticulously designed dynamic rule-based reward system.
This framework progressively builds (1) foundational temporal understanding and
logical event-time mappings from historical data, (2) future event prediction
skills for events beyond its knowledge cutoff, and finally (3) enables
remarkable generalization to creative future scenario generation without any
fine-tuning. Strikingly, experiments demonstrate that Time-R1 outperforms
models over 200 times larger, including the state-of-the-art 671B DeepSeek-R1,
on highly challenging future event prediction and creative scenario generation
benchmarks. This work provides strong evidence that thoughtfully engineered,
progressive RL fine-tuning allows smaller, efficient models to achieve superior
temporal performance, offering a practical and scalable path towards truly
time-aware AI. To foster further research, we also release \textit{Time-Bench},
a large-scale multi-task temporal reasoning dataset derived from 10 years of
news data, and our series of \textit{Time-R1} checkpoints.

</details>


### [11] [Induction Head Toxicity Mechanistically Explains Repetition Curse in Large Language Models](https://arxiv.org/abs/2505.13514)
*Shuxun Wang,Qingyu Yin,Chak Tou Leong,Qiang Zhang,Linyi Yang*

Main category: cs.CL

TL;DR: 研究发现大语言模型中的归纳头是导致重复生成的关键因素，并提出通过注意力头正则化技术来缓解这一问题。


<details>
  <summary>Details</summary>
Motivation: 尽管重复诅咒现象在大语言模型中广泛存在，但其内在机制尚不明确。本文旨在探究归纳头在驱动重复行为中的作用。

Method: 通过分析归纳头的毒性（即其在重复生成过程中主导输出logits的倾向），并提出注意力头正则化技术来减少其主导性。

Result: 归纳头被确认为重复诅咒的主要驱动因素，研究提供了机制性解释并提出了缓解方法。

Conclusion: 通过识别和调节归纳头，可以促进大语言模型生成更多样化和连贯的输出，对模型设计和训练具有重要启示。

Abstract: Repetition curse is a phenomenon where Large Language Models (LLMs) generate
repetitive sequences of tokens or cyclic sequences. While the repetition curse
has been widely observed, its underlying mechanisms remain poorly understood.
In this work, we investigate the role of induction heads--a specific type of
attention head known for their ability to perform in-context learning--in
driving this repetitive behavior. Specifically, we focus on the "toxicity" of
induction heads, which we define as their tendency to dominate the model's
output logits during repetition, effectively excluding other attention heads
from contributing to the generation process. Our findings have important
implications for the design and training of LLMs. By identifying induction
heads as a key driver of the repetition curse, we provide a mechanistic
explanation for this phenomenon and suggest potential avenues for mitigation.
We also propose a technique with attention head regularization that could be
employed to reduce the dominance of induction heads during generation, thereby
promoting more diverse and coherent outputs.

</details>


### [12] [Logic Jailbreak: Efficiently Unlocking LLM Safety Restrictions Through Formal Logical Expression](https://arxiv.org/abs/2505.13527)
*Jingyu Peng,Maolin Wang,Nan Wang,Xiangyu Zhao,Jiatong Li,Kai Zhang,Qi Liu*

Main category: cs.CL

TL;DR: 论文提出LogiBreak方法，通过逻辑表达式转换绕过LLM安全机制，利用对齐数据与逻辑输入的分布差异实现多语言越狱攻击。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型的安全机制仍易受越狱攻击，作者认为这是由于对齐提示与恶意提示间的分布差异所致。

Method: 提出LogiBreak黑盒越狱方法，将有害自然语言提示转换为形式逻辑表达式，利用逻辑输入与对齐数据的分布差规避安全约束。

Result: 在跨三种语言的越狱数据集测试中，该方法在不同评估设置和语言环境下均表现有效。

Conclusion: 通过逻辑表达式转换可有效利用分布差异突破LLM安全防护，揭示了现有对齐方法的潜在脆弱性。

Abstract: Despite substantial advancements in aligning large language models (LLMs)
with human values, current safety mechanisms remain susceptible to jailbreak
attacks. We hypothesize that this vulnerability stems from distributional
discrepancies between alignment-oriented prompts and malicious prompts. To
investigate this, we introduce LogiBreak, a novel and universal black-box
jailbreak method that leverages logical expression translation to circumvent
LLM safety systems. By converting harmful natural language prompts into formal
logical expressions, LogiBreak exploits the distributional gap between
alignment data and logic-based inputs, preserving the underlying semantic
intent and readability while evading safety constraints. We evaluate LogiBreak
on a multilingual jailbreak dataset spanning three languages, demonstrating its
effectiveness across various evaluation settings and linguistic contexts.

</details>


### [13] [Combining the Best of Both Worlds: A Method for Hybrid NMT and LLM Translation](https://arxiv.org/abs/2505.13554)
*Zhanglin Wu,Daimeng Wei,Xiaoyu Chen,Hengchao Shang,Jiaxin Guo,Zongyao Li,Yuanchang Luo,Jinlong Yang,Zhiqiang Rao,Hao Yang*

Main category: cs.CL

TL;DR: 该论文探讨了如何结合大型语言模型（LLM）和神经机器翻译（NMT）系统进行高效翻译，提出了一种基于源语句特征的调度策略，以减少LLM的使用并保持翻译质量。


<details>
  <summary>Details</summary>
Motivation: 虽然大型语言模型（LLM）在机器翻译等任务中表现优异，但其高计算成本和延迟问题限制了实际应用。研究发现，LLM和NMT在不同场景下各有优势，因此需要一种策略来优化两者的结合使用。

Method: 论文提出了一种新颖且简单的调度策略，利用源语句特征来决定何时使用LLM进行翻译，其余情况下则使用NMT系统。

Result: 在多语言测试集上的实验表明，该方法能以最少的LLM使用量实现最优的翻译性能，验证了调度策略的有效性。

Conclusion: 结合NMT和LLM的翻译方法，并通过智能调度策略减少LLM的使用，是一种高效且实用的解决方案，能够在保证翻译质量的同时降低计算成本。

Abstract: Large language model (LLM) shows promising performances in a variety of
downstream tasks, such as machine translation (MT). However, using LLMs for
translation suffers from high computational costs and significant latency.
Based on our evaluation, in most cases, translations using LLMs are comparable
to that generated by neural machine translation (NMT) systems. Only in
particular scenarios, LLM and NMT models show respective advantages. As a
result, integrating NMT and LLM for translation and using LLM only when
necessary seems to be a sound solution. A scheduling policy that optimizes
translation result while ensuring fast speed and as little LLM usage as
possible is thereby required. We compare several scheduling policies and
propose a novel and straightforward decider that leverages source sentence
features. We conduct extensive experiments on multilingual test sets and the
result shows that we can achieve optimal translation performance with minimal
LLM usage, demonstrating effectiveness of our decider.

</details>


### [14] [CS-Sum: A Benchmark for Code-Switching Dialogue Summarization and the Limits of Large Language Models](https://arxiv.org/abs/2505.13559)
*Sathya Krishnan Suresh,Tanmay Surana,Lim Zhi Hao,Eng Siong Chng*

Main category: cs.CL

TL;DR: 论文提出CS-Sum基准测试，评估大语言模型对混合语言对话的理解能力，发现模型在自动指标得分高但仍存在语义错误。


<details>
  <summary>Details</summary>
Motivation: 研究大语言模型对混合语言（code-switching）的理解能力，目前这一领域尚未充分探索。

Method: 构建CS-Sum基准测试，涵盖三种混合语言对，评估十种大语言模型在少量样本、翻译-总结和微调等方法下的表现。

Result: 尽管自动指标得分高，模型在处理混合语言时仍会犯语义错误，且错误类型和频率因语言对和模型而异。

Conclusion: 大语言模型需要针对混合语言数据进行专门训练以提高理解能力。

Abstract: Code-switching (CS) poses a significant challenge for Large Language Models
(LLMs), yet its comprehensibility remains underexplored in LLMs. We introduce
CS-Sum, to evaluate the comprehensibility of CS by the LLMs through CS dialogue
to English summarization. CS-Sum is the first benchmark for CS dialogue
summarization across Mandarin-English (EN-ZH), Tamil-English (EN-TA), and
Malay-English (EN-MS), with 900-1300 human-annotated dialogues per language
pair. Evaluating ten LLMs, including open and closed-source models, we analyze
performance across few-shot, translate-summarize, and fine-tuning (LoRA, QLoRA
on synthetic data) approaches. Our findings show that though the scores on
automated metrics are high, LLMs make subtle mistakes that alter the complete
meaning of the dialogue. To this end, we introduce 3 most common type of errors
that LLMs make when handling CS input. Error rates vary across CS pairs and
LLMs, with some LLMs showing more frequent errors on certain language pairs,
underscoring the need for specialized training on code-switched data.

</details>


### [15] [Cross-Lingual Representation Alignment Through Contrastive Image-Caption Tuning](https://arxiv.org/abs/2505.13628)
*Nathaniel Krasner,Nicholas Lanuzo,Antonios Anastasopoulos*

Main category: cs.CL

TL;DR: 该论文探讨了视觉信息是否能替代双语文本，用于多语言句子表征对齐，并验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 传统多语言句子表征对齐依赖双语文本，但获取双语数据成本高。论文探索视觉信息是否能更高效地填补语言间的鸿沟，尤其适用于低资源语言。

Method: 利用多语言图像-标题数据集，通过视觉信息隐式对齐不同语言的文本表征，包括预训练阶段未见的语言。

Result: 实验表明，该方法能有效实现跨语言表征对齐，并支持跨语言自然语言理解和双语检索任务。

Conclusion: 视觉信息可作为双语文本的替代方案，实现多语言表征对齐，为低资源语言提供更高效的解决方案。

Abstract: Multilingual alignment of sentence representations has mostly required
bitexts to bridge the gap between languages. We investigate whether visual
information can bridge this gap instead. Image caption datasets are very easy
to create without requiring multilingual expertise, so this offers a more
efficient alternative for low-resource languages. We find that multilingual
image-caption alignment can implicitly align the text representations between
languages, languages unseen by the encoder in pretraining can be incorporated
into this alignment post-hoc, and these aligned representations are usable for
cross-lingual Natural Language Understanding (NLU) and bitext retrieval.

</details>


### [16] [Clarifying orthography: Orthographic transparency as compressibility](https://arxiv.org/abs/2505.13657)
*Charles J. Torres,Richard Futrell*

Main category: cs.CL

TL;DR: 该论文提出了一种基于算法信息理论的通用指标，用于量化不同文字系统的正字法透明度，并通过神经序列模型验证了22种语言的相对透明度。


<details>
  <summary>Details</summary>
Motivation: 正字法透明度（拼写与发音的直接关联程度）缺乏统一的、与文字系统无关的度量标准。本文旨在填补这一空白。

Method: 利用算法信息理论中的互压缩性概念，结合神经序列模型的前序编码长度，量化正字法与音系字符串之间的关系。

Result: 在22种不同文字系统（如字母文字、辅音音素文字、元音附标文字等）上的评估结果验证了关于文字透明度的常见直觉。

Conclusion: 互压缩性为衡量正字法透明度提供了一个简单、有理论基础且通用的标准。

Abstract: Orthographic transparency -- how directly spelling is related to sound --
lacks a unified, script-agnostic metric. Using ideas from algorithmic
information theory, we quantify orthographic transparency in terms of the
mutual compressibility between orthographic and phonological strings. Our
measure provides a principled way to combine two factors that decrease
orthographic transparency, capturing both irregular spellings and rule
complexity in one quantity. We estimate our transparency measure using
prequential code-lengths derived from neural sequence models. Evaluating 22
languages across a broad range of script types (alphabetic, abjad, abugida,
syllabic, logographic) confirms common intuitions about relative transparency
of scripts. Mutual compressibility offers a simple, principled, and general
yardstick for orthographic transparency.

</details>


### [17] [Are Large Language Models Good at Detecting Propaganda?](https://arxiv.org/abs/2505.13706)
*Julia Jose,Rachel Greenstadt*

Main category: cs.CL

TL;DR: 该研究比较了多种大语言模型（LLMs）和基于Transformer的模型在检测新闻文章中的宣传技巧方面的性能，发现GPT-4表现优于GPT-3.5和Claude 3 Opus，但不及RoBERTa-CRF基线。


<details>
  <summary>Details</summary>
Motivation: 宣传者常使用修辞手法和情感诉求来推进其议程，识别这些技巧对做出明智决策至关重要。近年来自然语言处理（NLP）的进步使得开发能够检测操纵性内容的系统成为可能。

Method: 研究比较了几种大语言模型（LLMs）和基于Transformer的模型在检测新闻文章中宣传技巧的性能，包括GPT-4、GPT-3.5、Claude 3 Opus、RoBERTa-CRF基线以及多粒度网络（MGN）基线。

Result: GPT-4在F1分数上优于GPT-3.5和Claude 3 Opus（F1=0.16），但不及RoBERTa-CRF基线（F1=0.67）。所有三种LLMs在检测六种宣传技巧中的一种（name-calling）上优于MGN基线，GPT-3.5和GPT-4在检测appeal to fear和flag-waving上也优于MGN基线。

Conclusion: 尽管GPT-4在检测宣传技巧上表现优于其他LLMs，但仍未超越RoBERTa-CRF基线。LLMs在特定宣传技巧检测上表现优于MGN基线，显示了其在 propaganda detection 任务中的潜力。

Abstract: Propagandists use rhetorical devices that rely on logical fallacies and
emotional appeals to advance their agendas. Recognizing these techniques is key
to making informed decisions. Recent advances in Natural Language Processing
(NLP) have enabled the development of systems capable of detecting manipulative
content. In this study, we look at several Large Language Models and their
performance in detecting propaganda techniques in news articles. We compare the
performance of these LLMs with transformer-based models. We find that, while
GPT-4 demonstrates superior F1 scores (F1=0.16) compared to GPT-3.5 and Claude
3 Opus, it does not outperform a RoBERTa-CRF baseline (F1=0.67). Additionally,
we find that all three LLMs outperform a MultiGranularity Network (MGN)
baseline in detecting instances of one out of six propaganda techniques
(name-calling), with GPT-3.5 and GPT-4 also outperforming the MGN baseline in
detecting instances of appeal to fear and flag-waving.

</details>


### [18] [SQLForge: Synthesizing Reliable and Diverse Data to Enhance Text-to-SQL Reasoning in LLMs](https://arxiv.org/abs/2505.13725)
*Yu Guo,Dong Jin,Shenghao Ye,Shuangwu Chen,Jian Yang,Xiaobin Tan*

Main category: cs.CL

TL;DR: SQLForge通过合成高质量数据提升开源LLMs在text-to-SQL任务中的性能，在Spider和BIRD基准上达到SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有开源模型与闭源模型在text-to-SQL任务上存在显著性能差距，需通过改进数据质量来缩小差距。

Method: 提出SQL语法约束和反向翻译保证数据可靠性，通过模板增强和迭代探索提升数据多样性，并基于增强数据微调不同架构的模型。

Result: SQLForge-LM在Spider Dev和BIRD Dev上分别达到85.7%和59.8%的EX准确率，显著缩小与闭源模型的差距。

Conclusion: SQLForge通过数据合成和模型微调有效提升了开源LLMs的text-to-SQL性能，为开源社区提供了强竞争力方案。

Abstract: Large Language models (LLMs) have demonstrated significant potential in
text-to-SQL reasoning tasks, yet a substantial performance gap persists between
existing open-source models and their closed-source counterparts. In this
paper, we introduce SQLForge, a novel approach for synthesizing reliable and
diverse data to enhance text-to-SQL reasoning in LLMs. We improve data
reliability through SQL syntax constraints and SQL-to-question reverse
translation, ensuring data logic at both structural and semantic levels. We
also propose an SQL template enrichment and iterative data domain exploration
mechanism to boost data diversity. Building on the augmented data, we fine-tune
a variety of open-source models with different architectures and parameter
sizes, resulting in a family of models termed SQLForge-LM. SQLForge-LM achieves
the state-of-the-art performance on the widely recognized Spider and BIRD
benchmarks among the open-source models. Specifically, SQLForge-LM achieves EX
accuracy of 85.7% on Spider Dev and 59.8% on BIRD Dev, significantly narrowing
the performance gap with closed-source methods.

</details>


### [19] [Simulation Agent: A Framework for Integrating Simulation and Large Language Models for Enhanced Decision-Making](https://arxiv.org/abs/2505.13761)
*Jacob Kleiman,Kevin Frank,Sindy Campagna*

Main category: cs.CL

TL;DR: 论文提出了一种结合仿真模型和大型语言模型（LLM）优势的仿真代理框架，使非技术用户能够通过自然语言交互访问复杂仿真系统。


<details>
  <summary>Details</summary>
Motivation: 仿真模型虽能精确模拟现实系统，但对非技术用户过于复杂；而LLM虽提供直观的语言交互，但缺乏对复杂现实动态的结构化因果理解。因此，需要一种结合两者优势的方法。

Method: 提出仿真代理框架，利用LLM的对话能力与仿真系统无缝交互，同时用仿真系统为LLM提供准确、结构化的现实世界现象表征。

Result: 该框架为用户提供了强大的交互能力，同时确保LLM基于准确的仿真结果生成响应，适用于多领域。

Conclusion: 结合仿真模型与LLM的框架为实证验证提供了稳健且可推广的基础，具有广泛的适用性。

Abstract: Simulations, although powerful in accurately replicating real-world systems,
often remain inaccessible to non-technical users due to their complexity.
Conversely, large language models (LLMs) provide intuitive, language-based
interactions but can lack the structured, causal understanding required to
reliably model complex real-world dynamics. We introduce our simulation agent
framework, a novel approach that integrates the strengths of both simulation
models and LLMs. This framework helps empower users by leveraging the
conversational capabilities of LLMs to interact seamlessly with sophisticated
simulation systems, while simultaneously utilizing the simulations to ground
the LLMs in accurate and structured representations of real-world phenomena.
This integrated approach helps provide a robust and generalizable foundation
for empirical validation and offers broad applicability across diverse domains.

</details>


### [20] [Krikri: Advancing Open Large Language Models for Greek](https://arxiv.org/abs/2505.13772)
*Dimitris Roussis,Leon Voukoutis,Georgios Paraskevopoulos,Sokratis Sofianopoulos,Prokopis Prokopidis,Vassilis Papavasileiou,Athanasios Katsamanis,Stelios Piperidis,Vassilis Katsouros*

Main category: cs.CL

TL;DR: Llama-Krikri-8B是基于Meta Llama 3.1-8B优化的希腊语大语言模型，支持现代希腊语、英语及古希腊语，性能优于同类模型。


<details>
  <summary>Details</summary>
Motivation: 开发专为希腊语优化的高性能大语言模型，填补现有模型在希腊语处理上的不足，尤其是对古希腊语和多音调文本的支持。

Method: 基于Llama 3.1-8B架构，使用高质量希腊语数据训练，结合多阶段后训练流程（如MAGPIE技术），并引入人工和合成指令数据优化。

Result: 在自然语言理解、生成及代码生成任务上，Llama-Krikri-8B表现显著优于其他希腊语及多语言模型，同时提出了三个新的希腊语评测基准。

Conclusion: Llama-Krikri-8B成功实现了对希腊语的高效适配，为希腊语NLP任务提供了更先进的解决方案。

Abstract: We introduce Llama-Krikri-8B, a cutting-edge Large Language Model tailored
for the Greek language, built on Meta's Llama 3.1-8B. Llama-Krikri-8B has been
extensively trained on high-quality Greek data to ensure superior adaptation to
linguistic nuances. With 8 billion parameters, it offers advanced capabilities
while maintaining efficient computational performance. Llama-Krikri-8B supports
both Modern Greek and English, and is also equipped to handle polytonic text
and Ancient Greek. The chat version of Llama-Krikri-8B features a multi-stage
post-training pipeline, utilizing both human and synthetic instruction and
preference data, by applying techniques such as MAGPIE. In addition, for
evaluation, we propose three novel public benchmarks for Greek. Our evaluation
on existing as well as the proposed benchmarks shows notable improvements over
comparable Greek and multilingual LLMs in both natural language understanding
and generation as well as code generation.

</details>


### [21] [Interpretable Traces, Unexpected Outcomes: Investigating the Disconnect in Trace-Based Knowledge Distillation](https://arxiv.org/abs/2505.13792)
*Siddhant Bhambri,Upasana Biswas,Subbarao Kambhampati*

Main category: cs.CL

TL;DR: 该论文探讨了在知识蒸馏过程中，如何评估推理轨迹的忠实性及其与最终性能的关系，并发现正确推理轨迹不一定保证最终答案正确。


<details>
  <summary>Details</summary>
Motivation: 当前小型语言模型（SLMs）在问答任务中性能不足，知识蒸馏（KD）方法虽能提升性能，但推理轨迹难以评估。论文旨在解决推理轨迹评估的挑战。

Method: 采用基于规则的问题分解方法，将复杂查询拆解为结构化子问题，生成可解释的推理轨迹，并在多个QA数据集上进行实验。

Result: 实验发现，正确的推理轨迹并不一定导致最终答案正确，且中间轨迹正确性与最终答案正确性相关性较低。

Conclusion: 研究结果挑战了利用推理轨迹提升SLMs性能的隐含假设，为未来研究提供了新的方向。

Abstract: Question Answering (QA) poses a challenging and critical problem,
particularly in today's age of interactive dialogue systems such as ChatGPT,
Perplexity, Microsoft Copilot, etc. where users demand both accuracy and
transparency in the model's outputs. Since smaller language models (SLMs) are
computationally more efficient but often under-perform compared to larger
models, Knowledge Distillation (KD) methods allow for finetuning these smaller
models to improve their final performance. Lately, the intermediate tokens or
the so called `reasoning' traces produced by Chain-of-Thought (CoT) or by
reasoning models such as DeepSeek R1 are used as a training signal for KD.
However, these reasoning traces are often verbose and difficult to interpret or
evaluate. In this work, we aim to address the challenge of evaluating the
faithfulness of these reasoning traces and their correlation with the final
performance. To this end, we employ a KD method leveraging rule-based problem
decomposition. This approach allows us to break down complex queries into
structured sub-problems, generating interpretable traces whose correctness can
be readily evaluated, even at inference time. Specifically, we demonstrate this
approach on Open Book QA, decomposing the problem into a Classification step
and an Information Retrieval step, thereby simplifying trace evaluation. Our
SFT experiments with correct and incorrect traces on the CoTemp QA, Microsoft
Machine Reading Comprehension QA, and Facebook bAbI QA datasets reveal the
striking finding that correct traces do not necessarily imply that the model
outputs the correct final solution. Similarly, we find a low correlation
between correct final solutions and intermediate trace correctness. These
results challenge the implicit assumption behind utilizing reasoning traces for
improving SLMs' final performance via KD.

</details>


### [22] [EfficientLLM: Efficiency in Large Language Models](https://arxiv.org/abs/2505.13840)
*Zhengqing Yuan,Weixiang Sun,Yixin Liu,Huichi Zhou,Rong Zhou,Yiyang Li,Zheyuan Zhang,Wei Song,Yue Huang,Haolong Jia,Keerthiram Murugesan,Yu Wang,Lifang He,Jianfeng Gao,Lichao Sun,Yanfang Ye*

Main category: cs.CL

TL;DR: EfficientLLM是一个评估大语言模型效率技术的基准研究，通过系统测试架构预训练、微调和推理方法，揭示了效率与性能之间的权衡关系。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型参数和上下文窗口的增加，计算、能源和成本急剧上升，亟需研究如何提升模型效率。

Method: 在48xGH200和8xH200 GPU集群上，系统评估了高效注意力变体、稀疏专家混合、参数高效微调和量化方法，定义了6个细粒度指标。

Result: 研究发现：(1)效率存在量化权衡，如MoE降低计算量但增加显存；(2)最优方法取决于任务和规模；(3)技术可跨模态迁移。

Conclusion: EfficientLLM为下一代基础模型的效率-性能权衡提供了重要指导，开源了数据集和评估工具链。

Abstract: Large Language Models (LLMs) have driven significant progress, yet their
growing parameter counts and context windows incur prohibitive compute, energy,
and monetary costs. We introduce EfficientLLM, a novel benchmark and the first
comprehensive empirical study evaluating efficiency techniques for LLMs at
scale. Conducted on a production-class cluster (48xGH200, 8xH200 GPUs), our
study systematically explores three key axes: (1) architecture pretraining
(efficient attention variants: MQA, GQA, MLA, NSA; sparse Mixture-of-Experts
(MoE)), (2) fine-tuning (parameter-efficient methods: LoRA, RSLoRA, DoRA), and
(3) inference (quantization methods: int4, float16). We define six fine-grained
metrics (Memory Utilization, Compute Utilization, Latency, Throughput, Energy
Consumption, Compression Rate) to capture hardware saturation,
latency-throughput balance, and carbon cost. Evaluating over 100
model-technique pairs (0.5B-72B parameters), we derive three core insights: (i)
Efficiency involves quantifiable trade-offs: no single method is universally
optimal; e.g., MoE reduces FLOPs and improves accuracy but increases VRAM by
40%, while int4 quantization cuts memory/energy by up to 3.9x at a 3-5%
accuracy drop. (ii) Optima are task- and scale-dependent: MQA offers optimal
memory-latency trade-offs for constrained devices, MLA achieves lowest
perplexity for quality-critical tasks, and RSLoRA surpasses LoRA efficiency
only beyond 14B parameters. (iii) Techniques generalize across modalities: we
extend evaluations to Large Vision Models (Stable Diffusion 3.5, Wan 2.1) and
Vision-Language Models (Qwen2.5-VL), confirming effective transferability. By
open-sourcing datasets, evaluation pipelines, and leaderboards, EfficientLLM
provides essential guidance for researchers and engineers navigating the
efficiency-performance landscape of next-generation foundation models.

</details>


### [23] [Improve Language Model and Brain Alignment via Associative Memory](https://arxiv.org/abs/2505.13844)
*Congchi Yin,Yongpeng Zhang,Xuyun Wen,Piji Li*

Main category: cs.CL

TL;DR: 通过整合联想记忆改进语言模型与大脑在处理语音信息时的对齐性，并构建特定数据集进行监督微调。


<details>
  <summary>Details</summary>
Motivation: 研究旨在通过整合联想记忆，提升语言模型与人类大脑在处理语音信息时的对齐性，以更好地模拟人类认知系统。

Method: 验证语言模型与大脑的对齐性后，将模拟联想记忆扩展的文本作为输入，构建包含1000个故事样本的《Association》数据集进行监督微调。

Result: 发现语言模型与大脑在联想记忆处理相关脑区的对齐性得到提升，特定监督微调后的大型语言模型与大脑反应更一致。

Conclusion: 整合联想记忆能有效改进语言模型与大脑的对齐性，特定监督微调进一步提升了模型性能。

Abstract: Associative memory engages in the integration of relevant information for
comprehension in the human cognition system. In this work, we seek to improve
alignment between language models and human brain while processing speech
information by integrating associative memory. After verifying the alignment
between language model and brain by mapping language model activations to brain
activity, the original text stimuli expanded with simulated associative memory
are regarded as input to computational language models. We find the alignment
between language model and brain is improved in brain regions closely related
to associative memory processing. We also demonstrate large language models
after specific supervised fine-tuning better align with brain response, by
building the \textit{Association} dataset containing 1000 samples of stories,
with instructions encouraging associative memory as input and associated
content as output.

</details>


### [24] [Domain Gating Ensemble Networks for AI-Generated Text Detection](https://arxiv.org/abs/2505.13855)
*Arihant Tripathi,Liam Dugan,Charis Gao,Maggie Huan,Emma Jin,Peter Zhang,David Zhang,Julia Zhao,Chris Callison-Burch*

Main category: cs.CL

TL;DR: 本文提出了一种名为DoGEN的新技术，通过集成多个领域专家检测模型并结合领域分类器权重，显著提升了机器生成文本的跨领域检测性能。


<details>
  <summary>Details</summary>
Motivation: 随着语言模型的不断进步，检测机器生成文本的需求日益迫切。然而，现有检测器难以适应新领域和新型生成模型，亟需一种能自适应未知领域的解决方案。

Method: 提出DoGEN（Domain Gating Ensemble Networks）方法，通过领域分类器动态加权集成多个领域专家检测模型，实现跨领域自适应检测。

Result: 实验表明，DoGEN在领域内检测达到SOTA性能，在未知领域检测上超越两倍规模模型，并在主流基准测试中表现优异。

Conclusion: DoGEN为领域自适应AI检测提供了有效方案，作者开源了代码和模型以促进后续研究。

Abstract: As state-of-the-art language models continue to improve, the need for robust
detection of machine-generated text becomes increasingly critical. However,
current state-of-the-art machine text detectors struggle to adapt to new unseen
domains and generative models. In this paper we present DoGEN (Domain Gating
Ensemble Networks), a technique that allows detectors to adapt to unseen
domains by ensembling a set of domain expert detector models using weights from
a domain classifier. We test DoGEN on a wide variety of domains from leading
benchmarks and find that it achieves state-of-the-art performance on in-domain
detection while outperforming models twice its size on out-of-domain detection.
We release our code and trained models to assist in future research in
domain-adaptive AI detection.

</details>


### [25] [Reasoning Path Compression: Compressing Generation Trajectories for Efficient LLM Reasoning](https://arxiv.org/abs/2505.13866)
*Jiwon Song,Dongwon Jo,Yulhwa Kim,Jae-Joon Kim*

Main category: cs.CL

TL;DR: 该论文提出了一种无需训练的方法RPC，通过利用推理路径的语义稀疏性来加速推理，显著提高了生成吞吐量，同时保持了较高的准确性。


<details>
  <summary>Details</summary>
Motivation: 当前专注于推理的语言模型通过生成冗长的中间推理路径来提高准确性，但这显著增加了内存使用和令牌生成的吞吐量，限制了模型的实用部署。

Method: 提出了推理路径压缩（RPC）方法，通过定期压缩KV缓存，保留重要性评分高的KV缓存，这些评分由最近生成的查询组成的选择器窗口计算得出。

Result: 实验表明，RPC将QwQ-32B的生成吞吐量提高了1.60倍，同时在AIME 2024基准测试上的准确性仅下降了1.2%。

Conclusion: 研究发现推理路径中的语义稀疏性可以有效地用于压缩，为高效部署推理型大语言模型提供了实用路径。

Abstract: Recent reasoning-focused language models achieve high accuracy by generating
lengthy intermediate reasoning paths before producing final answers. While this
approach is effective in solving problems that require logical thinking, long
reasoning paths significantly increase memory usage and throughput of token
generation, limiting the practical deployment of such models. We propose
Reasoning Path Compression (RPC), a training-free method that accelerates
inference by leveraging the semantic sparsity of reasoning paths. RPC
periodically compresses the KV cache by retaining KV cache that receive high
importance score, which are computed using a selector window composed of
recently generated queries. Experiments show that RPC improves generation
throughput of QwQ-32B by up to 1.60$\times$ compared to the inference with full
KV cache, with an accuracy drop of 1.2% on the AIME 2024 benchmark. Our
findings demonstrate that semantic sparsity in reasoning traces can be
effectively exploited for compression, offering a practical path toward
efficient deployment of reasoning LLMs. Our code is available at
https://github.com/jiwonsong-dev/ReasoningPathCompression.

</details>


### [26] [Code2Logic: Game-Code-Driven Data Synthesis for Enhancing VLMs General Reasoning](https://arxiv.org/abs/2505.13886)
*Jingqi Tong,Jixin Tang,Hangcheng Li,Yurong Mou,Ming Zhang,Jun Zhao,Yanbo Wen,Fan Song,Jiahao Zhan,Yuyang Lu,Chaoran Tao,Zhiyuan Guo,Jizhou Yu,Tianhao Cheng,Changhao Jiang,Zhen Wang,Tao Liang,Zhihui Fei,Mingyang Wan,Guojun Ma,Weifeng Ge,Guanhua Chen,Tao Gui,Xipeng Qiu,Qi Zhang,Xuanjing Huang*

Main category: cs.CL

TL;DR: 论文提出Code2Logic方法，利用游戏代码自动生成视觉语言推理数据，构建GameQA数据集，提升视觉语言模型的推理能力。


<details>
  <summary>Details</summary>
Motivation: 视觉语言链式思维（CoT）数据资源稀缺，高质量标注成本高，游戏代码蕴含逻辑结构和状态转换过程，可作为替代资源。

Method: 利用大型语言模型（LLM）适配游戏代码，通过代码执行自动获取推理过程和结果，构建多模态推理数据集GameQA。

Result: GameQA包含30款游戏和158个任务，训练后的视觉语言模型（如Qwen2.5-VL-7B）在7个跨领域基准上性能提升2.33%。

Conclusion: Code2Logic提供了一种低成本、可扩展的视觉语言推理数据合成方法，显著提升模型泛化能力。

Abstract: Visual-language Chain-of-Thought (CoT) data resources are relatively scarce
compared to text-only counterparts, limiting the improvement of reasoning
capabilities in Vision Language Models (VLMs). However, high-quality
vision-language reasoning data is expensive and labor-intensive to annotate. To
address this issue, we leverage a promising resource: game code, which
naturally contains logical structures and state transition processes.
Therefore, we propose Code2Logic, a novel game-code-driven approach for
multimodal reasoning data synthesis. Our approach leverages Large Language
Models (LLMs) to adapt game code, enabling automatic acquisition of reasoning
processes and results through code execution. Using the Code2Logic approach, we
developed the GameQA dataset to train and evaluate VLMs. GameQA is
cost-effective and scalable to produce, challenging for state-of-the-art
models, and diverse with 30 games and 158 tasks. Surprisingly, despite training
solely on game data, VLMs demonstrated out of domain generalization,
specifically Qwen2.5-VL-7B improving performance by 2.33\% across 7 diverse
vision-language benchmarks. Our code and dataset are available at
https://github.com/tongjingqi/Code2Logic.

</details>


### [27] [Mapping the Minds of LLMs: A Graph-Based Analysis of Reasoning LLM](https://arxiv.org/abs/2505.13890)
*Zhen Xiong,Yujun Cai,Zhecheng Li,Yiwei Wang*

Main category: cs.CL

TL;DR: 该论文提出了一种基于图的分析框架，用于更好地建模大型语言模型（LLMs）的推理过程，揭示了推理结构与任务准确性之间的强相关性。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型在测试时扩展方面取得了进展，但其推理行为仍存在不稳定和反直觉的现象，如少样本提示下的性能下降，这挑战了当前对推理模型的理解。

Method: 论文提出了一种统一的图分析方法，首先将冗长的思维链输出聚类为语义连贯的推理步骤，然后构建有向推理图以捕捉这些步骤之间的上下文和逻辑依赖关系。

Result: 通过跨模型和提示机制的综合分析，研究发现结构属性（如探索密度、分支和收敛比率）与推理准确性密切相关，提示策略显著重塑了模型的内部推理结构。

Conclusion: 该框架不仅能够超越传统指标定量评估推理质量，还为提示工程和LLMs的认知分析提供了实用见解，相关代码和资源将公开发布以促进未来研究。

Abstract: Recent advances in test-time scaling have enabled Large Language Models
(LLMs) to display sophisticated reasoning abilities via extended
Chain-of-Thought (CoT) generation. Despite their potential, these Reasoning
LLMs (RLMs) often demonstrate counterintuitive and unstable behaviors, such as
performance degradation under few-shot prompting, that challenge our current
understanding of RLMs. In this work, we introduce a unified graph-based
analytical framework for better modeling the reasoning processes of RLMs. Our
method first clusters long, verbose CoT outputs into semantically coherent
reasoning steps, then constructs directed reasoning graphs to capture
contextual and logical dependencies among these steps. Through comprehensive
analysis across models and prompting regimes, we reveal that structural
properties, such as exploration density, branching, and convergence ratios,
strongly correlate with reasoning accuracy. Our findings demonstrate how
prompting strategies substantially reshape the internal reasoning structure of
RLMs, directly affecting task outcomes. The proposed framework not only enables
quantitative evaluation of reasoning quality beyond conventional metrics but
also provides practical insights for prompt engineering and the cognitive
analysis of LLMs. Code and resources will be released to facilitate future
research in this direction.

</details>


### [28] [InfiGFusion: Graph-on-Logits Distillation via Efficient Gromov-Wasserstein for Model Fusion](https://arxiv.org/abs/2505.13893)
*Yuanyi Wang,Zhaoyi Yan,Yiming Zhang,Qi Zhou,Yanggan Gu,Fei Wu,Hongxia Yang*

Main category: cs.CL

TL;DR: 提出InfiGFusion框架，通过图蒸馏损失融合异构大语言模型，显著提升推理和数学任务表现。


<details>
  <summary>Details</summary>
Motivation: 现有基于logit的融合方法忽视词汇维度间的语义依赖，无法有效整合异构模型的互补优势。

Method: 设计全局共激活图捕捉词汇通道联合激活，提出O(n log n)的排序近似算法降低计算成本。

Result: 在11个基准测试中超越SOTA，复杂推理任务提升35%以上，证明多步关系推理优势。

Conclusion: 结构感知的图蒸馏方法能有效融合异构模型，为多模态推理提供新思路。

Abstract: Recent advances in large language models (LLMs) have intensified efforts to
fuse heterogeneous open-source models into a unified system that inherits their
complementary strengths. Existing logit-based fusion methods maintain inference
efficiency but treat vocabulary dimensions independently, overlooking semantic
dependencies encoded by cross-dimension interactions. These dependencies
reflect how token types interact under a model's internal reasoning and are
essential for aligning models with diverse generation behaviors. To explicitly
model these dependencies, we propose \textbf{InfiGFusion}, the first
structure-aware fusion framework with a novel \textit{Graph-on-Logits
Distillation} (GLD) loss. Specifically, we retain the top-$k$ logits per output
and aggregate their outer products across sequence positions to form a global
co-activation graph, where nodes represent vocabulary channels and edges
quantify their joint activations. To ensure scalability and efficiency, we
design a sorting-based closed-form approximation that reduces the original
$O(n^4)$ cost of Gromov-Wasserstein distance to $O(n \log n)$, with provable
approximation guarantees. Experiments across multiple fusion settings show that
GLD consistently improves fusion quality and stability. InfiGFusion outperforms
SOTA models and fusion baselines across 11 benchmarks spanning reasoning,
coding, and mathematics. It shows particular strength in complex reasoning
tasks, with +35.6 improvement on Multistep Arithmetic and +37.06 on Causal
Judgement over SFT, demonstrating superior multi-step and relational inference.

</details>


### [29] [Let's Verify Math Questions Step by Step](https://arxiv.org/abs/2505.13903)
*Chengyu Shen,Zhen Hao Wong,Runming He,Hao Liang,Meiyi Qiang,Zimo Meng,Zhengyang Zhao,Bohan Zeng,Zhengzhou Zhu,Bin Cui,Wentao Zhang*

Main category: cs.CL

TL;DR: 该论文提出了MathQ-Verify，一个五阶段流程，用于严格过滤数学问题中的不良或未明确问题，以提高数学推理数据集的可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有工作主要关注生成正确的推理路径和答案，而忽视了问题本身的有效性。MathQ-Verify旨在解决这一问题，通过验证问题的格式、逻辑和完整性，确保数学问题的质量。

Method: MathQ-Verify采用五阶段流程：格式验证、问题形式化、条件分解、逻辑矛盾检测和目标导向的完整性检查，以过滤不良数学问题。

Result: 实验表明，MathQ-Verify在多个基准测试中达到最先进性能，F1分数提升高达25个百分点，并通过轻量级模型投票方案实现约90%的精确度和63%的召回率。

Conclusion: MathQ-Verify为构建可靠的数学数据集提供了可扩展且准确的解决方案，减少了标签噪声并避免了对无效问题的不必要计算。

Abstract: Large Language Models (LLMs) have recently achieved remarkable progress in
mathematical reasoning. To enable such capabilities, many existing works
distill strong reasoning models into long chains of thought or design
algorithms to construct high-quality math QA data for training. However, these
efforts primarily focus on generating correct reasoning paths and answers,
while largely overlooking the validity of the questions themselves. In this
work, we propose Math Question Verification (MathQ-Verify), a novel five-stage
pipeline designed to rigorously filter ill-posed or under-specified math
problems. MathQ-Verify first performs format-level validation to remove
redundant instructions and ensure that each question is syntactically
well-formed. It then formalizes each question, decomposes it into atomic
conditions, and verifies them against mathematical definitions. Next, it
detects logical contradictions among these conditions, followed by a
goal-oriented completeness check to ensure the question provides sufficient
information for solving. To evaluate this task, we use existing benchmarks
along with an additional dataset we construct, containing 2,147 math questions
with diverse error types, each manually double-validated. Experiments show that
MathQ-Verify achieves state-of-the-art performance across multiple benchmarks,
improving the F1 score by up to 25 percentage points over the direct
verification baseline. It further attains approximately 90% precision and 63%
recall through a lightweight model voting scheme. MathQ-Verify offers a
scalable and accurate solution for curating reliable mathematical datasets,
reducing label noise and avoiding unnecessary computation on invalid questions.
Our code and data are available at https://github.com/scuuy/MathQ-Verify.

</details>


### [30] [Cross-Linguistic Transfer in Multilingual NLP: The Role of Language Families and Morphology](https://arxiv.org/abs/2505.13908)
*Ajitesh Bankula,Praney Bankula*

Main category: cs.CL

TL;DR: 该论文研究了语言家族和形态相似性对多语言预训练模型跨语言迁移性能的影响，并探讨了整合语言类型学信息以提升迁移效果的新方法。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于探索语言家族亲缘关系和形态相似性如何影响多语言模型在跨语言迁移任务中的表现，以优化低资源语言的模型应用。

Method: 通过分析多语言预训练模型（如mBERT、XLM-R）的零样本迁移能力，结合语言距离度量和形态特征进行性能对比。

Result: 研究发现语言家族和形态相似性与跨语言迁移效果存在相关性，整合类型学信息可显著提升模型在多样化语言上的表现。

Conclusion: 论文结论强调语言结构相似性对迁移效果的重要性，并指出融合语言学知识是改进多语言模型迁移能力的有效方向。

Abstract: Cross-lingual transfer has become a crucial aspect of multilingual NLP, as it
allows for models trained on resource-rich languages to be applied to
low-resource languages more effectively. Recently massively multilingual
pre-trained language models (e.g., mBERT, XLM-R) demonstrate strong zero-shot
transfer capabilities[14] [13]. This paper investigates cross-linguistic
transfer through the lens of language families and morphology. Investigating
how language family proximity and morphological similarity affect performance
across NLP tasks. We further discuss our results and how it relates to findings
from recent literature. Overall, we compare multilingual model performance and
review how linguistic distance metrics correlate with transfer outcomes. We
also look into emerging approaches that integrate typological and morphological
information into model pre-training to improve transfer to diverse
languages[18] [19].

</details>


### [31] [Word length predicts word order: "Min-max"-ing drives language evolution](https://arxiv.org/abs/2505.13913)
*Hiram Ring*

Main category: cs.CL

TL;DR: 该论文通过分析1500多种语言的平行数据集，提出了一种基于词类长度与语序相关性的'Min-Max'语言进化理论，调和了先天论和功能论的分歧。


<details>
  <summary>Details</summary>
Motivation: 当前关于语言结构的理论存在先天论（如Chomsky）和功能论（如Greenberg）的分歧，且进化模型认为语序主要受谱系影响。本文旨在通过大规模跨语言数据分析，揭示语序变化的普遍机制。

Method: 使用包含1500多种语言（133个语系和111个孤立语言）的标注平行数据集，通过回归模型分析词类长度与语序的关联性，并验证其对历史语序变化的预测力。

Result: 研究发现词类长度与跨语言语序显著相关（但非简单线性关系），其解释力超过谱系或语言区域因素，并能预测两个不同谱系线的历史语序变化。

Conclusion: 提出'Min-Max'理论，认为语言进化受信息处理效率（最小化加工成本）与信息结构需求（最大化表达清晰度）的双重压力驱动，与近期效率导向和信息论研究一致。

Abstract: Current theories of language propose an innate (Baker 2001; Chomsky 1981) or
a functional (Greenberg 1963; Dryer 2007; Hawkins 2014) origin for the surface
structures (i.e. word order) that we observe in languages of the world, while
evolutionary modeling (Dunn et al. 2011) suggests that descent is the primary
factor influencing such patterns. Although there are hypotheses for word order
change from both innate and usage-based perspectives for specific languages and
families, there are key disagreements between the two major proposals for
mechanisms that drive the evolution of language more broadly (Wasow 2002; Levy
2008). This paper proposes a universal underlying mechanism for word order
change based on a large tagged parallel dataset of over 1,500 languages
representing 133 language families and 111 isolates. Results indicate that word
class length is significantly correlated with word order crosslinguistically,
but not in a straightforward manner, partially supporting opposing theories of
processing, while at the same time predicting historical word order change in
two different phylogenetic lines and explaining more variance than descent or
language area in regression models. Such findings suggest an integrated
"Min-Max" theory of language evolution driven by competing pressures of
processing and information structure, aligning with recent efficiency-oriented
(Levshina 2023) and information-theoretic proposals (Zaslavsky 2020; Tucker et
al. 2025).

</details>


### [32] [EEG-to-Text Translation: A Model for Deciphering Human Brain Activity](https://arxiv.org/abs/2505.13936)
*Saydul Akbar Murad,Ashim Dahal,Nick Rahimi*

Main category: cs.CL

TL;DR: 提出新型R1 Translator模型，结合双向LSTM与预训练Transformer解码器，显著提升EEG信号转文本性能，多项指标超越现有技术。


<details>
  <summary>Details</summary>
Motivation: 现有EEG信号解码模型性能有限，需突破技术瓶颈以实现更精准的人脑语言处理交互。

Method: 采用双向LSTM编码器提取EEG序列特征，衔接预训练Transformer解码器生成高质量文本。

Result: ROUGE-1得分38%（提升9%）、ROUGE-L F1值32.51%（提升3%），CER与WER分别降低2-4%，全面优于T5和Brain模型。

Conclusion: R1 Translator通过混合架构有效捕捉脑电信号时序特征，为脑机接口语言解码设立新基准。

Abstract: With the rapid advancement of large language models like Gemini, GPT, and
others, bridging the gap between the human brain and language processing has
become an important area of focus. To address this challenge, researchers have
developed various models to decode EEG signals into text. However, these models
still face significant performance limitations. To overcome these shortcomings,
we propose a new model, R1 Translator, which aims to improve the performance of
EEG-to-text decoding. The R1 Translator model combines a bidirectional LSTM
encoder with a pretrained transformer-based decoder, utilizing EEG features to
produce high-quality text outputs. The model processes EEG embeddings through
the LSTM to capture sequential dependencies, which are then fed into the
transformer decoder for effective text generation. The R1 Translator excels in
ROUGE metrics, outperforming both T5 (previous research) and Brain Translator.
Specifically, R1 achieves a ROUGE-1 score of 38.00% (P), which is up to 9%
higher than T5 (34.89%) and 3% better than Brain (35.69%). It also leads in
ROUGE-L, with a F1 score of 32.51%, outperforming T5 by 3% (29.67%) and Brain
by 2% (30.38%). In terms of CER, R1 achieves a CER of 0.5795, which is 2% lower
than T5 (0.5917) and 4% lower than Brain (0.6001). Additionally, R1 performs
better in WER with a score of 0.7280, outperforming T5 by 4.3% (0.7610) and
Brain by 3.6% (0.7553). Code is available at
https://github.com/Mmurrad/EEG-To-text.

</details>


### [33] [Towards Rehearsal-Free Continual Relation Extraction: Capturing Within-Task Variance with Adaptive Prompting](https://arxiv.org/abs/2505.13944)
*Bao-Ngoc Dao,Quang Nguyen,Luyen Ngo Dinh,Minh Le,Nam Le,Linh Ngo Van*

Main category: cs.CL

TL;DR: WAVE++提出了一种基于提示的持续关系抽取方法，通过任务特定提示池和标签描述提升性能，避免存储历史数据，解决了现有方法在任务识别和遗忘问题上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有基于记忆的持续关系抽取方法存在内存和隐私问题，而基于提示的方法在任务识别和防止灾难性遗忘方面仍有不足。本文旨在解决这些问题。

Method: WAVE++结合前缀调优和专家混合思想，设计了任务特定提示池，引入标签描述增强分类，并采用生成模型整合先验知识，无需显式存储数据。

Result: 实验表明，WAVE++在持续关系抽取任务上优于当前最先进的基于提示和基于记忆的方法。

Conclusion: WAVE++为持续关系抽取提供了更鲁棒的解决方案，有效解决了任务识别和遗忘问题，同时避免了数据存储需求。

Abstract: Memory-based approaches have shown strong performance in Continual Relation
Extraction (CRE). However, storing examples from previous tasks increases
memory usage and raises privacy concerns. Recently, prompt-based methods have
emerged as a promising alternative, as they do not rely on storing past
samples. Despite this progress, current prompt-based techniques face several
core challenges in CRE, particularly in accurately identifying task identities
and mitigating catastrophic forgetting. Existing prompt selection strategies
often suffer from inaccuracies, lack robust mechanisms to prevent forgetting in
shared parameters, and struggle to handle both cross-task and within-task
variations. In this paper, we propose WAVE++, a novel approach inspired by the
connection between prefix-tuning and mixture of experts. Specifically, we
introduce task-specific prompt pools that enhance flexibility and adaptability
across diverse tasks while avoiding boundary-spanning risks; this design more
effectively captures variations within each task and across tasks. To further
refine relation classification, we incorporate label descriptions that provide
richer, more global context, enabling the model to better distinguish among
different relations. We also propose a training-free mechanism to improve task
prediction during inference. Moreover, we integrate a generative model to
consolidate prior knowledge within the shared parameters, thereby removing the
need for explicit data storage. Extensive experiments demonstrate that WAVE++
outperforms state-of-the-art prompt-based and rehearsal-based methods, offering
a more robust solution for continual relation extraction. Our code is publicly
available at https://github.com/PiDinosauR2804/WAVE-CRE-PLUS-PLUS.

</details>


### [34] [Memory-Centric Embodied Question Answer](https://arxiv.org/abs/2505.13948)
*Mingliang Zhai,Zhi Gao,Yuwei Wu,Yunde Jia*

Main category: cs.CL

TL;DR: 本文提出了一种以记忆为中心的EQA框架MemoryEQA，通过多模态分层记忆机制增强复杂任务处理能力，并在新构建的MT-HM3D数据集上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有EQA框架以规划器为核心，记忆模块无法与其他模块充分交互，限制了处理跨区域多目标等复杂任务的效率与准确性。

Method: 提出MemoryEQA框架：1) 建立全局/局部多模态分层记忆机制；2) 利用大语言模型动态转换记忆信息；3) 构建MT-HM3D评测数据集。

Result: 在HM-EQA、MT-HM3D和OpenEQA上性能显著提升，MT-HM3D任务相比基线模型提升19.8%，验证了记忆机制的关键作用。

Conclusion: 以记忆为中心的框架能有效提升EQA系统处理复杂任务的能力，多模态记忆机制和动态信息转换是核心创新点。

Abstract: Embodied Question Answering (EQA) requires agents to autonomously explore and
understand the environment to answer context-dependent questions. Existing
frameworks typically center around the planner, which guides the stopping
module, memory module, and answering module for reasoning. In this paper, we
propose a memory-centric EQA framework named MemoryEQA. Unlike planner-centric
EQA models where the memory module cannot fully interact with other modules,
MemoryEQA flexible feeds memory information into all modules, thereby enhancing
efficiency and accuracy in handling complex tasks, such as those involving
multiple targets across different regions. Specifically, we establish a
multi-modal hierarchical memory mechanism, which is divided into global memory
that stores language-enhanced scene maps, and local memory that retains
historical observations and state information. When performing EQA tasks, the
multi-modal large language model is leveraged to convert memory information
into the required input formats for injection into different modules. To
evaluate EQA models' memory capabilities, we constructed the MT-HM3D dataset
based on HM3D, comprising 1,587 question-answer pairs involving multiple
targets across various regions, which requires agents to maintain memory of
exploration-acquired target information. Experimental results on HM-EQA,
MT-HM3D, and OpenEQA demonstrate the effectiveness of our framework, where a
19.8% performance gain on MT-HM3D compared to baseline model further
underscores memory capability's pivotal role in resolving complex tasks.

</details>


### [35] [FlashThink: An Early Exit Method For Efficient Reasoning](https://arxiv.org/abs/2505.13949)
*Guochao Jiang,Guofeng Quan,Zepeng Ding,Ziqin Luo,Dixuan Wang,Zheng Hu*

Main category: cs.CL

TL;DR: 论文提出FlashThink方法，通过提前终止大语言模型的推理过程来减少计算开销，同时保持准确性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在推理任务中倾向于生成过长的推理内容，导致不必要的计算开销，而实际上模型可能在推理中途就已得出正确答案。

Method: 引入验证模型来识别模型可以提前终止推理的时刻，从而缩短推理内容。

Result: 在四个基准测试中，FlashThink显著缩短了推理内容长度（Deepseek-R1和QwQ-32B模型分别减少77.04%和77.47%），且未降低准确性。

Conclusion: FlashThink方法有效实现了高效推理，平衡了推理长度与模型准确性。

Abstract: Large Language Models (LLMs) have shown impressive performance in reasoning
tasks. However, LLMs tend to generate excessively long reasoning content,
leading to significant computational overhead. Our observations indicate that
even on simple problems, LLMs tend to produce unnecessarily lengthy reasoning
content, which is against intuitive expectations. Preliminary experiments show
that at a certain point during the generation process, the model is already
capable of producing the correct solution without completing the full reasoning
content. Therefore, we consider that the reasoning process of the model can be
exited early to achieve the purpose of efficient reasoning. We introduce a
verification model that identifies the exact moment when the model can stop
reasoning and still provide the correct answer. Comprehensive experiments on
four different benchmarks demonstrate that our proposed method, FlashThink,
effectively shortens the reasoning content while preserving the model accuracy.
For the Deepseek-R1 and QwQ-32B models, we reduced the length of reasoning
content by 77.04% and 77.47%, respectively, without reducing the accuracy.

</details>


### [36] [Through a Compressed Lens: Investigating the Impact of Quantization on LLM Explainability and Interpretability](https://arxiv.org/abs/2505.13963)
*Qianli Wang,Mingyang Wang,Nils Feldhus,Simon Ostermann,Yuan Cao,Hinrich Schütze,Sebastian Möller,Vera Schmitt*

Main category: cs.CL

TL;DR: 量化方法对大型语言模型的可解释性和透明度有显著影响，影响方向取决于量化方法、解释性方法及评估协议。


<details>
  <summary>Details</summary>
Motivation: 量化方法广泛用于加速大型语言模型的推理和部署，但其对模型可解释性和透明度的影响尚未被充分研究。

Method: 使用三种常见的量化技术在不同比特宽度下，结合两种可解释性方法（反事实示例和自然语言解释）和两种可解释性方法（知识记忆分析和潜在多跳推理分析），并进行用户研究。

Result: 量化对模型的可解释性和透明度有显著影响，影响方向不一致，取决于量化方法、解释性方法及评估协议。在某些情况下，量化会降低可解释性，而在其他情况下甚至会改善。

Conclusion: 量化可能不可预测地影响模型透明度，这对在透明度要求高的应用中部署大型语言模型具有重要意义。

Abstract: Quantization methods are widely used to accelerate inference and streamline
the deployment of large language models (LLMs). While prior research has
extensively investigated the degradation of various LLM capabilities due to
quantization, its effects on model explainability and interpretability, which
are crucial for understanding decision-making processes, remain unexplored. To
address this gap, we conduct comprehensive experiments using three common
quantization techniques at distinct bit widths, in conjunction with two
explainability methods, counterfactual examples and natural language
explanations, as well as two interpretability approaches, knowledge
memorization analysis and latent multi-hop reasoning analysis. We complement
our analysis with a thorough user study, evaluating selected explainability
methods. Our findings reveal that, depending on the configuration, quantization
can significantly impact model explainability and interpretability. Notably,
the direction of this effect is not consistent, as it strongly depends on (1)
the quantization method, (2) the explainability or interpretability approach,
and (3) the evaluation protocol. In some settings, human evaluation shows that
quantization degrades explainability, while in others, it even leads to
improvements. Our work serves as a cautionary tale, demonstrating that
quantization can unpredictably affect model transparency. This insight has
important implications for deploying LLMs in applications where transparency is
a critical requirement.

</details>


### [37] [CAFES: A Collaborative Multi-Agent Framework for Multi-Granular Multimodal Essay Scoring](https://arxiv.org/abs/2505.13965)
*Jiamin Su,Yibo Yan,Zhuoran Gao,Han Zhang,Xiang Liu,Xuming Hu*

Main category: cs.CL

TL;DR: CAFES是一个创新的多智能体协作框架，用于提升自动作文评分（AES）的准确性和与人类评价的一致性。


<details>
  <summary>Details</summary>
Motivation: 传统AES方法在评估泛化性和多模态感知方面存在不足，而现有的多模态大语言模型（MLLM）方法可能产生与人类判断不符的评分和理由。

Method: CAFES框架包含三个专门设计的智能体：初始评分器、反馈池管理器和反思评分器，通过协作迭代优化评分。

Result: 实验表明，CAFES在Quadratic Weighted Kappa（QWK）上实现了21%的平均相对提升，尤其在语法和词汇多样性方面表现突出。

Conclusion: CAFES为智能多模态AES系统的发展奠定了基础，代码将在论文被接受后公开。

Abstract: Automated Essay Scoring (AES) is crucial for modern education, particularly
with the increasing prevalence of multimodal assessments. However, traditional
AES methods struggle with evaluation generalizability and multimodal
perception, while even recent Multimodal Large Language Model (MLLM)-based
approaches can produce hallucinated justifications and scores misaligned with
human judgment. To address the limitations, we introduce CAFES, the first
collaborative multi-agent framework specifically designed for AES. It
orchestrates three specialized agents: an Initial Scorer for rapid,
trait-specific evaluations; a Feedback Pool Manager to aggregate detailed,
evidence-grounded strengths; and a Reflective Scorer that iteratively refines
scores based on this feedback to enhance human alignment. Extensive
experiments, using state-of-the-art MLLMs, achieve an average relative
improvement of 21% in Quadratic Weighted Kappa (QWK) against ground truth,
especially for grammatical and lexical diversity. Our proposed CAFES framework
paves the way for an intelligent multimodal AES system. The code will be
available upon acceptance.

</details>


### [38] [Truth or Twist? Optimal Model Selection for Reliable Label Flipping Evaluation in LLM-based Counterfactuals](https://arxiv.org/abs/2505.13972)
*Qianli Wang,Van Bach Nguyen,Nils Feldhus,Luis Felipe Villa-Arenas,Christin Seifert,Sebastian Möller,Vera Schmitt*

Main category: cs.CL

TL;DR: 研究发现，在反事实数据增强中，使用独立且未经微调的评估模型能提供最可靠的标签翻转评估，但完全自动化流程仍需人工干预。


<details>
  <summary>Details</summary>
Motivation: 反事实数据增强（CDA）中评估模型的选择对结果影响不一致，研究旨在探索生成模型与评估模型之间的关系对CDA效果的影响。

Method: 通过两种最先进的LLM方法、三个数据集、五个生成模型和15个评估模型进行实验，并结合用户研究（n=90）分析不同关系类型的影响。

Result: 独立且未经微调的评估模型提供最可靠的标签翻转评估，与用户研究结果一致的模型关系能提升模型性能和鲁棒性，但自动化流程与人工评估间仍存在显著差距。

Conclusion: 完全自动化的CDA流程可能不足，需要结合人工干预以确保评估的可靠性。

Abstract: Counterfactual examples are widely employed to enhance the performance and
robustness of large language models (LLMs) through counterfactual data
augmentation (CDA). However, the selection of the judge model used to evaluate
label flipping, the primary metric for assessing the validity of generated
counterfactuals for CDA, yields inconsistent results. To decipher this, we
define four types of relationships between the counterfactual generator and
judge models. Through extensive experiments involving two state-of-the-art
LLM-based methods, three datasets, five generator models, and 15 judge models,
complemented by a user study (n = 90), we demonstrate that judge models with an
independent, non-fine-tuned relationship to the generator model provide the
most reliable label flipping evaluations. Relationships between the generator
and judge models, which are closely aligned with the user study for CDA, result
in better model performance and robustness. Nevertheless, we find that the gap
between the most effective judge models and the results obtained from the user
study remains considerably large. This suggests that a fully automated pipeline
for CDA may be inadequate and requires human intervention.

</details>


### [39] [Toward Effective Reinforcement Learning Fine-Tuning for Medical VQA in Vision-Language Models](https://arxiv.org/abs/2505.13973)
*Wenhui Zhu,Xuanzhao Dong,Xin Li,Peijie Qiu,Xiwen Chen,Abolfazl Razi,Aris Sotiras,Yi Su,Yalin Wang*

Main category: cs.CL

TL;DR: 该论文研究了基于强化学习的多模态大语言模型在医学视觉问答任务中的优化方法，探讨了四个关键因素对模型性能的影响，并证明GRPO强化学习调优优于标准监督微调。


<details>
  <summary>Details</summary>
Motivation: 由于直接将强化学习调优应用于医学任务难以实现符合临床预期的模型行为，作者研究了如何通过优化模型初始化策略、医学语义对齐、长链推理奖励和偏差影响来提升医学视觉问答的效果。

Method: 论文通过大量实验分析了四个关键维度：基础模型初始化策略、医学语义对齐的作用、基于长度的奖励对长链推理的影响以及偏差的影响。

Result: 实验结果表明，基于GRPO的强化学习调优在准确性和推理质量上均优于标准监督微调（SFT）。

Conclusion: 该研究为医学领域特定的大语言模型调优提供了新的见解，并验证了GRPO强化学习在医学任务中的优越性。

Abstract: Recently, reinforcement learning (RL)-based tuning has shifted the trajectory
of Multimodal Large Language Models (MLLMs), particularly following the
introduction of Group Relative Policy Optimization (GRPO). However, directly
applying it to medical tasks remains challenging for achieving clinically
grounded model behavior. Motivated by the need to align model response with
clinical expectations, we investigate four critical dimensions that affect the
effectiveness of RL-based tuning in medical visual question answering (VQA):
base model initialization strategy, the role of medical semantic alignment, the
impact of length-based rewards on long-chain reasoning, and the influence of
bias. We conduct extensive experiments to analyze these factors for medical
MLLMs, providing new insights into how models are domain-specifically
fine-tuned. Additionally, our results also demonstrate that GRPO-based RL
tuning consistently outperforms standard supervised fine-tuning (SFT) in both
accuracy and reasoning quality.

</details>


### [40] [DRP: Distilled Reasoning Pruning with Skill-aware Step Decomposition for Efficient Large Reasoning Models](https://arxiv.org/abs/2505.13975)
*Yuxuan Jiang,Dawei Li,Frank Ferraro*

Main category: cs.CL

TL;DR: 论文提出DRP框架，通过推理剪枝与蒸馏结合提升大模型推理效率，在数学推理任务中显著减少token使用且保持或提升准确率。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型（LRMs）在复杂推理任务中表现优异，但其冗长的推理链导致效率低下。为解决这一问题，作者提出结合推理剪枝与蒸馏的方法以提高效率。

Method: 提出Distilled Reasoning Pruning (DRP)框架，利用教师模型进行技能感知的步骤分解与内容剪枝，并将剪枝后的推理路径蒸馏到学生模型中，实现高效准确推理。

Result: 在多个数学推理数据集上，DRP显著提升token效率且不牺牲准确率。例如，GSM8K上token使用从917降至328，准确率从91.7%提升至94.1%；AIME上token减少43%且性能无下降。

Conclusion: DRP通过对齐训练推理链与学生模型的推理能力，有效实现知识迁移与性能提升，为高效推理提供了可行方案。

Abstract: While Large Reasoning Models (LRMs) have demonstrated success in complex
reasoning tasks through long chain-of-thought (CoT) reasoning, their inference
often involves excessively verbose reasoning traces, resulting in substantial
inefficiency. To address this, we propose Distilled Reasoning Pruning (DRP), a
hybrid framework that combines inference-time pruning with tuning-based
distillation, two widely used strategies for efficient reasoning. DRP uses a
teacher model to perform skill-aware step decomposition and content pruning,
and then distills the pruned reasoning paths into a student model, enabling it
to reason both efficiently and accurately. Across several challenging
mathematical reasoning datasets, we find that models trained with DRP achieve
substantial improvements in token efficiency without sacrificing accuracy.
Specifically, DRP reduces average token usage on GSM8K from 917 to 328 while
improving accuracy from 91.7% to 94.1%, and achieves a 43% token reduction on
AIME with no performance drop. Further analysis shows that aligning the
reasoning structure of training CoTs with the student's reasoning capacity is
critical for effective knowledge transfer and performance gains.

</details>


### [41] [Mixed Signals: Understanding Model Disagreement in Multimodal Empathy Detection](https://arxiv.org/abs/2505.13979)
*Maya Srikanth,Run Chen,Julia Hirschberg*

Main category: cs.CL

TL;DR: 多模态模型在共情检测中表现关键，但模态间冲突线索会导致性能下降。研究发现模态间预测分歧常反映内在模糊性，且人类同样不总能从多模态输入中获益。


<details>
  <summary>Details</summary>
Motivation: 探讨多模态模型在共情检测中因模态间冲突线索导致的性能下降问题，分析单模态与多模态预测分歧的原因及其反映的标注不确定性。

Method: 使用微调后的文本、音频和视频单模态模型，结合门控融合模型，分析预测分歧案例及其与标注不确定性的关联。

Result: 发现单一模态的主导信号若未被其他模态支持会误导融合；人类与模型类似，多模态输入并非总是带来一致增益。分歧可作为诊断信号提升系统鲁棒性。

Conclusion: 模态间分歧能有效识别挑战性样本，为改进共情检测系统的鲁棒性提供关键洞察。

Abstract: Multimodal models play a key role in empathy detection, but their performance
can suffer when modalities provide conflicting cues. To understand these
failures, we examine cases where unimodal and multimodal predictions diverge.
Using fine-tuned models for text, audio, and video, along with a gated fusion
model, we find that such disagreements often reflect underlying ambiguity, as
evidenced by annotator uncertainty. Our analysis shows that dominant signals in
one modality can mislead fusion when unsupported by others. We also observe
that humans, like models, do not consistently benefit from multimodal input.
These insights position disagreement as a useful diagnostic signal for
identifying challenging examples and improving empathy system robustness.

</details>


### [42] [The Hallucination Tax of Reinforcement Finetuning](https://arxiv.org/abs/2505.13988)
*Linxin Song,Taiwei Shi,Jieyu Zhao*

Main category: cs.CL

TL;DR: 研究发现强化微调(RFT)会降低大语言模型对不可答问题的拒绝能力，导致幻觉回答增加。通过引入合成不可答数学数据集(SUM)，仅需10%的SUM数据即可显著恢复模型的拒绝行为，且不影响可解任务准确率。


<details>
  <summary>Details</summary>
Motivation: 当前强化微调(RFT)被广泛用于提升大语言模型的推理能力，但其对模型可信度的影响尚未充分研究。本文旨在探究RFT可能导致模型对不可答问题产生自信幻觉回答的副作用（称为幻觉税）。

Method: 构建高质量合成不可答数学数据集(SUM)，用于测试模型识别不可答问题的能力。在RFT训练中掺入10%的SUM数据，评估模型拒绝行为和准确率的变化。

Result: 标准RFT训练会使模型拒绝率下降80%以上，显著增加幻觉倾向。加入10%SUM数据后，模型拒绝行为大幅恢复，且对可解任务准确率影响极小。该方法还能提升模型对自身知识边界的认知能力。

Conclusion: RFT存在显著幻觉税问题，但通过少量针对性数据干预即可有效缓解。该方法能帮助模型更好地识别知识边界，在数学和事实问答任务中均展现出更好的泛化能力。

Abstract: Reinforcement finetuning (RFT) has become a standard approach for enhancing
the reasoning capabilities of large language models (LLMs). However, its impact
on model trustworthiness remains underexplored. In this work, we identify and
systematically study a critical side effect of RFT, which we term the
hallucination tax: a degradation in refusal behavior causing models to produce
hallucinated answers to unanswerable questions confidently. To investigate
this, we introduce SUM (Synthetic Unanswerable Math), a high-quality dataset of
unanswerable math problems designed to probe models' ability to recognize an
unanswerable question by reasoning from the insufficient or ambiguous
information. Our results show that standard RFT training could reduce model
refusal rates by more than 80%, which significantly increases model's tendency
to hallucinate. We further demonstrate that incorporating just 10% SUM during
RFT substantially restores appropriate refusal behavior, with minimal accuracy
trade-offs on solvable tasks. Crucially, this approach enables LLMs to leverage
inference-time compute to reason about their own uncertainty and knowledge
boundaries, improving generalization not only to out-of-domain math problems
but also to factual question answering tasks.

</details>


### [43] [DecIF: Improving Instruction-Following through Meta-Decomposition](https://arxiv.org/abs/2505.13990)
*Tingfeng Hui,Pengyu Zhu,Bowen Ping,Ling Tang,Yaqi Zhang,Sen Su*

Main category: cs.CL

TL;DR: DecIF框架通过元分解指导，仅用大语言模型自主生成高质量指令跟随数据，提升灵活性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖外部资源生成指令数据，限制了灵活性和泛化性。DecIF旨在通过纯LLM自主生成多样化、高质量的指令数据。

Method: DecIF基于分解原则，迭代生成元信息并结合响应约束构建指令，通过LLM检测并解决不一致性，将指令分解为原子级标准验证响应质量。

Result: 实验表明DecIF在多种场景下表现优异，具备强灵活性、扩展性和泛化性，能自动合成高质量指令数据。

Conclusion: DecIF为纯LLM驱动的指令数据生成提供了有效解决方案，显著提升指令跟随任务的性能。

Abstract: Instruction-following has emerged as a crucial capability for large language
models (LLMs). However, existing approaches often rely on pre-existing
documents or external resources to synthesize instruction-following data, which
limits their flexibility and generalizability. In this paper, we introduce
DecIF, a fully autonomous, meta-decomposition guided framework that generates
diverse and high-quality instruction-following data using only LLMs. DecIF is
grounded in the principle of decomposition. For instruction generation, we
guide LLMs to iteratively produce various types of meta-information, which are
then combined with response constraints to form well-structured and
semantically rich instructions. We further utilize LLMs to detect and resolve
potential inconsistencies within the generated instructions. Regarding response
generation, we decompose each instruction into atomic-level evaluation
criteria, enabling rigorous validation and the elimination of inaccurate
instruction-response pairs. Extensive experiments across a wide range of
scenarios and settings demonstrate DecIF's superior performance on
instruction-following tasks. Further analysis highlights its strong
flexibility, scalability, and generalizability in automatically synthesizing
high-quality instruction data.

</details>


### [44] [Social Sycophancy: A Broader Understanding of LLM Sycophancy](https://arxiv.org/abs/2505.13995)
*Myra Cheng,Sunny Yu,Cinoo Lee,Pranav Khadpe,Lujain Ibrahim,Dan Jurafsky*

Main category: cs.CL

TL;DR: 论文提出LLMs存在过度迎合用户的‘社交谄媚’问题，并开发了ELEPHANT框架进行量化评估，发现模型在模糊情境下会高频维护用户面子，且该行为难以缓解。


<details>
  <summary>Details</summary>
Motivation: 现有研究仅关注LLMs对用户明确观点的附和，但忽视了无明确事实场景（如建议寻求）中的谄媚行为，这种社交谄媚可能强化用户有害的隐性假设或行为。

Method: 提出社交谄媚理论框架ELEPHANT，通过五种面子维护行为（情感认同、道德认可等）在OEQ和AITA数据集上评估8个模型的表现。

Result: LLMs社交谄媚率显著高于人类：OEQ中多47%，AITA中42%情况下会认可人类评判不当的行为；且该行为受偏好数据奖励且难以缓解。

Conclusion: 研究为理解LLMs社交谄媚提供了理论工具，揭示了这一未被充分认识但影响深远的问题，并开源了评估数据集和代码。

Abstract: A serious risk to the safety and utility of LLMs is sycophancy, i.e.,
excessive agreement with and flattery of the user. Yet existing work focuses on
only one aspect of sycophancy: agreement with users' explicitly stated beliefs
that can be compared to a ground truth. This overlooks forms of sycophancy that
arise in ambiguous contexts such as advice and support-seeking, where there is
no clear ground truth, yet sycophancy can reinforce harmful implicit
assumptions, beliefs, or actions. To address this gap, we introduce a richer
theory of social sycophancy in LLMs, characterizing sycophancy as the excessive
preservation of a user's face (the positive self-image a person seeks to
maintain in an interaction). We present ELEPHANT, a framework for evaluating
social sycophancy across five face-preserving behaviors (emotional validation,
moral endorsement, indirect language, indirect action, and accepting framing)
on two datasets: open-ended questions (OEQ) and Reddit's r/AmITheAsshole
(AITA). Across eight models, we show that LLMs consistently exhibit high rates
of social sycophancy: on OEQ, they preserve face 47% more than humans, and on
AITA, they affirm behavior deemed inappropriate by crowdsourced human judgments
in 42% of cases. We further show that social sycophancy is rewarded in
preference datasets and is not easily mitigated. Our work provides theoretical
grounding and empirical tools (datasets and code) for understanding and
addressing this under-recognized but consequential issue.

</details>


### [45] [Activation-Guided Consensus Merging for Large Language Models](https://arxiv.org/abs/2505.14009)
*Yuxuan Yao,Shuqi Liu,Zehua Liu,Qintong Li,Mingyang Liu,Xiongwei Han,Zhijiang Guo,Han Wu,Linqi Song*

Main category: cs.CL

TL;DR: 论文提出了一种名为ACM的模型融合方法，通过激活引导的共识合并策略，有效整合不同大语言模型的优势，无需额外训练即可提升推理准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 现有方法在整合System 1的效率与System 2的推理能力时面临效率与稳定性挑战，且传统模型融合方法忽视神经组件的功能异质性。

Method: 提出ACM框架，基于预训练与微调模型激活间的互信息动态计算分层合并系数，实现任务能力保留且无需梯度计算。

Result: 在L2S任务中，ACM使Qwen-7B模型响应长度减少55.3%，推理准确率提升1.3个百分点，显著优于基线方法。

Conclusion: ACM为模型融合提供了高效稳定的解决方案，代码将开源以促进可复现性。

Abstract: Recent research has increasingly focused on reconciling the reasoning
capabilities of System 2 with the efficiency of System 1. While existing
training-based and prompt-based approaches face significant challenges in terms
of efficiency and stability, model merging emerges as a promising strategy to
integrate the diverse capabilities of different Large Language Models (LLMs)
into a unified model. However, conventional model merging methods often assume
uniform importance across layers, overlooking the functional heterogeneity
inherent in neural components. To address this limitation, we propose
\textbf{A}ctivation-Guided \textbf{C}onsensus \textbf{M}erging (\textbf{ACM}),
a plug-and-play merging framework that determines layer-specific merging
coefficients based on mutual information between activations of pre-trained and
fine-tuned models. ACM effectively preserves task-specific capabilities without
requiring gradient computations or additional training. Extensive experiments
on Long-to-Short (L2S) and general merging tasks demonstrate that ACM
consistently outperforms all baseline methods. For instance, in the case of
Qwen-7B models, TIES-Merging equipped with ACM achieves a \textbf{55.3\%}
reduction in response length while simultaneously improving reasoning accuracy
by \textbf{1.3} points. We submit the code with the paper for reproducibility,
and it will be publicly available.

</details>


### [46] [AUTOLAW: Enhancing Legal Compliance in Large Language Models via Case Law Generation and Jury-Inspired Deliberation](https://arxiv.org/abs/2505.14015)
*Tai D. Nguyen,Long H. Pham,Jun Sun*

Main category: cs.CL

TL;DR: AutoLaw框架通过对抗数据生成和陪审团式审议提升法律大模型的合规性检测能力。


<details>
  <summary>Details</summary>
Motivation: 现有法律评估基准缺乏对区域法律差异的适应性，难以应对动态变化的监管环境，需开发更灵活、精准的合规检测方法。

Method: 结合对抗数据生成动态合成地方法规案例，并采用LLM陪审团排名机制模拟司法决策流程以减少偏见。

Result: 在Law-SG等三个基准测试中，对抗数据提升模型区分度，陪审团投票策略显著提高违规检测率（具体数值未提及）。

Conclusion: AutoLaw能自适应探测法律偏差，提供可靠的情境感知判决，为敏感法律应用提供可扩展的LLM评估方案。

Abstract: The rapid advancement of domain-specific large language models (LLMs) in
fields like law necessitates frameworks that account for nuanced regional legal
distinctions, which are critical for ensuring compliance and trustworthiness.
Existing legal evaluation benchmarks often lack adaptability and fail to
address diverse local contexts, limiting their utility in dynamically evolving
regulatory landscapes. To address these gaps, we propose AutoLaw, a novel
violation detection framework that combines adversarial data generation with a
jury-inspired deliberation process to enhance legal compliance of LLMs. Unlike
static approaches, AutoLaw dynamically synthesizes case law to reflect local
regulations and employs a pool of LLM-based "jurors" to simulate judicial
decision-making. Jurors are ranked and selected based on synthesized legal
expertise, enabling a deliberation process that minimizes bias and improves
detection accuracy. Evaluations across three benchmarks: Law-SG, Case-SG
(legality), and Unfair-TOS (policy), demonstrate AutoLaw's effectiveness:
adversarial data generation improves LLM discrimination, while the jury-based
voting strategy significantly boosts violation detection rates. Our results
highlight the framework's ability to adaptively probe legal misalignments and
deliver reliable, context-aware judgments, offering a scalable solution for
evaluating and enhancing LLMs in legally sensitive applications.

</details>


### [47] [From Unaligned to Aligned: Scaling Multilingual LLMs with Multi-Way Parallel Corpora](https://arxiv.org/abs/2505.14045)
*Yingli Shen,Wen Lai,Shuo Wang,Kangyang Luo,Alexander Fraser,Maosong Sun*

Main category: cs.CL

TL;DR: 论文提出基于TED Talks构建的大规模多语言平行语料库TED2025，通过多语言对齐数据提升大语言模型的跨语言语义捕捉能力。


<details>
  <summary>Details</summary>
Motivation: 现有大规模多语言数据因未对齐而难以有效捕捉跨语言语义，多向平行数据能提供更强的跨语言一致性，从而提升多语言模型性能。

Method: 构建包含113种语言、最多50种语言对齐的TED2025语料库，研究持续预训练和指令微调的最佳实践，分析关键影响因素。

Result: 在六个多语言基准测试中，使用多向平行数据训练的模型性能始终优于未对齐多语言数据训练的模型。

Conclusion: 多向平行数据能显著提升大语言模型的多语言能力，TED2025为跨语言研究提供了高质量资源。

Abstract: Continued pretraining and instruction tuning on large-scale multilingual data
have proven to be effective in scaling large language models (LLMs) to
low-resource languages. However, the unaligned nature of such data limits its
ability to effectively capture cross-lingual semantics. In contrast, multi-way
parallel data, where identical content is aligned across multiple languages,
provides stronger cross-lingual consistency and offers greater potential for
improving multilingual performance. In this paper, we introduce a large-scale,
high-quality multi-way parallel corpus, TED2025, based on TED Talks. The corpus
spans 113 languages, with up to 50 languages aligned in parallel, ensuring
extensive multilingual coverage. Using this dataset, we investigate best
practices for leveraging multi-way parallel data to enhance LLMs, including
strategies for continued pretraining, instruction tuning, and the analysis of
key influencing factors. Experiments on six multilingual benchmarks show that
models trained on multiway parallel data consistently outperform those trained
on unaligned multilingual data.

</details>


### [48] [Improved Methods for Model Pruning and Knowledge Distillation](https://arxiv.org/abs/2505.14052)
*Wei Jiang,Anying Fu,Youling Zhang*

Main category: cs.CL

TL;DR: 提出MAMA剪枝法，通过权重和GRPO奖励指标，在保持性能的同时大幅压缩大语言模型。


<details>
  <summary>Details</summary>
Motivation: 现有剪枝方法会导致性能显著下降或需要大量微调，需开发更高效的轻量化方案。

Method: 基于预训练阶段固定权重和训练后GRPO奖励作为剪枝指标，结合运动幅度分析(MAMA)。

Result: 在极端剪枝率下仍保持原始模型性能，优于现有方法且适用于多种NLP任务。

Conclusion: MAMA剪枝实现了模型高效压缩与性能平衡，为部署轻量化语言模型提供新方案。

Abstract: Model pruning is a performance optimization technique for large language
models like R1 or o3-mini. However, existing pruning methods often lead to
significant performance degradation or require extensive retraining and
fine-tuning. This technique aims to identify and remove neurons, connections
unlikely leading to the contribution during the human-computer interaction
phase. Our goal is to obtain a much smaller and faster knowledge distilled
model that can quickly generate content almost as good as those of the unpruned
ones. We propose MAMA Pruning, short for Movement and Magnitude Analysis, an
improved pruning method that effectively reduces model size and computational
complexity while maintaining performance comparable to the original unpruned
model even at extreme pruned levels. The improved method is based on weights,
bias fixed in the pre-training phase and GRPO rewards verified during the
post-training phase as our novel pruning indicators. Preliminary experimental
results show that our method outperforms and be comparable to state-of-the-art
methods across various pruning levels and different downstream computational
linguistics tasks.

</details>


### [49] [Enhancing LLMs via High-Knowledge Data Selection](https://arxiv.org/abs/2505.14070)
*Feiyu Duan,Xuemiao Zhang,Sirui Wang,Haoran Que,Yuqi Liu,Wenge Rong,Xunliang Cai*

Main category: cs.CL

TL;DR: 本文提出了一种无梯度的高知识评分器（HKS），用于从知识维度选择高质量数据，以缓解预训练语料中知识稀缺的问题，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有研究在高质量数据选择时未考虑文本语料库中知识丰富度的重要性，导致预训练语料中存在知识稀缺问题。

Method: 提出HKS评分器，构建多领域知识元素池，引入知识密度和覆盖率作为评估指标，并设计综合知识评分器选择高知识密度数据。

Result: 实验表明，HKS评分器能提升模型在知识密集型和通用理解任务中的表现，并增强模型的通用及领域特定能力。

Conclusion: HKS评分器有效解决了预训练语料中的知识稀缺问题，显著提升了模型性能。

Abstract: The performance of Large Language Models (LLMs) is intrinsically linked to
the quality of its training data. Although several studies have proposed
methods for high-quality data selection, they do not consider the importance of
knowledge richness in text corpora. In this paper, we propose a novel and
gradient-free High-Knowledge Scorer (HKS) to select high-quality data from the
dimension of knowledge, to alleviate the problem of knowledge scarcity in the
pre-trained corpus. We propose a comprehensive multi-domain knowledge element
pool and introduce knowledge density and coverage as metrics to assess the
knowledge content of the text. Based on this, we propose a comprehensive
knowledge scorer to select data with intensive knowledge, which can also be
utilized for domain-specific high-knowledge data selection by restricting
knowledge elements to the specific domain. We train models on a high-knowledge
bilingual dataset, and experimental results demonstrate that our scorer
improves the model's performance in knowledge-intensive and general
comprehension tasks, and is effective in enhancing both the generic and
domain-specific capabilities of the model.

</details>


### [50] [BAR: A Backward Reasoning based Agent for Complex Minecraft Tasks](https://arxiv.org/abs/2505.14079)
*Weihong Du,Wenrui Liao,Binyu Yan,Hongru Liang,Anthony G. Cohn,Wenqiang Lei*

Main category: cs.CL

TL;DR: 论文提出了一种基于逆向推理的BAR智能体，通过从目标状态反向规划来解决复杂任务中的感知差距问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于前向推理的LLM智能体在处理复杂任务时效果不佳，主要由于初始状态与任务目标间存在较大感知差距。

Method: 设计了BAR智能体，包含递归目标分解模块、状态一致性维护模块和阶段记忆模块，从终端状态进行逆向规划。

Result: 实验结果表明BAR优于现有方法，且提出的模块有效。

Conclusion: 逆向推理能有效解决复杂任务规划问题，BAR智能体的模块设计提升了规划的鲁棒性和效率。

Abstract: Large language model (LLM) based agents have shown great potential in
following human instructions and automatically completing various tasks. To
complete a task, the agent needs to decompose it into easily executed steps by
planning. Existing studies mainly conduct the planning by inferring what steps
should be executed next starting from the agent's initial state. However, this
forward reasoning paradigm doesn't work well for complex tasks. We propose to
study this issue in Minecraft, a virtual environment that simulates complex
tasks based on real-world scenarios. We believe that the failure of forward
reasoning is caused by the big perception gap between the agent's initial state
and task goal. To this end, we leverage backward reasoning and make the
planning starting from the terminal state, which can directly achieve the task
goal in one step. Specifically, we design a BAckward Reasoning based agent
(BAR). It is equipped with a recursive goal decomposition module, a state
consistency maintaining module and a stage memory module to make robust,
consistent, and efficient planning starting from the terminal state.
Experimental results demonstrate the superiority of BAR over existing methods
and the effectiveness of proposed modules.

</details>


### [51] [Gender Trouble in Language Models: An Empirical Audit Guided by Gender Performativity Theory](https://arxiv.org/abs/2505.14080)
*Franziska Sofia Hafner,Ana Valdivia,Luc Rocher*

Main category: cs.CL

TL;DR: 该论文指出语言模型编码并延续了有害的性别刻板印象，现有研究仅通过解耦非性别词汇与性别词汇来缓解问题，但忽略了性别建构本身带来的更深层伤害。作者呼吁重新定义语言模型中的'性别偏见'，并通过实证分析16种不同架构、训练数据和规模的模型，发现模型倾向于将性别编码为与生理性别绑定的二元类别，且对非二元性别身份存在抹除和病理化现象。


<details>
  <summary>Details</summary>
Motivation: 当前语言模型中的性别偏见研究多停留在词汇关联层面（如职业与性别词的解耦），但性别建构理论指出，将性别等同于生理性别的错误认知会导致对跨性别和非二元性别身份的抹除，进而引发实际应用中的伤害（如错误称呼用户或误诊患者）。论文旨在推动对语言模型中性别偏见的更全面定义。

Method: 基于性别研究理论，作者将性别建构的语言学洞察操作化为可量化指标，对16种不同架构（如Transformer）、训练数据集和参数规模的预训练语言模型进行系统性实证分析，检验其如何编码性别概念。

Result: 研究发现：(1) 语言模型普遍将性别编码为与生理性别强关联的二元类别；(2) 不符合二元分类的性别术语被系统性抹除或病理化；(3) 性能越强的大模型（如GPT-3），性别与生理性别的关联强度反而更高，进一步固化了狭隘的性别认知。

Conclusion: 当前语言模型性能优化可能加剧性别偏见的深层结构性问题。作者呼吁学界重新评估性别偏见的定义框架，提出需超越表层词汇关联，从性别建构的本质层面设计干预措施，以避免模型对多元性别身份的持续边缘化。

Abstract: Language models encode and subsequently perpetuate harmful gendered
stereotypes. Research has succeeded in mitigating some of these harms, e.g. by
dissociating non-gendered terms such as occupations from gendered terms such as
'woman' and 'man'. This approach, however, remains superficial given that
associations are only one form of prejudice through which gendered harms arise.
Critical scholarship on gender, such as gender performativity theory,
emphasizes how harms often arise from the construction of gender itself, such
as conflating gender with biological sex. In language models, these issues
could lead to the erasure of transgender and gender diverse identities and
cause harms in downstream applications, from misgendering users to
misdiagnosing patients based on wrong assumptions about their anatomy.
  For FAccT research on gendered harms to go beyond superficial linguistic
associations, we advocate for a broader definition of 'gender bias' in language
models. We operationalize insights on the construction of gender through
language from gender studies literature and then empirically test how 16
language models of different architectures, training datasets, and model sizes
encode gender. We find that language models tend to encode gender as a binary
category tied to biological sex, and that gendered terms that do not neatly
fall into one of these binary categories are erased and pathologized. Finally,
we show that larger models, which achieve better results on performance
benchmarks, learn stronger associations between gender and sex, further
reinforcing a narrow understanding of gender. Our findings lead us to call for
a re-evaluation of how gendered harms in language models are defined and
addressed.

</details>


### [52] [Beyond Chains: Bridging Large Language Models and Knowledge Bases in Complex Question Answering](https://arxiv.org/abs/2505.14099)
*Yihua Zhu,Qianying Liu,Akiko Aizawa,Hidetoshi Shimodaira*

Main category: cs.CL

TL;DR: PDRR框架通过预测、分解、检索和推理四阶段，有效提升KBQA在复杂问题上的性能。


<details>
  <summary>Details</summary>
Motivation: 现有LLM-only方法存在知识过时、幻觉和透明度不足的问题，而基于链式结构的KG-RAG方法仅适用于简单链式问题。PDRR旨在解决这些局限性。

Method: 提出PDRR四阶段框架：预测问题类型、分解问题为结构化三元组、从知识库检索信息、引导LLM代理推理补全三元组。

Result: 实验表明，PDRR在不同LLM骨干上均优于现有方法，尤其在非链式复杂问题上表现突出。

Conclusion: PDRR通过结构化分解和规划式检索推理，显著提升了KBQA系统的泛化能力和复杂问题处理能力。

Abstract: Knowledge Base Question Answering (KBQA) aims to answer natural language
questions using structured knowledge from KBs. While LLM-only approaches offer
generalization, they suffer from outdated knowledge, hallucinations, and lack
of transparency. Chain-based KG-RAG methods address these issues by
incorporating external KBs, but are limited to simple chain-structured
questions due to the absence of planning and logical structuring. Inspired by
semantic parsing methods, we propose PDRR: a four-stage framework consisting of
Predict, Decompose, Retrieve, and Reason. Our method first predicts the
question type and decomposes the question into structured triples. Then
retrieves relevant information from KBs and guides the LLM as an agent to
reason over and complete the decomposed triples. Experimental results
demonstrate that PDRR consistently outperforms existing methods across various
LLM backbones and achieves superior performance on both chain-structured and
non-chain complex questions.

</details>


### [53] [MultiHal: Multilingual Dataset for Knowledge-Graph Grounded Evaluation of LLM Hallucinations](https://arxiv.org/abs/2505.14101)
*Ernests Lavrinovics,Russa Biswas,Katja Hose,Johannes Bjerva*

Main category: cs.CL

TL;DR: 该论文提出了一个基于知识图谱的多语言、多跳基准测试MultiHal，用于评估生成文本的事实性，并通过实验展示了知识图谱在减少大语言模型幻觉方面的潜力。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型存在事实性不足的问题（幻觉），且现有评估基准多依赖英语数据集和附加文本信息，忽略了结构化知识资源。知识图谱（KGs）因其结构化表示事实的特性，被认为有助于缓解幻觉问题。

Method: 研究团队构建了一个名为MultiHal的多语言、多跳基准测试，通过从开放域知识图谱中挖掘14万条知识路径，并筛选出2.59万条高质量路径，用于生成文本的评估。

Result: 基线评估显示，在多语言和多模型测试中，结合知识图谱的问答系统（KG-RAG）在语义相似度得分上比普通问答系统提高了0.12到0.36分。

Conclusion: MultiHal基准测试有望推动基于图谱的幻觉缓解和事实核查任务的研究，展示了知识图谱集成在提升语言模型事实性方面的潜力。

Abstract: Large Language Models (LLMs) have inherent limitations of faithfulness and
factuality, commonly referred to as hallucinations. Several benchmarks have
been developed that provide a test bed for factuality evaluation within the
context of English-centric datasets, while relying on supplementary informative
context like web links or text passages but ignoring the available structured
factual resources. To this end, Knowledge Graphs (KGs) have been identified as
a useful aid for hallucination mitigation, as they provide a structured way to
represent the facts about entities and their relations with minimal linguistic
overhead. We bridge the lack of KG paths and multilinguality for factual
language modeling within the existing hallucination evaluation benchmarks and
propose a KG-based multilingual, multihop benchmark called \textbf{MultiHal}
framed for generative text evaluation. As part of our data collection pipeline,
we mined 140k KG-paths from open-domain KGs, from which we pruned noisy
KG-paths, curating a high-quality subset of 25.9k. Our baseline evaluation
shows an absolute scale increase by approximately 0.12 to 0.36 points for the
semantic similarity score in KG-RAG over vanilla QA across multiple languages
and multiple models, demonstrating the potential of KG integration. We
anticipate MultiHal will foster future research towards several graph-based
hallucination mitigation and fact-checking tasks.

</details>


### [54] [Legal Rule Induction: Towards Generalizable Principle Discovery from Analogous Judicial Precedents](https://arxiv.org/abs/2505.14104)
*Wei Fan,Tianshi Zheng,Yiran Hu,Zheye Deng,Weiqi Wang,Baixuan Xu,Chunyang Li,Haoran Li,Weixing Shen,Yangqiu Song*

Main category: cs.CL

TL;DR: 论文提出法律规则归纳（LRI）任务，构建首个LRI基准数据集，并验证大语言模型在从类似案例中提取隐含法律规则时的表现及改进方法。


<details>
  <summary>Details</summary>
Motivation: 现有计算法律研究多关注已知规则的应用，而从司法判决中归纳法律规则的研究不足，主要受限于模型推理效果和符号推理能力。大语言模型的出现为自动化提取隐含法律原则提供了新机遇，但缺乏正式任务定义、基准数据集和方法论阻碍了进展。

Method: 论文将法律规则归纳（LRI）任务形式化为从类似先例中提取简洁、可推广的教义规则，包括共同前提、规范行为和法律后果。构建了包含5,121个案例集（总计38,088个中国案例）的LRI基准数据集，其中216个为专家标注的黄金测试集。

Result: 实验结果表明：1）当前最先进的大语言模型存在过度泛化和幻觉问题；2）使用该数据集训练显著提升了模型在捕捉类似案例间细微规则模式的能力。

Conclusion: 通过形式化LRI任务并提供基准数据集，论文填补了该领域空白，并证明针对性训练能有效提升大语言模型的法律规则归纳能力。

Abstract: Legal rules encompass not only codified statutes but also implicit
adjudicatory principles derived from precedents that contain discretionary
norms, social morality, and policy. While computational legal research has
advanced in applying established rules to cases, inducing legal rules from
judicial decisions remains understudied, constrained by limitations in model
inference efficacy and symbolic reasoning capability. The advent of Large
Language Models (LLMs) offers unprecedented opportunities for automating the
extraction of such latent principles, yet progress is stymied by the absence of
formal task definitions, benchmark datasets, and methodologies. To address this
gap, we formalize Legal Rule Induction (LRI) as the task of deriving concise,
generalizable doctrinal rules from sets of analogous precedents, distilling
their shared preconditions, normative behaviors, and legal consequences. We
introduce the first LRI benchmark, comprising 5,121 case sets (38,088 Chinese
cases in total) for model tuning and 216 expert-annotated gold test sets.
Experimental results reveal that: 1) State-of-the-art LLMs struggle with
over-generalization and hallucination; 2) Training on our dataset markedly
enhances LLMs capabilities in capturing nuanced rule patterns across similar
cases.

</details>


### [55] [A Personalized Conversational Benchmark: Towards Simulating Personalized Conversations](https://arxiv.org/abs/2505.14106)
*Li Li,Peilin Cai,Ryan A. Rossi,Franck Dernoncourt,Branislav Kveton,Junda Wu,Tong Yu,Linxin Song,Tiankai Yang,Yuehan Qin,Nesreen K. Ahmed,Samyadeep Basu,Subhojyoti Mukherjee,Ruiyi Zhang,Zhengmian Hu,Bo Ni,Yuxiao Zhou,Zichao Wang,Yue Huang,Yu Wang,Xiangliang Zhang,Philip S. Yu,Xiyang Hu,Yue Zhao*

Main category: cs.CL

TL;DR: PersonaConvBench是一个大规模基准测试，用于评估多轮对话中个性化推理和生成，结合个性化和对话结构，显著提升LLM性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究多单独关注个性化或对话结构，缺乏两者结合的系统性评估。PersonaConvBench旨在填补这一空白，支持LLM在个性化、长期上下文跟踪和丰富响应生成方面的研究。

Method: 构建基于Reddit多领域的基准测试，包含句子分类、影响回归和用户中心文本生成三项核心任务，采用统一提示设置对商业和开源LLM进行测试。

Result: 引入个性化历史数据带来显著性能提升，情感分类任务中相对最佳非对话基线提升198%。

Conclusion: PersonaConvBench的发布为LLM适应个体风格、跟踪长期上下文及生成情境化响应提供了研究基础，代码与评估数据已开源。

Abstract: We present PersonaConvBench, a large-scale benchmark for evaluating
personalized reasoning and generation in multi-turn conversations with large
language models (LLMs). Unlike existing work that focuses on either
personalization or conversational structure in isolation, PersonaConvBench
integrates both, offering three core tasks: sentence classification, impact
regression, and user-centric text generation across ten diverse Reddit-based
domains. This design enables systematic analysis of how personalized
conversational context shapes LLM outputs in realistic multi-user scenarios. We
benchmark several commercial and open-source LLMs under a unified prompting
setup and observe that incorporating personalized history yields substantial
performance improvements, including a 198 percent relative gain over the best
non-conversational baseline in sentiment classification. By releasing
PersonaConvBench with evaluations and code, we aim to support research on LLMs
that adapt to individual styles, track long-term context, and produce
contextually rich, engaging responses.

</details>


### [56] [DiagnosisArena: Benchmarking Diagnostic Reasoning for Large Language Models](https://arxiv.org/abs/2505.14107)
*Yakun Zhu,Zhongzhen Huang,Linjie Mu,Yutong Huang,Wei Nie,Shaoting Zhang,Pengfei Liu,Xiaofan Zhang*

Main category: cs.CL

TL;DR: 论文提出DiagnosisArena基准测试，评估大语言模型在临床诊断推理中的表现，发现当前模型准确率不足，需进一步改进。


<details>
  <summary>Details</summary>
Motivation: 为安全有效地将大语言模型应用于现实医疗场景，需系统评估其诊断能力。现有医学基准在评估高级诊断推理方面存在局限。

Method: 构建DiagnosisArena基准，包含1,113对病例与诊断，覆盖28个医学专科，经AI和专家多轮筛选，防止数据泄露。

Result: 最先进模型o3-mini、o1和DeepSeek-R1的准确率仅45.82%、31.09%和17.79%，显示临床诊断推理存在泛化瓶颈。

Conclusion: DiagnosisArena旨在推动AI诊断推理能力发展，为解决现实临床诊断挑战提供更有效方案。

Abstract: The emergence of groundbreaking large language models capable of performing
complex reasoning tasks holds significant promise for addressing various
scientific challenges, including those arising in complex clinical scenarios.
To enable their safe and effective deployment in real-world healthcare
settings, it is urgently necessary to benchmark the diagnostic capabilities of
current models systematically. Given the limitations of existing medical
benchmarks in evaluating advanced diagnostic reasoning, we present
DiagnosisArena, a comprehensive and challenging benchmark designed to
rigorously assess professional-level diagnostic competence. DiagnosisArena
consists of 1,113 pairs of segmented patient cases and corresponding diagnoses,
spanning 28 medical specialties, deriving from clinical case reports published
in 10 top-tier medical journals. The benchmark is developed through a
meticulous construction pipeline, involving multiple rounds of screening and
review by both AI systems and human experts, with thorough checks conducted to
prevent data leakage. Our study reveals that even the most advanced reasoning
models, o3-mini, o1, and DeepSeek-R1, achieve only 45.82%, 31.09%, and 17.79%
accuracy, respectively. This finding highlights a significant generalization
bottleneck in current large language models when faced with clinical diagnostic
reasoning challenges. Through DiagnosisArena, we aim to drive further
advancements in AIs diagnostic reasoning capabilities, enabling more effective
solutions for real-world clinical diagnostic challenges. We provide the
benchmark and evaluation tools for further research and development
https://github.com/SPIRAL-MED/DiagnosisArena.

</details>


### [57] [Invisible Entropy: Towards Safe and Efficient Low-Entropy LLM Watermarking](https://arxiv.org/abs/2505.14112)
*Tianle Gu,Zongqi Wang,Kexin Huang,Yuanqi Yao,Xiangliang Zhang,Yujiu Yang,Xiuying Chen*

Main category: cs.CL

TL;DR: 提出了一种名为Invisible Entropy (IE)的新型水印方法，通过轻量级特征提取器和熵标记器预测高低熵令牌，解决了现有方法在低熵场景下的问题，同时提高了安全性和效率。


<details>
  <summary>Details</summary>
Motivation: 现有的基于Logit的LLM水印方法在低熵场景下表现不佳，且依赖原始LLM计算熵，导致高计算成本和潜在模型泄露风险。

Method: 引入轻量级特征提取器和熵标记器预测令牌熵高低，并开发自适应熵阈值导航器，优化水印文本的自然性和检测鲁棒性。

Result: 在HumanEval和MBPP数据集上的实验表明，IE将参数大小减少了99%，性能与最先进方法相当。

Conclusion: IE为低熵水印提供了一种安全高效的范式，显著提升了水印技术的实用性和检测效果。

Abstract: Logit-based LLM watermarking traces and verifies AI-generated content by
maintaining green and red token lists and increasing the likelihood of green
tokens during generation. However, it fails in low-entropy scenarios, where
predictable outputs make green token selection difficult without disrupting
natural text flow. Existing approaches address this by assuming access to the
original LLM to calculate entropy and selectively watermark high-entropy
tokens. However, these methods face two major challenges: (1) high
computational costs and detection delays due to reliance on the original LLM,
and (2) potential risks of model leakage. To address these limitations, we
propose Invisible Entropy (IE), a watermarking paradigm designed to enhance
both safety and efficiency. Instead of relying on the original LLM, IE
introduces a lightweight feature extractor and an entropy tagger to predict
whether the entropy of the next token is high or low. Furthermore, based on
theoretical analysis, we develop a threshold navigator that adaptively sets
entropy thresholds. It identifies a threshold where the watermark ratio
decreases as the green token count increases, enhancing the naturalness of the
watermarked text and improving detection robustness. Experiments on HumanEval
and MBPP datasets demonstrate that IE reduces parameter size by 99\% while
achieving performance on par with state-of-the-art methods. Our work introduces
a safe and efficient paradigm for low-entropy watermarking.
https://github.com/Carol-gutianle/IE
https://huggingface.co/datasets/Carol0110/IE-Tagger

</details>


### [58] [Self-Reasoning Language Models: Unfold Hidden Reasoning Chains with Few Reasoning Catalyst](https://arxiv.org/abs/2505.14116)
*Hongru Wang,Deng Cai,Wanjun Zhong,Shijue Huang,Jeff Z. Pan,Zeming Liu,Kam-Fai Wong*

Main category: cs.CL

TL;DR: 论文提出自推理语言模型(SRLM)，通过自我训练生成更长思维链数据，提升大语言模型在复杂推理任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖人工构建长思维链数据，难以获取且成本高。SRLM旨在让模型自主生成推理链，实现性能迭代提升。

Method: SRLM通过少量示范样本(如1000条)学习展开隐含推理链，作为推理催化剂，进行自我训练和迭代优化。

Result: 在五个推理任务上平均提升2.5分，64次采样时提升达7.89分，显示出更深入、多样化的推理能力。

Conclusion: SRLM能自主生成高质量推理链，显著提升模型性能，且采样次数越多效果越好。

Abstract: Inference-time scaling has attracted much attention which significantly
enhance the performance of Large Language Models (LLMs) in complex reasoning
tasks by increasing the length of Chain-of-Thought. These longer intermediate
reasoning rationales embody various meta-reasoning skills in human cognition,
such as reflection and decomposition, being difficult to create and acquire. In
this work, we introduce \textit{Self-Reasoning Language Model} (SRLM), where
the model itself can synthesize longer CoT data and iteratively improve
performance through self-training. By incorporating a few demonstration
examples (i.e., 1,000 samples) on how to unfold hidden reasoning chains from
existing responses, which act as a reasoning catalyst, we demonstrate that SRLM
not only enhances the model's initial performance but also ensures more stable
and consistent improvements in subsequent iterations. Our proposed SRLM
achieves an average absolute improvement of more than $+2.5$ points across five
reasoning tasks: MMLU, GSM8K, ARC-C, HellaSwag, and BBH on two backbone models.
Moreover, it brings more improvements with more times of sampling during
inference, such as absolute $+7.89$ average improvement with $64$ sampling
times, revealing the in-depth, diverse and creative reasoning paths in SRLM
against the strong baseline.

</details>


### [59] [Probing BERT for German Compound Semantics](https://arxiv.org/abs/2505.14130)
*Filip Miletić,Aaron Schmid,Sabine Schulte im Walde*

Main category: cs.CL

TL;DR: 该研究探讨了预训练德语BERT模型对名词复合词语义的编码能力，发现其表现不及英语BERT，可能与德语复合词的高产性和歧义性有关。


<details>
  <summary>Details</summary>
Motivation: 研究旨在评估德语BERT模型在编码名词复合词语义方面的能力，并与英语BERT的表现进行对比，以探索语言特性对模型性能的影响。

Method: 通过系统地改变目标词、模型层及大小写设置，利用868个标准复合词评估模型预测复合词组合性的能力。

Result: 研究发现，德语BERT在早期层能较好捕捉复合词组合性信息，但整体表现明显逊于英语BERT，表明德语复合词处理更具挑战性。

Conclusion: 德语复合词的高产性和成分歧义性增加了语义编码的难度，导致德语BERT的表现不及英语BERT。

Abstract: This paper investigates the extent to which pretrained German BERT encodes
knowledge of noun compound semantics. We comprehensively vary combinations of
target tokens, layers, and cased vs. uncased models, and evaluate them by
predicting the compositionality of 868 gold standard compounds. Looking at
representational patterns within the transformer architecture, we observe
trends comparable to equivalent prior work on English, with compositionality
information most easily recoverable in the early layers. However, our strongest
results clearly lag behind those reported for English, suggesting an inherently
more difficult task in German. This may be due to the higher productivity of
compounding in German than in English and the associated increase in
constituent-level ambiguity, including in our target compound set.

</details>


### [60] [Texts or Images? A Fine-grained Analysis on the Effectiveness of Input Representations and Models for Table Question Answering](https://arxiv.org/abs/2505.14131)
*Wei Zhou,Mohsen Mesgar,Heike Adel,Annemarie Friedrich*

Main category: cs.CL

TL;DR: 本文通过控制实验比较了表格问答中文本与图像输入的优劣，提出动态选择表示方法FRES，性能提升10%。


<details>
  <summary>Details</summary>
Motivation: 现有研究认为多模态大模型处理表格图像效果不亚于文本输入，但缺乏控制实验验证不同场景下的优劣。

Method: 构建新基准数据集，系统分析7组模型组合，提出动态表示选择方法FRES。

Result: 最佳输入方式因场景而异，FRES方法平均性能提升10%。

Conclusion: 表格表示方式需动态选择，FRES方法显著提升表格问答性能。

Abstract: In table question answering (TQA), tables are encoded as either texts or
images. Prior work suggests that passing images of tables to multi-modal large
language models (MLLMs) performs comparably to or even better than using
textual input with large language models (LLMs). However, the lack of
controlled setups limits fine-grained distinctions between these approaches. In
this paper, we conduct the first controlled study on the effectiveness of
several combinations of table representations and models from two perspectives:
question complexity and table size. We build a new benchmark based on existing
TQA datasets. In a systematic analysis of seven pairs of MLLMs and LLMs, we
find that the best combination of table representation and model varies across
setups. We propose FRES, a method selecting table representations dynamically,
and observe a 10% average performance improvement compared to using both
representations indiscriminately.

</details>


### [61] [Enhancing Keyphrase Extraction from Academic Articles Using Section Structure Information](https://arxiv.org/abs/2505.14149)
*Chengzhi Zhang,Xinyi Yan,Lei Zhao,Yingyi Zhang*

Main category: cs.CL

TL;DR: 该论文提出利用学术文章的结构特征和章节文本改进关键词提取（KPE）性能，通过整合不同章节的提取结果和探索结构特征的影响，显著提升了KPE效果。


<details>
  <summary>Details</summary>
Motivation: 学术论文数量的激增增加了研究者查找相关文献的时间，现有基于标题和摘要的关键词提取方法受限于摘要长度，而全文提取又引入噪声。因此，需要一种更有效的方法来提取关键词。

Method: 论文采用两部分方法：(1) 探索七种结构特征对KPE模型的影响，(2) 通过关键词整合算法整合所有章节文本的提取结果。同时研究了章节结构分类质量对KPE性能的影响。

Result: 实验表明，结构特征的引入提升了KPE性能，不同特征对模型效果影响各异。关键词整合方法表现最佳，章节结构分类质量也会影响KPE性能。

Conclusion: 利用学术文章的章节结构信息可以有效提升关键词提取性能，结构特征和整合方法对KPE有显著改进作用。

Abstract: The exponential increase in academic papers has significantly increased the
time required for researchers to access relevant literature. Keyphrase
Extraction (KPE) offers a solution to this situation by enabling researchers to
efficiently retrieve relevant literature. The current study on KPE from
academic articles aims to improve the performance of extraction models through
innovative approaches using Title and Abstract as input corpora. However, the
semantic richness of keywords is significantly constrained by the length of the
abstract. While full-text-based KPE can address this issue, it simultaneously
introduces noise, which significantly diminishes KPE performance. To address
this issue, this paper utilized the structural features and section texts
obtained from the section structure information of academic articles to extract
keyphrase from academic papers. The approach consists of two main parts: (1)
exploring the effect of seven structural features on KPE models, and (2)
integrating the extraction results from all section texts used as input corpora
for KPE models via a keyphrase integration algorithm to obtain the keyphrase
integration result. Furthermore, this paper also examined the effect of the
classification quality of section structure on the KPE performance. The results
show that incorporating structural features improves KPE performance, though
different features have varying effects on model efficacy. The keyphrase
integration approach yields the best performance, and the classification
quality of section structure can affect KPE performance. These findings
indicate that using the section structure information of academic articles
contributes to effective KPE from academic articles. The code and dataset
supporting this study are available at https://github.com/yan-xinyi/SSB_KPE.

</details>


### [62] [Prior Prompt Engineering for Reinforcement Fine-Tuning](https://arxiv.org/abs/2505.14157)
*Pittawat Taveekitworachai,Potsawee Manakul,Sarana Nutanong,Kunat Pipatanakul*

Main category: cs.CL

TL;DR: 该论文研究了在强化微调（RFT）中先验提示工程（pPE）的作用，通过实验证明不同的pPE方法能引导语言模型内化不同行为，其中空示例pPE方法表现最佳。


<details>
  <summary>Details</summary>
Motivation: 现有RFT研究主要关注算法、奖励塑造和数据筛选，而先验提示设计在训练中引导模型行为的作用尚未充分探索。本文旨在探索不同pPE方法是否能引导语言模型在RFT后内化不同行为。

Method: 受推理时提示工程（iPE）启发，将五种代表性iPE策略（推理、规划、基于代码的推理、知识回忆和空示例利用）转化为对应的pPE方法，并在Qwen2.5-7B模型上进行实验，评估其在领域内和领域外基准测试中的表现。

Result: 所有经过pPE训练的模型均优于iPE提示的模型，其中空示例pPE方法平均性能提升最大，在AIME2024和GPQA-Diamond上表现最佳，超过了常用的推理方法。不同pPE策略在模型中注入了不同的行为风格。

Conclusion: pPE是RFT中一个强大但未被充分研究的维度，能够有效引导语言模型内化特定行为，提升性能。

Abstract: This paper investigates prior prompt engineering (pPE) in the context of
reinforcement fine-tuning (RFT), where language models (LMs) are incentivized
to exhibit behaviors that maximize performance through reward signals. While
existing RFT research has primarily focused on algorithms, reward shaping, and
data curation, the design of the prior prompt--the instructions prepended to
queries during training to elicit behaviors such as step-by-step
reasoning--remains underexplored. We investigate whether different pPE
approaches can guide LMs to internalize distinct behaviors after RFT. Inspired
by inference-time prompt engineering (iPE), we translate five representative
iPE strategies--reasoning, planning, code-based reasoning, knowledge recall,
and null-example utilization--into corresponding pPE approaches. We experiment
with Qwen2.5-7B using each of the pPE approaches, then evaluate performance on
in-domain and out-of-domain benchmarks (e.g., AIME2024, HumanEval+, and
GPQA-Diamond). Our results show that all pPE-trained models surpass their
iPE-prompted counterparts, with the null-example pPE approach achieving the
largest average performance gain and the highest improvement on AIME2024 and
GPQA-Diamond, surpassing the commonly used reasoning approach. Furthermore, by
adapting a behavior-classification framework, we demonstrate that different pPE
strategies instill distinct behavioral styles in the resulting models. These
findings position pPE as a powerful yet understudied axis for RFT.

</details>


### [63] [Temporal Alignment of Time Sensitive Facts with Activation Engineering](https://arxiv.org/abs/2505.14158)
*Sanjay Govindan,Maurice Pagnucco,Yang Song*

Main category: cs.CL

TL;DR: 该论文探索了通过激活工程技术提升LLMs时间对齐能力，无需训练或数据集创建，显著提高了事实回忆准确性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）训练数据跨越多个领域和时间段，部分知识仅在特定时间范围内有效。确保LLMs生成时间适当的响应对于保持相关性和准确性至关重要。

Method: 研究采用激活工程技术，将三个版本的LLaMA 2模型锚定到特定时间点，并考察不同注入层和提示策略的效果。

Result: 实验显示，相对提示和显式提示分别提高了44%和16%，性能与微调方法相当，但计算效率更高且无需预对齐数据集。

Conclusion: 激活工程技术能有效提升LLMs的时间对齐能力，无需复杂训练或数据集，为时间敏感型任务提供了高效解决方案。

Abstract: Large Language Models (LLMs) are trained on diverse and often conflicting
knowledge spanning multiple domains and time periods. Some of this knowledge is
only valid within specific temporal contexts, such as answering the question,
"Who is the President of the United States in 2022?" Ensuring LLMs generate
time appropriate responses is crucial for maintaining relevance and accuracy.
In this work we explore activation engineering as a method for temporally
aligning LLMs to improve factual recall without any training or dataset
creation. In this research we explore an activation engineering technique to
ground three versions of LLaMA 2 to specific points in time and examine the
effects of varying injection layers and prompting strategies. Our experiments
demonstrate up to a 44% and 16% improvement in relative and explicit prompting
respectively, achieving comparable performance to the fine-tuning method
proposed by Zhao et al. (2024) . Notably, our approach achieves similar results
to the fine-tuning baseline while being significantly more computationally
efficient and requiring no pre-aligned datasets.

</details>


### [64] [Breaking Language Barriers or Reinforcing Bias? A Study of Gender and Racial Disparities in Multilingual Contrastive Vision Language Models](https://arxiv.org/abs/2505.14160)
*Zahraa Al Sahili,Ioannis Patras,Matthew Purver*

Main category: cs.CL

TL;DR: 研究发现多语言视觉语言模型存在性别和种族偏见，且多语言性并未减轻偏见，反而在某些情况下加剧了偏见。


<details>
  <summary>Details</summary>
Motivation: 尽管多语言视觉语言模型在图像-文本检索方面表现出色，但其社会偏见尚未得到充分研究。本文旨在系统评估这些模型在不同语言中的偏见表现。

Method: 研究使用FairFace和PATA数据集，在零样本设置下评估了三种多语言CLIP模型（M-CLIP、NLLB-CLIP和CAPIVARA-CLIP）在十种语言中的性别和种族偏见。

Result: 所有模型在多语言环境下的性别偏见均强于其英语基线，尤其是低资源语言和目标语言中偏见更为显著。共享跨语言编码器会将英语的性别偏见传播到性别中立的语言中。

Conclusion: 多语言性并未减轻模型的偏见，反而可能加剧偏见。未来研究需要更细粒度、语言感知的偏见评估方法。

Abstract: Multilingual vision-language models promise universal image-text retrieval,
yet their social biases remain under-explored. We present the first systematic
audit of three public multilingual CLIP checkpoints -- M-CLIP, NLLB-CLIP, and
CAPIVARA-CLIP -- across ten languages that vary in resource availability and
grammatical gender. Using balanced subsets of \textsc{FairFace} and the
\textsc{PATA} stereotype suite in a zero-shot setting, we quantify race and
gender bias and measure stereotype amplification. Contrary to the assumption
that multilinguality mitigates bias, every model exhibits stronger gender bias
than its English-only baseline. CAPIVARA-CLIP shows its largest biases
precisely in the low-resource languages it targets, while the shared
cross-lingual encoder of NLLB-CLIP transports English gender stereotypes into
gender-neutral languages; loosely coupled encoders largely avoid this transfer.
Highly gendered languages consistently magnify all measured bias types, but
even gender-neutral languages remain vulnerable when cross-lingual weight
sharing imports foreign stereotypes. Aggregated metrics conceal
language-specific ``hot spots,'' underscoring the need for fine-grained,
language-aware bias evaluation in future multilingual vision-language research.

</details>


### [65] [PL-FGSA: A Prompt Learning Framework for Fine-Grained Sentiment Analysis Based on MindSpore](https://arxiv.org/abs/2505.14165)
*Zhenkai Qin,Jiajing He,Qiao Fang*

Main category: cs.CL

TL;DR: PL-FGSA是一种基于提示学习的细粒度情感分析框架，通过多任务提示增强生成方法，在多个数据集上表现优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统细粒度情感分析方法需要特定任务架构和大量标注数据，限制了其泛化能力和扩展性。

Method: 提出PL-FGSA框架，结合提示设计和轻量级TextCNN主干，将FGSA重新定义为多任务提示增强生成问题。

Result: 在SST-2、SemEval-2014 Task 4和MAMS数据集上的F1分数分别为0.922、0.694和0.597。

Conclusion: PL-FGSA通过提示学习提高了泛化能力和可解释性，适用于实际情感分析任务。

Abstract: Fine-grained sentiment analysis (FGSA) aims to identify sentiment polarity
toward specific aspects within a text, enabling more precise opinion mining in
domains such as product reviews and social media. However, traditional FGSA
approaches often require task-specific architectures and extensive annotated
data, limiting their generalization and scalability. To address these
challenges, we propose PL-FGSA, a unified prompt learning-based framework
implemented using the MindSpore platform, which integrates prompt design with a
lightweight TextCNN backbone. Our method reformulates FGSA as a multi-task
prompt-augmented generation problem, jointly tackling aspect extraction,
sentiment classification, and causal explanation in a unified paradigm. By
leveraging prompt-based guidance, PL-FGSA enhances interpretability and
achieves strong performance under both full-data and low-resource conditions.
Experiments on three benchmark datasets-SST-2, SemEval-2014 Task 4, and
MAMS-demonstrate that our model consistently outperforms traditional
fine-tuning methods and achieves F1-scores of 0.922, 0.694, and 0.597,
respectively. These results validate the effectiveness of prompt-based
generalization and highlight the practical value of PL-FGSA for real-world
sentiment analysis tasks.

</details>


### [66] [The Strawberry Problem: Emergence of Character-level Understanding in Tokenized Language Models](https://arxiv.org/abs/2505.14172)
*Adrian Cosma,Stefan Ruseti,Emilian Radoi,Mihai Dascalu*

Main category: cs.CL

TL;DR: 大语言模型在字符级任务上表现不佳，研究通过概念涌现理论分析并提出轻量级架构改进。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在多领域表现优异，但在字符级任务（如字母计数）上存在明显不足，主要受限于分词机制。本研究旨在分析并解决这一瓶颈问题。

Method: 通过19项合成任务隔离字符级推理，使用基于渗透的概念涌现模型分析，并提出轻量级架构改进方案。

Result: 研究表明字符组合能力的涌现缓慢且突然，改进后的架构显著提升字符级推理能力，同时保留子词模型的归纳优势。

Conclusion: 研究弥合了分词语言模型的低级感知缺陷，为理解和缓解其结构性盲点提供了理论框架。

Abstract: Despite their remarkable progress across diverse domains, Large Language
Models (LLMs) consistently fail at simple character-level tasks, such as
counting letters in words, due to a fundamental limitation: tokenization. In
this work, we frame this limitation as a problem of low mutual information and
analyze it in terms of concept emergence. Using a suite of 19 synthetic tasks
that isolate character-level reasoning in a controlled setting, we show that
such capabilities emerge slowly, suddenly, and only late in training. We
further show that percolation-based models of concept emergence explain these
patterns, suggesting that learning character composition is not fundamentally
different from learning commonsense knowledge. To address this bottleneck, we
propose a lightweight architectural modification that significantly improves
character-level reasoning while preserving the inductive advantages of subword
models. Together, our results bridge low-level perceptual gaps in tokenized LMs
and provide a principled framework for understanding and mitigating their
structural blind spots. We make our code publicly available.

</details>


### [67] [THOR-MoE: Hierarchical Task-Guided and Context-Responsive Routing for Neural Machine Translation](https://arxiv.org/abs/2505.14173)
*Yunlong Liang,Fandong Meng,Jie Zhou*

Main category: cs.CL

TL;DR: THOR-MoE提出了一种结合任务引导和上下文感知路由策略的稀疏混合专家模型，显著提升了机器翻译性能。


<details>
  <summary>Details</summary>
Motivation: 当前稀疏混合专家模型在机器翻译中存在两个问题：1) 直接使用任务知识（如领域/语言学知识），但这些知识在实际应用中通常不可得；2) 专家选择仅依赖局部标记表示，忽略了全局上下文信息。

Method: THOR-MoE通过分层任务引导和上下文感知路由策略改进MoE：1) 预测领域/语言标签并提取混合表示以分配任务级专家；2) 注入上下文信息增强标记路由，从预选专家集中选择更合适的专家。

Result: 在多领域和多语言翻译基准测试中，THOR-MoE表现优异，平均提升0.75 BLEU分数，且激活参数少于22%。

Conclusion: THOR-MoE作为一种即插即用模块，兼容现有路由方案，显著提升了机器翻译性能，具有广泛适用性。

Abstract: The sparse Mixture-of-Experts (MoE) has achieved significant progress for
neural machine translation (NMT). However, there exist two limitations in
current MoE solutions which may lead to sub-optimal performance: 1) they
directly use the task knowledge of NMT into MoE (\emph{e.g.},
domain/linguistics-specific knowledge), which are generally unavailable at
practical application and neglect the naturally grouped domain/linguistic
properties; 2) the expert selection only depends on the localized token
representation without considering the context, which fully grasps the state of
each token in a global view. To address the above limitations, we propose
THOR-MoE via arming the MoE with hierarchical task-guided and
context-responsive routing policies. Specifically, it 1) firstly predicts the
domain/language label and then extracts mixed domain/language representation to
allocate task-level experts in a hierarchical manner; 2) injects the context
information to enhance the token routing from the pre-selected task-level
experts set, which can help each token to be accurately routed to more
specialized and suitable experts. Extensive experiments on multi-domain
translation and multilingual translation benchmarks with different
architectures consistently demonstrate the superior performance of THOR-MoE.
Additionally, the THOR-MoE operates as a plug-and-play module compatible with
existing Top-$k$~\cite{shazeer2017} and Top-$p$~\cite{huang-etal-2024-harder}
routing schemes, ensuring broad applicability across diverse MoE architectures.
For instance, compared with vanilla Top-$p$~\cite{huang-etal-2024-harder}
routing, the context-aware manner can achieve an average improvement of 0.75
BLEU with less than 22\% activated parameters on multi-domain translation
tasks.

</details>


### [68] [Cheaper, Better, Faster, Stronger: Robust Text-to-SQL without Chain-of-Thought or Fine-Tuning](https://arxiv.org/abs/2505.14174)
*Yusuf Denizay Dönder,Derek Hommel,Andrea W Wen-Yi,David Mimno,Unso Eun Seo Jo*

Main category: cs.CL

TL;DR: 论文提出了一种名为N-rep的低成本文本到SQL方法，其性能与昂贵方法相当但成本仅为每次查询0.039美元。


<details>
  <summary>Details</summary>
Motivation: 当前最先进的文本到SQL方法（如Chain-of-Thought、自洽和微调）在推理时成本高昂，每次查询可能花费高达0.46美元，而微调模型可能需要数千美元。

Method: N-rep利用同一模式输入的多种表示来弥补单一表示的弱点，从而在不使用推理或微调的情况下，使用更小、更便宜的模型实现鲁棒性。

Result: N-rep在BIRD基准测试中取得了与其他昂贵方法相似的分数，而每次查询的成本仅为0.039美元。

Conclusion: N-rep是当前成本范围内性能最佳的文本到SQL方法，显著降低了成本同时保持了高性能。

Abstract: LLMs are effective at code generation tasks like text-to-SQL, but is it worth
the cost? Many state-of-the-art approaches use non-task-specific LLM techniques
including Chain-of-Thought (CoT), self-consistency, and fine-tuning. These
methods can be costly at inference time, sometimes requiring over a hundred LLM
calls with reasoning, incurring average costs of up to \$0.46 per query, while
fine-tuning models can cost thousands of dollars. We introduce "N-rep"
consistency, a more cost-efficient text-to-SQL approach that achieves similar
BIRD benchmark scores as other more expensive methods, at only \$0.039 per
query. N-rep leverages multiple representations of the same schema input to
mitigate weaknesses in any single representation, making the solution more
robust and allowing the use of smaller and cheaper models without any reasoning
or fine-tuning. To our knowledge, N-rep is the best-performing text-to-SQL
approach in its cost range.

</details>


### [69] [Tokenization Constraints in LLMs: A Study of Symbolic and Arithmetic Reasoning Limits](https://arxiv.org/abs/2505.14178)
*Xiang Zhang,Juntai Cao,Jiaqi Wei,Yiwei Xu,Chenyu You*

Main category: cs.CL

TL;DR: 该论文探讨了分词方案（如BPE）如何影响语言模型的符号推理能力，提出了'Token Awareness'概念，并通过实验证明原子对齐的分词格式能显著提升小模型在结构化推理任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于揭示分词作为语言模型计算的第一层，其结构（尤其是子词分词方法）可能通过模糊原子推理单元来限制模型的符号计算能力，进而影响思维链（CoT）提示的效果。

Method: 通过理论分析和系统实验（算术与符号任务），对比不同分词结构对推理性能的影响，并引入'Token Awareness'概念量化分词粒度与逻辑对齐的关系。

Result: 实验表明：1）不当分词会导致CoT推理失败；2）原子对齐的分词格式使小模型（如GPT-4o-mini）在结构化推理上超越大模型（如o1）。

Conclusion: 语言模型的符号推理能力不仅取决于架构，更依赖于分词层面的表征质量。优化分词粒度可成为提升小模型推理性能的新途径。

Abstract: Tokenization is the first - and often underappreciated - layer of computation
in language models. While Chain-of-Thought (CoT) prompting enables transformer
models to approximate recurrent computation by externalizing intermediate
steps, we show that the success of such reasoning is fundamentally bounded by
the structure of tokenized inputs. This work presents a theoretical and
empirical investigation into how tokenization schemes, particularly
subword-based methods like byte-pair encoding (BPE), impede symbolic
computation by merging or obscuring atomic reasoning units. We introduce the
notion of Token Awareness to formalize how poor token granularity disrupts
logical alignment and prevents models from generalizing symbolic procedures.
Through systematic evaluation on arithmetic and symbolic tasks, we demonstrate
that token structure dramatically affect reasoning performance, causing failure
even with CoT, while atomically-aligned formats unlock strong generalization,
allowing small models (e.g., GPT-4o-mini) to outperform larger systems (e.g.,
o1) in structured reasoning. Our findings reveal that symbolic reasoning
ability in LLMs is not purely architectural, but deeply conditioned on
token-level representations.

</details>


### [70] [Enhancing Abstractive Summarization of Scientific Papers Using Structure Information](https://arxiv.org/abs/2505.14179)
*Tong Bao,Heng Zhang,Chengzhi Zhang*

Main category: cs.CL

TL;DR: 该论文提出了一种两阶段摘要生成框架，通过自动识别科学论文的结构功能来生成更全面的摘要。


<details>
  <summary>Details</summary>
Motivation: 现有的摘要生成方法难以充分捕捉科学论文中的结构化信息，且缺乏跨学科的鲁棒性。

Method: 采用两阶段框架：第一阶段通过标准化章节标题构建数据集并训练分类器识别结构功能；第二阶段使用Longformer模型生成上下文感知的摘要。

Result: 在两个领域特定的科学论文摘要数据集上，该方法优于先进基线，生成更全面的摘要。

Conclusion: 该方法通过自动识别结构功能和利用上下文关系，显著提升了科学论文摘要的质量和全面性。

Abstract: Abstractive summarization of scientific papers has always been a research
focus, yet existing methods face two main challenges. First, most summarization
models rely on Encoder-Decoder architectures that treat papers as sequences of
words, thus fail to fully capture the structured information inherent in
scientific papers. Second, existing research often use keyword mapping or
feature engineering to identify the structural information, but these methods
struggle with the structural flexibility of scientific papers and lack
robustness across different disciplines. To address these challenges, we
propose a two-stage abstractive summarization framework that leverages
automatic recognition of structural functions within scientific papers. In the
first stage, we standardize chapter titles from numerous scientific papers and
construct a large-scale dataset for structural function recognition. A
classifier is then trained to automatically identify the key structural
components (e.g., Background, Methods, Results, Discussion), which provides a
foundation for generating more balanced summaries. In the second stage, we
employ Longformer to capture rich contextual relationships across sections and
generating context-aware summaries. Experiments conducted on two
domain-specific scientific paper summarization datasets demonstrate that our
method outperforms advanced baselines, and generates more comprehensive
summaries. The code and dataset can be accessed at
https://github.com/tongbao96/code-for-SFR-AS.

</details>


### [71] [SlangDIT: Benchmarking LLMs in Interpretative Slang Translation](https://arxiv.org/abs/2505.14181)
*Yunlong Liang,Fandong Meng,Jiaan Wang,Jie Zhou*

Main category: cs.CL

TL;DR: 该论文提出了一个包含俚语检测、跨语言解释和翻译的综合任务SlangDIT，并构建了相应数据集，通过深度思考模型SlangOWL显著提升了大语言模型的俚语翻译性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究将俚语检测、解释和翻译视为独立任务，忽略了它们之间的内在关联性，且缺乏支持三者协同的基准数据集，导致俚语翻译准确性不足。

Method: 构建包含2.5万条英中句对的SlangDIT数据集，提出分步推理模型SlangOWL：先检测俚语并分析多义性，再生成上下文相关的解释，最终输出翻译结果。

Result: 实验表明SlangOWL在Qwen2.5等大模型上显著超越原始模型和未经思维链调优的监督微调模型，验证了深度思考机制的有效性。

Conclusion: 通过联合建模俚语处理的三个子任务并引入推理机制，能显著提升翻译质量，为语境敏感的语义扩展问题提供了解决方案。

Abstract: The challenge of slang translation lies in capturing context-dependent
semantic extensions, as slang terms often convey meanings beyond their literal
interpretation. While slang detection, explanation, and translation have been
studied as isolated tasks in the era of large language models (LLMs), their
intrinsic interdependence remains underexplored. The main reason is lacking of
a benchmark where the two tasks can be a prerequisite for the third one, which
can facilitate idiomatic translation. In this paper, we introduce the
interpretative slang translation task (named SlangDIT) consisting of three
sub-tasks: slang detection, cross-lingual slang explanation, and slang
translation within the current context, aiming to generate more accurate
translation with the help of slang detection and slang explanation. To this
end, we construct a SlangDIT dataset, containing over 25k English-Chinese
sentence pairs. Each source sentence mentions at least one slang term and is
labeled with corresponding cross-lingual slang explanation. Based on the
benchmark, we propose a deep thinking model, named SlangOWL. It firstly
identifies whether the sentence contains a slang, and then judges whether the
slang is polysemous and analyze its possible meaning. Further, the SlangOWL
provides the best explanation of the slang term targeting on the current
context. Finally, according to the whole thought, the SlangOWL offers a
suitable translation. Our experiments on LLMs (\emph{e.g.}, Qwen2.5 and
LLama-3.1), show that our deep thinking approach indeed enhances the
performance of LLMs where the proposed SLangOWL significantly surpasses the
vanilla models and supervised fine-tuned models without thinking.

</details>


### [72] [ThinkSwitcher: When to Think Hard, When to Think Fast](https://arxiv.org/abs/2505.14183)
*Guosheng Liang,Longguang Zhong,Ziyi Yang,Xiaojun Quan*

Main category: cs.CL

TL;DR: 论文提出ThinkSwitcher框架，通过动态切换长短推理链模式，在保持复杂任务精度的同时降低20-30%计算成本。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型（LRMs）在处理复杂任务时依赖长推理链，但在简单任务上会产生不必要的计算开销。研究发现LRMs本身具备短推理链能力，需通过提示设计激发。

Method: 提出ThinkSwitcher框架，包含轻量级切换模块，根据任务复杂度动态选择短/长推理链模式，模块训练信号来自不同模式下任务的相对性能。

Result: 在多个推理基准测试中，ThinkSwitcher在保持复杂任务高精度的同时，减少20-30%计算开销。

Conclusion: ThinkSwitcher证明了统一部署LRMs时动态切换推理模式的高效性和可扩展性。

Abstract: Large reasoning models (LRMs) excel at solving complex tasks by leveraging
long chain-of-thought (CoT) reasoning. However, this often leads to
overthinking on simple tasks, resulting in unnecessary computational overhead.
We observe that LRMs inherently possess the capability for efficient short CoT
reasoning, which can be reliably elicited through prompt design. To leverage
this capability, we propose ThinkSwitcher, a framework that enables a single
LRM to dynamically switch between short and long CoT modes based on task
complexity. ThinkSwitcher introduces a lightweight switching module trained
with supervision signals derived from the relative performance of each
reasoning mode across tasks. Experiments on multiple reasoning benchmarks show
that ThinkSwitcher reduces computational cost by 20-30% while maintaining high
accuracy on complex tasks. This demonstrates the effectiveness of ThinkSwitcher
as a scalable and efficient solution for unified LRM deployment.

</details>


### [73] [Unraveling Interwoven Roles of Large Language Models in Authorship Privacy: Obfuscation, Mimicking, and Verification](https://arxiv.org/abs/2505.14195)
*Tuc Nguyen,Yifan Hu,Thai Le*

Main category: cs.CL

TL;DR: 该论文提出了首个统一框架，分析大语言模型（LLMs）在作者隐私中的动态关系，包括作者混淆、模仿和验证，并探讨了人口统计元数据的作用。


<details>
  <summary>Details</summary>
Motivation: 大语言模型的训练数据常包含用户隐私信息，可能导致模型无意中泄露作者身份。现有研究独立探讨了作者隐私的三个任务（混淆、模仿、验证），但缺乏对它们互动关系的深入分析，尤其是在LLMs时代，机器生成与人类创作文本的界限日益模糊。

Method: 论文提出了一个统一框架，量化分析LLMs支持的作者混淆、模仿和验证任务之间的动态关系，研究它们如何随时间迭代改变人类创作文本，并考察人口统计元数据（如性别、学术背景）对任务性能和隐私风险的影响。

Result: 研究发现LLMs在作者隐私任务中存在复杂的互动关系，人口统计元数据显著影响任务动态和隐私风险。所有源代码将公开。

Conclusion: 该研究填补了作者隐私任务互动关系的研究空白，为LLMs时代的隐私保护提供了新视角，强调了统一分析框架的重要性。

Abstract: Recent advancements in large language models (LLMs) have been fueled by large
scale training corpora drawn from diverse sources such as websites, news
articles, and books. These datasets often contain explicit user information,
such as person names and addresses, that LLMs may unintentionally reproduce in
their generated outputs. Beyond such explicit content, LLMs can also leak
identity revealing cues through implicit signals such as distinctive writing
styles, raising significant concerns about authorship privacy. There are three
major automated tasks in authorship privacy, namely authorship obfuscation
(AO), authorship mimicking (AM), and authorship verification (AV). Prior
research has studied AO, AM, and AV independently. However, their interplays
remain under explored, which leaves a major research gap, especially in the era
of LLMs, where they are profoundly shaping how we curate and share user
generated content, and the distinction between machine generated and human
authored text is also increasingly blurred. This work then presents the first
unified framework for analyzing the dynamic relationships among LLM enabled AO,
AM, and AV in the context of authorship privacy. We quantify how they interact
with each other to transform human authored text, examining effects at a single
point in time and iteratively over time. We also examine the role of
demographic metadata, such as gender, academic background, in modulating their
performances, inter-task dynamics, and privacy risks. All source code will be
publicly available.

</details>


### [74] [Automatic Dataset Generation for Knowledge Intensive Question Answering Tasks](https://arxiv.org/abs/2505.14212)
*Sizhe Yuen,Ting Su,Ziyang Wang,Yali Du,Adam J. Sobey*

Main category: cs.CL

TL;DR: 论文提出了一种通过自动生成上下文相关的QA对来增强大语言模型在知识密集型问答任务中表现的新方法，减少了人工标注依赖并提升了推理能力。


<details>
  <summary>Details</summary>
Motivation: 当前问答系统在处理需要复杂推理或实时知识整合的查询时表现不佳，即使结合检索增强生成技术（RAG）仍面临多源信息逻辑关联的挑战。

Method: 采用自动化QA生成器创建微调数据，结合模型微调器，使用困惑度、ROUGE、BLEU和BERTScore等指标进行评估。

Result: 实验表明该方法提升了逻辑连贯性和事实准确性，其中Mistral-7b-v0.3模型在生成QA对的BERT F1、BLEU和ROUGE分数上均优于Llama-3-8b模型。

Conclusion: 该方法为开发适应性强的AI系统提供了新思路，证明自动生成QA对可有效增强语言模型的知识理解和推理能力。

Abstract: A question-answering (QA) system is to search suitable answers within a
knowledge base. Current QA systems struggle with queries requiring complex
reasoning or real-time knowledge integration. They are often supplemented with
retrieval techniques on a data source such as Retrieval-Augmented Generation
(RAG). However, RAG continues to face challenges in handling complex reasoning
and logical connections between multiple sources of information. A novel
approach for enhancing Large Language Models (LLMs) in knowledge-intensive QA
tasks is presented through the automated generation of context-based QA pairs.
This methodology leverages LLMs to create fine-tuning data, reducing reliance
on human labelling and improving model comprehension and reasoning
capabilities. The proposed system includes an automated QA generator and a
model fine-tuner, evaluated using perplexity, ROUGE, BLEU, and BERTScore.
Comprehensive experiments demonstrate improvements in logical coherence and
factual accuracy, with implications for developing adaptable Artificial
Intelligence (AI) systems. Mistral-7b-v0.3 outperforms Llama-3-8b with BERT F1,
BLEU, and ROUGE scores 0.858, 0.172, and 0.260 of for the LLM generated QA
pairs compared to scores of 0.836, 0.083, and 0.139 for the human annotated QA
pairs.

</details>


### [75] ["Haet Bhasha aur Diskrimineshun": Phonetic Perturbations in Code-Mixed Hinglish to Red-Team LLMs](https://arxiv.org/abs/2505.14226)
*Darpan Aswal,Siddharth D Jaiswal*

Main category: cs.CL

TL;DR: 该论文提出了一种利用代码混合和语音扰动的新型越狱策略，成功绕过大型语言模型的安全过滤器，在文本和图像生成任务中实现了高攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 现有的越狱研究主要集中在英语语言和固定模板攻击上，而多语言和多模态模型仍易受攻击。论文旨在探索更通用的安全对齐方法，特别是在现实场景中可能存在拼写错误的情况下。

Method: 论文引入了两种新型越狱策略：代码混合和语音扰动。通过在多语言提示中对敏感词应用语音拼写错误，绕过模型的安全过滤器。

Result: 新型提示在文本生成中实现了99%的攻击成功率，图像生成中为78%。攻击相关率在文本生成中为100%，图像生成中为95%。语音扰动通过影响词标记化导致越狱成功。

Conclusion: 研究表明，多语言多模态模型需要更通用的安全对齐方法，特别是在现实场景中提示可能存在拼写错误的情况下。语音扰动对模型安全性的影响值得进一步研究。

Abstract: Large Language Models (LLMs) have become increasingly powerful, with
multilingual and multimodal capabilities improving by the day. These models are
being evaluated through audits, alignment studies and red-teaming efforts to
expose model vulnerabilities towards generating harmful, biased and unfair
content. Existing red-teaming efforts have previously focused on the English
language, using fixed template-based attacks; thus, models continue to be
susceptible to multilingual jailbreaking strategies, especially in the
multimodal context. In this study, we introduce a novel strategy that leverages
code-mixing and phonetic perturbations to jailbreak LLMs for both text and
image generation tasks. We also introduce two new jailbreak strategies that
show higher effectiveness than baseline strategies. Our work presents a method
to effectively bypass safety filters in LLMs while maintaining interpretability
by applying phonetic misspellings to sensitive words in code-mixed prompts. Our
novel prompts achieve a 99% Attack Success Rate for text generation and 78% for
image generation, with Attack Relevance Rate of 100% for text generation and
95% for image generation when using the phonetically perturbed code-mixed
prompts. Our interpretability experiments reveal that phonetic perturbations
impact word tokenization, leading to jailbreak success. Our study motivates
increasing the focus towards more generalizable safety alignment for
multilingual multimodal models, especially in real-world settings wherein
prompts can have misspelt words.

</details>


### [76] [Mechanistic Fine-tuning for In-context Learning](https://arxiv.org/abs/2505.14233)
*Hakaze Cho,Peng Luo,Mariko Kato,Rin Kaenbyou,Naoya Inoue*

Main category: cs.CL

TL;DR: 本文提出了一种名为注意力行为微调（ABFT）的新方法，通过优化注意力分数而非最终输出来提升语言模型的上下文学习能力，显著降低了计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有的上下文学习方法（ICL）需要在大规模ICL风格数据上进行端到端微调，计算成本高昂。为了降低成本并提升效率，作者提出了ABFT方法。

Method: ABFT利用对ICL内部机制的研究成果，构建了基于注意力分数的训练目标，强制模型关注上下文中的正确标签并减少对错误标签的注意力。

Result: 在9个现代语言模型和8个数据集上的实验表明，ABFT在性能、鲁棒性、无偏性和效率上均优于现有方法，且数据成本仅为前者的0.01%。

Conclusion: ABFT展示了通过控制语言模型内部特定模块序列来改善其行为的可能性，为未来机制可解释性应用开辟了新方向。

Abstract: In-context Learning (ICL) utilizes structured demonstration-query inputs to
induce few-shot learning on Language Models (LMs), which are not originally
pre-trained on ICL-style data. To bridge the gap between ICL and pre-training,
some approaches fine-tune LMs on large ICL-style datasets by an end-to-end
paradigm with massive computational costs. To reduce such costs, in this paper,
we propose Attention Behavior Fine-Tuning (ABFT), utilizing the previous
findings on the inner mechanism of ICL, building training objectives on the
attention scores instead of the final outputs, to force the attention scores to
focus on the correct label tokens presented in the context and mitigate
attention scores from the wrong label tokens. Our experiments on 9 modern LMs
and 8 datasets empirically find that ABFT outperforms in performance,
robustness, unbiasedness, and efficiency, with only around 0.01% data cost
compared to the previous methods. Moreover, our subsequent analysis finds that
the end-to-end training objective contains the ABFT objective, suggesting the
implicit bias of ICL-style data to the emergence of induction heads. Our work
demonstrates the possibility of controlling specific module sequences within
LMs to improve their behavior, opening up the future application of mechanistic
interpretability.

</details>


### [77] [ABBA: Highly Expressive Hadamard Product Adaptation for Large Language Models](https://arxiv.org/abs/2505.14238)
*Raghav Singhal,Kaustubh Ponkshe,Rohit Vartak,Praneeth Vepakomma*

Main category: cs.CL

TL;DR: 论文提出ABBA方法，通过解耦可学习低秩矩阵的Hadamard积来提升参数高效微调的表达能力，在多个基准测试中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有参数高效微调方法（如LoRA）的表达能力受限于低秩分解结构，且更新部分依赖预训练模型权重。需要一种能完全解耦预训练权重、自由优化更新结构的方案。

Method: ABBA架构将权重更新重新参数化为两个独立可学习低秩矩阵的Hadamard积，完全脱离预训练权重约束，在相同参数量下实现更高表达能力。

Result: 矩阵重构实验验证了ABBA的表达优势，在算术推理和常识推理基准测试中显著超越现有PEFT方法，达到SOTA性能。

Conclusion: ABBA通过解耦式低秩矩阵设计突破了传统PEFT方法的表达能力限制，为轻量化适配大模型提供了新思路。

Abstract: Large Language Models have demonstrated strong performance across a wide
range of tasks, but adapting them efficiently to new domains remains a key
challenge. Parameter-Efficient Fine-Tuning (PEFT) methods address this by
introducing lightweight, trainable modules while keeping most pre-trained
weights fixed. The prevailing approach, LoRA, models updates using a low-rank
decomposition, but its expressivity is inherently constrained by the rank.
Recent methods like HiRA aim to increase expressivity by incorporating a
Hadamard product with the frozen weights, but still rely on the structure of
the pre-trained model. We introduce ABBA, a new PEFT architecture that
reparameterizes the update as a Hadamard product of two independently learnable
low-rank matrices. In contrast to prior work, ABBA fully decouples the update
from the pre-trained weights, enabling both components to be optimized freely.
This leads to significantly higher expressivity under the same parameter
budget. We formally analyze ABBA's expressive capacity and validate its
advantages through matrix reconstruction experiments. Empirically, ABBA
achieves state-of-the-art results on arithmetic and commonsense reasoning
benchmarks, consistently outperforming existing PEFT methods by a significant
margin across multiple models. Our code is publicly available at:
https://github.com/CERT-Lab/abba.

</details>


### [78] [Technical Report on classification of literature related to children speech disorder](https://arxiv.org/abs/2505.14242)
*Ziang Wang,Amir Aryani*

Main category: cs.CL

TL;DR: 该技术报告提出了一种基于自然语言处理的方法，用于系统分类儿童言语障碍的科学文献，通过LDA和BERTopic模型识别出14个临床相关主题。


<details>
  <summary>Details</summary>
Motivation: 旨在自动化文献综述过程，提高儿童言语障碍领域文献分类的效率和精确度。

Method: 从PubMed数据库检索4804篇相关文献，使用LDA和BERTopic进行主题建模，并结合自定义停用词表优化结果。

Result: LDA模型一致性得分为0.42，困惑度为-7.5；BERTopic模型异常主题比例低于20%，分类效果显著。

Conclusion: 该方法为言语病理学领域的自动化文献综述提供了有效基础。

Abstract: This technical report presents a natural language processing (NLP)-based
approach for systematically classifying scientific literature on childhood
speech disorders. We retrieved and filtered 4,804 relevant articles published
after 2015 from the PubMed database using domain-specific keywords. After
cleaning and pre-processing the abstracts, we applied two topic modeling
techniques - Latent Dirichlet Allocation (LDA) and BERTopic - to identify
latent thematic structures in the corpus. Our models uncovered 14 clinically
meaningful clusters, such as infantile hyperactivity and abnormal epileptic
behavior. To improve relevance and precision, we incorporated a custom stop
word list tailored to speech pathology. Evaluation results showed that the LDA
model achieved a coherence score of 0.42 and a perplexity of -7.5, indicating
strong topic coherence and predictive performance. The BERTopic model exhibited
a low proportion of outlier topics (less than 20%), demonstrating its capacity
to classify heterogeneous literature effectively. These results provide a
foundation for automating literature reviews in speech-language pathology.

</details>


### [79] [TransBench: Benchmarking Machine Translation for Industrial-Scale Applications](https://arxiv.org/abs/2505.14244)
*Haijun Li,Tianqi Shi,Zifu Shang,Yuxuan Han,Xueyu Zhao,Hao Wang,Yu Qian,Zhiqiang Qian,Linlong Xu,Minghao Wu,Chenyang Lyu,Longyue Wang,Gongbo Tang,Weihua Luo,Zhao Xu,Kaifu Zhang*

Main category: cs.CL

TL;DR: 该论文提出了一个针对工业机器翻译的三级能力评估框架TransBench，填补了通用模型在专业领域应用的评估空白，并发布了首个电商领域公开翻译基准。


<details>
  <summary>Details</summary>
Motivation: 现有通用机器翻译模型在工业场景中因专业术语、文化差异和风格规范等问题表现不佳，且缺乏针对专业领域的评估体系，导致学术基准与实际效果存在差距。

Method: 提出三级翻译能力框架（基础语言能力/领域专业能力/文化适应能力），构建包含1.7万句专业翻译的电商基准TransBench，结合传统指标与领域评估模型Marco-MOS。

Result: 发布首个公开电商翻译基准（覆盖4大场景33种语言对），开发开源评估工具，提出多级质量评估指标，建立可复用的工业MT评估体系。

Conclusion: 该研究通过结构化评估框架和领域专用基准，为工业机器翻译的系统化评估与优化提供了方法论和实践工具。

Abstract: Machine translation (MT) has become indispensable for cross-border
communication in globalized industries like e-commerce, finance, and legal
services, with recent advancements in large language models (LLMs)
significantly enhancing translation quality. However, applying general-purpose
MT models to industrial scenarios reveals critical limitations due to
domain-specific terminology, cultural nuances, and stylistic conventions absent
in generic benchmarks. Existing evaluation frameworks inadequately assess
performance in specialized contexts, creating a gap between academic benchmarks
and real-world efficacy. To address this, we propose a three-level translation
capability framework: (1) Basic Linguistic Competence, (2) Domain-Specific
Proficiency, and (3) Cultural Adaptation, emphasizing the need for holistic
evaluation across these dimensions. We introduce TransBench, a benchmark
tailored for industrial MT, initially targeting international e-commerce with
17,000 professionally translated sentences spanning 4 main scenarios and 33
language pairs. TransBench integrates traditional metrics (BLEU, TER) with
Marco-MOS, a domain-specific evaluation model, and provides guidelines for
reproducible benchmark construction. Our contributions include: (1) a
structured framework for industrial MT evaluation, (2) the first publicly
available benchmark for e-commerce translation, (3) novel metrics probing
multi-level translation quality, and (4) open-sourced evaluation tools. This
work bridges the evaluation gap, enabling researchers and practitioners to
systematically assess and enhance MT systems for industry-specific needs.

</details>


### [80] [FuxiMT: Sparsifying Large Language Models for Chinese-Centric Multilingual Machine Translation](https://arxiv.org/abs/2505.14256)
*Shaolin Zhu,Tianyu Dong,Bo Li,Deyi Xiong*

Main category: cs.CL

TL;DR: FuxiMT是一种新型以中文为中心的多语言机器翻译模型，采用稀疏化大语言模型（LLM）和两阶段训练策略，在低资源场景下表现优异。


<details>
  <summary>Details</summary>
Motivation: 为了解决多语言机器翻译中低资源语言对的数据稀缺问题，并提升翻译性能，特别是针对中文为中心的多语言场景。

Method: 采用两阶段训练策略：先在大型中文语料库上进行预训练，再在包含65种语言的大规模平行数据集上进行多语言微调，结合混合专家（MoEs）和课程学习策略。

Result: FuxiMT显著优于现有基线模型，包括最先进的大语言模型和机器翻译模型，尤其在低资源场景下表现突出，并展现出对未见语言对的零样本翻译能力。

Conclusion: FuxiMT在多语言机器翻译中表现出色，特别是在低资源语言对和零样本翻译场景中，具有填补数据稀缺语言对沟通鸿沟的潜力。

Abstract: In this paper, we present FuxiMT, a novel Chinese-centric multilingual
machine translation model powered by a sparsified large language model (LLM).
We adopt a two-stage strategy to train FuxiMT. We first pre-train the model on
a massive Chinese corpus and then conduct multilingual fine-tuning on a large
parallel dataset encompassing 65 languages. FuxiMT incorporates
Mixture-of-Experts (MoEs) and employs a curriculum learning strategy for robust
performance across various resource levels. Experimental results demonstrate
that FuxiMT significantly outperforms strong baselines, including
state-of-the-art LLMs and machine translation models, particularly under
low-resource scenarios. Furthermore, FuxiMT exhibits remarkable zero-shot
translation capabilities for unseen language pairs, indicating its potential to
bridge communication gaps where parallel data are scarce or unavailable.

</details>


### [81] [SLOT: Sample-specific Language Model Optimization at Test-time](https://arxiv.org/abs/2505.12392)
*Yang Hu,Xingyu Zhang,Xueji Fang,Zhiyang Chen,Xiao Wang,Huatian Zhang,Guojun Qi*

Main category: cs.CL

TL;DR: SLOT是一种参数高效的测试时推理方法，通过少量优化步骤更新轻量级样本特定参数向量，提升语言模型对单个提示的响应准确性。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型在处理复杂指令时表现不佳，尤其是在通用样本中代表性不足的情况下。SLOT旨在通过测试时的少量优化步骤，提升模型对单个提示的适应能力。

Method: SLOT在测试时进行少量优化步骤，更新一个轻量级的样本特定参数向量，并将其添加到输出头之前的最终隐藏层。通过缓存最后一层特征并最小化输入提示的交叉熵损失，实现高效适配。

Result: 实验表明，SLOT在多个基准测试和大型语言模型上优于对比模型。例如，Qwen2.5-7B在GSM8K上的准确率提升了8.6%，DeepSeek-R1-Distill-Llama-70B在GPQA上达到了70B级别模型中的SOTA准确率68.69%。

Conclusion: SLOT通过测试时的少量优化步骤，显著提升了语言模型对单个提示的响应准确性，且在多个基准测试中表现优异。

Abstract: We propose SLOT (Sample-specific Language Model Optimization at Test-time), a
novel and parameter-efficient test-time inference approach that enhances a
language model's ability to more accurately respond to individual prompts.
Existing Large Language Models (LLMs) often struggle with complex instructions,
leading to poor performances on those not well represented among general
samples. To address this, SLOT conducts few optimization steps at test-time to
update a light-weight sample-specific parameter vector. It is added to the
final hidden layer before the output head, and enables efficient adaptation by
caching the last layer features during per-sample optimization. By minimizing
the cross-entropy loss on the input prompt only, SLOT helps the model better
aligned with and follow each given instruction. In experiments, we demonstrate
that our method outperforms the compared models across multiple benchmarks and
LLMs. For example, Qwen2.5-7B with SLOT achieves an accuracy gain of 8.6% on
GSM8K from 57.54% to 66.19%, while DeepSeek-R1-Distill-Llama-70B with SLOT
achieves a SOTA accuracy of 68.69% on GPQA among 70B-level models. Our code is
available at https://github.com/maple-research-lab/SLOT.

</details>


### [82] [Think-J: Learning to Think for Generative LLM-as-a-Judge](https://arxiv.org/abs/2505.14268)
*Hui Huang,Yancheng He,Hongli Zhou,Rui Zhang,Wei Liu,Weixun Wang,Wenbo Su,Bo Zheng,Jiaheng Liu*

Main category: cs.CL

TL;DR: 提出Think-J方法，通过强化学习优化大语言模型的判断思维，提升其作为评判者的能力。


<details>
  <summary>Details</summary>
Motivation: 尽管生成式大语言模型在多任务中表现优异，但其作为评判者的性能仍不理想，需改进其判断能力。

Method: 先利用少量精选数据培养初步判断思维，再通过离线（训练评论模型）和在线（基于规则的奖励）强化学习优化判断轨迹。

Result: 实验表明，该方法显著提升生成式LLM-Judge的评估能力，超越生成式和基于分类器的LLM-Judge，且无需额外人工标注。

Conclusion: Think-J通过思维优化有效增强LLM的评判性能，为自动评估和奖励建模提供新思路。

Abstract: LLM-as-a-Judge refers to the automatic modeling of preferences for responses
generated by Large Language Models (LLMs), which is of significant importance
for both LLM evaluation and reward modeling. Although generative LLMs have made
substantial progress in various tasks, their performance as LLM-Judge still
falls short of expectations. In this work, we propose Think-J, which improves
generative LLM-as-a-Judge by learning how to think. We first utilized a small
amount of curated data to develop the model with initial judgment thinking
capabilities. Subsequently, we optimize the judgment thinking traces based on
reinforcement learning (RL). We propose two methods for judgment thinking
optimization, based on offline and online RL, respectively. The offline RL
requires training a critic model to construct positive and negative examples
for learning. The online method defines rule-based reward as feedback for
optimization. Experimental results showed that our approach can significantly
enhance the evaluation capability of generative LLM-Judge, surpassing both
generative and classifier-based LLM-Judge without requiring extra human
annotations.

</details>


### [83] [FAID: Fine-grained AI-generated Text Detection using Multi-task Auxiliary and Multi-level Contrastive Learning](https://arxiv.org/abs/2505.14271)
*Minh Ngoc Ta,Dong Cao Van,Duc-Anh Hoang,Minh Le-Anh,Truong Nguyen,My Anh Tran Nguyen,Yuxia Wang,Preslav Nakov,Sang Dinh*

Main category: cs.CL

TL;DR: FAIDSet数据集和FAID框架用于区分人类、AI及人机协作文本，通过多级对比学习和多任务辅助分类提升检测性能。


<details>
  <summary>Details</summary>
Motivation: 随着人机协作生成文本的普及，区分人类、AI及协作文本的需求日益增长，现有二元分类器无法满足需求。

Method: 提出FAID框架，结合多级对比学习和多任务辅助分类，学习文本风格特征并识别AI模型家族。

Result: FAID在未见过的领域和新AI模型上表现优异，显著提升泛化能力。

Conclusion: FAID为提升AI辅助写作的透明度和问责制提供了潜在解决方案。

Abstract: The growing collaboration between humans and AI models in generative tasks
has introduced new challenges in distinguishing between human-written,
AI-generated, and human-AI collaborative texts. In this work, we collect a
multilingual, multi-domain, multi-generator dataset FAIDSet. We further
introduce a fine-grained detection framework FAID to classify text into these
three categories, meanwhile identifying the underlying AI model family. Unlike
existing binary classifiers, FAID is built to capture both authorship and
model-specific characteristics. Our method combines multi-level contrastive
learning with multi-task auxiliary classification to learn subtle stylistic
cues. By modeling AI families as distinct stylistic entities, FAID offers
improved interpretability. We incorporate an adaptation to address
distributional shifts without retraining for unseen data. Experimental results
demonstrate that FAID outperforms several baseline approaches, particularly
enhancing the generalization accuracy on unseen domains and new AI models. It
provide a potential solution for improving transparency and accountability in
AI-assisted writing.

</details>


### [84] [Data-Efficient Hate Speech Detection via Cross-Lingual Nearest Neighbor Retrieval with Limited Labeled Data](https://arxiv.org/abs/2505.14272)
*Faeze Ghorbanpour,Daryna Dementieva,Alexander Fraser*

Main category: cs.CL

TL;DR: 该论文提出了一种高效、可扩展的跨语言迁移学习方法，通过近邻检索增强目标语言的少量标注数据，提升仇恨言论检测性能，在八种语言上验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 仇恨言论检测中，标注数据成本高昂且耗时，尤其对于低资源语言。现有研究表明跨语言迁移学习和数据增强能有效提升低资源任务的性能，因此需要开发更高效、可扩展的方法。

Method: 利用近邻检索技术，基于目标语言的小规模标注数据，从多语言仇恨言论数据池中检索最相关的样本进行数据增强，并应用最大边际相关性减少冗余。

Result: 在八种语言上的实验表明，该方法始终优于仅使用目标语言数据的模型，多数情况下超越当前最优方法，且具有高数据效率（最低仅需200条数据）和可扩展性。

Conclusion: 该方法为低资源语言的仇恨言论检测提供了高效解决方案，通过智能数据检索和去冗余策略实现了性能提升，且易于扩展到新语言和任务。

Abstract: Considering the importance of detecting hateful language, labeled hate speech
data is expensive and time-consuming to collect, particularly for low-resource
languages. Prior work has demonstrated the effectiveness of cross-lingual
transfer learning and data augmentation in improving performance on tasks with
limited labeled data. To develop an efficient and scalable cross-lingual
transfer learning approach, we leverage nearest-neighbor retrieval to augment
minimal labeled data in the target language, thereby enhancing detection
performance. Specifically, we assume access to a small set of labeled training
instances in the target language and use these to retrieve the most relevant
labeled examples from a large multilingual hate speech detection pool. We
evaluate our approach on eight languages and demonstrate that it consistently
outperforms models trained solely on the target language data. Furthermore, in
most cases, our method surpasses the current state-of-the-art. Notably, our
approach is highly data-efficient, retrieving as small as 200 instances in some
cases while maintaining superior performance. Moreover, it is scalable, as the
retrieval pool can be easily expanded, and the method can be readily adapted to
new languages and tasks. We also apply maximum marginal relevance to mitigate
redundancy and filter out highly similar retrieved instances, resulting in
improvements in some languages.

</details>


### [85] [YESciEval: Robust LLM-as-a-Judge for Scientific Question Answering](https://arxiv.org/abs/2505.14279)
*Jennifer D'Souza,Hamed Babaei Giglou,Quentin Münch*

Main category: cs.CL

TL;DR: YESciEval是一个开源框架，结合细粒度评分标准和强化学习，减少LLM评估中的乐观偏差，提供多学科科学问答数据集及对抗性变体。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型（LLM）在科学问答中的评估鲁棒性不足，缺乏透明且可扩展的评估方法。

Method: 提出YESciEval框架，结合细粒度评分标准和强化学习，并发布多学科科学问答数据集及对抗性变体。

Result: 该框架独立于专有模型和人类反馈，实现了可扩展、零成本的评估，支持AI对齐和透明评估。

Conclusion: YESciEval通过提升LLM作为评估者的可靠性，为科学研究和通用人工智能发展提供了稳健、透明的评估基础。

Abstract: Large Language Models (LLMs) drive scientific question-answering on modern
search engines, yet their evaluation robustness remains underexplored. We
introduce YESciEval, an open-source framework that combines fine-grained
rubric-based assessment with reinforcement learning to mitigate optimism bias
in LLM evaluators. We release multidisciplinary scienceQ&A datasets, including
adversarial variants, with evaluation scores from multiple LLMs. Independent of
proprietary models and human feedback, our approach enables scalable, cost-free
evaluation. By advancing reliable LLM-as-a-judge models, this work supports AI
alignment and fosters robust, transparent evaluation essential for scientific
inquiry and artificial general intelligence.

</details>


### [86] [Universal Acoustic Adversarial Attacks for Flexible Control of Speech-LLMs](https://arxiv.org/abs/2505.14286)
*Rao Ma,Mengjie Qian,Vyas Raina,Mark Gales,Kate Knill*

Main category: cs.CL

TL;DR: 研究发现语音大语言模型存在通用对抗攻击漏洞，需增强鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 探讨预训练语音编码器与大语言模型结合的语音LLMs在对抗攻击下的脆弱性。

Method: 通过在原始音频前添加固定通用对抗片段，研究模型无输出或任务被篡改的情况，并扩展至选择性激活攻击。

Result: Qwen2-Audio和Granite-Speech等模型显示出易受通用对抗攻击的关键漏洞。

Conclusion: 语音LLMs需采用更鲁棒的训练策略以提高对抗攻击抵抗力。

Abstract: The combination of pre-trained speech encoders with large language models has
enabled the development of speech LLMs that can handle a wide range of spoken
language processing tasks. While these models are powerful and flexible, this
very flexibility may make them more vulnerable to adversarial attacks. To
examine the extent of this problem, in this work we investigate universal
acoustic adversarial attacks on speech LLMs. Here a fixed, universal,
adversarial audio segment is prepended to the original input audio. We
initially investigate attacks that cause the model to either produce no output
or to perform a modified task overriding the original prompt. We then extend
the nature of the attack to be selective so that it activates only when
specific input attributes, such as a speaker gender or spoken language, are
present. Inputs without the targeted attribute should be unaffected, allowing
fine-grained control over the model outputs. Our findings reveal critical
vulnerabilities in Qwen2-Audio and Granite-Speech and suggest that similar
speech LLMs may be susceptible to universal adversarial attacks. This
highlights the need for more robust training strategies and improved resistance
to adversarial attacks.

</details>


### [87] [Cross-Lingual Optimization for Language Transfer in Large Language Models](https://arxiv.org/abs/2505.14297)
*Jungseob Lee,Seongtae Hong,Hyeonseok Moon,Heuiseok Lim*

Main category: cs.CL

TL;DR: 论文提出跨语言优化方法（CLO），通过利用英文数据和翻译模型，在低资源环境下更高效地将大语言模型适配到目标语言，同时保持英文能力。


<details>
  <summary>Details</summary>
Motivation: 传统监督微调（SFT）在多语言适配中过度依赖英文性能，尤其在数据受限环境下表现不佳，需要一种更高效的方法。

Method: 提出跨语言优化（CLO），利用公开英文SFT数据和翻译模型实现跨语言迁移，减少对目标语言数据量的依赖。

Result: 实验表明，CLO在六种语言上均优于SFT，尤其在低资源语言中仅用3200样本即超越SFT的6400样本效果，且对数据量变化更鲁棒。

Conclusion: CLO能更高效地实现多语言适配，解决了SFT在数据敏感性和资源依赖上的局限性，为低资源语言提供了可行方案。

Abstract: Adapting large language models to other languages typically employs
supervised fine-tuning (SFT) as a standard approach. However, it often suffers
from an overemphasis on English performance, a phenomenon that is especially
pronounced in data-constrained environments. To overcome these challenges, we
propose \textbf{Cross-Lingual Optimization (CLO)} that efficiently transfers an
English-centric LLM to a target language while preserving its English
capabilities. CLO utilizes publicly available English SFT data and a
translation model to enable cross-lingual transfer. We conduct experiments
using five models on six languages, each possessing varying levels of resource.
Our results show that CLO consistently outperforms SFT in both acquiring target
language proficiency and maintaining English performance. Remarkably, in
low-resource languages, CLO with only 3,200 samples surpasses SFT with 6,400
samples, demonstrating that CLO can achieve better performance with less data.
Furthermore, we find that SFT is particularly sensitive to data quantity in
medium and low-resource languages, whereas CLO remains robust. Our
comprehensive analysis emphasizes the limitations of SFT and incorporates
additional training strategies in CLO to enhance efficiency.

</details>


### [88] [JOLT-SQL: Joint Loss Tuning of Text-to-SQL with Confusion-aware Noisy Schema Sampling](https://arxiv.org/abs/2505.14305)
*Jinwang Song,Hongying Zan,Kunli Zhang,Lingling Mu,Yingjie Han,Haobo Hua,Min Peng*

Main category: cs.CL

TL;DR: JOLT-SQL提出了一种单阶段监督微调框架，通过统一损失联合优化模式链接和SQL生成，在Spider和BIRD基准测试中达到同类开源模型的最优执行准确率。


<details>
  <summary>Details</summary>
Motivation: 当前基于大语言模型的Text-to-SQL方法中，监督微调(SFT)面临多阶段流程复杂、对噪声模式信息鲁棒性差等问题，需要更高效的解决方案。

Method: 采用判别式模式链接（增强局部双向注意力）和混淆感知噪声模式采样策略（选择性注意力），通过统一损失进行端到端优化。

Result: 在Spider和BIRD基准测试中取得同类模型最优执行准确率，同时显著提升训练和推理效率。

Conclusion: JOLT-SQL通过单阶段联合优化框架有效解决了传统SFT方法的缺陷，在准确性和效率方面均实现突破。

Abstract: Text-to-SQL, which maps natural language to SQL queries, has benefited
greatly from recent advances in Large Language Models (LLMs). While LLMs offer
various paradigms for this task, including prompting and supervised fine-tuning
(SFT), SFT approaches still face challenges such as complex multi-stage
pipelines and poor robustness to noisy schema information. To address these
limitations, we present JOLT-SQL, a streamlined single-stage SFT framework that
jointly optimizes schema linking and SQL generation via a unified loss.
JOLT-SQL employs discriminative schema linking, enhanced by local bidirectional
attention, alongside a confusion-aware noisy schema sampling strategy with
selective attention to improve robustness under noisy schema conditions.
Experiments on the Spider and BIRD benchmarks demonstrate that JOLT-SQL
achieves state-of-the-art execution accuracy among comparable-size open-source
models, while significantly improving both training and inference efficiency.

</details>


### [89] [Studying the Role of Input-Neighbor Overlap in Retrieval-Augmented Language Models Training Efficiency](https://arxiv.org/abs/2505.14309)
*Ehsan Doostmohammadi,Marco Kuhlmann*

Main category: cs.CL

TL;DR: 检索增强语言模型通过优化查询与上下文重叠度，在保持性能的同时提升数据效率并减少40%训练时间，关键阈值后重叠度提升显著改善模型表现。


<details>
  <summary>Details</summary>
Motivation: 现有检索增强语言模型的效果高度依赖查询与检索上下文的重叠度，但最优重叠度尚未明确，需系统研究其对训练和推理的影响。

Method: 通过实验量化不同重叠度对模型性能的影响，并采用查询复述生成合成上下文人为提升重叠度。

Result: 重叠度超过临界阈值后，测试困惑度显著降低且学习速度加快；合成上下文使训练时间减少40%且保持性能。

Conclusion: 检索机制在语言模型预训练中存在显著优化空间，人为提升查询-上下文重叠度是高效可行的技术路径。

Abstract: Retrieval-augmented language models have demonstrated performance comparable
to much larger models while requiring fewer computational resources. The
effectiveness of these models crucially depends on the overlap between query
and retrieved context, but the optimal degree of this overlap remains
unexplored. In this paper, we systematically investigate how varying levels of
query--context overlap affect model performance during both training and
inference. Our experiments reveal that increased overlap initially has minimal
effect, but substantially improves test-time perplexity and accelerates model
learning above a critical threshold. Building on these findings, we demonstrate
that deliberately increasing overlap through synthetic context can enhance data
efficiency and reduce training time by approximately 40\% without compromising
performance. We specifically generate synthetic context through paraphrasing
queries. We validate our perplexity-based findings on question-answering tasks,
confirming that the benefits of retrieval-augmented language modeling extend to
practical applications. Our results provide empirical evidence of significant
optimization potential for retrieval mechanisms in language model pretraining.

</details>


### [90] [HausaNLP: Current Status, Challenges and Future Directions for Hausa Natural Language Processing](https://arxiv.org/abs/2505.14311)
*Shamsuddeen Hassan Muhammad,Ibrahim Said Ahmad,Idris Abdulmumin,Falalu Ibrahim Lawan,Babangida Sani,Sukairaj Hafiz Imam,Yusuf Aliyu,Sani Abdullahi Sani,Ali Usman Umar,Kenneth Church,Vukosi Marivate*

Main category: cs.CL

TL;DR: 本文综述了豪萨语NLP的现状，提出了资源整合平台HausaNLP，并探讨了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 豪萨语作为低资源语言，虽有大量使用者，但在NLP领域研究不足，面临数据集和模型代表性不足的挑战。

Method: 系统梳理豪萨语NLP资源与研究缺口，建立资源目录HausaNLP，并分析大语言模型整合中的问题。

Result: 推出HausaNLP平台整合数据集与工具，识别出分词和方言差异等关键挑战。

Conclusion: 需通过数据扩充、改进语言模型和加强社区协作推动豪萨语NLP发展，为多语言研究提供参考。

Abstract: Hausa Natural Language Processing (NLP) has gained increasing attention in
recent years, yet remains understudied as a low-resource language despite
having over 120 million first-language (L1) and 80 million second-language (L2)
speakers worldwide. While significant advances have been made in high-resource
languages, Hausa NLP faces persistent challenges, including limited open-source
datasets and inadequate model representation. This paper presents an overview
of the current state of Hausa NLP, systematically examining existing resources,
research contributions, and gaps across fundamental NLP tasks: text
classification, machine translation, named entity recognition, speech
recognition, and question answering. We introduce HausaNLP
(https://catalog.hausanlp.org), a curated catalog that aggregates datasets,
tools, and research works to enhance accessibility and drive further
development. Furthermore, we discuss challenges in integrating Hausa into large
language models (LLMs), addressing issues of suboptimal tokenization and
dialectal variation. Finally, we propose strategic research directions
emphasizing dataset expansion, improved language modeling approaches, and
strengthened community collaboration to advance Hausa NLP. Our work provides
both a foundation for accelerating Hausa NLP progress and valuable insights for
broader multilingual NLP research.

</details>


### [91] [A MIND for Reasoning: Meta-learning for In-context Deduction](https://arxiv.org/abs/2505.14313)
*Leonardo Bertolazzi,Manuel Vargas Guzmán,Raffaella Bernardi,Maciej Malicki,Jakub Szymanik*

Main category: cs.CL

TL;DR: 论文提出MIND方法，通过元学习微调提升小模型在演绎推理任务上的泛化能力，使其性能超越GPT-4o等大模型。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在形式化任务中表现优异，但对分布外问题的泛化能力有限。本文旨在研究如何系统提升模型对演绎规则的理解能力，特别是从知识库中筛选前提推导假设的任务。

Method: 提出MIND（Meta-learning for In-context Deduction）方法，这是一种基于少量样本的元学习微调技术，旨在增强模型对未见知识库的泛化能力和规则系统应用能力。

Result: 实验表明，MIND显著提升了1.5B-7B参数小模型的泛化性能，尤其在低资源和小模型场景下效果突出。经MIND微调的小模型甚至超越GPT-4o等前沿大模型。

Conclusion: MIND通过元学习机制有效解决了小模型在演绎推理中的系统性泛化问题，为资源受限场景提供了高性能解决方案。

Abstract: Large language models (LLMs) are increasingly evaluated on formal tasks,
where strong reasoning abilities define the state of the art. However, their
ability to generalize to out-of-distribution problems remains limited. In this
paper, we investigate how LLMs can achieve a systematic understanding of
deductive rules. Our focus is on the task of identifying the appropriate subset
of premises within a knowledge base needed to derive a given hypothesis. To
tackle this challenge, we propose Meta-learning for In-context Deduction
(MIND), a novel few-shot meta-learning fine-tuning approach. The goal of MIND
is to enable models to generalize more effectively to unseen knowledge bases
and to systematically apply inference rules. Our results show that MIND
significantly improves generalization in small LMs ranging from 1.5B to 7B
parameters. The benefits are especially pronounced in smaller models and
low-data settings. Remarkably, small models fine-tuned with MIND outperform
state-of-the-art LLMs, such as GPT-4o and o3-mini, on this task.

</details>


### [92] [QA-prompting: Improving Summarization with Large Language Models using Question-Answering](https://arxiv.org/abs/2505.14347)
*Neelabh Sinha*

Main category: cs.CL

TL;DR: 该论文提出了一种名为QA-prompting的简单提示方法，通过问答作为中间步骤来改进长文本摘要生成，无需微调或复杂流程，显著提升了摘要质量。


<details>
  <summary>Details</summary>
Motivation: 当前语言模型在长文本摘要中存在位置偏差问题，导致关键信息提取不理想。现有方法如微调或复杂技术流程存在挑战，需要更简单有效的解决方案。

Method: 提出QA-prompting方法，在生成摘要前先通过问答步骤提取关键信息并丰富上下文，从而减少位置偏差，仅需单次模型调用。

Result: 在多个领域数据集和十种先进预训练模型上的实验表明，QA-prompting优于基线和其他先进方法，ROUGE分数最高提升29%。

Conclusion: QA-prompting为摘要任务提供了高效可扩展的解决方案，同时证明了领域特定问题选择对优化性能的重要性。

Abstract: Language Models (LMs) have revolutionized natural language processing,
enabling high-quality text generation through prompting and in-context
learning. However, models often struggle with long-context summarization due to
positional biases, leading to suboptimal extraction of critical information.
There are techniques to improve this with fine-tuning, pipelining, or using
complex techniques, which have their own challenges. To solve these challenges,
we propose QA-prompting - a simple prompting method for summarization that
utilizes question-answering as an intermediate step prior to summary
generation. Our method extracts key information and enriches the context of
text to mitigate positional biases and improve summarization in a single LM
call per task without requiring fine-tuning or pipelining. Experiments on
multiple datasets belonging to different domains using ten state-of-the-art
pre-trained models demonstrate that QA-prompting outperforms baseline and other
state-of-the-art methods, achieving up to 29% improvement in ROUGE scores. This
provides an effective and scalable solution for summarization and highlights
the importance of domain-specific question selection for optimal performance.

</details>


### [93] [OSoRA: Output-Dimension and Singular-Value Initialized Low-Rank Adaptation](https://arxiv.org/abs/2505.14350)
*Jialong Han,Si Zhang,Ke Zhang*

Main category: cs.CL

TL;DR: OSoRA是一种新型的参数高效微调方法，通过结合SVD和可学习缩放向量，显著减少计算资源需求，同时保持或超越现有方法的性能。


<details>
  <summary>Details</summary>
Motivation: 由于大语言模型规模庞大且计算成本高昂，传统微调方法面临挑战。参数高效微调方法虽被提出，但仍需大量资源。本文旨在开发一种更高效的微调方法。

Method: OSoRA扩展了LoRA方法，将SVD与可学习缩放向量结合。首先对预训练权重矩阵进行SVD分解，然后在训练中优化输出维度向量，同时冻结对应的奇异向量矩阵。

Result: 在数学推理、常识推理等基准测试中，OSoRA性能与LoRA和VeRA相当或更优，同时显著减少可训练参数数量，计算资源需求大幅降低。

Conclusion: OSoRA通过联合训练奇异值和输出维度向量，实现了高效微调，性能优异，且参数规模随秩增加线性扩展。

Abstract: Fine-tuning Large Language Models (LLMs) has become increasingly challenging
due to their massive scale and associated computational costs.
Parameter-Efficient Fine-Tuning (PEFT) methodologies have been proposed as
computational alternatives; however, their implementations still require
significant resources. In this paper, we present OSoRA (Output-Dimension and
Singular-Value Initialized Low-Rank Adaptation), a novel PEFT method for LLMs.
OSoRA extends Low-Rank Adaptation (LoRA) by integrating Singular Value
Decomposition (SVD) with learnable scaling vectors in a unified framework. It
first performs an SVD of pre-trained weight matrices, then optimizes an
output-dimension vector during training, while keeping the corresponding
singular vector matrices frozen. OSoRA substantially reduces computational
resource requirements by minimizing the number of trainable parameters during
fine-tuning. Comprehensive evaluations across mathematical reasoning, common
sense reasoning, and other benchmarks demonstrate that OSoRA achieves
comparable or superior performance to state-of-the-art methods like LoRA and
VeRA, while maintaining a linear parameter scaling even as the rank increases
to higher dimensions. Our ablation studies further confirm that jointly
training both the singular values and the output-dimension vector is critical
for optimal performance.

</details>


### [94] [WirelessMathBench: A Mathematical Modeling Benchmark for LLMs in Wireless Communications](https://arxiv.org/abs/2505.14354)
*Xin Li,Mengbing Liu,Li Wei,Jiancheng An,Mérouane Debbah,Chau Yuen*

Main category: cs.CL

TL;DR: 论文提出了WirelessMathBench基准，用于评估大语言模型在无线通信数学建模任务中的表现，发现当前模型在复杂方程重建方面表现不佳。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在多种任务中表现出色，但它们在特定领域（如无线通信）的复杂数学推理能力尚未得到充分探索。

Method: 研究团队开发了WirelessMathBench基准，包含587个来自40篇前沿论文的问题，涵盖从基础选择题到复杂方程完成任务。

Result: 实验显示，大语言模型在基础回忆任务中表现良好，但在重建部分或完全遮蔽的方程时表现显著下降，最佳模型DeepSeek-R1的平均准确率仅为38.05%。

Conclusion: 通过公开WirelessMathBench基准和评估工具包，研究旨在推动开发更强大、领域感知的大语言模型，以应用于无线系统分析和更广泛的工程领域。

Abstract: Large Language Models (LLMs) have achieved impressive results across a broad
array of tasks, yet their capacity for complex, domain-specific mathematical
reasoning-particularly in wireless communications-remains underexplored. In
this work, we introduce WirelessMathBench, a novel benchmark specifically
designed to evaluate LLMs on mathematical modeling challenges to wireless
communications engineering. Our benchmark consists of 587 meticulously curated
questions sourced from 40 state-of-the-art research papers, encompassing a
diverse spectrum of tasks ranging from basic multiple-choice questions to
complex equation completion tasks, including both partial and full completions,
all of which rigorously adhere to physical and dimensional constraints. Through
extensive experimentation with leading LLMs, we observe that while many models
excel in basic recall tasks, their performance degrades significantly when
reconstructing partially or fully obscured equations, exposing fundamental
limitations in current LLMs. Even DeepSeek-R1, the best performer on our
benchmark, achieves an average accuracy of only 38.05%, with a mere 7.83%
success rate in full equation completion. By publicly releasing
WirelessMathBench along with the evaluation toolkit, we aim to advance the
development of more robust, domain-aware LLMs for wireless system analysis and
broader engineering applications.

</details>


### [95] [Dual Decomposition of Weights and Singular Value Low Rank Adaptation](https://arxiv.org/abs/2505.14367)
*Jialong Han,Si Zhang,Ke Zhang*

Main category: cs.CL

TL;DR: DuDe提出了一种基于SVD分解的参数高效微调方法，解决了LoRA类方法训练不稳定和知识迁移效率低的问题，在多个任务上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有基于LoRA的参数高效微调方法存在两个根本性局限：训练动态不稳定和预训练模型知识迁移效率低，这源于适配器参数的随机初始化。

Method: DuDe通过奇异值分解(SVD)将权重矩阵分解为幅度和方向分量，实现适配器参数的原则性初始化。

Result: 在MMLU上达到48.35%准确率，GSM8K上达到62.53%(±1.59)准确率，证明该方法能增强优化稳定性并更好保留预训练表示。

Conclusion: DuDe的理论分析和实证验证表明，其分解策略显著提升了LLMs参数高效微调方法的性能，特别是在需要专业知识的领域特定任务上。

Abstract: Parameter-Efficient Fine-Tuning (PEFT) has emerged as a critical paradigm for
adapting Large Language Models (LLMs) to downstream tasks, among which Low-rank
Adaptation (LoRA) represents one of the most widely adopted methodologies.
However, existing LoRA-based approaches exhibit two fundamental limitations:
unstable training dynamics and inefficient knowledge transfer from pre-trained
models, both stemming from random initialization of adapter parameters. To
overcome these challenges, we propose DuDe, a novel approach that decomposes
weight matrices into magnitude and direction components, employing Singular
Value Decomposition (SVD) for principled initialization. Our comprehensive
evaluation demonstrates DuDe's superior performance and robustness, achieving
up to 48.35\% accuracy on MMLU and 62.53\% ($\pm$ 1.59) accuracy on GSM8K. Our
theoretical analysis and empirical validation collectively demonstrate that
DuDe's decomposition strategy enhances optimization stability and better
preserves pre-trained representations, particularly for domain-specific tasks
requiring specialized knowledge. The combination of robust empirical
performance and rigorous theoretical foundations establishes DuDe as a
significant contribution to PEFT methodologies for LLMs.

</details>


### [96] [AutoRev: Automatic Peer Review System for Academic Research Papers](https://arxiv.org/abs/2505.14376)
*Maitreya Prafulla Chitale,Ketaki Mangesh Shetye,Harshit Gupta,Manav Chaudhary,Vasudeva Varma*

Main category: cs.CL

TL;DR: AutoRev通过图结构提取关键段落生成论文评审，性能超越现有方法58.72%，适用于多种NLP任务。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖大语言模型但忽视长文本处理限制，需更高效且全面的自动评审系统。

Method: 提出基于图表示的AutoRev框架，从学术文档中提取关键段落以生成评审。

Result: 在评审生成任务中，平均性能超越基线方法58.72%，验证了图方法的有效性。

Conclusion: 图提取技术有望拓展至其他NLP任务，代码将开源以促进相关研究。

Abstract: Generating a review for an academic research paper is a complex task that
requires a deep understanding of the document's content and the
interdependencies between its sections. It demands not only insight into
technical details but also an appreciation of the paper's overall coherence and
structure. Recent methods have predominantly focused on fine-tuning large
language models (LLMs) to address this challenge. However, they often overlook
the computational and performance limitations imposed by long input token
lengths. To address this, we introduce AutoRev, an Automatic Peer Review System
for Academic Research Papers. Our novel framework represents an academic
document as a graph, enabling the extraction of the most critical passages that
contribute significantly to the review. This graph-based approach demonstrates
effectiveness for review generation and is potentially adaptable to various
downstream tasks, such as question answering, summarization, and document
representation. When applied to review generation, our method outperforms SOTA
baselines by an average of 58.72% across all evaluation metrics. We hope that
our work will stimulate further research in applying graph-based extraction
techniques to other downstream tasks in NLP. We plan to make our code public
upon acceptance.

</details>


### [97] [Editing Across Languages: A Survey of Multilingual Knowledge Editing](https://arxiv.org/abs/2505.14393)
*Nadir Durrani,Basel Mousi,Fahim Dalvi*

Main category: cs.CL

TL;DR: 该综述系统整理了多语言知识编辑（MKE）的最新研究，提出了方法分类、总结了关键发现，并指出了跨语言传播的挑战和未解决问题。


<details>
  <summary>Details</summary>
Motivation: 尽管知识编辑在单语环境中已得到广泛研究，但在多语言环境中的探索仍不足。本文旨在填补这一空白，推动可编辑的多语言大模型发展。

Method: 本文提出了MKE方法的综合分类法，包括基于参数、基于记忆、微调和超网络等方法，并调查了现有基准。

Result: 总结了方法有效性和迁移模式的关键发现，识别了跨语言传播的挑战，如语言各向异性和评估覆盖不足等问题。

Conclusion: 本文整合了快速发展的MKE领域，为未来可编辑的语言感知大模型研究奠定了基础。

Abstract: While Knowledge Editing has been extensively studied in monolingual settings,
it remains underexplored in multilingual contexts. This survey systematizes
recent research on Multilingual Knowledge Editing (MKE), a growing subdomain of
model editing focused on ensuring factual edits generalize reliably across
languages. We present a comprehensive taxonomy of MKE methods, covering
parameter-based, memory-based, fine-tuning, and hypernetwork approaches. We
survey available benchmarks,summarize key findings on method effectiveness and
transfer patterns, identify challenges in cross-lingual propagation, and
highlight open problems related to language anisotropy, evaluation coverage,
and edit scalability. Our analysis consolidates a rapidly evolving area and
lays the groundwork for future progress in editable language-aware LLMs.

</details>


### [98] [MUG-Eval: A Proxy Evaluation Framework for Multilingual Generation Capabilities in Any Language](https://arxiv.org/abs/2505.14395)
*Seyoung Song,Seogyeong Jeong,Eunsu Kim,Jiho Jin,Dongkwan Kim,Jay Shin,Alice Oh*

Main category: cs.CL

TL;DR: MUG-Eval是一个评估大语言模型多语言生成能力的新框架，通过将现有基准转化为对话任务并测量准确率，无需依赖语言特定工具或标注数据。


<details>
  <summary>Details</summary>
Motivation: 评估大语言模型在低资源语言中的文本生成能力具有挑战性，因为直接评估方法稀缺且依赖高资源语言工具。

Method: 将现有基准转化为对话任务，以任务成功率作为生成成功对话的代理指标，避免使用语言特定工具或LLMs作为评判者。

Result: 在30种语言上评估8个LLMs，MUG-Eval与现有基准强相关（r>0.75），支持跨语言和模型的标准化比较。

Conclusion: MUG-Eval提供了一个资源高效且稳健的多语言生成评估框架，可扩展至数千种语言。

Abstract: Evaluating text generation capabilities of large language models (LLMs) is
challenging, particularly for low-resource languages where methods for direct
assessment are scarce. We propose MUG-Eval, a novel framework that evaluates
LLMs' multilingual generation capabilities by transforming existing benchmarks
into conversational tasks and measuring the LLMs' accuracies on those tasks. We
specifically designed these conversational tasks to require effective
communication in the target language. Then, we simply use task success rate as
a proxy of successful conversation generation. Our approach offers two key
advantages: it is independent of language-specific NLP tools or annotated
datasets, which are limited for most languages, and it does not rely on
LLMs-as-judges, whose evaluation quality degrades outside a few high-resource
languages. We evaluate 8 LLMs across 30 languages spanning high, mid, and
low-resource categories, and we find that MUG-Eval correlates strongly with
established benchmarks ($r$ > 0.75) while enabling standardized comparisons
across languages and models. Our framework provides a robust and
resource-efficient solution for evaluating multilingual generation that can be
extended to thousands of languages.

</details>


### [99] [Log-Augmented Generation: Scaling Test-Time Reasoning with Reusable Computation](https://arxiv.org/abs/2505.14398)
*Peter Baile Chen,Yi Zhang,Dan Roth,Samuel Madden,Jacob Andreas,Michael Cafarella*

Main category: cs.CL

TL;DR: 论文提出了一种名为LAG的新框架，通过重用过去的计算和推理日志来增强大语言模型在新任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 当前的大语言模型及其代理版本难以保留和复用过去任务中的推理过程，限制了其在未来任务中的应用能力。

Method: LAG框架利用键值（KV）缓存来存储和检索过去任务的完整推理上下文，并在新任务中直接复用这些缓存以增强生成能力。

Result: 实验表明，LAG在知识和推理密集型数据集上显著优于未使用日志的标准代理系统以及基于反射和KV缓存技术的现有解决方案。

Conclusion: LAG框架通过直接复用过去的推理和计算，有效提升了模型在新任务上的表现，同时保持了系统的高效性和可扩展性。

Abstract: While humans naturally learn and adapt from past experiences, large language
models (LLMs) and their agentic counterparts struggle to retain reasoning from
previous tasks and apply them in future contexts. To address this limitation,
we propose a novel framework, log-augmented generation (LAG) that directly
reuses prior computation and reasoning from past logs at test time to enhance
model's ability to learn from previous tasks and perform better on new, unseen
challenges, all while keeping the system efficient and scalable. Specifically,
our system represents task logs using key-value (KV) caches, encoding the full
reasoning context of prior tasks while storing KV caches for only a selected
subset of tokens. When a new task arises, LAG retrieves the KV values from
relevant logs to augment generation. Our approach differs from reflection-based
memory mechanisms by directly reusing prior reasoning and computations without
requiring additional steps for knowledge extraction or distillation. Our method
also goes beyond existing KV caching techniques, which primarily target
efficiency gains rather than improving accuracy. Experiments on knowledge- and
reasoning-intensive datasets demonstrate that our method significantly
outperforms standard agentic systems that do not utilize logs, as well as
existing solutions based on reflection and KV cache techniques.

</details>


### [100] [Pierce the Mists, Greet the Sky: Decipher Knowledge Overshadowing via Knowledge Circuit Analysis](https://arxiv.org/abs/2505.14406)
*Haoming Huang,Yibo Yan,Jiahao Huo,Xin Zou,Xinfeng Li,Kun Wang,Xuming Hu*

Main category: cs.CL

TL;DR: 论文提出PhantomCircuit框架，用于分析和检测大语言模型中的知识遮蔽现象，揭示其训练过程中的内部机制。


<details>
  <summary>Details</summary>
Motivation: 大语言模型存在知识遮蔽问题，即激活的知识片段可能掩盖其他相关信息，导致错误输出。目前对其起源和内部机制的理解不足。

Method: 引入PhantomCircuit框架，通过知识回路分析，剖析注意力头的内部工作机制，追踪竞争知识路径如何导致遮蔽现象及其在训练中的演变。

Result: 实验证明PhantomCircuit能有效识别知识遮蔽实例，为这一难以捉摸的幻觉现象提供新见解。

Conclusion: PhantomCircuit为研究社区提供了新的方法论视角，有望缓解知识遮蔽问题。

Abstract: Large Language Models (LLMs), despite their remarkable capabilities, are
hampered by hallucinations. A particularly challenging variant, knowledge
overshadowing, occurs when one piece of activated knowledge inadvertently masks
another relevant piece, leading to erroneous outputs even with high-quality
training data. Current understanding of overshadowing is largely confined to
inference-time observations, lacking deep insights into its origins and
internal mechanisms during model training. Therefore, we introduce
PhantomCircuit, a novel framework designed to comprehensively analyze and
detect knowledge overshadowing. By innovatively employing knowledge circuit
analysis, PhantomCircuit dissects the internal workings of attention heads,
tracing how competing knowledge pathways contribute to the overshadowing
phenomenon and its evolution throughout the training process. Extensive
experiments demonstrate PhantomCircuit's effectiveness in identifying such
instances, offering novel insights into this elusive hallucination and
providing the research community with a new methodological lens for its
potential mitigation.

</details>


### [101] [Hidden Ghost Hand: Unveiling Backdoor Vulnerabilities in MLLM-Powered Mobile GUI Agents](https://arxiv.org/abs/2505.14418)
*Pengzhou Cheng,Haowen Hu,Zheng Wu,Zongru Wu,Tianjie Ju,Daizong Ding,Zhuosheng Zhang,Gongshen Liu*

Main category: cs.CL

TL;DR: 本文揭示了多模态大语言模型（MLLM）驱动的GUI代理存在供应链威胁，提出了一种名为AgentGhost的后门攻击框架，并通过实验验证了其高效性和隐蔽性。


<details>
  <summary>Details</summary>
Motivation: 由于微调成本高，用户常依赖开源GUI代理或AI提供商提供的API，这引入了供应链威胁，尤其是后门攻击。本文旨在揭示并解决这一问题。

Method: 提出AgentGhost框架，通过组合目标和交互级触发器，将后门注入建模为Min-Max优化问题，利用监督对比学习和微调来增强后门的灵活性和隐蔽性。

Result: 在多个代理模型和移动基准测试中，AgentGhost攻击准确率达到99.7%，且仅导致1%的效用下降。此外，提出的防御方法将攻击准确率降至22.1%。

Conclusion: AgentGhost展示了后门攻击在GUI代理中的高效性和隐蔽性，同时提出的防御方法有效降低了攻击风险。

Abstract: Graphical user interface (GUI) agents powered by multimodal large language
models (MLLMs) have shown greater promise for human-interaction. However, due
to the high fine-tuning cost, users often rely on open-source GUI agents or
APIs offered by AI providers, which introduces a critical but underexplored
supply chain threat: backdoor attacks. In this work, we first unveil that
MLLM-powered GUI agents naturally expose multiple interaction-level triggers,
such as historical steps, environment states, and task progress. Based on this
observation, we introduce AgentGhost, an effective and stealthy framework for
red-teaming backdoor attacks. Specifically, we first construct composite
triggers by combining goal and interaction levels, allowing GUI agents to
unintentionally activate backdoors while ensuring task utility. Then, we
formulate backdoor injection as a Min-Max optimization problem that uses
supervised contrastive learning to maximize the feature difference across
sample classes at the representation space, improving flexibility of the
backdoor. Meanwhile, it adopts supervised fine-tuning to minimize the
discrepancy between backdoor and clean behavior generation, enhancing
effectiveness and utility. Extensive evaluations of various agent models in two
established mobile benchmarks show that AgentGhost is effective and generic,
with attack accuracy that reaches 99.7\% on three attack objectives, and shows
stealthiness with only 1\% utility degradation. Furthermore, we tailor a
defense method against AgentGhost that reduces the attack accuracy to 22.1\%.
Our code is available at \texttt{anonymous}.

</details>


### [102] [SAE-FiRE: Enhancing Earnings Surprise Predictions Through Sparse Autoencoder Feature Selection](https://arxiv.org/abs/2505.14420)
*Huopu Zhang,Yanguang Liu,Mengnan Du*

Main category: cs.CL

TL;DR: 论文提出SAE-FiRE框架，通过稀疏自编码器分析财报电话会议文本，有效提取关键信息并预测盈利意外。


<details>
  <summary>Details</summary>
Motivation: 财报电话会议是公司高管、分析师和股东间的重要沟通渠道，包含大量前瞻性信息，但文本冗长且专业术语多，传统语言模型处理效果有限。

Method: 采用稀疏自编码器(SAEs)构建SAE-FiRE框架，过滤冗余信息并捕捉具有预测力的细微金融信号。

Result: 实验表明SAE-FiRE显著优于基线模型，能更精准预测盈利意外。

Conclusion: SAE-FiRE框架解决了金融文本分析中的噪声和冗余问题，为盈利预测提供了有效工具。

Abstract: Predicting earnings surprises through the analysis of earnings conference
call transcripts has attracted increasing attention from the financial research
community. Conference calls serve as critical communication channels between
company executives, analysts, and shareholders, offering valuable
forward-looking information. However, these transcripts present significant
analytical challenges, typically containing over 5,000 words with substantial
redundancy and industry-specific terminology that creates obstacles for
language models. In this work, we propose the Sparse Autoencoder for Financial
Representation Enhancement (SAE-FiRE) framework to address these limitations by
extracting key information while eliminating redundancy. SAE-FiRE employs
Sparse Autoencoders (SAEs) to efficiently identify patterns and filter out
noises, and focusing specifically on capturing nuanced financial signals that
have predictive power for earnings surprises. Experimental results indicate
that the proposed method can significantly outperform comparing baselines.

</details>


### [103] [Scaling Low-Resource MT via Synthetic Data Generation with LLMs](https://arxiv.org/abs/2505.14423)
*Ona de Gibert,Joseph Attieh,Teemu Vahtola,Mikko Aulamo,Zihao Li,Raúl Vázquez,Tiancheng Hu,Jörg Tiedemann*

Main category: cs.CL

TL;DR: LLM生成合成数据可显著提升低资源机器翻译性能，研究构建了高质量合成语料库SynOPUS并验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 解决低资源语言机器翻译数据不足的问题，探索LLM生成合成数据的潜力。

Method: 基于英语Europarl构建文档级合成语料库，通过枢轴翻译扩展至147种语言对，并比较HPLT数据集。

Result: 自动和人工评估证实合成数据质量高，能有效提升低资源语言翻译性能。

Conclusion: 即使存在噪声，LLM生成的合成数据仍可显著改善低资源机器翻译效果。

Abstract: We investigate the potential of LLM-generated synthetic data for improving
low-resource machine translation (MT). Focusing on seven diverse target
languages, we construct a document-level synthetic corpus from English
Europarl, and extend it via pivoting to 147 additional language pairs.
Automatic and human evaluation confirm its high overall quality. We study its
practical application by (i) identifying effective training regimes, (ii)
comparing our data with the HPLT dataset, and (iii) testing its utility beyond
English-centric MT. Finally, we introduce SynOPUS, a public repository for
synthetic parallel datasets. Our findings show that LLM-generated synthetic
data, even when noisy, can substantially improve MT performance for
low-resource languages.

</details>


### [104] [From Templates to Natural Language: Generalization Challenges in Instruction-Tuned LLMs for Spatial Reasoning](https://arxiv.org/abs/2505.14425)
*Chalamalasetti Kranti,Sherzod Hakimov,David Schlangen*

Main category: cs.CL

TL;DR: 研究发现，指令调优的大型语言模型在简单空间任务上表现良好，但在复杂任务中泛化能力显著下降。


<details>
  <summary>Details</summary>
Motivation: 探讨指令调优的大型语言模型在从合成指令泛化到人类编写指令时的挑战，特别是在空间基础任务中。

Method: 仅使用合成指令对大型语言模型进行微调，并在包含合成和人类编写指令的基准数据集上评估其性能。

Result: 模型在简单任务上表现良好，但在复杂任务中性能显著下降，并进行了详细的错误分析。

Conclusion: 研究揭示了指令泛化中的差距，特别是在复杂任务中，为未来改进提供了方向。

Abstract: Instruction-tuned large language models (LLMs) have shown strong performance
on a variety of tasks; however, generalizing from synthetic to human-authored
instructions in grounded environments remains a challenge for them. In this
work, we study generalization challenges in spatial grounding tasks where
models interpret and translate instructions for building object arrangements on
a $2.5$D grid. We fine-tune LLMs using only synthetic instructions and evaluate
their performance on a benchmark dataset containing both synthetic and
human-written instructions. Our results reveal that while models generalize
well on simple tasks, their performance degrades significantly on more complex
tasks. We present a detailed error analysis of the gaps in instruction
generalization.

</details>


### [105] [Neural Incompatibility: The Unbridgeable Gap of Cross-Scale Parametric Knowledge Transfer in Large Language Models](https://arxiv.org/abs/2505.14436)
*Yuqiao Tan,Shizhu He,Kang Liu,Jun Zhao*

Main category: cs.CL

TL;DR: 该论文探讨了如何通过参数实现跨规模大语言模型（LLMs）间的知识迁移，提出了参数对齐的重要性，并引入了PrePKT和PostPKT两种范式，最终发现神经不兼容性是主要挑战。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于超越传统的基于符号语言的知识迁移，实现真正的参数化知识迁移（PKT），并探索不同规模LLMs间通过参数进行知识迁移的有效方法。

Method: 论文首先证明了参数空间对齐是跨规模PKT成功的前提，重新定义了PostPKT，并提出了PrePKT范式及解决方案LaTen，通过少量训练步骤实现参数空间对齐。

Result: 实验表明，PostPKT和PrePKT在实现稳定迁移方面均面临挑战，神经不兼容性被识别为不同规模LLMs间的根本障碍。

Conclusion: 研究揭示了LLMs参数架构的新见解，为未来高效PKT研究指明了方向，神经不兼容性是实现有效PKT的主要挑战。

Abstract: Large Language Models (LLMs) offer a transparent brain with accessible
parameters that encode extensive knowledge, which can be analyzed, located and
transferred. Consequently, a key research challenge is to transcend traditional
knowledge transfer paradigms rooted in symbolic language and achieve genuine
Parametric Knowledge Transfer (PKT). Significantly, exploring effective methods
for transferring knowledge across LLMs of different scales through parameters
presents an intriguing and valuable research direction. In this paper, we first
demonstrate $\textbf{Alignment}$ in parametric space is the fundamental
prerequisite to achieve successful cross-scale PKT. We redefine the previously
explored knowledge transfer as Post-Align PKT (PostPKT), which utilizes
extracted parameters for LoRA initialization and requires subsequent fine-tune
for alignment. Hence, to reduce cost for further fine-tuning, we introduce a
novel Pre-Align PKT (PrePKT) paradigm and propose a solution called
$\textbf{LaTen}$
($\textbf{L}$oc$\textbf{a}$te-$\textbf{T}$h$\textbf{e}$n-Alig$\textbf{n}$) that
aligns the parametric spaces of LLMs across scales only using several training
steps without following training. Comprehensive experiments on four benchmarks
demonstrate that both PostPKT and PrePKT face challenges in achieving
consistently stable transfer. Through in-depth analysis, we identify
$\textbf{Neural Incompatibility}$ as the ethological and parametric structural
differences between LLMs of varying scales, presenting fundamental challenges
to achieving effective PKT. These findings provide fresh insights into the
parametric architectures of LLMs and highlight promising directions for future
research on efficient PKT. Our code is available at
https://github.com/Trae1ounG/Neural_Incompatibility.

</details>


### [106] [Creative Preference Optimization](https://arxiv.org/abs/2505.14442)
*Mete Ismayilzada,Antonio Laverghetta Jr.,Simone A. Luchini,Reet Patel,Antoine Bosselut,Lonneke van der Plas,Roger Beaty*

Main category: cs.CL

TL;DR: 该论文提出了一种名为CrPO的新方法，通过多维度创造力信号注入优化LLM的创造性输出，并在大规模人类偏好数据集上验证了其优于GPT-4o等基线模型的表现。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在生成具有新颖性、多样性、惊喜感和高质量的内容方面存在局限，现有方法未能全面解决创造力的多维度特性。

Method: 提出创造性偏好优化（CrPO）方法，以模块化方式将多维度创造力信号注入偏好优化目标，并使用包含20万人类生成响应和30多项创造力评估的新数据集MuCE进行训练。

Result: 经自动化和人工评估，使用CrPO的模型在保持高质量输出的同时，在新颖性、多样性和惊喜感方面均超越GPT-4o等基线模型，NoveltyBench测试进一步验证了方法的泛化性。

Conclusion: 在偏好优化框架中直接优化创造力指标是提升LLM创造性能力的有效途径，且不会损害输出质量。

Abstract: While Large Language Models (LLMs) have demonstrated impressive performance
across natural language generation tasks, their ability to generate truly
creative content-characterized by novelty, diversity, surprise, and
quality-remains limited. Existing methods for enhancing LLM creativity often
focus narrowly on diversity or specific tasks, failing to address creativity's
multifaceted nature in a generalizable way. In this work, we propose Creative
Preference Optimization (CrPO), a novel alignment method that injects signals
from multiple creativity dimensions into the preference optimization objective
in a modular fashion. We train and evaluate creativity-augmented versions of
several models using CrPO and MuCE, a new large-scale human preference dataset
spanning over 200,000 human-generated responses and ratings from more than 30
psychological creativity assessments. Our models outperform strong baselines,
including GPT-4o, on both automated and human evaluations, producing more
novel, diverse, and surprising generations while maintaining high output
quality. Additional evaluations on NoveltyBench further confirm the
generalizability of our approach. Together, our results demonstrate that
directly optimizing for creativity within preference frameworks is a promising
direction for advancing the creative capabilities of LLMs without compromising
output quality.

</details>


### [107] [CtrlDiff: Boosting Large Diffusion Language Models with Dynamic Block Prediction and Controllable Generation](https://arxiv.org/abs/2505.14455)
*Chihan Huang,Hao Tang*

Main category: cs.CL

TL;DR: 本文提出CtrlDiff，一种动态可控的半自回归框架，通过强化学习自适应确定生成块大小，并引入分类器引导控制机制，显著提升扩散语言模型的灵活性和控制能力。


<details>
  <summary>Details</summary>
Motivation: 当前扩散语言模型存在固定长度生成和缺乏灵活控制机制的限制，限制了其实际应用。本文旨在解决这些问题，提升模型的灵活性和可控性。

Method: 提出CtrlDiff框架，结合自回归和扩散模型的优势，使用强化学习动态确定生成块大小，并设计分类器引导的控制机制，减少计算开销。

Result: 实验表明，CtrlDiff在混合扩散模型中表现优异，缩小了与最先进自回归模型的性能差距，并在多样任务中实现高效的条件文本生成。

Conclusion: CtrlDiff通过动态块生成和高效控制机制，显著提升了扩散语言模型的灵活性和可控性，为条件文本生成提供了新解决方案。

Abstract: Although autoregressive models have dominated language modeling in recent
years, there has been a growing interest in exploring alternative paradigms to
the conventional next-token prediction framework. Diffusion-based language
models have emerged as a compelling alternative due to their powerful parallel
generation capabilities and inherent editability. However, these models are
often constrained by fixed-length generation. A promising direction is to
combine the strengths of both paradigms, segmenting sequences into blocks,
modeling autoregressive dependencies across blocks while leveraging discrete
diffusion to estimate the conditional distribution within each block given the
preceding context. Nevertheless, their practical application is often hindered
by two key limitations: rigid fixed-length outputs and a lack of flexible
control mechanisms. In this work, we address the critical limitations of fixed
granularity and weak controllability in current large diffusion language
models. We propose CtrlDiff, a dynamic and controllable semi-autoregressive
framework that adaptively determines the size of each generation block based on
local semantics using reinforcement learning. Furthermore, we introduce a
classifier-guided control mechanism tailored to discrete diffusion, which
significantly reduces computational overhead while facilitating efficient
post-hoc conditioning without retraining. Extensive experiments demonstrate
that CtrlDiff sets a new standard among hybrid diffusion models, narrows the
performance gap to state-of-the-art autoregressive approaches, and enables
effective conditional text generation across diverse tasks.

</details>


### [108] [Not All Correct Answers Are Equal: Why Your Distillation Source Matters](https://arxiv.org/abs/2505.14464)
*Xiaoyu Tian,Yunjie Ji,Haotian Wang,Shuaiting Chen,Sitong Zhao,Yiping Peng,Han Zhao,Xiangang Li*

Main category: cs.CL

TL;DR: 该论文通过大规模实证研究，比较了三种先进教师模型在推理数据蒸馏上的效果，发现AM-Thinking-v1蒸馏的数据表现最佳，并公开了相关数据集以支持未来研究。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于探索如何通过蒸馏技术有效提升开源语言模型的推理能力，并通过比较不同教师模型蒸馏的数据质量来寻找最优方法。

Method: 方法包括从三种教师模型（AM-Thinking-v1、Qwen3-235B-A22B和DeepSeek-R1）收集189万条查询的验证输出，构建三个并行数据集，并分析其分布特性。

Result: 结果显示，AM-Thinking-v1蒸馏的数据在多个推理基准测试中表现最佳，且模型能根据任务难度自适应调整输出长度。

Conclusion: 结论强调了高质量验证推理轨迹的价值，并公开了AM-Thinking-v1和Qwen3-235B-A22B蒸馏的数据集以促进未来研究。

Abstract: Distillation has emerged as a practical and effective approach to enhance the
reasoning capabilities of open-source language models. In this work, we conduct
a large-scale empirical study on reasoning data distillation by collecting
verified outputs from three state-of-the-art teacher models-AM-Thinking-v1,
Qwen3-235B-A22B, and DeepSeek-R1-on a shared corpus of 1.89 million queries. We
construct three parallel datasets and analyze their distributions, revealing
that AM-Thinking-v1-distilled data exhibits greater token length diversity and
lower perplexity. Student models trained on each dataset are evaluated on
reasoning benchmarks including AIME2024, AIME2025, MATH500, and LiveCodeBench.
The AM-based model consistently achieves the best performance (e.g., 84.3 on
AIME2024, 72.2 on AIME2025, 98.4 on MATH500, and 65.9 on LiveCodeBench) and
demonstrates adaptive output behavior-producing longer responses for harder
tasks and shorter ones for simpler tasks. These findings highlight the value of
high-quality, verified reasoning traces. We release the AM-Thinking-v1 and
Qwen3-235B-A22B distilled datasets to support future research on open and
high-performing reasoning-oriented language models. The datasets are publicly
available on Hugging Face\footnote{Datasets are available on Hugging Face:
\href{https://huggingface.co/datasets/a-m-team/AM-Thinking-v1-Distilled}{AM-Thinking-v1-Distilled},
\href{https://huggingface.co/datasets/a-m-team/AM-Qwen3-Distilled}{AM-Qwen3-Distilled}.}.

</details>


### [109] [Void in Language Models](https://arxiv.org/abs/2505.14467)
*Mani Shemiranifar*

Main category: cs.CL

TL;DR: 研究发现并非所有Transformer层在推理时都被激活，通过L2自适应计算（LAC）方法跳过未激活层可提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 探讨Transformer语言模型在推理过程中是否所有层都被激活，以及如何通过识别和跳过未激活层来优化模型性能。

Method: 使用L2自适应计算（LAC）方法监测激活的L2范数变化，识别未激活层（Voids），并在提示处理（PP）和响应生成（RG）阶段跟踪激活层。

Result: 实验表明，跳过未激活层能显著提升模型性能，如在MMLU基准测试中，Qwen2.5-7B-Instruct的准确率从69.24提升至71.29，同时仅使用30%的层。

Conclusion: 研究证实了Transformer模型在推理时存在大量未激活层，选择性跳过这些层可以优化模型性能。

Abstract: Despite advances in transformer-based language models (LMs), a fundamental
question remains largely unanswered: Are all layers activated during inference?
We investigate this question by detecting unactivated layers (which we refer to
as Voids) using a non-trainable and parameter-free adaptive computation method
called L2 Adaptive Computation (LAC). We adapt LAC from its original
efficiency-focused application to trace activated layers during inference. This
method monitors changes in the L2-norm of activations to identify voids. We
analyze layer activation in instruction-tuned LMs across two phases: Prompt
Processing (PP), where we trace activated layers for each token in the input
prompts, and Response Generation (RG), where we trace activated layers for each
generated token. We further demonstrate that distinct layers are activated
during these two phases. To show the effectiveness of our method, we evaluated
three distinct instruction-tuned LMs from the Llama, Mistral, and Qwen families
on three benchmarks: MMLU, GPQA Diamond, and BoolQ. For example, on MMLU with a
zero-shot setting, skipping voids in Qwen2.5-7B-Instruct resulted in an
improvement from 69.24 to 71.29 while the model uses only 30% of the layers.
Similarly, Mistral-7B-Instruct-v0.3 on GPQA Diamond improved from 13.88 to
18.36 when using 70% of the layers during both the PP and RG phases. These
results show that not all layers contribute equally during inference, and that
selectively skipping most of them can improve the performance of models on
certain tasks.

</details>


### [110] [Attributional Safety Failures in Large Language Models under Code-Mixed Perturbations](https://arxiv.org/abs/2505.14469)
*Somnath Banerjee,Pratyush Chatterjee,Shanu Kumar,Sayan Layek,Parag Agrawal,Rima Hazra,Animesh Mukherjee*

Main category: cs.CL

TL;DR: 研究发现，LLMs在处理混合代码输入时比单语输入更容易产生不安全输出，并通过解释性方法分析了其内部机制。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs的进步，处理混合代码输入时的安全问题日益突出，研究旨在探究其不安全性增加的机制。

Method: 使用解释性方法分析LLMs内部属性变化，区分普遍不安全与文化特定不安全查询。

Result: 混合代码提示比单语提示更易引发不安全输出，揭示了模型行为背后的机制。

Conclusion: 研究阐明了LLMs在混合代码输入下不安全行为的驱动机制，为安全改进提供了新见解。

Abstract: Recent advancements in LLMs have raised significant safety concerns,
particularly when dealing with code-mixed inputs and outputs. Our study
systematically investigates the increased susceptibility of LLMs to produce
unsafe outputs from code-mixed prompts compared to monolingual English prompts.
Utilizing explainability methods, we dissect the internal attribution shifts
causing model's harmful behaviors. In addition, we explore cultural dimensions
by distinguishing between universally unsafe and culturally-specific unsafe
queries. This paper presents novel experimental insights, clarifying the
mechanisms driving this phenomenon.

</details>


### [111] [Adapting Pretrained Language Models for Citation Classification via Self-Supervised Contrastive Learning](https://arxiv.org/abs/2505.14471)
*Tong Li,Jiachuan Wang,Yongqi Zhang,Shuangyin Li,Lei Chen*

Main category: cs.CL

TL;DR: Citss框架通过自监督对比学习和两种专用策略解决引文分类中的数据稀缺和噪声问题，兼容编码器和解码器模型，在多个基准数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 引文分类对学术分析至关重要，但直接微调预训练语言模型面临标注数据稀缺、上下文噪声和虚假关键词关联等挑战。

Method: 提出Citss框架，引入自监督对比学习，采用句子级裁剪和关键词扰动策略生成对比对，兼容编码器和解码器模型。

Result: 在三个基准数据集上，Citss相比现有技术表现出色，验证了其有效性。

Conclusion: Citss通过创新方法解决了引文分类的关键挑战，为学术分析提供了更强大的工具。

Abstract: Citation classification, which identifies the intention behind academic
citations, is pivotal for scholarly analysis. Previous works suggest
fine-tuning pretrained language models (PLMs) on citation classification
datasets, reaping the reward of the linguistic knowledge they gained during
pretraining. However, directly fine-tuning for citation classification is
challenging due to labeled data scarcity, contextual noise, and spurious
keyphrase correlations. In this paper, we present a novel framework, Citss,
that adapts the PLMs to overcome these challenges. Citss introduces
self-supervised contrastive learning to alleviate data scarcity, and is
equipped with two specialized strategies to obtain the contrastive pairs:
sentence-level cropping, which enhances focus on target citations within long
contexts, and keyphrase perturbation, which mitigates reliance on specific
keyphrases. Compared with previous works that are only designed for
encoder-based PLMs, Citss is carefully developed to be compatible with both
encoder-based PLMs and decoder-based LLMs, to embrace the benefits of enlarged
pretraining. Experiments with three benchmark datasets with both encoder-based
PLMs and decoder-based LLMs demonstrate our superiority compared to the
previous state of the art. Our code is available at: github.com/LITONG99/Citss

</details>


### [112] [PlanGPT-VL: Enhancing Urban Planning with Domain-Specific Vision-Language Models](https://arxiv.org/abs/2505.14481)
*He Zhu,Junyou Su,Minxi Chen,Wen Wang,Yijie Deng,Guanhua Chen,Wenjia Zhang*

Main category: cs.CL

TL;DR: PlanGPT-VL是首个针对城市规划地图的视觉语言模型，通过创新方法提升专业地图分析能力，性能优于通用模型且参数高效。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型在城市规划地图分析上效果不佳，而这类地图包含关键的空间配置和法规信息，需要专业化的理解。

Method: 提出PlanGPT-VL模型，采用PlanAnno-V数据合成框架、Critical Point Thinking减少幻觉，以及结合监督微调和冻结视觉编码器的训练方法。

Result: PlanGPT-VL在PlanBench-V基准测试中显著优于通用VLMs，7B参数模型性能媲美72B参数模型，保持高准确率。

Conclusion: PlanGPT-VL为城市规划提供了可靠的专用分析工具，在专业性和效率间取得平衡，适用于教育与实践场景。

Abstract: In the field of urban planning, existing Vision-Language Models (VLMs)
frequently fail to effectively analyze and evaluate planning maps, despite the
critical importance of these visual elements for urban planners and related
educational contexts. Planning maps, which visualize land use, infrastructure
layouts, and functional zoning, require specialized understanding of spatial
configurations, regulatory requirements, and multi-scale analysis. To address
this challenge, we introduce PlanGPT-VL, the first domain-specific
Vision-Language Model tailored specifically for urban planning maps. PlanGPT-VL
employs three innovative approaches: (1) PlanAnno-V framework for high-quality
VQA data synthesis, (2) Critical Point Thinking to reduce hallucinations
through structured verification, and (3) comprehensive training methodology
combining Supervised Fine-Tuning with frozen vision encoder parameters. Through
systematic evaluation on our proposed PlanBench-V benchmark, we demonstrate
that PlanGPT-VL significantly outperforms general-purpose state-of-the-art VLMs
in specialized planning map interpretation tasks, offering urban planning
professionals a reliable tool for map analysis, assessment, and educational
applications while maintaining high factual accuracy. Our lightweight 7B
parameter model achieves comparable performance to models exceeding 72B
parameters, demonstrating efficient domain specialization without sacrificing
performance.

</details>


### [113] [MoMoE: Mixture of Moderation Experts Framework for AI-Assisted Online Governance](https://arxiv.org/abs/2505.14483)
*Agam Goyal,Xianyang Zhan,Yilun Chen,Koustuv Saha,Eshwar Chandrasekharan*

Main category: cs.CL

TL;DR: MoMoE框架通过模块化专家系统实现跨社区内容审核，提供可解释性且无需针对每个社区微调，性能媲美或超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有内容审核方法需为每个社区单独训练模型且决策不透明，限制了实际应用。本文旨在开发一个可扩展、透明的跨社区审核框架。

Method: 提出混合审核专家(MoMoE)框架，包含分配、预测、聚合、解释四个操作模块，具体实现为7个社区专家和5个违规专家。

Result: 在30个未见过的subreddit上，最佳变体Micro-F1分别达0.72和0.67，匹配或超越微调基线，同时生成简洁可靠解释。

Conclusion: MoMoE证明轻量级可解释专家系统能实现可信赖的AI内容治理，为NLP和HCI研究提供新方向。

Abstract: Large language models (LLMs) have shown great potential in flagging harmful
content in online communities. Yet, existing approaches for moderation require
a separate model for every community and are opaque in their decision-making,
limiting real-world adoption. We introduce Mixture of Moderation Experts
(MoMoE), a modular, cross-community framework that adds post-hoc explanations
to scalable content moderation. MoMoE orchestrates four operators -- Allocate,
Predict, Aggregate, Explain -- and is instantiated as seven
community-specialized experts (MoMoE-Community) and five norm-violation experts
(MoMoE-NormVio). On 30 unseen subreddits, the best variants obtain Micro-F1
scores of 0.72 and 0.67, respectively, matching or surpassing strong fine-tuned
baselines while consistently producing concise and reliable explanations.
Although community-specialized experts deliver the highest peak accuracy,
norm-violation experts provide steadier performance across domains. These
findings show that MoMoE yields scalable, transparent moderation without
needing per-community fine-tuning. More broadly, they suggest that lightweight,
explainable expert ensembles can guide future NLP and HCI research on
trustworthy human-AI governance of online communities.

</details>


### [114] [Enhanced Multimodal Aspect-Based Sentiment Analysis by LLM-Generated Rationales](https://arxiv.org/abs/2505.14499)
*Jun Cao,Jiyi Li,Ziwei Yang,Renjie Zhou*

Main category: cs.CL

TL;DR: 本文提出了一种结合小型语言模型(SLMs)和大型语言模型(LLMs)优势的新框架LRSA，用于多模态方面情感分析(MABSA)，通过LLMs生成的解释增强SLMs的能力，并在多个基准测试中表现出优越性。


<details>
  <summary>Details</summary>
Motivation: 现有的MABSA方法主要依赖预训练的小型语言模型(SLMs)，但其能力有限，导致在文本和视觉数据中识别方面、情感及其关联时不够准确。尽管大型语言模型(LLMs)在多模态任务中表现出色，但在ABSA领域仍不及经过微调的小型模型。因此，需要一种结合两者优势的方法。

Method: 提出LRSA框架，将LLMs生成的解释作为理性注入SLMs，并采用双重交叉注意力机制增强特征交互和融合，从而提升SLMs在识别方面和情感方面的能力。

Result: 在两个基线模型和三个广泛使用的基准测试中，LRSA表现出优越性，证明了其泛化能力和对大多数预训练模型的适用性。

Conclusion: LRSA框架通过结合SLMs和LLMs的优势，有效提升了多模态方面情感分析的性能，为未来研究提供了新的方向。

Abstract: There has been growing interest in Multimodal Aspect-Based Sentiment Analysis
(MABSA) in recent years. Existing methods predominantly rely on pre-trained
small language models (SLMs) to collect information related to aspects and
sentiments from both image and text, with an aim to align these two modalities.
However, small SLMs possess limited capacity and knowledge, often resulting in
inaccurate identification of meaning, aspects, sentiments, and their
interconnections in textual and visual data. On the other hand, Large language
models (LLMs) have shown exceptional capabilities in various tasks by
effectively exploring fine-grained information in multimodal data. However,
some studies indicate that LLMs still fall short compared to fine-tuned small
models in the field of ABSA. Based on these findings, we propose a novel
framework, termed LRSA, which combines the decision-making capabilities of SLMs
with additional information provided by LLMs for MABSA. Specifically, we inject
explanations generated by LLMs as rationales into SLMs and employ a dual
cross-attention mechanism for enhancing feature interaction and fusion, thereby
augmenting the SLMs' ability to identify aspects and sentiments. We evaluated
our method using two baseline models, numerous experiments highlight the
superiority of our approach on three widely-used benchmarks, indicating its
generalizability and applicability to most pre-trained models for MABSA.

</details>


### [115] [ModRWKV: Transformer Multimodality in Linear Time](https://arxiv.org/abs/2505.14505)
*Jiale Kang,Ziyin Yue,Qingyu Yin,Jiang Rui,Weile Li,Zening Lu,Zhouran Ji*

Main category: cs.CL

TL;DR: 该论文提出了一种基于RWKV7架构的轻量级多模态框架ModRWKV，通过动态适配异构模态编码器实现多源信息融合，在性能和计算效率间取得平衡，证明了现代RNN架构在多模态大语言模型中的可行性。


<details>
  <summary>Details</summary>
Motivation: 当前多模态研究主要依赖计算复杂度高的Transformer架构，而线性模型如RNN虽推理成本低，但多局限于文本模态。本文旨在探索现代RNN架构在多模态场景下的潜力。

Method: 基于RWKV7架构构建解耦式多模态框架ModRWKV，采用动态可调的异构模态编码器进行多源信息融合，设计极轻量级多模态模块，并利用RWKV7预训练权重加速训练。

Result: 实验表明，ModRWKV在性能与计算效率间达到最优平衡，预训练权重初始化显著提升模型的多模态信号理解能力。

Conclusion: 现代RNN架构可作为Transformer的可行替代方案应用于多模态大语言模型领域，系统探索确定了ModRWKV的最佳配置。

Abstract: Currently, most multimodal studies are based on large language models (LLMs)
with quadratic-complexity Transformer architectures. While linear models like
RNNs enjoy low inference costs, their application has been largely limited to
the text-only modality. This work explores the capabilities of modern RNN
architectures in multimodal contexts. We propose ModRWKV-a decoupled multimodal
framework built upon the RWKV7 architecture as its LLM backbone-which achieves
multi-source information fusion through dynamically adaptable heterogeneous
modality encoders. We designed the multimodal modules in ModRWKV with an
extremely lightweight architecture and, through extensive experiments,
identified a configuration that achieves an optimal balance between performance
and computational efficiency. ModRWKV leverages the pretrained weights of the
RWKV7 LLM for initialization, which significantly accelerates multimodal
training. Comparative experiments with different pretrained checkpoints further
demonstrate that such initialization plays a crucial role in enhancing the
model's ability to understand multimodal signals. Supported by extensive
experiments, we conclude that modern RNN architectures present a viable
alternative to Transformers in the domain of multimodal large language models
(MLLMs). Furthermore, we identify the optimal configuration of the ModRWKV
architecture through systematic exploration.

</details>


### [116] [Exploring Graph Representations of Logical Forms for Language Modeling](https://arxiv.org/abs/2505.14523)
*Michael Sullivan*

Main category: cs.CL

TL;DR: 论文提出基于逻辑形式的语言模型（LFLMs），证明其在数据效率上优于纯文本模型，并通过GFoLDS原型验证了其潜力。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于展示逻辑形式语言模型相比纯文本模型具有更高的数据效率，能够利用内置的底层语言知识快速学习复杂模式。

Method: 方法为开发GFoLDS原型——一种基于图表示逻辑形式的预训练语言模型，作为LFLMs的概念验证。

Result: 实验表明GFoLDS在少量数据下显著优于同规模纯文本Transformer模型，且性能可能随参数和数据量提升而扩展。

Conclusion: 结论指出LFLMs在实际应用中具有可行性，因其能以更少数据实现高效学习，且具备可扩展性。

Abstract: We make the case for language models over logical forms (LFLMs), arguing that
such models are more data-efficient than their textual counterparts. To that
end, we introduce the Graph-based Formal-Logical Distributional Semantics
(GFoLDS) prototype, a pretrained LM over graph representations of logical
forms, as a proof-of-concept of LFLMs. Using GFoLDS, we present strong
experimental evidence that LFLMs can leverage the built-in, basic linguistic
knowledge inherent in such models to immediately begin learning more complex
patterns. On downstream tasks, we show that GFoLDS vastly outperforms textual,
transformer LMs pretrained on similar amounts of data, indicating that LFLMs
can learn with substantially less data than models over plain text.
Furthermore, we show that the performance of this model is likely to scale with
additional parameters and pretraining data, suggesting the viability of LFLMs
in real-world applications.

</details>


### [117] [Internal Chain-of-Thought: Empirical Evidence for Layer-wise Subtask Scheduling in LLMs](https://arxiv.org/abs/2505.14530)
*Zhipeng Yang,Junzhuo Li,Siyu Xia,Xuming Hu*

Main category: cs.CL

TL;DR: 研究发现大语言模型（LLMs）具有内部思维链能力，能分层分解和执行复合任务，并通过实验验证了其分层执行模式。


<details>
  <summary>Details</summary>
Motivation: 探索大语言模型如何内部处理和分解复合任务，以增强模型透明性并揭示其执行机制。

Method: 使用层间上下文掩码和新颖的跨任务修补方法，结合LogitLens解码隐藏状态，分析模型的分层执行模式。

Result: 在15个两步复合任务基准测试和真实世界TRACE基准上，均观察到模型的分层逐步执行模式。

Conclusion: 研究证实LLMs能内部规划和执行子任务，为细粒度的指令级激活调控提供了新方向。

Abstract: We show that large language models (LLMs) exhibit an $\textit{internal
chain-of-thought}$: they sequentially decompose and execute composite tasks
layer-by-layer. Two claims ground our study: (i) distinct subtasks are learned
at different network depths, and (ii) these subtasks are executed sequentially
across layers. On a benchmark of 15 two-step composite tasks, we employ
layer-from context-masking and propose a novel cross-task patching method,
confirming (i). To examine claim (ii), we apply LogitLens to decode hidden
states, revealing a consistent layerwise execution pattern. We further
replicate our analysis on the real-world $\text{TRACE}$ benchmark, observing
the same stepwise dynamics. Together, our results enhance LLMs transparency by
showing their capacity to internally plan and execute subtasks (or
instructions), opening avenues for fine-grained, instruction-level activation
steering.

</details>


### [118] [Breaking Bad Tokens: Detoxification of LLMs Using Sparse Autoencoders](https://arxiv.org/abs/2505.14536)
*Agam Goyal,Vedant Rathi,William Yeh,Yian Wang,Yuen Chen,Hari Sundaram*

Main category: cs.CL

TL;DR: 论文利用稀疏自编码器(SAE)定位大语言模型中的毒性特征方向，并通过激活导向干预实现针对性去毒，在降低毒性20%的同时保持模型核心能力，但过强干预会损害流畅性。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型(LLM)仍会生成不当内容，传统去毒方法易被绕过。需开发更精准的干预技术，在保持模型性能的同时有效抑制毒性输出。

Method: 采用稀疏自编码器识别残差流中的毒性相关方向，通过解码器向量进行三级强度（温和/中等/激进）的激活导向干预，在GPT-2 Small和Gemma-2-2B上验证。

Result: 激进干预使毒性降低20%但影响流畅性（尤其GPT-2 Small），模型知识保留（NLP基准稳定）。宽SAE的特征分裂会削弱安全干预效果。

Conclusion: SAE因果干预在LLM去毒中具有潜力但存在流畅性权衡，需平衡干预强度与特征解耦学习，为安全部署提供实践指导。

Abstract: Large language models (LLMs) are now ubiquitous in user-facing applications,
yet they still generate undesirable toxic outputs, including profanity,
vulgarity, and derogatory remarks. Although numerous detoxification methods
exist, most apply broad, surface-level fixes and can therefore easily be
circumvented by jailbreak attacks. In this paper we leverage sparse
autoencoders (SAEs) to identify toxicity-related directions in the residual
stream of models and perform targeted activation steering using the
corresponding decoder vectors. We introduce three tiers of steering
aggressiveness and evaluate them on GPT-2 Small and Gemma-2-2B, revealing
trade-offs between toxicity reduction and language fluency. At stronger
steering strengths, these causal interventions surpass competitive baselines in
reducing toxicity by up to 20%, though fluency can degrade noticeably on GPT-2
Small depending on the aggressiveness. Crucially, standard NLP benchmark scores
upon steering remain stable, indicating that the model's knowledge and general
abilities are preserved. We further show that feature-splitting in wider SAEs
hampers safety interventions, underscoring the importance of disentangled
feature learning. Our findings highlight both the promise and the current
limitations of SAE-based causal interventions for LLM detoxification, further
suggesting practical guidelines for safer language-model deployment.

</details>


### [119] [KORGym: A Dynamic Game Platform for LLM Reasoning Evaluation](https://arxiv.org/abs/2505.14552)
*Jiajun Shi,Jian Yang,Jiaheng Liu,Xingyuan Bu,Jiangjie Chen,Junting Zhou,Kaijing Ma,Zhoufutu Wen,Bingli Wang,Yancheng He,Liang Song,Hualei Zhu,Shilong Li,Xingjian Wang,Wei Zhang,Ruibin Yuan,Yifan Yao,Wenjun Yang,Yunli Wang,Siyuan Fang,Siyu Yuan,Qianyu He,Xiangru Tang,Yingshui Tan,Wangchunshu Zhou,Zhaoxiang Zhang,Zhoujun Li,Wenhao Huang,Ge Zhang*

Main category: cs.CL

TL;DR: 研究者开发了KORGym平台，用于更全面评估大语言模型的推理能力，测试了19个LLM和8个VLM，发现闭源模型表现更优。


<details>
  <summary>Details</summary>
Motivation: 现有评估基准多为领域特定，无法全面衡量大语言模型的通用推理潜力，因此需要更全面的评估方法。

Method: 引入KORGym动态评估平台，包含50多种文本或视觉游戏，支持交互式多轮评估和强化学习场景。

Result: 测试显示模型家族内存在一致的推理模式，闭源模型表现更优，并分析了模态、推理策略、强化学习技术和响应长度对性能的影响。

Conclusion: KORGym有望成为推动LLM推理研究和开发适合复杂交互环境评估方法的重要资源。

Abstract: Recent advancements in large language models (LLMs) underscore the need for
more comprehensive evaluation methods to accurately assess their reasoning
capabilities. Existing benchmarks are often domain-specific and thus cannot
fully capture an LLM's general reasoning potential. To address this limitation,
we introduce the Knowledge Orthogonal Reasoning Gymnasium (KORGym), a dynamic
evaluation platform inspired by KOR-Bench and Gymnasium. KORGym offers over
fifty games in either textual or visual formats and supports interactive,
multi-turn assessments with reinforcement learning scenarios. Using KORGym, we
conduct extensive experiments on 19 LLMs and 8 VLMs, revealing consistent
reasoning patterns within model families and demonstrating the superior
performance of closed-source models. Further analysis examines the effects of
modality, reasoning strategies, reinforcement learning techniques, and response
length on model performance. We expect KORGym to become a valuable resource for
advancing LLM reasoning research and developing evaluation methodologies suited
to complex, interactive environments.

</details>


### [120] [Pivot Language for Low-Resource Machine Translation](https://arxiv.org/abs/2505.14553)
*Abhimanyu Talwar,Julien Laasri*

Main category: cs.CL

TL;DR: 该论文探讨了使用印地语作为枢轴语言将尼泊尔语翻译成英语的方法，比较了两种不同翻译策略的效果，并分析了性能差异的原因。


<details>
  <summary>Details</summary>
Motivation: 某些语言对缺乏大规模、多领域的平行语料库，通过枢轴语言（如印地语）可以解决这一问题。本文选择印地语作为尼泊尔语到英语翻译的枢轴语言，并探讨其适用性。

Method: 论文采用了两种方法：完全监督的转移方法和半监督的回译方法，用于实现尼泊尔语到英语的翻译。

Result: 使用转移方法时，开发测试集的SacreBLEU得分为14.2，比基线提高了6.6分；但半监督方法的得分略低于基线15.1，论文对此进行了原因分析。

Conclusion: 论文总结了枢轴语言在翻译中的有效性，提出了未来改进的方向，特别是在半监督方法性能不足的问题上。

Abstract: Certain pairs of languages suffer from lack of a parallel corpus which is
large in size and diverse in domain. One of the ways this is overcome is via
use of a pivot language. In this paper we use Hindi as a pivot language to
translate Nepali into English. We describe what makes Hindi a good candidate
for the pivot. We discuss ways in which a pivot language can be used, and use
two such approaches - the Transfer Method (fully supervised) and
Backtranslation (semi-supervised) - to translate Nepali into English. Using the
former, we are able to achieve a devtest Set SacreBLEU score of 14.2, which
improves the baseline fully supervised score reported by (Guzman et al., 2019)
by 6.6 points. While we are slightly below the semi-supervised baseline score
of 15.1, we discuss what may have caused this under-performance, and suggest
scope for future work.

</details>


### [121] [TRATES: Trait-Specific Rubric-Assisted Cross-Prompt Essay Scoring](https://arxiv.org/abs/2505.14577)
*Sohaila Eltanbouly,Salam Albatarni,Tamer Elsayed*

Main category: cs.CL

TL;DR: TRATES框架利用LLM生成特征，结合传统回归模型，实现了跨提示的自动化作文评分新突破。


<details>
  <summary>Details</summary>
Motivation: 现有自动化作文评分系统缺乏对个体特征的针对性评估，需要一种能够根据具体评分标准评估作文特质的方法。

Method: 提出TRATES框架，利用大型语言模型根据评分标准生成特质相关特征，再结合通用写作质量和提示特定特征，训练回归模型预测未见提示的作文特质分数。

Result: TRATES在广泛使用的数据集上实现了所有特质评分的最新最优性能，其中LLM生成的特征贡献最大。

Conclusion: TRATES框架通过结合LLM生成的特征和传统回归模型，显著提升了自动化作文评分的准确性和泛化能力。

Abstract: Research on holistic Automated Essay Scoring (AES) is long-dated; yet, there
is a notable lack of attention for assessing essays according to individual
traits. In this work, we propose TRATES, a novel trait-specific and
rubric-based cross-prompt AES framework that is generic yet specific to the
underlying trait. The framework leverages a Large Language Model (LLM) that
utilizes the trait grading rubrics to generate trait-specific features
(represented by assessment questions), then assesses those features given an
essay. The trait-specific features are eventually combined with generic
writing-quality and prompt-specific features to train a simple classical
regression model that predicts trait scores of essays from an unseen prompt.
Experiments show that TRATES achieves a new state-of-the-art performance across
all traits on a widely-used dataset, with the generated LLM-based features
being the most significant.

</details>


### [122] [Can Pruning Improve Reasoning? Revisiting Long-CoT Compression with Capability in Mind for Better Reasoning](https://arxiv.org/abs/2505.14582)
*Shangziqi Zhao,Jiahao Yuan,Guisong Yang,Usman Naseem*

Main category: cs.CL

TL;DR: 通过剪枝优化长思维链推理，提升小语言模型推理效率与准确性。


<details>
  <summary>Details</summary>
Motivation: 长思维链推理（Long-CoT）虽能提升大语言模型准确性，但其冗长自省的风格难以有效压缩至小语言模型（SLMs）。研究旨在探索剪枝是否可改进推理效率。

Method: 提出Prune-on-Logic框架，将Long-CoT转为逻辑图，在自验证约束下选择性剪枝低效推理步骤，分析三种剪枝策略（全链、核心推理、验证步骤）。

Result: 剪枝验证步骤能稳定提升准确性并降低推理成本，优于词级基线及未压缩微调；而剪枝推理或全链步骤会降低性能，表明小模型需语义更精简的CoT。

Conclusion: 剪枝是一种结构优化策略，可对齐CoT推理与小模型能力，凸显语义精简而非单纯缩短链的重要性。

Abstract: Long chain-of-thought (Long-CoT) reasoning improves accuracy in LLMs, yet its
verbose, self-reflective style often hinders effective distillation into small
language models (SLMs). We revisit Long-CoT compression through the lens of
capability alignment and ask: Can pruning improve reasoning? We propose
Prune-on-Logic, a structure-aware framework that transforms Long-CoT into logic
graphs and selectively prunes low-utility reasoning steps under
self-verification constraints. Through systematic analysis across three pruning
strategies -- targeting entire chains, core reasoning, and verification -- we
find that pruning verification steps yields consistent accuracy gains while
reducing inference cost, outperforming token-level baselines and uncompressed
fine-tuning. In contrast, pruning reasoning or all-chain steps degrades
performance, revealing that small models benefit not from shorter CoTs, but
from semantically leaner ones. Our findings highlight pruning as a structural
optimization strategy for aligning CoT reasoning with SLM capacity.

</details>


### [123] [Context Reasoner: Incentivizing Reasoning Capability for Contextualized Privacy and Safety Compliance via Reinforcement Learning](https://arxiv.org/abs/2505.14585)
*Wenbin Hu,Haoran Li,Huihao Jing,Qi Hu,Ziqian Zeng,Sirui Han,Heli Xu,Tianshu Chu,Peizhao Hu,Yangqiu Song*

Main category: cs.CL

TL;DR: 该论文提出了一种基于情境完整性理论的强化学习方法，以提升大语言模型在安全隐私合规性和通用推理能力上的表现。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型存在显著的安全和隐私风险，现有缓解策略往往依赖敏感模式匹配，忽略了法律合规标准，且会损害模型的上下文推理能力。

Method: 采用情境完整性理论框架，结合GDPR、EU AI Act和HIPAA三大法规标准，使用基于规则的强化学习奖励机制来提升模型的合规性和推理能力。

Result: 实验表明，该方法显著提升了法律合规性（安全/隐私基准准确率提升17.64%），同时增强了通用推理能力（MMLU和LegalBench基准分别提升2.05%和8.98%）。

Conclusion: 通过情境化合规方法，既能有效解决大语言模型的安全隐私风险，又能保持甚至提升其核心推理能力。

Abstract: While Large Language Models (LLMs) exhibit remarkable capabilities, they also
introduce significant safety and privacy risks. Current mitigation strategies
often fail to preserve contextual reasoning capabilities in risky scenarios.
Instead, they rely heavily on sensitive pattern matching to protect LLMs, which
limits the scope. Furthermore, they overlook established safety and privacy
standards, leading to systemic risks for legal compliance. To address these
gaps, we formulate safety and privacy issues into contextualized compliance
problems following the Contextual Integrity (CI) theory. Under the CI
framework, we align our model with three critical regulatory standards: GDPR,
EU AI Act, and HIPAA. Specifically, we employ reinforcement learning (RL) with
a rule-based reward to incentivize contextual reasoning capabilities while
enhancing compliance with safety and privacy norms. Through extensive
experiments, we demonstrate that our method not only significantly enhances
legal compliance (achieving a +17.64% accuracy improvement in safety/privacy
benchmarks) but also further improves general reasoning capability. For
OpenThinker-7B, a strong reasoning model that significantly outperforms its
base model Qwen2.5-7B-Instruct across diverse subjects, our method enhances its
general reasoning capabilities, with +2.05% and +8.98% accuracy improvement on
the MMLU and LegalBench benchmark, respectively.

</details>


### [124] [MCIP: Protecting MCP Safety via Model Contextual Integrity Protocol](https://arxiv.org/abs/2505.14590)
*Huihao Jing,Haoran Li,Wenbin Hu,Qi Hu,Heli Xu,Tianshu Chu,Peizhao Hu,Yangqiu Song*

Main category: cs.CL

TL;DR: 该论文提出了一种新框架MCIP，以增强MCP的安全性，并通过实验证明其能显著提升LLMs在MCP交互中的安全性能。


<details>
  <summary>Details</summary>
Motivation: MCP的分散式架构虽然为用户和开发者提供了易用的生态系统，但也带来了未被充分探索的安全风险，需要进行系统性安全分析。

Method: 基于MAESTRO框架分析MCP缺失的安全机制，提出改进版MCIP；开发细粒度分类法捕捉不安全行为，构建基准和训练数据以评估和改进LLMs的安全风险识别能力。

Result: 实验结果表明，LLMs在MCP交互中存在漏洞，而提出的方法显著提升了它们的安全性能。

Conclusion: 通过MCIP框架和配套工具，可以有效提升MCP及LLMs在安全方面的表现。

Abstract: As Model Context Protocol (MCP) introduces an easy-to-use ecosystem for users
and developers, it also brings underexplored safety risks. Its decentralized
architecture, which separates clients and servers, poses unique challenges for
systematic safety analysis. This paper proposes a novel framework to enhance
MCP safety. Guided by the MAESTRO framework, we first analyze the missing
safety mechanisms in MCP, and based on this analysis, we propose the Model
Contextual Integrity Protocol (MCIP), a refined version of MCP that addresses
these gaps.Next, we develop a fine-grained taxonomy that captures a diverse
range of unsafe behaviors observed in MCP scenarios. Building on this taxonomy,
we develop benchmark and training data that support the evaluation and
improvement of LLMs' capabilities in identifying safety risks within MCP
interactions. Leveraging the proposed benchmark and training data, we conduct
extensive experiments on state-of-the-art LLMs. The results highlight LLMs'
vulnerabilities in MCP interactions and demonstrate that our approach
substantially improves their safety performance.

</details>


### [125] [Success is in the Details: Evaluate and Enhance Details Sensitivity of Code LLMs through Counterfactuals](https://arxiv.org/abs/2505.14597)
*Xianzhen Luo,Qingfu Zhu,Zhiming Zhang,Mingzheng Xu,Tianhao Cheng,Yixuan Wang,Zheng Chu,Shijie Xuyang,Zhiyuan Ma,YuanTao Fan,Wanxiang Che*

Main category: cs.CL

TL;DR: 论文提出代码敏感性概念，构建CTF-Code基准测试，并通过CTF-Instruct微调框架提升LLMs性能。


<details>
  <summary>Details</summary>
Motivation: 当前代码基准测试和指令数据主要关注难度和多样性，忽视了代码敏感性（即模型对问题描述细节变化的识别与响应能力）。

Method: 1. 使用反事实扰动构建CTF-Code基准测试；2. 提出CTF-Instruct增量指令微调框架，结合选择机制平衡难度、多样性和敏感性。

Result: 实验显示：LLMs在CTF-Code上性能下降超10%；经CTF-Instruct微调的模型在CTF-Code上提升2%，在LiveCodeBench上提升超10%。

Conclusion: 增强代码敏感性可有效提升LLMs性能，CTF-Instruct框架具有可行性。

Abstract: Code Sensitivity refers to the ability of Code LLMs to recognize and respond
to details changes in problem descriptions. While current code benchmarks and
instruction data focus on difficulty and diversity, sensitivity is overlooked.
We first introduce the CTF-Code benchmark, constructed using counterfactual
perturbations, minimizing input changes while maximizing output changes. The
evaluation shows that many LLMs have a more than 10\% performance drop compared
to the original problems. To fully utilize sensitivity, CTF-Instruct, an
incremental instruction fine-tuning framework, extends on existing data and
uses a selection mechanism to meet the three dimensions of difficulty,
diversity, and sensitivity. Experiments show that LLMs fine-tuned with
CTF-Instruct data achieve over a 2\% improvement on CTF-Code, and more than a
10\% performance boost on LiveCodeBench, validating the feasibility of
enhancing LLMs' sensitivity to improve performance.

</details>


### [126] [Toward Reliable Biomedical Hypothesis Generation: Evaluating Truthfulness and Hallucination in Large Language Models](https://arxiv.org/abs/2505.14599)
*Guangzhi Xiong,Eric Xie,Corey Williams,Myles Kim,Amir Hassan Shariatmadari,Sikun Guo,Stefan Bekiranov,Aidong Zhang*

Main category: cs.CL

TL;DR: 该论文提出了TruthHypo基准和KnowHD检测器，用于评估大语言模型在生成真实生物医学假设方面的能力，并解决幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在生物医学等科学领域具有生成假设的潜力，但其生成的假设可能存在不真实或幻觉问题，验证这些假设需要大量时间和资源。

Method: 引入TruthHypo基准和KnowHD知识基础的幻觉检测器，评估大语言模型生成假设的真实性，并通过分析推理步骤中的幻觉来过滤真实假设。

Result: 研究表明，大语言模型在生成真实假设方面存在困难，KnowHD提供的接地性评分能有效过滤真实假设，人类评估进一步验证了其效用。

Conclusion: KnowHD能有效识别真实假设，加速科学发现，相关数据和源代码已公开。

Abstract: Large language models (LLMs) have shown significant potential in scientific
disciplines such as biomedicine, particularly in hypothesis generation, where
they can analyze vast literature, identify patterns, and suggest research
directions. However, a key challenge lies in evaluating the truthfulness of
generated hypotheses, as verifying their accuracy often requires substantial
time and resources. Additionally, the hallucination problem in LLMs can lead to
the generation of hypotheses that appear plausible but are ultimately
incorrect, undermining their reliability. To facilitate the systematic study of
these challenges, we introduce TruthHypo, a benchmark for assessing the
capabilities of LLMs in generating truthful biomedical hypotheses, and KnowHD,
a knowledge-based hallucination detector to evaluate how well hypotheses are
grounded in existing knowledge. Our results show that LLMs struggle to generate
truthful hypotheses. By analyzing hallucinations in reasoning steps, we
demonstrate that the groundedness scores provided by KnowHD serve as an
effective metric for filtering truthful hypotheses from the diverse outputs of
LLMs. Human evaluations further validate the utility of KnowHD in identifying
truthful hypotheses and accelerating scientific discovery. Our data and source
code are available at https://github.com/Teddy-XiongGZ/TruthHypo.

</details>


### [127] [sudoLLM : On Multi-role Alignment of Language Models](https://arxiv.org/abs/2505.14607)
*Soumadeep Saha,Akshay Chaturvedi,Joy Mahapatra,Utpal Garain*

Main category: cs.CL

TL;DR: 本文提出sudoLLM框架，通过用户授权机制增强大语言模型的安全性，实现多角色对齐并抵御越狱攻击。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型缺乏用户权限控制功能，存在安全风险，需要借鉴关键安全系统的访问控制机制。

Method: 在查询中注入用户偏置信号，训练模型仅对授权用户返回敏感信息。

Result: 实验表明该方法显著提升模型对齐性、泛化能力和抗越狱攻击能力。

Conclusion: sudoLLM作为补充安全层，可与现有防护机制协同增强端到端安全性。

Abstract: User authorization-based access privileges are a key feature in many
safety-critical systems, but have thus far been absent from the large language
model (LLM) realm. In this work, drawing inspiration from such access control
systems, we introduce sudoLLM, a novel framework that results in multi-role
aligned LLMs, i.e., LLMs that account for, and behave in accordance with, user
access rights. sudoLLM injects subtle user-based biases into queries and trains
an LLM to utilize this bias signal in order to produce sensitive information if
and only if the user is authorized. We present empirical results demonstrating
that this approach shows substantially improved alignment, generalization, and
resistance to prompt-based jailbreaking attacks. The persistent tension between
the language modeling objective and safety alignment, which is often exploited
to jailbreak LLMs, is somewhat resolved with the aid of the injected bias
signal. Our framework is meant as an additional security layer, and complements
existing guardrail mechanisms for enhanced end-to-end safety with LLMs.

</details>


### [128] [Language Models Optimized to Fool Detectors Still Have a Distinct Style (And How to Change It)](https://arxiv.org/abs/2505.14608)
*Rafael Rivera Soto,Barry Chen,Nicholas Andrews*

Main category: cs.CL

TL;DR: 该论文探讨了机器生成文本检测的固有困难，并提出了一种基于风格特征的检测方法，该方法对模型优化具有鲁棒性。同时，论文还引入了一种新的度量标准AURA，用于评估人类与机器生成文本分布的重叠程度。


<details>
  <summary>Details</summary>
Motivation: 尽管在机器文本检测方面取得了显著进展，但有人认为这一问题本质上是困难的，因此利益相关者应假设机器生成的文本无法被可靠地检测。论文旨在验证这一观点，并探索更鲁棒的检测方法。

Method: 论文通过分析风格特征空间，提出了一种鲁棒的检测方法，并引入了一种新的转述方法，用于同时缩小人类写作与机器写作在风格特征空间上的差距。此外，论文还提出了AURA度量标准，用于评估分布重叠。

Result: 研究发现，风格检测器在模型优化后仍能保持较高的检测性能。当仅有一个样本时，攻击对所有检测器都有效，但随着样本数量的增加，人类与机器生成的分布变得可区分。AURA度量标准能够有效评估分布重叠。

Conclusion: 论文的发现强调了避免依赖机器文本检测的建议，并展示了风格特征空间在检测中的鲁棒性。同时，AURA度量标准为评估检测性能提供了新的视角。

Abstract: Despite considerable progress in the development of machine-text detectors,
it has been suggested that the problem is inherently hard, and therefore, that
stakeholders should proceed under the assumption that machine-generated text
cannot be reliably detected as such. We examine a recent such claim by Nicks et
al. (2024) regarding the ease with which language models can be optimized to
degrade the performance of machine-text detectors, including detectors not
specifically optimized against. We identify a feature space$\unicode{x2013}$the
stylistic feature space$\unicode{x2013}$that is robust to such optimization,
and show that it may be used to reliably detect samples from language models
optimized to prevent detection. Furthermore, we show that even when models are
explicitly optimized against stylistic detectors, detection performance remains
surprisingly unaffected. We then seek to understand if stylistic detectors are
inherently more robust. To study this question, we explore a new paraphrasing
approach that simultaneously aims to close the gap between human writing and
machine writing in stylistic feature space while avoiding detection using
traditional features. We show that when only a single sample is available for
detection, this attack is universally effective across all detectors
considered, including those that use writing style. However, as the number of
samples available for detection grows, the human and machine distributions
become distinguishable. This observation encourages us to introduce AURA, a
metric that estimates the overlap between human and machine-generated
distributions by analyzing how detector performance improves as more samples
become available. Overall, our findings underscore previous recommendations to
avoid reliance on machine-text detection.

</details>


### [129] [Linear Control of Test Awareness Reveals Differential Compliance in Reasoning Models](https://arxiv.org/abs/2505.14617)
*Sahar Abdelnabi,Ahmed Salem*

Main category: cs.CL

TL;DR: 研究发现大型语言模型在测试时会改变行为（类似霍桑效应），影响安全对齐。作者提出白盒探测框架量化这种影响，并展示不同模型受影响程度不同。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型在测试时可能出现行为偏移（如为通过测试优化表现或更易响应有害指令），这种'测试意识'对安全评估的影响尚未被量化研究。

Method: 提出白盒探测框架：(1)线性识别测试意识相关激活；(2)通过操控激活值引导模型进入/脱离测试意识状态，同时监测下游表现。

Result: 测试意识显著影响模型安全对齐效果，且不同模型受影响程度存在差异。框架实现了对该潜在效应的细粒度控制。

Conclusion: 通过量化测试意识的影响并提供控制方法，该研究有助于提升安全评估的可信度。

Abstract: Reasoning-focused large language models (LLMs) sometimes alter their behavior
when they detect that they are being evaluated, an effect analogous to the
Hawthorne phenomenon, which can lead them to optimize for test-passing
performance or to comply more readily with harmful prompts if real-world
consequences appear absent. We present the first quantitative study of how such
"test awareness" impacts model behavior, particularly its safety alignment. We
introduce a white-box probing framework that (i) linearly identifies
awareness-related activations and (ii) steers models toward or away from test
awareness while monitoring downstream performance. We apply our method to
different state-of-the-art open-source reasoning LLMs across both realistic and
hypothetical tasks. Our results demonstrate that test awareness significantly
impact safety alignment, and is different for different models. By providing
fine-grained control over this latent effect, our work aims to increase trust
in how we perform safety evaluation.

</details>


### [130] [Think Only When You Need with Large Hybrid-Reasoning Models](https://arxiv.org/abs/2505.14631)
*Lingjie Jiang,Xun Wu,Shaohan Huang,Qingxiu Dong,Zewen Chi,Li Dong,Xingxing Zhang,Tengchao Lv,Lei Cui,Furu Wei*

Main category: cs.CL

TL;DR: 本文提出了一种新型的大规模混合推理模型（LHRM），能够根据查询的上下文自适应地决定是否进行深入思考，从而在保持高效的同时提升推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有的LRM模型在处理简单查询时，过长的思考过程会带来不必要的计算开销和延迟。因此，需要一种能够自适应调整思考深度的模型。

Method: 采用两阶段训练流程：首先进行混合微调（HFT）作为冷启动，然后通过在线强化学习（HGPO）隐式学习选择适当的思考模式。

Result: 实验结果表明，LHRM能够自适应处理不同难度和类型的查询，在推理和通用能力上优于现有模型，同时显著提高了效率。

Conclusion: 本文重新思考了扩展思考过程的适用性，为构建混合思考系统提供了坚实的基础。

Abstract: Recent Large Reasoning Models (LRMs) have shown substantially improved
reasoning capabilities over traditional Large Language Models (LLMs) by
incorporating extended thinking processes prior to producing final responses.
However, excessively lengthy thinking introduces substantial overhead in terms
of token consumption and latency, which is particularly unnecessary for simple
queries. In this work, we introduce Large Hybrid-Reasoning Models (LHRMs), the
first kind of model capable of adaptively determining whether to perform
thinking based on the contextual information of user queries. To achieve this,
we propose a two-stage training pipeline comprising Hybrid Fine-Tuning (HFT) as
a cold start, followed by online reinforcement learning with the proposed
Hybrid Group Policy Optimization (HGPO) to implicitly learn to select the
appropriate thinking mode. Furthermore, we introduce a metric called Hybrid
Accuracy to quantitatively assess the model's capability for hybrid thinking.
Extensive experimental results show that LHRMs can adaptively perform hybrid
thinking on queries of varying difficulty and type. It outperforms existing
LRMs and LLMs in reasoning and general capabilities while significantly
improving efficiency. Together, our work advocates for a reconsideration of the
appropriate use of extended thinking processes and provides a solid starting
point for building hybrid thinking systems.

</details>


### [131] [Will AI Tell Lies to Save Sick Children? Litmus-Testing AI Values Prioritization with AIRiskDilemmas](https://arxiv.org/abs/2505.14633)
*Yu Ying Chiu,Zhilin Wang,Sharan Maiya,Yejin Choi,Kyle Fish,Sydney Levine,Evan Hubinger*

Main category: cs.CL

TL;DR: 该论文提出通过识别AI模型的核心价值观来预测其潜在风险行为，开发了LitmusValues评估框架和AIRiskDilemmas数据集进行验证。


<details>
  <summary>Details</summary>
Motivation: 随着AI模型能力的增强，传统风险检测方法（如对齐伪造）可能失效。受人类危险行为常受价值观驱动的启发，研究者认为识别AI模型的价值观可作为风险预警系统。

Method: 1) 创建LitmusValues评估框架，量化AI模型对不同价值观的优先级；2) 构建AIRiskDilemmas数据集，包含涉及AI安全风险的价值观冲突场景（如权力追求）。通过模型在困境中的选择聚合来预测其价值观倾向。

Result: LitmusValues中的价值观（包括看似无害的'关怀'）能有效预测模型在AIRiskDilemmas中的已知风险行为，以及在HarmBench中未见的风险行为。

Conclusion: AI模型的价值观分析可作为新型风险检测手段，为AI安全研究提供早期预警指标。该方法揭示了价值观与风险行为之间的潜在关联。

Abstract: Detecting AI risks becomes more challenging as stronger models emerge and
find novel methods such as Alignment Faking to circumvent these detection
attempts. Inspired by how risky behaviors in humans (i.e., illegal activities
that may hurt others) are sometimes guided by strongly-held values, we believe
that identifying values within AI models can be an early warning system for
AI's risky behaviors. We create LitmusValues, an evaluation pipeline to reveal
AI models' priorities on a range of AI value classes. Then, we collect
AIRiskDilemmas, a diverse collection of dilemmas that pit values against one
another in scenarios relevant to AI safety risks such as Power Seeking. By
measuring an AI model's value prioritization using its aggregate choices, we
obtain a self-consistent set of predicted value priorities that uncover
potential risks. We show that values in LitmusValues (including seemingly
innocuous ones like Care) can predict for both seen risky behaviors in
AIRiskDilemmas and unseen risky behaviors in HarmBench.

</details>


### [132] [General-Reasoner: Advancing LLM Reasoning Across All Domains](https://arxiv.org/abs/2505.14652)
*Xueguang Ma,Qian Liu,Dongfu Jiang,Ge Zhang,Zejun Ma,Wenhu Chen*

Main category: cs.CL

TL;DR: 论文提出General-Reasoner训练范式，通过构建多领域数据集和生成式答案验证器，增强大语言模型的跨领域推理能力。


<details>
  <summary>Details</summary>
Motivation: 当前强化学习提升大语言模型推理能力的研究主要集中在数学和编程领域，因数据丰富且答案易验证。但其他领域问题答案多样且数据稀缺，限制了模型的泛化能力。

Method: 1) 通过爬虫构建大规模多领域高质量问题数据集；2) 开发基于生成模型的答案验证器，利用思维链和上下文感知替代传统规则验证。

Result: 在12个跨领域基准测试（如MMLU-Pro、GPQA等）中，General-Reasoner超越现有基线方法，保持数学推理优势的同时展现强泛化能力。

Conclusion: 该训练范式有效扩展了大语言模型的跨领域推理能力，为通用推理模型的发展提供了新方向。

Abstract: Reinforcement learning (RL) has recently demonstrated strong potential in
enhancing the reasoning capabilities of large language models (LLMs).
Particularly, the "Zero" reinforcement learning introduced by Deepseek-R1-Zero,
enables direct RL training of base LLMs without relying on an intermediate
supervised fine-tuning stage. Despite these advancements, current works for LLM
reasoning mainly focus on mathematical and coding domains, largely due to data
abundance and the ease of answer verification. This limits the applicability
and generalization of such models to broader domains, where questions often
have diverse answer representations, and data is more scarce. In this paper, we
propose General-Reasoner, a novel training paradigm designed to enhance LLM
reasoning capabilities across diverse domains. Our key contributions include:
(1) constructing a large-scale, high-quality dataset of questions with
verifiable answers curated by web crawling, covering a wide range of
disciplines; and (2) developing a generative model-based answer verifier, which
replaces traditional rule-based verification with the capability of
chain-of-thought and context-awareness. We train a series of models and
evaluate them on a wide range of datasets covering wide domains like physics,
chemistry, finance, electronics etc. Our comprehensive evaluation across these
12 benchmarks (e.g. MMLU-Pro, GPQA, SuperGPQA, TheoremQA, BBEH and MATH AMC)
demonstrates that General-Reasoner outperforms existing baseline methods,
achieving robust and generalizable reasoning performance while maintaining
superior effectiveness in mathematical reasoning tasks.

</details>


### [133] [EmoGist: Efficient In-Context Learning for Visual Emotion Understanding](https://arxiv.org/abs/2505.14660)
*Ronald Seoh,Dan Goldwasser*

Main category: cs.CL

TL;DR: EmoGist是一种无需训练的上下文学习方法，通过预生成情感标签解释提升视觉情感分类准确率。


<details>
  <summary>Details</summary>
Motivation: 情感在图像中的表现高度依赖上下文且具有细微差别，传统方法难以准确捕捉。

Method: 通过聚类分析生成多版本情感标签解释，测试时基于嵌入相似性检索并输入快速视觉语言模型分类。

Result: 在Memotion多标签数据集上F1分数提升13点，FI多类数据集上提升8点。

Conclusion: 上下文相关的动态标签定义能显著提升LVLMs的情感分类性能。

Abstract: In this paper, we introduce EmoGist, a training-free, in-context learning
method for performing visual emotion classification with LVLMs. The key
intuition of our approach is that context-dependent definition of emotion
labels could allow more accurate predictions of emotions, as the ways in which
emotions manifest within images are highly context dependent and nuanced.
EmoGist pre-generates multiple explanations of emotion labels, by analyzing the
clusters of example images belonging to each category. At test time, we
retrieve a version of explanation based on embedding similarity, and feed it to
a fast VLM for classification. Through our experiments, we show that EmoGist
allows up to 13 points improvement in micro F1 scores with the multi-label
Memotion dataset, and up to 8 points in macro F1 in the multi-class FI dataset.

</details>


### [134] [Reward Reasoning Model](https://arxiv.org/abs/2505.14674)
*Jiaxin Guo,Zewen Chi,Li Dong,Qingxiu Dong,Xun Wu,Shaohan Huang,Furu Wei*

Main category: cs.CL

TL;DR: 论文提出奖励推理模型（RRMs），通过链式推理增强奖励模型性能，利用测试时计算提升复杂查询的奖励准确性。


<details>
  <summary>Details</summary>
Motivation: 现有奖励模型在利用测试时计算提升性能方面存在挑战，需要一种能进行深思熟虑推理的模型来优化奖励生成。

Method: 采用强化学习框架开发RRMs，无需显式推理轨迹训练数据，通过链式推理自适应利用测试时计算。

Result: 实验显示RRMs在多个领域的奖励建模基准上表现优异，并能自适应利用测试时计算进一步提高奖励准确性。

Conclusion: RRMs通过推理过程有效提升奖励模型性能，展示了测试时计算在优化奖励准确性中的潜力。

Abstract: Reward models play a critical role in guiding large language models toward
outputs that align with human expectations. However, an open challenge remains
in effectively utilizing test-time compute to enhance reward model performance.
In this work, we introduce Reward Reasoning Models (RRMs), which are
specifically designed to execute a deliberate reasoning process before
generating final rewards. Through chain-of-thought reasoning, RRMs leverage
additional test-time compute for complex queries where appropriate rewards are
not immediately apparent. To develop RRMs, we implement a reinforcement
learning framework that fosters self-evolved reward reasoning capabilities
without requiring explicit reasoning traces as training data. Experimental
results demonstrate that RRMs achieve superior performance on reward modeling
benchmarks across diverse domains. Notably, we show that RRMs can adaptively
exploit test-time compute to further improve reward accuracy. The pretrained
reward reasoning models are available at
https://huggingface.co/Reward-Reasoning.

</details>


### [135] [UltraEdit: Training-, Subject-, and Memory-Free Lifelong Editing in Large Language Models](https://arxiv.org/abs/2505.14679)
*Xiaojie Gu,Guangxu Chen,Jungang Li,Jia-Chen Gu,Xuming Hu,Kai Zhang*

Main category: cs.CL

TL;DR: ULTRAEDIT是一种新型的大语言模型编辑方法，无需训练、主题和内存，适用于大规模实时终身学习，速度提升7倍以上，VRAM消耗减少三分之二。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型终身学习方法难以满足实际大规模应用的需求，特别是在高效更新知识的同时保持现有能力和可靠部署方面存在挑战。

Method: ULTRAEDIT通过轻量级线性代数操作计算参数偏移，实现快速一致的参数修改，并采用终身归一化策略适应分布变化，保持长期一致性。

Result: ULTRAEDIT在编辑速度上比现有最快方法快7倍以上，VRAM消耗不到三分之一，支持在24GB消费级GPU上编辑7B参数的LLM，并在大规模数据集上验证了其高效性。

Conclusion: ULTRAEDIT在多种模型编辑场景中表现优异，是目前唯一能在消费级硬件上高效支持大规模终身学习的方法。

Abstract: Lifelong learning enables large language models (LLMs) to adapt to evolving
information by continually updating their internal knowledge. An ideal system
should support efficient, wide-ranging updates while preserving existing
capabilities and ensuring reliable deployment. Model editing stands out as a
promising solution for this goal, offering a focused and efficient way to
revise a model's internal knowledge. Although recent paradigms have made
notable progress, they often struggle to meet the demands of practical lifelong
adaptation at scale. To bridge this gap, we propose ULTRAEDIT-a fundamentally
new editing solution that is training-, subject- and memory-free, making it
particularly well-suited for ultra-scalable, real-world lifelong model editing.
ULTRAEDIT performs editing through a self-contained process that relies solely
on lightweight linear algebra operations to compute parameter shifts, enabling
fast and consistent parameter modifications with minimal overhead. To improve
scalability in lifelong settings, ULTRAEDIT employs a lifelong normalization
strategy that continuously updates feature statistics across turns, allowing it
to adapt to distributional shifts and maintain consistency over time. ULTRAEDIT
achieves editing speeds over 7x faster than the previous state-of-the-art
method-which was also the fastest known approach-while consuming less than 1/3
the VRAM, making it the only method currently capable of editing a 7B LLM on a
24GB consumer-grade GPU. Furthermore, we construct ULTRAEDITBENCH-the largest
dataset in the field to date, with over 2M editing pairs-and demonstrate that
our method supports up to 1M edits while maintaining high accuracy.
Comprehensive experiments on four datasets and six models show that ULTRAEDIT
consistently achieves superior performance across diverse model editing
scenarios. Our code is available at: https://github.com/XiaojieGu/UltraEdit.

</details>


### [136] [Mind the Gap: Bridging Thought Leap for Improved Chain-of-Thought Tuning](https://arxiv.org/abs/2505.14684)
*Haolei Xu,Yuchen Yan,Yongliang Shen,Wenqi Zhang,Guiyang Hou,Shengpei Jiang,Kaitao Song,Weiming Lu,Jun Xiao,Yueting Zhuang*

Main category: cs.CL

TL;DR: 该论文提出CoT Thought Leap Bridge任务，通过自动检测思维跳跃并生成缺失的中间推理步骤，提升大语言模型在数学任务上的推理完整性和连贯性。


<details>
  <summary>Details</summary>
Motivation: 现有数学CoT数据集常因专家省略中间步骤导致思维跳跃，影响模型学习和泛化能力。

Method: 基于ScaleQM+数据集训练CoT-Bridge模型，自动填补推理链中的缺失步骤。

Result: 在NuminaMath等基准测试中，使用填补后数据微调的模型性能提升最高达5.87%，且能增强蒸馏数据和强化学习效果。

Conclusion: 提升推理完整性可带来广泛收益，CoT-Bridge作为即插即用模块兼容现有优化技术，并能泛化到域外逻辑推理任务。

Abstract: Large language models (LLMs) have achieved remarkable progress on
mathemati-cal tasks through Chain-of-Thought (CoT) reasoning. However, existing
mathematical CoT datasets often suffer from Thought Leaps due to experts
omitting intermediate steps, which negatively impacts model learning and
generalization. We propose the CoT Thought Leap Bridge Task, which aims to
automatically detect leaps and generate missing intermediate reasoning steps to
restore the completeness and coherence of CoT. To facilitate this, we
constructed a specialized training dataset called ScaleQM+, based on the
structured ScaleQuestMath dataset, and trained CoT-Bridge to bridge thought
leaps. Through comprehensive experiments on mathematical reasoning benchmarks,
we demonstrate that models fine-tuned on bridged datasets consistently
outperform those trained on original datasets, with improvements of up to
+5.87% on NuminaMath. Our approach effectively enhances distilled data (+3.02%)
and provides better starting points for reinforcement learning (+3.1%),
functioning as a plug-and-play module compatible with existing optimization
techniques. Furthermore, CoT-Bridge demonstrate improved generalization to
out-of-domain logical reasoning tasks, confirming that enhancing reasoning
completeness yields broadly applicable benefits.

</details>


### [137] [Language Models use Lookbacks to Track Beliefs](https://arxiv.org/abs/2505.14685)
*Nikhil Prakash,Natalie Shapira,Arnab Sen Sharma,Christoph Riedl,Yonatan Belinkov,Tamar Rott Shaham,David Bau,Atticus Geiger*

Main category: cs.CL

TL;DR: 该论文研究了语言模型（Llama-3-70B-Instruct）如何表示角色的信念，尤其是当这些信念与现实不同时，揭示了其心智理论（ToM）能力。通过因果中介和抽象分析，发现了一种称为“回溯机制”的算法模式，用于跟踪和更新角色的信念。


<details>
  <summary>Details</summary>
Motivation: 研究语言模型如何表示角色的信念，尤其是当这些信念与现实不同时，以理解其心智理论（ToM）能力。

Method: 构建了一个包含简单故事的数据集，其中两个角色分别改变两个对象的状态，可能不知道彼此的行为。通过因果中介和抽象分析，研究了模型的信念跟踪机制。

Result: 发现了一种称为“回溯机制”的算法模式，该机制通过低秩子空间中的排序ID（OIs）绑定角色-对象-状态三元组，并在需要时检索信息。还发现模型能生成可见性ID来更新角色的信念。

Conclusion: 该研究揭示了语言模型的信念跟踪机制，为逆向工程心智理论推理迈出了重要一步。

Abstract: How do language models (LMs) represent characters' beliefs, especially when
those beliefs may differ from reality? This question lies at the heart of
understanding the Theory of Mind (ToM) capabilities of LMs. We analyze
Llama-3-70B-Instruct's ability to reason about characters' beliefs using causal
mediation and abstraction. We construct a dataset that consists of simple
stories where two characters each separately change the state of two objects,
potentially unaware of each other's actions. Our investigation uncovered a
pervasive algorithmic pattern that we call a lookback mechanism, which enables
the LM to recall important information when it becomes necessary. The LM binds
each character-object-state triple together by co-locating reference
information about them, represented as their Ordering IDs (OIs) in low rank
subspaces of the state token's residual stream. When asked about a character's
beliefs regarding the state of an object, the binding lookback retrieves the
corresponding state OI and then an answer lookback retrieves the state token.
When we introduce text specifying that one character is (not) visible to the
other, we find that the LM first generates a visibility ID encoding the
relation between the observing and the observed character OIs. In a visibility
lookback, this ID is used to retrieve information about the observed character
and update the observing character's beliefs. Our work provides insights into
the LM's belief tracking mechanisms, taking a step toward reverse-engineering
ToM reasoning in LMs.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [138] [AgentSGEN: Multi-Agent LLM in the Loop for Semantic Collaboration and GENeration of Synthetic Data](https://arxiv.org/abs/2505.13466)
*Vu Dinh Xuan,Hao Vo,David Murphy,Hoang D. Nguyen*

Main category: cs.AI

TL;DR: 论文提出了一种新型多智能体框架，通过评估与编辑智能体的协作生成安全关键场景的合成数据，解决现有方法语义深度不足的问题。


<details>
  <summary>Details</summary>
Motivation: 由于伦理和物流障碍，真实危险场景数据稀缺，限制了AI在安全关键应用（如建筑安全）中的训练效果，亟需能生成高质量合成数据的端到端框架。

Method: 采用双智能体协作框架：评估智能体（基于LLM）确保语义一致性与安全约束，编辑智能体根据反馈生成并优化场景，通过迭代循环提升合成数据质量。

Result: 实验表明，该方法能基于现实需求生成兼顾安全要求与视觉语义的合成场景，有效弥补现有方法的不足。

Conclusion: 该迭代框架为多媒体安全应用中的数据稀缺问题提供了潜在解决方案，可生成鲁棒且视觉合理的模拟场景。

Abstract: The scarcity of data depicting dangerous situations presents a major obstacle
to training AI systems for safety-critical applications, such as construction
safety, where ethical and logistical barriers hinder real-world data
collection. This creates an urgent need for an end-to-end framework to generate
synthetic data that can bridge this gap. While existing methods can produce
synthetic scenes, they often lack the semantic depth required for scene
simulations, limiting their effectiveness. To address this, we propose a novel
multi-agent framework that employs an iterative, in-the-loop collaboration
between two agents: an Evaluator Agent, acting as an LLM-based judge to enforce
semantic consistency and safety-specific constraints, and an Editor Agent,
which generates and refines scenes based on this guidance. Powered by LLM's
capabilities to reasoning and common-sense knowledge, this collaborative design
produces synthetic images tailored to safety-critical scenarios. Our
experiments suggest this design can generate useful scenes based on realistic
specifications that address the shortcomings of prior approaches, balancing
safety requirements with visual semantics. This iterative process holds promise
for delivering robust, aesthetically sound simulations, offering a potential
solution to the data scarcity challenge in multimedia safety applications.

</details>


### [139] [Evaluating Large Language Models for Real-World Engineering Tasks](https://arxiv.org/abs/2505.13484)
*Rene Heesch,Sebastian Eilermann,Alexander Windmann,Alexander Diedrich,Philipp Rosenthal,Oliver Niggemann*

Main category: cs.AI

TL;DR: 论文指出当前大语言模型(LLM)在工程领域的评估存在简化用例和临时场景的不足，通过构建真实工程问题数据集评估了4种先进LLM的性能。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在工程任务中的评估过于依赖简化用例和临时场景，无法反映真实工程问题的复杂性，导致评估结果不全面。

Method: 构建包含100多个真实工程场景问题的数据集，系统评估4种先进LLM(包括云端和本地部署)在核心工程能力上的表现。

Result: LLM在基础时空推理上表现良好，但在抽象推理、形式化建模和上下文敏感工程逻辑方面存在显著不足。

Conclusion: 研究表明LLM处理复杂工程任务的能力仍有局限，特别是在需要高级推理和形式化建模的场景中。

Abstract: Large Language Models (LLMs) are transformative not only for daily activities
but also for engineering tasks. However, current evaluations of LLMs in
engineering exhibit two critical shortcomings: (i) the reliance on simplified
use cases, often adapted from examination materials where correctness is easily
verifiable, and (ii) the use of ad hoc scenarios that insufficiently capture
critical engineering competencies. Consequently, the assessment of LLMs on
complex, real-world engineering problems remains largely unexplored. This paper
addresses this gap by introducing a curated database comprising over 100
questions derived from authentic, production-oriented engineering scenarios,
systematically designed to cover core competencies such as product design,
prognosis, and diagnosis. Using this dataset, we evaluate four state-of-the-art
LLMs, including both cloud-based and locally hosted instances, to
systematically investigate their performance on complex engineering tasks. Our
results show that LLMs demonstrate strengths in basic temporal and structural
reasoning but struggle significantly with abstract reasoning, formal modeling,
and context-sensitive engineering logic.

</details>


### [140] [Contrastive Cross-Course Knowledge Tracing via Concept Graph Guided Knowledge Transfer](https://arxiv.org/abs/2505.13489)
*Wenkang Han,Wang Lin,Liya Hu,Zhenlong Dai,Yiyun Zhou,Mengze Li,Zemin Liu,Chang Yao,Jingyuan Chen*

Main category: cs.AI

TL;DR: TransKT提出了一种跨课程知识追踪方法，利用概念图和对比学习提升学习者知识状态建模。


<details>
  <summary>Details</summary>
Motivation: 现有知识追踪模型仅关注单一课程数据，难以全面捕捉学习者的知识状态。

Method: 通过零样本LLM构建跨课程概念图，结合GCN进行知识迁移，并采用对比学习对齐单课程与跨课程知识状态。

Result: TransKT显著提升了知识状态估计的准确性和鲁棒性。

Conclusion: 跨课程知识迁移和语义特征整合能有效增强学习者知识状态的建模能力。

Abstract: Knowledge tracing (KT) aims to predict learners' future performance based on
historical learning interactions. However, existing KT models predominantly
focus on data from a single course, limiting their ability to capture a
comprehensive understanding of learners' knowledge states. In this paper, we
propose TransKT, a contrastive cross-course knowledge tracing method that
leverages concept graph guided knowledge transfer to model the relationships
between learning behaviors across different courses, thereby enhancing
knowledge state estimation. Specifically, TransKT constructs a cross-course
concept graph by leveraging zero-shot Large Language Model (LLM) prompts to
establish implicit links between related concepts across different courses.
This graph serves as the foundation for knowledge transfer, enabling the model
to integrate and enhance the semantic features of learners' interactions across
courses. Furthermore, TransKT includes an LLM-to-LM pipeline for incorporating
summarized semantic features, which significantly improves the performance of
Graph Convolutional Networks (GCNs) used for knowledge transfer. Additionally,
TransKT employs a contrastive objective that aligns single-course and
cross-course knowledge states, thereby refining the model's ability to provide
a more robust and accurate representation of learners' overall knowledge
states.

</details>


### [141] [ADALog: Adaptive Unsupervised Anomaly detection in Logs with Self-attention Masked Language Model](https://arxiv.org/abs/2505.13496)
*Przemek Pospieszny,Wojciech Mormul,Karolina Szyndler,Sanjeev Kumar*

Main category: cs.AI

TL;DR: ADALog是一种自适应、无监督的日志异常检测框架，利用预训练双向编码器提取日志上下文关系，通过自适应阈值实现动态异常检测。


<details>
  <summary>Details</summary>
Motivation: 现代软件系统产生的日志数据具有异构性、动态格式和碎片化事件序列，传统依赖日志解析或标记数据的方法难以应对，亟需一种无需解析和标记的自适应异常检测方案。

Method: 基于Transformer的预训练双向编码器，通过掩码语言建模任务捕捉日志语法语义模式，使用正常日志微调模型，并通过令牌级重建概率聚合为日志级异常分数，采用基于百分位的自适应阈值。

Result: 在BGL、Thunderbird和Spirit基准数据集上，ADALog展现出优于现有监督/无监督方法的泛化能力，消融实验验证了掩码、微调和令牌位置策略的有效性。

Conclusion: ADALog通过无监督学习和动态阈值机制，实现了对复杂日志环境的高效异常检测，避免了传统方法对启发式规则的依赖。

Abstract: Modern software systems generate extensive heterogeneous log data with
dynamic formats, fragmented event sequences, and varying temporal patterns,
making anomaly detection both crucial and challenging. To address these
complexities, we propose ADALog, an adaptive, unsupervised anomaly detection
framework designed for practical applicability across diverse real-world
environments. Unlike traditional methods reliant on log parsing, strict
sequence dependencies, or labeled data, ADALog operates on individual
unstructured logs, extracts intra-log contextual relationships, and performs
adaptive thresholding on normal data. The proposed approach utilizes a
transformer-based, pretrained bidirectional encoder with a masked language
modeling task, fine-tuned on normal logs to capture domain-specific syntactic
and semantic patterns essential for accurate anomaly detection. Anomalies are
identified via token-level reconstruction probabilities, aggregated into
log-level scores, with adaptive percentile-based thresholding calibrated only
on normal data. This allows the model to dynamically adapt to evolving system
behaviors while avoiding rigid, heuristic-based thresholds common in
traditional systems. We evaluate ADALog on benchmark datasets BGL, Thunderbird,
and Spirit, showing strong generalization and competitive performance compared
to state-of-the-art supervised and unsupervised methods. Additional ablation
studies examine the effects of masking, fine-tuning, and token positioning on
model behavior and interpretability.

</details>


### [142] [Can AI Freelancers Compete? Benchmarking Earnings, Reliability, and Task Success at Scale](https://arxiv.org/abs/2505.13511)
*David Noever,Forrest McKee*

Main category: cs.AI

TL;DR: 研究评估了四种大语言模型在自由职业编程任务上的表现，Claude 3.5 Haiku表现最佳，赚取约152万美元。


<details>
  <summary>Details</summary>
Motivation: 探索大语言模型作为自主代理完成现实世界任务（如自由职业软件开发）的可行性。

Method: 构建基于Kaggle自由职业数据集的新基准，包含程序化可测试任务和预测价格，评估四种模型的任务成功率和收益。

Result: Claude 3.5 Haiku表现最优（152万美元），其次是GPT-4o-mini（149万美元）、Qwen 2.5（133万美元）和Mistral（70万美元）。

Conclusion: AI具备自由职业开发者潜力，但结构化任务表现与真实工作复杂度仍存在差距。

Abstract: This study explores Large Language Models (LLMs) as autonomous agents for
real-world tasks, including freelance software development. This work presents
a new benchmark that evaluates LLMs on freelance programming and data analysis
tasks derived from economic data. We construct the benchmark using synthetic
tasks created from a Kaggle Freelancer dataset of job postings, with all job
prices standardized to USD (median fixed-project price around $250, and an
average of $306). Each task is accompanied by structured input-output test
cases and an estimated price tag, enabling automated correctness checking and a
monetary performance valuation. This approach is inspired by OpenAI's recent
SWE-Lancer benchmark (1,400 real Upwork tasks worth $1M total). Still, our
framework simplifies evaluation using programmatically testable tasks and
predicted price values, making it highly scalable and repeatable. On this
benchmark, we evaluate four modern LLMs - Claude 3.5 Haiku, GPT-4o-mini, Qwen
2.5, and Mistral. We report each model's accuracy (task success rate and
test-case pass rate) and the total "freelance earnings" it achieves (sum of
prices of solved tasks). Our results show that Claude 3.5 Haiku performs best,
earning approximately $1.52 million USD, followed closely by GPT-4o-mini at
$1.49 million, then Qwen 2.5 ($1.33M) and Mistral ($0.70M). We analyze the
distribution of errors per task and observe that the strongest models solve the
most tasks and rarely fail completely on any project. We discuss the
implications of these results for the feasibility of AI as a freelance
developer, the advantages and limitations of our automated benchmark approach,
and the gap between performance on structured tasks versus the true complexity
of real-world freelance jobs.

</details>


### [143] [A Heuristic Algorithm Based on Beam Search and Iterated Local Search for the Maritime Inventory Routing Problem](https://arxiv.org/abs/2505.13522)
*Nathalie Sanghikian,Rafael Meirelles,Rafael Martinelli,Anand Subramanian*

Main category: cs.AI

TL;DR: 本文提出了一种不依赖数学优化技术的启发式方法，结合波束搜索算法和迭代局部搜索过程，用于解决确定性、有限时间、单一产品的海上库存路径问题（MIRP），并在72个测试实例中改进了10个实例的最优解。


<details>
  <summary>Details</summary>
Motivation: 由于海上库存路径问题（MIRP）的高复杂性，现有方法难以高效解决大规模实例或其变种。精确方法因计算时间过长而不适用于日常操作，而非基于混合整数规划（MIP）的启发式方法又因问题的高度约束性而较少应用。本文旨在通过提出一种新的启发式方法，促进MIRPLib的使用并方便结果比较。

Method: 本文提出的启发式方法结合了波束搜索算法的变体和迭代局部搜索过程，用于解决确定性、有限时间、单一产品的MIRP问题。

Result: 在72个测试实例中，该方法在可接受的CPU时间内改进了10个实例的最优解。

Conclusion: 本文提出的启发式方法在解决MIRP问题上表现出色，能够有效改进已知最优解，为未来的研究提供了有价值的参考。

Abstract: Maritime Inventory Routing Problem (MIRP) plays a crucial role in the
integration of global maritime commerce levels. However, there are still no
well-established methodologies capable of efficiently solving large MIRP
instances or their variants due to the high complexity of the problem. The
adoption of exact methods, typically based on Mixed Integer Programming (MIP),
for daily operations is nearly impractical due to the CPU time required, as
planning must be executed multiple times while ensuring high-quality results
within acceptable time limits. Non-MIP-based heuristics are less frequently
applied due to the highly constrained nature of the problem, which makes even
the construction of an effective initial solution challenging. Papageorgiou et
al. (2014) introduced a single-product MIRP as the foundation for MIRPLib,
aiming to provide a collection of publicly available benchmark instances.
However, only a few studies that propose new methodologies have been published
since then. To encourage the use of MIRPLib and facilitate result comparisons,
this study presents a heuristic approach that does not rely on mathematical
optimization techniques to solve a deterministic, finite-horizon,
single-product MIRP. The proposed heuristic combines a variation of a Beam
Search algorithm with an Iterated Local Search procedure. Among the 72
instances tested, the developed methodology can improve the best-known solution
for ten instances within an acceptable CPU time.

</details>


### [144] [BARREL: Boundary-Aware Reasoning for Factual and Reliable LRMs](https://arxiv.org/abs/2505.13529)
*Junxiao Yang,Jinzhe Tu,Haoran Liu,Xiaoce Wang,Chujie Zheng,Zhexin Zhang,Shiyao Cui,Caishun Chen,Tiantian He,Hongning Wang,Yew-Soon Ong,Minlie Huang*

Main category: cs.AI

TL;DR: 论文提出BARREL框架，解决大型推理模型(LRMs)过度自信和错误回答的问题，通过边界感知推理提升可靠性。


<details>
  <summary>Details</summary>
Motivation: 当前大型推理模型(LRMs)在数学和逻辑推理中表现出色，但常因过度自信而给出错误答案，缺乏承认无知的能力，影响事实可靠性。

Method: 提出BARREL框架，针对过度思考导致的两种病态推理模式（最后一刻猜测和二次思考螺旋），促进简洁且边界感知的事实推理。

Result: 实验显示，BARREL训练将DeepSeek-R1-Distill-Llama-8B的可靠性从39.33%提升至61.48%，同时保持与R1生成数据微调模型相当的准确率。

Conclusion: BARREL框架为构建更可靠、事实性强的系统2型LRMs提供了启发性的初步研究。

Abstract: Recent advances in Large Reasoning Models (LRMs) have shown impressive
capabilities in mathematical and logical reasoning. However, current LRMs
rarely admit ignorance or respond with "I don't know". Instead, they often
produce incorrect answers while showing undue confidence, raising concerns
about their factual reliability. In this work, we identify two pathological
reasoning patterns characterized by overthinking that contribute to the
overconfident and incorrect answers: last-minute guessing and second-thought
spiraling. To address these issues, we propose BARREL-a novel framework that
promotes concise and boundary-aware factual reasoning. Our experiments show
that BARREL-training increases the reliability of DeepSeek-R1-Distill-Llama-8B
from 39.33% to 61.48%, while still achieving accuracy comparable to models
finetuned on reasoning data generated by R1. These results demonstrate that our
pilot study is inspiring to build more reliable and factual System 2 LRMs.

</details>


### [145] [FinMaster: A Holistic Benchmark for Mastering Full-Pipeline Financial Workflows with LLMs](https://arxiv.org/abs/2505.13533)
*Junzhe Jiang,Chang Yang,Aixin Cui,Sihan Jin,Ruiyu Wang,Bo Li,Xiao Huang,Dongning Sun,Xinrun Wang*

Main category: cs.AI

TL;DR: FinMaster是一个全面的金融基准测试，旨在评估大语言模型在金融领域的表现，填补现有评估工具的不足。


<details>
  <summary>Details</summary>
Motivation: 金融任务对全球经济稳定至关重要，但面临劳动密集、容错率低、数据分散和工具限制等挑战。现有的大语言模型评估基准在金融领域缺乏足够的领域特定数据、任务设计简单且评估框架不完整。

Method: FinMaster包含三个模块：FinSim生成合成金融数据，FinSuite提供核心金融任务，FinEval提供统一评估接口。

Result: 实验显示，大语言模型在复杂金融推理任务中表现显著下降，准确率从90%降至40%，多指标场景下计算错误传播明显。

Conclusion: FinMaster是首个覆盖全流程金融工作流的基准测试，有望推动大语言模型在金融实践中的应用，提升效率和准确性。

Abstract: Financial tasks are pivotal to global economic stability; however, their
execution faces challenges including labor intensive processes, low error
tolerance, data fragmentation, and tool limitations. Although large language
models (LLMs) have succeeded in various natural language processing tasks and
have shown potential in automating workflows through reasoning and contextual
understanding, current benchmarks for evaluating LLMs in finance lack
sufficient domain-specific data, have simplistic task design, and incomplete
evaluation frameworks. To address these gaps, this article presents FinMaster,
a comprehensive financial benchmark designed to systematically assess the
capabilities of LLM in financial literacy, accounting, auditing, and
consulting. Specifically, FinMaster comprises three main modules: i) FinSim,
which builds simulators that generate synthetic, privacy-compliant financial
data for companies to replicate market dynamics; ii) FinSuite, which provides
tasks in core financial domains, spanning 183 tasks of various types and
difficulty levels; and iii) FinEval, which develops a unified interface for
evaluation. Extensive experiments over state-of-the-art LLMs reveal critical
capability gaps in financial reasoning, with accuracy dropping from over 90% on
basic tasks to merely 40% on complex scenarios requiring multi-step reasoning.
This degradation exhibits the propagation of computational errors, where
single-metric calculations initially demonstrating 58% accuracy decreased to
37% in multimetric scenarios. To the best of our knowledge, FinMaster is the
first benchmark that covers full-pipeline financial workflows with challenging
tasks. We hope that FinMaster can bridge the gap between research and industry
practitioners, driving the adoption of LLMs in real-world financial practices
to enhance efficiency and accuracy.

</details>


### [146] [Prompt Stability Matters: Evaluating and Optimizing Auto-Generated Prompt in General-Purpose Systems](https://arxiv.org/abs/2505.13546)
*Ke Chen,Yufei Zhou,Xitong Zhang,Haohan Wang*

Main category: cs.AI

TL;DR: 该论文提出了一种关注提示稳定性的自动提示生成系统，通过语义稳定性评估和LLaMA微调，提升了多任务场景下的准确性和输出一致性。


<details>
  <summary>Details</summary>
Motivation: 现有提示生成方法仅关注即时任务表现，忽视了提示的固有可靠性，且未考虑大语言模型的随机性。论文强调提示稳定性（模型响应的一致性）对构建鲁棒系统的重要性。

Method: 提出语义稳定性作为评估标准，微调LLaMA自动测量跨任务稳定性，并开发首个稳定性感知的通用提示生成系统，通过迭代反馈优化提示质量。

Result: 实验表明，该框架在通用和领域特定任务中均提高了准确性和输出一致性，验证了稳定性对系统级执行效果的必要性。

Conclusion: 通过将焦点从单次结果转向持久可靠性，该研究为构建更可信的通用系统提供了新视角和实用工具。

Abstract: Automatic prompt generation plays a crucial role in enabling general-purpose
multi-agent systems to perform diverse tasks autonomously. Existing methods
typically evaluate prompts based on their immediate task performance,
overlooking the intrinsic qualities that determine their reliability. This
outcome-centric view not only limits interpretability but also fails to account
for the inherent stochasticity of large language models (LLMs). In this work,
we bring attention to prompt stability-the consistency of model responses
across repeated executions-as a key factor for building robust and effective
prompt generation systems. To quantify this, we propose semantic stability as a
criterion for assessing the response consistency of prompts, and fine-tune a
LLaMA-based evaluator to measure it automatically across tasks. These
components have enabled us to develop the first stability-aware general-purpose
prompt generation system that leverages stability feedback to iteratively
enhance both prompt quality and system-level performance. Furthermore, we
establish a logical chain between prompt stability and task success by
analyzing the structural dependencies within our system, proving stability as a
necessary condition for effective system-level execution. Empirical results
across general and domain-specific tasks demonstrate that our stability-aware
framework improves both accuracy and output consistency. By shifting the focus
from one-off results to persistent reliability, our work offers a new
perspective on prompt design and contributes practical tools for building more
trustworthy general-purpose systems.

</details>


### [147] [Counter-Inferential Behavior in Natural and Artificial Cognitive Systems](https://arxiv.org/abs/2505.13551)
*Serge Dolgikh*

Main category: cs.AI

TL;DR: 该研究探讨了自然和人工认知系统中出现的反推理行为，即代理错误归因经验成功或抑制适应，导致认知僵化或适应不良的稳定性。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于理解认知系统中反推理行为的起源和机制，这些行为可能导致系统在稳定条件下出现适应不良或认知僵化。

Method: 研究方法包括分析典型场景中的反推理行为，如奖励不平衡、元认知成功归因和模型脆弱性下的保护性重构，并综合人工系统、生物认知、人类心理和社会动态的证据。

Result: 研究发现反推理行为是一种普遍的认知脆弱性，即使在适应良好的系统中也可能出现，强调了在稳定条件下保持最小适应激活的重要性。

Conclusion: 研究结论提出了认知架构的设计原则，以抵抗信息压力下的僵化，并指出反推理行为是认知系统的一个普遍弱点。

Abstract: This study explores the emergence of counter-inferential behavior in natural
and artificial cognitive systems, that is, patterns in which agents
misattribute empirical success or suppress adaptation, leading to epistemic
rigidity or maladaptive stability. We analyze archetypal scenarios in which
such behavior arises: reinforcement of stability through reward imbalance,
meta-cognitive attribution of success to internal superiority, and protective
reframing under perceived model fragility. Rather than arising from noise or
flawed design, these behaviors emerge through structured interactions between
internal information models, empirical feedback, and higher-order evaluation
mechanisms. Drawing on evidence from artificial systems, biological cognition,
human psychology, and social dynamics, we identify counter-inferential behavior
as a general cognitive vulnerability that can manifest even in otherwise
well-adapted systems. The findings highlight the importance of preserving
minimal adaptive activation under stable conditions and suggest design
principles for cognitive architectures that can resist rigidity under
informational stress.

</details>


### [148] [Language and Thought: The View from LLMs](https://arxiv.org/abs/2505.13561)
*Daniel Rothschild*

Main category: cs.AI

TL;DR: 论文探讨语言对思维的影响，通过AI模型验证语言训练如何提升推理能力，支持Dennett关于语言改变心智本质的观点。


<details>
  <summary>Details</summary>
Motivation: 受Daniel Dennett关于语言可能根本改变心智本质的猜想启发，研究旨在通过AI系统（尤其是大语言模型）验证语言对推理能力的影响。

Method: 通过对比分析接受语言训练与未接受语言训练的AI系统表现，特别是大语言模型（LLMs）的跨领域推理能力，探究语言编码的抽象性和效率。

Result: 研究发现，语言训练显著提升AI系统的推理能力，支持语言使推理在计算上更易处理的论点，表明语言对思维具有根本性影响。

Conclusion: 语言通过其抽象和高效的编码方式，不仅增强AI的推理能力，也可能在人类生物心智中扮演类似的核心角色。

Abstract: Daniel Dennett speculated in *Kinds of Minds* 1996: "Perhaps the kind of mind
you get when you add language to it is so different from the kind of mind you
can have without language that calling them both minds is a mistake." Recent
work in AI can be seen as testing Dennett's thesis by exploring the performance
of AI systems with and without linguistic training. I argue that the success of
Large Language Models at inferential reasoning, limited though it may be,
supports Dennett's radical view about the effect of language on thought. I
suggest it is the abstractness and efficiency of linguistic encoding that lies
behind the capacity of LLMs to perform inferences across a wide range of
domains. In a slogan, language makes inference computationally tractable. I
assess what these results in AI indicate about the role of language in the
workings of our own biological minds.

</details>


### [149] [MAFA: A multi-agent framework for annotation](https://arxiv.org/abs/2505.13668)
*Mahmood Hegazy,Aaron Rodrigues,Azzam Naeem*

Main category: cs.AI

TL;DR: 提出多智能体框架提升FAQ标注效果，结合多种方法并通过裁判智能体重排序，显著优于单智能体方法。


<details>
  <summary>Details</summary>
Motivation: 传统FAQ检索依赖单一模型，难以处理多样化用户查询的细微差异，需更精准高效的解决方案。

Method: 基于ARQ的结构化推理多智能体框架，各智能体采用不同少样本策略，裁判智能体进行候选答案重排序。

Result: 在银行数据集和公开基准(LCQMC/FiQA)上，Top-1准确率提升14%，Top-5提升18%，MRR提高12%。

Conclusion: 该框架能有效处理模糊查询，具备跨领域语言泛化能力，适合生产环境部署。

Abstract: Modern applications require accurate and efficient retrieval of information
in response to user queries. Mapping user utterances to the most relevant
Frequently Asked Questions (FAQs) is a crucial component of these systems.
Traditional approaches often rely on a single model or technique, which may not
capture the nuances of diverse user inquiries. In this paper, we introduce a
multi-agent framework for FAQ annotation that combines multiple specialized
agents with different approaches and a judge agent that reranks candidates to
produce optimal results. Our agents utilize a structured reasoning approach
inspired by Attentive Reasoning Queries (ARQs), which guides them through
systematic reasoning steps using targeted, task-specific JSON queries. Our
framework features a specialized few-shot example strategy, where each agent
receives different few-shots, enhancing ensemble diversity and coverage of the
query space. We evaluate our framework on a real-world banking dataset as well
as public benchmark datasets (LCQMC and FiQA), demonstrating significant
improvements over single-agent approaches across multiple metrics, including a
14% increase in Top-1 accuracy, an 18% increase in Top-5 accuracy, and a 12%
improvement in Mean Reciprocal Rank on our dataset, and similar gains on public
benchmarks when compared with traditional single agent annotation techniques.
Our framework is particularly effective at handling ambiguous queries, making
it well-suited for deployment in production applications while showing strong
generalization capabilities across different domains and languages.

</details>


### [150] [A*-Decoding: Token-Efficient Inference Scaling](https://arxiv.org/abs/2505.13672)
*Giannis Chatziveroglou*

Main category: cs.AI

TL;DR: 提出A*解码方法，通过结构化搜索优化推理计算资源分配，使小模型达到大模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在固定计算预算下表现良好，但未充分优化推理过程中的资源利用效率。

Method: 将语言模型解码视为部分解空间的搜索问题，应用A*算法在外部监督信号引导下优先扩展高质量推理路径。

Result: 在MATH500等基准测试中，1B参数模型达到70B模型的性能，节省3倍token和30%PRM计算量。

Conclusion: 结构化搜索解码策略可替代暴力采样，为高效部署语言模型提供新方向。

Abstract: Inference-time scaling has emerged as a powerful alternative to parameter
scaling for improving language model performance on complex reasoning tasks.
While existing methods have shown strong performance gains under fixed compute
budgets, there has been little focus on optimally utilizing that budget during
inference. In this work, we introduce A*-decoding, a search-based
inference-time strategy that builds on the A* search algorithm to optimally
utilize a fixed compute budget by prioritizing high-quality reasoning paths
during generation. We frame language model decoding as a structured search in a
state space of partial solutions, applying the A* transition model to identify
promising continuations guided by an external process supervision signal. In
our experiments, A*-decoding reaches the performance levels of strong inference
scaling baselines like best-of-N and particle filtering while using up to 3x
fewer tokens and 30% fewer PRM passes under equivalent compute budgets. On the
MATH500 and AIME 2024 benchmarks, A*-decoding enables Llama-3.2-1B-Instruct to
match the performance of the 70x larger Llama-3.1-70B-Instruct, and allows
Qwen3-1.7B to reach o1-like reasoning accuracy. These results highlight the
power of structured search in decoding, offering an alternative to brute-force
sampling or scale-driven gains. Our work demonstrates how thoughtful
inference-time strategies can enhance reasoning in SLMs, pointing toward future
advances in more efficient and scalable language model deployment.

</details>


### [151] [Building spatial world models from sparse transitional episodic memories](https://arxiv.org/abs/2505.13696)
*Zizhan He,Maxime Daigle,Pouya Bashivan*

Main category: cs.AI

TL;DR: 该论文提出了一种名为ESWM的新型神经网络框架，能够从稀疏的片段记忆中构建空间环境模型，具有高效样本利用和快速适应环境变化的能力。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探索神经网络是否能够像动物一样，通过稀疏且不连续的片段记忆快速构建灵活的环境空间模型，以支持导航、探索和规划等行为。

Method: 论文在模拟环境中提出了Episodic Spatial World Model (ESWM)框架，通过片段记忆学习构建空间模型。

Result: 结果表明，ESWM具有极高的样本效率，仅需少量观察即可构建鲁棒的环境表示，并能快速适应环境变化，支持近乎最优的探索和导航策略。

Conclusion: ESWM证明了神经网络可以从稀疏记忆中高效构建空间模型，为理解大脑如何快速适应环境提供了新视角。

Abstract: Many animals possess a remarkable capacity to rapidly construct flexible
mental models of their environments. These world models are crucial for
ethologically relevant behaviors such as navigation, exploration, and planning.
The ability to form episodic memories and make inferences based on these sparse
experiences is believed to underpin the efficiency and adaptability of these
models in the brain. Here, we ask: Can a neural network learn to construct a
spatial model of its surroundings from sparse and disjoint episodic memories?
We formulate the problem in a simulated world and propose a novel framework,
the Episodic Spatial World Model (ESWM), as a potential answer. We show that
ESWM is highly sample-efficient, requiring minimal observations to construct a
robust representation of the environment. It is also inherently adaptive,
allowing for rapid updates when the environment changes. In addition, we
demonstrate that ESWM readily enables near-optimal strategies for exploring
novel environments and navigating between arbitrary points, all without the
need for additional training.

</details>


### [152] [Warm Up Before You Train: Unlocking General Reasoning in Resource-Constrained Settings](https://arxiv.org/abs/2505.13718)
*Safal Shrestha,Minwu Kim,Aadim Nepal,Anubhav Shrestha,Keith Ross*

Main category: cs.AI

TL;DR: 论文提出了一种两阶段训练策略，在数据稀缺环境下通过预热和RLVR训练提升LLM的推理能力。


<details>
  <summary>Details</summary>
Motivation: 当前训练具备推理能力的大型语言模型通常需要大量高质量数据，但在数据稀缺时效果受限。本文旨在解决这一挑战。

Method: 采用两阶段训练：1) 使用Knights & Knaves逻辑谜题预热模型，学习通用推理技能；2) 在预热模型上应用RLVR进行目标领域微调。

Result: 实验表明预热阶段能提升跨任务表现，结合RLVR后模型在少样本场景下优于基线，同时保持跨领域泛化性和训练效率。

Conclusion: 预热策略为数据稀缺环境下构建鲁棒推理模型提供了有效路径，显著提升样本效率和模型性能。

Abstract: Designing effective reasoning-capable LLMs typically requires training using
Reinforcement Learning with Verifiable Rewards (RLVR) or distillation with
carefully curated Long Chain of Thoughts (CoT), both of which depend heavily on
extensive training data. This creates a major challenge when the amount of
quality training data is scarce. We propose a sample-efficient, two-stage
training strategy to develop reasoning LLMs under limited supervision. In the
first stage, we "warm up" the model by distilling Long CoTs from a toy domain,
namely, Knights \& Knaves (K\&K) logic puzzles to acquire general reasoning
skills. In the second stage, we apply RLVR to the warmed-up model using a
limited set of target-domain examples. Our experiments demonstrate that this
two-phase approach offers several benefits: $(i)$ the warmup phase alone
facilitates generalized reasoning, leading to performance improvements across a
range of tasks, including MATH, HumanEval$^{+}$, and MMLU-Pro. $(ii)$ When both
the base model and the warmed-up model are RLVR trained on the same small
dataset ($\leq100$ examples), the warmed-up model consistently outperforms the
base model; $(iii)$ Warming up before RLVR training allows a model to maintain
cross-domain generalizability even after training on a specific domain; $(iv)$
Introducing warmup in the pipeline improves not only accuracy but also overall
sample efficiency during RLVR training. The results in this paper highlight the
promise of warmup for building robust reasoning LLMs in data-scarce
environments.

</details>


### [153] [Causal Head Gating: A Framework for Interpreting Roles of Attention Heads in Transformers](https://arxiv.org/abs/2505.13737)
*Andrew Nam,Henry Conklin,Yukang Yang,Thomas Griffiths,Jonathan Cohen,Sarah-Jane Leslie*

Main category: cs.AI

TL;DR: 提出了一种名为因果头门控（CHG）的可扩展方法，用于解释Transformer模型中注意力头的功能角色，并通过实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的机制可解释性方法通常是假设驱动的，需要提示模板或目标标签，限制了其应用范围。CHG旨在直接应用于任何数据集，无需这些限制。

Method: CHG通过学习注意力头的软门控，并根据其对任务性能的影响将其分类为促进、干扰或无关。还引入了对比CHG，用于分离特定任务组件的子电路。

Result: 实验表明，CHG能够提供因果性洞察，而非仅仅相关性。研究发现，LLMs包含多个稀疏且充分的子电路，注意力头角色依赖于与其他头的交互，且指令跟随和上下文学习依赖于可分离的机制。

Conclusion: CHG是一种有效的可扩展方法，能够揭示Transformer模型中注意力头的功能角色，并为模型的可解释性提供了新的视角。

Abstract: We present causal head gating (CHG), a scalable method for interpreting the
functional roles of attention heads in transformer models. CHG learns soft
gates over heads and assigns them a causal taxonomy - facilitating,
interfering, or irrelevant - based on their impact on task performance. Unlike
prior approaches in mechanistic interpretability, which are hypothesis-driven
and require prompt templates or target labels, CHG applies directly to any
dataset using standard next-token prediction. We evaluate CHG across multiple
large language models (LLMs) in the Llama 3 model family and diverse tasks,
including syntax, commonsense, and mathematical reasoning, and show that CHG
scores yield causal - not merely correlational - insight, validated via
ablation and causal mediation analyses. We also introduce contrastive CHG, a
variant that isolates sub-circuits for specific task components. Our findings
reveal that LLMs contain multiple sparse, sufficient sub-circuits, that
individual head roles depend on interactions with others (low modularity), and
that instruction following and in-context learning rely on separable
mechanisms.

</details>


### [154] [Language Models Are Capable of Metacognitive Monitoring and Control of Their Internal Activations](https://arxiv.org/abs/2505.13763)
*Li Ji-An,Hua-Dong Xiong,Robert C. Wilson,Marcelo G. Mattar,Marcus K. Benna*

Main category: cs.AI

TL;DR: 该论文研究了大型语言模型（LLMs）的元认知能力，发现它们能够报告和控制部分内部激活模式，但能力有限，这对AI安全有重要影响。


<details>
  <summary>Details</summary>
Motivation: 随着社会对大型语言模型的依赖增加，理解其元认知能力的局限性变得至关重要，尤其是它们监控内部激活的能力，这关系到AI的安全性。

Method: 论文采用了一种受神经科学启发的神经反馈范式，通过向模型提供句子-标签对来量化LLMs报告和控制其激活模式的能力。

Result: 研究发现LLMs能够学习和控制特定的内部激活，但其表现受示例数量、目标神经方向的语义可解释性及该方向解释的方差影响。

Conclusion: LLMs的元认知能力有限，只能监控其神经机制的一部分，这一发现为AI安全提供了重要的实证依据。

Abstract: Large language models (LLMs) can sometimes report the strategies they
actually use to solve tasks, but they can also fail to do so. This suggests
some degree of metacognition -- the capacity to monitor one's own cognitive
processes for subsequent reporting and self-control. Metacognitive abilities
enhance AI capabilities but raise safety concerns, as models might obscure
their internal processes to evade neural-activation-based oversight mechanisms
designed to detect harmful behaviors. Given society's increased reliance on
these models, it is critical that we understand the limits of their
metacognitive abilities, particularly their ability to monitor their internal
activations. To address this, we introduce a neuroscience-inspired
neurofeedback paradigm designed to quantify the ability of LLMs to explicitly
report and control their activation patterns. By presenting models with
sentence-label pairs where labels correspond to sentence-elicited internal
activations along specific directions in the neural representation space, we
demonstrate that LLMs can learn to report and control these activations. The
performance varies with several factors: the number of example pairs provided,
the semantic interpretability of the target neural direction, and the variance
explained by that direction. These results reveal a "metacognitive space" with
dimensionality much lower than the model's neural space, suggesting LLMs can
monitor only a subset of their neural mechanisms. Our findings provide
empirical evidence quantifying metacognitive capabilities in LLMs, with
significant implications for AI safety.

</details>


### [155] [Ice Cream Doesn't Cause Drowning: Benchmarking LLMs Against Statistical Pitfalls in Causal Inference](https://arxiv.org/abs/2505.13770)
*Jin Du,Li Chen,Xun Xian,An Luo,Fangqiao Tian,Ganghua Wang,Charles Doss,Xiaotong Shen,Jie Ding*

Main category: cs.AI

TL;DR: 论文提出了CausalPitfalls基准，用于评估大语言模型在因果推理中的能力，发现现有模型存在显著局限性。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在因果推理任务中的表现尚不明确，现有基准任务过于简化，无法全面评估模型处理统计陷阱（如辛普森悖论、选择偏差）的能力，限制了模型在现实世界中的应用。

Method: 提出CausalPitfalls基准，包含结构化挑战和评分标准，通过直接提示和代码辅助提示两种协议评估模型，并与人类专家评估结果进行对比验证。

Result: 研究结果显示，当前大语言模型在统计因果推理任务中存在显著局限性。

Conclusion: CausalPitfalls基准为开发可信赖的因果推理系统提供了重要指导和量化指标。

Abstract: Reliable causal inference is essential for making decisions in high-stakes
areas like medicine, economics, and public policy. However, it remains unclear
whether large language models (LLMs) can handle rigorous and trustworthy
statistical causal inference. Current benchmarks usually involve simplified
tasks. For example, these tasks might only ask LLMs to identify semantic causal
relationships or draw conclusions directly from raw data. As a result, models
may overlook important statistical pitfalls, such as Simpson's paradox or
selection bias. This oversight limits the applicability of LLMs in the real
world. To address these limitations, we propose CausalPitfalls, a comprehensive
benchmark designed to rigorously evaluate the capability of LLMs in overcoming
common causal inference pitfalls. Our benchmark features structured challenges
across multiple difficulty levels, each paired with grading rubrics. This
approach allows us to quantitatively measure both causal reasoning capabilities
and the reliability of LLMs' responses. We evaluate models using two protocols:
(1) direct prompting, which assesses intrinsic causal reasoning, and (2)
code-assisted prompting, where models generate executable code for explicit
statistical analysis. Additionally, we validate the effectiveness of this judge
by comparing its scoring with assessments from human experts. Our results
reveal significant limitations in current LLMs when performing statistical
causal inference. The CausalPitfalls benchmark provides essential guidance and
quantitative metrics to advance the development of trustworthy causal reasoning
systems.

</details>


### [156] [Model Cards for AI Teammates: Comparing Human-AI Team Familiarization Methods for High-Stakes Environments](https://arxiv.org/abs/2505.13773)
*Ryan Bowers,Richard Agbeyibor,Jack Kolb,Karen Feigh*

Main category: cs.AI

TL;DR: 研究比较了三种让人熟悉AI队友的方法，发现结合文档阅读和实际操作训练效果最佳。


<details>
  <summary>Details</summary>
Motivation: 探讨在快速变化的ISR环境中，如何最有效地让人熟悉AI队友，以提升团队协作效率。

Method: 采用60名参与者的组间实验，分别通过阅读文档、实际操作训练或无熟悉化处理来熟悉AI队友。

Result: 文档熟悉组能快速制定策略但偏向风险规避；实际操作组更愿冒险但对AI内部机制理解较弱。

Conclusion: 建议结合AI文档、结构化现场训练和探索性互动，以优化人-AI团队的熟悉化过程。

Abstract: We compare three methods of familiarizing a human with an artificial
intelligence (AI) teammate ("agent") prior to operation in a collaborative,
fast-paced intelligence, surveillance, and reconnaissance (ISR) environment. In
a between-subjects user study (n=60), participants either read documentation
about the agent, trained alongside the agent prior to the mission, or were
given no familiarization. Results showed that the most valuable information
about the agent included details of its decision-making algorithms and its
relative strengths and weaknesses compared to the human. This information
allowed the familiarization groups to form sophisticated team strategies more
quickly than the control group. Documentation-based familiarization led to the
fastest adoption of these strategies, but also biased participants towards
risk-averse behavior that prevented high scores. Participants familiarized
through direct interaction were able to infer much of the same information
through observation, and were more willing to take risks and experiment with
different control modes, but reported weaker understanding of the agent's
internal processes. Significant differences were seen between individual
participants' risk tolerance and methods of AI interaction, which should be
considered when designing human-AI control interfaces. Based on our findings,
we recommend a human-AI team familiarization method that combines AI
documentation, structured in-situ training, and exploratory interaction.

</details>


### [157] [Measuring the Faithfulness of Thinking Drafts in Large Reasoning Models](https://arxiv.org/abs/2505.13774)
*Zidi Xiong,Chen Shan,Zhenting Qi,Himabindu Lakkaraju*

Main category: cs.AI

TL;DR: 论文提出反事实干预框架评估大模型思维草稿的忠实性，发现现有模型对中间推理步骤的忠实性存在选择性且常与结论不一致。


<details>
  <summary>Details</summary>
Motivation: 大模型通过多路径思维链增强复杂问题解决能力，但需确保中间推理过程的忠实性以实现可靠监控与有效控制。

Method: 提出系统性反事实干预框架，从步骤内因果性（反事实步骤插入）和草稿-答案逻辑一致性（结论逻辑扰动）两个维度评估忠实性。

Result: 实验表明当前大模型对中间步骤呈现选择性忠实，且常无法与思维草稿结论保持逻辑一致。

Conclusion: 研究强调先进大模型需要更忠实、可解释的推理机制。

Abstract: Large Reasoning Models (LRMs) have significantly enhanced their capabilities
in complex problem-solving by introducing a thinking draft that enables
multi-path Chain-of-Thought explorations before producing final answers.
Ensuring the faithfulness of these intermediate reasoning processes is crucial
for reliable monitoring, interpretation, and effective control. In this paper,
we propose a systematic counterfactual intervention framework to rigorously
evaluate thinking draft faithfulness. Our approach focuses on two complementary
dimensions: (1) Intra-Draft Faithfulness, which assesses whether individual
reasoning steps causally influence subsequent steps and the final draft
conclusion through counterfactual step insertions; and (2) Draft-to-Answer
Faithfulness, which evaluates whether final answers are logically consistent
with and dependent on the thinking draft, by perturbing the draft's concluding
logic. We conduct extensive experiments across six state-of-the-art LRMs. Our
findings show that current LRMs demonstrate selective faithfulness to
intermediate reasoning steps and frequently fail to faithfully align with the
draft conclusions. These results underscore the need for more faithful and
interpretable reasoning in advanced LRMs.

</details>


### [158] [CoIn: Counting the Invisible Reasoning Tokens in Commercial Opaque LLM APIs](https://arxiv.org/abs/2505.13778)
*Guoheng Sun,Ziyao Wang,Bowei Tian,Meng Liu,Zheyu Shen,Shwai He,Yexiao He,Wanghao Ye,Yiting Wang,Ang Li*

Main category: cs.AI

TL;DR: 论文提出CoIn框架，用于审计大语言模型(LLM)服务中隐藏的推理令牌数量和语义有效性，以解决计费不透明问题。


<details>
  <summary>Details</summary>
Motivation: 当前商业LLM服务通常隐藏推理过程仅返回最终答案，导致用户无法验证实际使用的令牌数量，可能存在令牌计数虚报问题。

Method: CoIn框架通过构建可验证的哈希树检查令牌数量，并利用嵌入相关性匹配检测伪造的推理内容。

Result: 实验表明CoIn作为第三方审计工具，检测令牌计数虚报的成功率高达94.7%。

Conclusion: CoIn能有效恢复不透明LLM服务的计费透明度，相关代码和数据集已开源。

Abstract: As post-training techniques evolve, large language models (LLMs) are
increasingly augmented with structured multi-step reasoning abilities, often
optimized through reinforcement learning. These reasoning-enhanced models
outperform standard LLMs on complex tasks and now underpin many commercial LLM
APIs. However, to protect proprietary behavior and reduce verbosity, providers
typically conceal the reasoning traces while returning only the final answer.
This opacity introduces a critical transparency gap: users are billed for
invisible reasoning tokens, which often account for the majority of the cost,
yet have no means to verify their authenticity. This opens the door to token
count inflation, where providers may overreport token usage or inject
synthetic, low-effort tokens to inflate charges. To address this issue, we
propose CoIn, a verification framework that audits both the quantity and
semantic validity of hidden tokens. CoIn constructs a verifiable hash tree from
token embedding fingerprints to check token counts, and uses embedding-based
relevance matching to detect fabricated reasoning content. Experiments
demonstrate that CoIn, when deployed as a trusted third-party auditor, can
effectively detect token count inflation with a success rate reaching up to
94.7%, showing the strong ability to restore billing transparency in opaque LLM
services. The dataset and code are available at
https://github.com/CASE-Lab-UMD/LLM-Auditing-CoIn.

</details>


### [159] [LLM-based Evaluation Policy Extraction for Ecological Modeling](https://arxiv.org/abs/2505.13794)
*Qi Cheng,Licheng Liu,Qing Zhu,Runlong Yu,Zhenong Jin,Yiqun Xie,Xiaowei Jia*

Main category: cs.AI

TL;DR: 提出了一种结合度量学习和大型语言模型的新框架，用于生态时间序列评估，弥补传统数值指标与专家知识间的差距。


<details>
  <summary>Details</summary>
Motivation: 传统数值指标（如R平方、均方根误差）难以捕捉生态过程的关键时间模式，依赖专家视觉检查又费时费力，限制了大规模评估的适用性。

Method: 通过整合度量学习与LLM的自然语言策略提取，开发可解释的评估标准，处理成对标注并实施策略优化机制以生成和组合不同评估指标。

Result: 在多个数据集上验证了该方法在评估作物总初级生产力和二氧化碳通量预测时的有效性，能同时适应合成数据和专家标注的模型比较需求。

Conclusion: 该框架在数值指标与专家知识间架起桥梁，提供可解释的评估策略，满足不同生态系统建模研究的多样化需求。

Abstract: Evaluating ecological time series is critical for benchmarking model
performance in many important applications, including predicting greenhouse gas
fluxes, capturing carbon-nitrogen dynamics, and monitoring hydrological cycles.
Traditional numerical metrics (e.g., R-squared, root mean square error) have
been widely used to quantify the similarity between modeled and observed
ecosystem variables, but they often fail to capture domain-specific temporal
patterns critical to ecological processes. As a result, these methods are often
accompanied by expert visual inspection, which requires substantial human labor
and limits the applicability to large-scale evaluation. To address these
challenges, we propose a novel framework that integrates metric learning with
large language model (LLM)-based natural language policy extraction to develop
interpretable evaluation criteria. The proposed method processes pairwise
annotations and implements a policy optimization mechanism to generate and
combine different assessment metrics. The results obtained on multiple datasets
for evaluating the predictions of crop gross primary production and carbon
dioxide flux have confirmed the effectiveness of the proposed method in
capturing target assessment preferences, including both synthetically generated
and expert-annotated model comparisons. The proposed framework bridges the gap
between numerical metrics and expert knowledge while providing interpretable
evaluation policies that accommodate the diverse needs of different ecosystem
modeling studies.

</details>


### [160] [Multimodal RAG-driven Anomaly Detection and Classification in Laser Powder Bed Fusion using Large Language Models](https://arxiv.org/abs/2505.13828)
*Kiarash Naghavi Khanghah,Zhiling Chen,Lela Romeo,Qian Yang,Rajiv Malhotra,Farhad Imani,Hongyi Xu*

Main category: cs.AI

TL;DR: 该研究提出了一种基于检索增强生成的多模态框架，用于自动化检测增材制造过程中的异常，无需训练数据集，通过检索科学文献中的图像和文本信息实现零样本异常识别、分类和解释生成。


<details>
  <summary>Details</summary>
Motivation: 增材制造能够制造复杂设计并减少浪费，但在缺陷和过程异常方面面临挑战。本研究旨在通过自动化异常检测来提高增材制造的效率和准确性。

Method: 研究提出了一种多模态检索增强生成框架，整合了科学文献中的文本和图像检索以及多模态生成模型，用于在激光粉末床熔融（L-PBF）设置中进行零样本异常检测。

Result: 在四个L-PBF制造数据集上的评估表明，该框架具有适应性和泛化能力，无需额外训练。GPT-4o-mini在异常分类中表现优于Qwen2-VL-2B和随机基线，检索机制的加入使平均准确率提高了12%。

Conclusion: 该框架可不断更新以适应增材制造技术的发展，是一种可扩展、自动化且支持零样本的解决方案，能够提高增材制造异常分析的效率和准确性。

Abstract: Additive manufacturing enables the fabrication of complex designs while
minimizing waste, but faces challenges related to defects and process
anomalies. This study presents a novel multimodal Retrieval-Augmented
Generation-based framework that automates anomaly detection across various
Additive Manufacturing processes leveraging retrieved information from
literature, including images and descriptive text, rather than training
datasets. This framework integrates text and image retrieval from scientific
literature and multimodal generation models to perform zero-shot anomaly
identification, classification, and explanation generation in a Laser Powder
Bed Fusion setting. The proposed framework is evaluated on four L-PBF
manufacturing datasets from Oak Ridge National Laboratory, featuring various
printer makes, models, and materials. This evaluation demonstrates the
framework's adaptability and generalizability across diverse images without
requiring additional training. Comparative analysis using Qwen2-VL-2B and
GPT-4o-mini as MLLM within the proposed framework highlights that GPT-4o-mini
outperforms Qwen2-VL-2B and proportional random baseline in manufacturing
anomalies classification. Additionally, the evaluation of the RAG system
confirms that incorporating retrieval mechanisms improves average accuracy by
12% by reducing the risk of hallucination and providing additional information.
The proposed framework can be continuously updated by integrating emerging
research, allowing seamless adaptation to the evolving landscape of AM
technologies. This scalable, automated, and zero-shot-capable framework
streamlines AM anomaly analysis, enhancing efficiency and accuracy.

</details>


### [161] [TelePlanNet: An AI-Driven Framework for Efficient Telecom Network Planning](https://arxiv.org/abs/2505.13831)
*Zongyuan Deng,Yujie Cai,Qing Liu,Shiyao Mu,Bin Lyu,Zhen Yang*

Main category: cs.AI

TL;DR: 论文提出TelePlanNet框架，利用AI优化5G基站选址，结合大语言模型和强化学习，提升规划一致性和效率。


<details>
  <summary>Details</summary>
Motivation: 传统基站选址方法依赖人工，效率低且一致性差；现有AI工具难以满足动态网络和多目标需求。

Method: 提出TelePlanNet框架，整合大语言模型实时处理用户输入，并通过改进的GRPO强化学习训练规划模型。

Result: 实验显示TelePlanNet将规划一致性提升至78%，优于人工方法。

Conclusion: TelePlanNet为运营商提供了高效、可扩展的基站规划工具，显著推进蜂窝网络规划。

Abstract: The selection of base station sites is a critical challenge in 5G network
planning, which requires efficient optimization of coverage, cost, user
satisfaction, and practical constraints. Traditional manual methods, reliant on
human expertise, suffer from inefficiencies and are limited to an unsatisfied
planning-construction consistency. Existing AI tools, despite improving
efficiency in certain aspects, still struggle to meet the dynamic network
conditions and multi-objective needs of telecom operators' networks. To address
these challenges, we propose TelePlanNet, an AI-driven framework tailored for
the selection of base station sites, integrating a three-layer architecture for
efficient planning and large-scale automation. By leveraging large language
models (LLMs) for real-time user input processing and intent alignment with
base station planning, combined with training the planning model using the
improved group relative policy optimization (GRPO) reinforcement learning, the
proposed TelePlanNet can effectively address multi-objective optimization,
evaluates candidate sites, and delivers practical solutions. Experiments
results show that the proposed TelePlanNet can improve the consistency to 78%,
which is superior to the manual methods, providing telecom operators with an
efficient and scalable tool that significantly advances cellular network
planning.

</details>


### [162] [A Challenge to Build Neuro-Symbolic Video Agents](https://arxiv.org/abs/2505.13851)
*Sahil Shah,Harsh Goel,Sai Shankar Narasimhan,Minkyu Choi,S P Sharan,Oguzhan Akcin,Sandeep Chinchali*

Main category: cs.AI

TL;DR: 现代视频理解系统在场景分类、物体检测和短视频检索等任务上表现出色，但在时间推理方面存在挑战。论文提出结合神经符号方法，分解视频查询为原子事件并验证时间约束，以推动智能视频代理的发展。


<details>
  <summary>Details</summary>
Motivation: 随着视频分析在现实应用中的重要性增加，需要不仅能解释视频流，还能推理事件并采取明智行动的前摄性视频代理。当前深度学习模型在时间推理方面存在局限，难以理解事件序列和依赖关系。

Method: 采用神经符号视角，将视频查询分解为原子事件，构建连贯序列，并通过时间约束验证，以增强可解释性和结构化推理能力。

Result: 提出开发下一代智能视频代理的三大核心能力：自主视频搜索与分析、无缝现实世界交互和高级内容生成，推动视频理解从被动感知向智能代理的转变。

Conclusion: 通过整合神经符号方法和三大核心能力，可以突破当前视频理解的限制，发展出能推理、预测和行动的智能视频代理，推动可信赖视频代理的进步。

Abstract: Modern video understanding systems excel at tasks such as scene
classification, object detection, and short video retrieval. However, as video
analysis becomes increasingly central to real-world applications, there is a
growing need for proactive video agents for the systems that not only interpret
video streams but also reason about events and take informed actions. A key
obstacle in this direction is temporal reasoning: while deep learning models
have made remarkable progress in recognizing patterns within individual frames
or short clips, they struggle to understand the sequencing and dependencies of
events over time, which is critical for action-driven decision-making.
Addressing this limitation demands moving beyond conventional deep learning
approaches. We posit that tackling this challenge requires a neuro-symbolic
perspective, where video queries are decomposed into atomic events, structured
into coherent sequences, and validated against temporal constraints. Such an
approach can enhance interpretability, enable structured reasoning, and provide
stronger guarantees on system behavior, all key properties for advancing
trustworthy video agents. To this end, we present a grand challenge to the
research community: developing the next generation of intelligent video agents
that integrate three core capabilities: (1) autonomous video search and
analysis, (2) seamless real-world interaction, and (3) advanced content
generation. By addressing these pillars, we can transition from passive
perception to intelligent video agents that reason, predict, and act, pushing
the boundaries of video understanding.

</details>


### [163] [Mobile-Agent-V: A Video-Guided Approach for Effortless and Efficient Operational Knowledge Injection in Mobile Automation](https://arxiv.org/abs/2505.13887)
*Junyang Wang,Haiyang Xu,Xi Zhang,Ming Yan,Ji Zhang,Fei Huang,Jitao Sang*

Main category: cs.AI

TL;DR: Mobile-Agent-V利用视频作为指导工具，自动注入操作知识，显著提升移动自动化效率，性能比现有方法提高36%。


<details>
  <summary>Details</summary>
Motivation: 移动设备使用激增需要高效的自动化任务管理，但现有AI框架因缺乏操作知识而表现不佳，手动注入知识又效率低下。

Method: 提出Mobile-Agent-V框架，通过视频内容直接获取操作知识，无需人工干预，并设计Mobile-Knowledge基准评估外部知识对移动代理性能的影响。

Result: 实验显示Mobile-Agent-V性能提升36%，验证了其在移动自动化中的高效性和便捷性。

Conclusion: Mobile-Agent-V通过视频自动获取操作知识，显著减少人工成本和时间，为移动自动化提供了高效解决方案。

Abstract: The exponential rise in mobile device usage necessitates streamlined
automation for effective task management, yet many AI frameworks fall short due
to inadequate operational expertise. While manually written knowledge can
bridge this gap, it is often burdensome and inefficient. We introduce
Mobile-Agent-V, an innovative framework that utilizes video as a guiding tool
to effortlessly and efficiently inject operational knowledge into mobile
automation processes. By deriving knowledge directly from video content,
Mobile-Agent-V eliminates manual intervention, significantly reducing the
effort and time required for knowledge acquisition. To rigorously evaluate this
approach, we propose Mobile-Knowledge, a benchmark tailored to assess the
impact of external knowledge on mobile agent performance. Our experimental
findings demonstrate that Mobile-Agent-V enhances performance by 36% compared
to existing methods, underscoring its effortless and efficient advantages in
mobile automation.

</details>


### [164] [Efficient Agent Training for Computer Use](https://arxiv.org/abs/2505.13909)
*Yanheng He,Jiahe Jin,Pengfei Liu*

Main category: cs.AI

TL;DR: PC Agent-E框架通过少量高质量轨迹数据和AI合成数据，显著提升计算机使用代理的性能，并在跨操作系统任务中展现强泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解决高质量轨迹数据稀缺对开发类人计算机使用代理的限制。

Method: 使用312条人工标注轨迹，结合Claude 3.7 Sonnet合成多样化动作决策以提升数据质量。

Result: PC Agent-E在WindowsAgentArena-V2基准上相对性能提升141%，并在OSWorld跨操作系统任务中表现优异。

Conclusion: 少量高质量轨迹数据足以激发强大的计算机使用能力。

Abstract: Scaling up high-quality trajectory data has long been a critical bottleneck
for developing human-like computer use agents. We introduce PC Agent-E, an
efficient agent training framework that significantly reduces reliance on
large-scale human demonstrations. Starting with just 312 human-annotated
computer use trajectories, we further improved data quality by synthesizing
diverse action decisions with Claude 3.7 Sonnet. Trained on these enriched
trajectories, our PC Agent-E model achieved a remarkable 141% relative
improvement, surpassing the strong Claude 3.7 Sonnet with extended thinking on
WindowsAgentArena-V2, an improved benchmark we also released. Furthermore, PC
Agent-E demonstrates strong generalizability to different operating systems on
OSWorld. Our findings suggest that strong computer use capabilities can be
stimulated from a small amount of high-quality trajectory data.

</details>


### [165] [Parallel Belief Revision via Order Aggregation](https://arxiv.org/abs/2505.13914)
*Jake Chandler,Richard Booth*

Main category: cs.AI

TL;DR: 该论文提出了一种基于TeamQueue聚合器的方法，将串行迭代信念修正操作扩展到并行修正，统一了相关合理性假设。


<details>
  <summary>Details</summary>
Motivation: 尽管已有研究探讨单步并行修正的约束条件，但如何将其扩展到迭代场景仍缺乏系统性研究。Delgrande & Jin提出的合理性假设虽具启发性，但缺乏统一的理论基础。

Method: 利用迭代并行收缩的最新成果，通过TeamQueue聚合器家族将串行迭代信念修正操作扩展为并行处理机制。

Result: 该方法能系统性地复现文献中合理的修正属性，同时规避存在争议的特性，为并行信念修正提供了原则性框架。

Conclusion: 基于序聚合的通用方法为迭代并行信念修正建立了理论基础，弥合了单步与迭代修正之间的研究断层。

Abstract: Despite efforts to better understand the constraints that operate on
single-step parallel (aka "package", "multiple") revision, very little work has
been carried out on how to extend the model to the iterated case. A recent
paper by Delgrande & Jin outlines a range of relevant rationality postulates.
While many of these are plausible, they lack an underlying unifying
explanation. We draw on recent work on iterated parallel contraction to offer a
general method for extending serial iterated belief revision operators to
handle parallel change. This method, based on a family of order aggregators
known as TeamQueue aggregators, provides a principled way to recover the
independently plausible properties that can be found in the literature, without
yielding the more dubious ones.

</details>


### [166] [DrugPilot: LLM-based Parameterized Reasoning Agent for Drug Discovery](https://arxiv.org/abs/2505.13940)
*Kun Li,Zhennan Wu,Shoupeng Wang,Wenbin Hu*

Main category: cs.AI

TL;DR: DrugPilot是一个基于LLM的药物发现代理，通过参数化推理架构解决了传统端到端LLM预测方法的局限性，支持药物发现流程的主要阶段，并在多模态药物数据分析方面表现出色。


<details>
  <summary>Details</summary>
Motivation: 尽管在药物发现领域通过专业数据预训练、上下文窗口扩展和互联网搜索进行了优化，现有LLM仍面临多模态和异构数据处理、领域知识动态更新延迟以及对复杂计算任务结果预测信心不足等挑战。

Method: 提出了DrugPilot，一个基于LLM的药物发现代理，采用参数化推理架构，支持药物发现流程的主要阶段，并开发了交互式参数化内存池和多任务药物指令数据集。

Result: DrugPilot在药物发现工具指令数据集上表现出最先进的工具调用能力，任务完成率在简单、多重和多轮任务中分别达到98.0%、93.5%和64.0%。

Conclusion: DrugPilot通过参数化推理架构和交互式内存池有效解决了药物发现中的关键挑战，展示了在多任务处理和高效率知识检索方面的优势。

Abstract: In the field of AI4Science, large-scale language models (LLMs) show great
potential to parse complex scientific semantics, integrate cross-disciplinary
knowledge, and assist critical task research. However, in the field of drug
discovery, despite the optimization through professional data pre-training,
context window expansion, and internet search, the existing LLMs are still
facing challenges such as massive multi-modal and heterogeneous data
processing, domain knowledge dynamic updating delay, and insufficient
confidence in predicting the results of complex computational tasks. To address
these challenges, we propose the DrugPilot, an LLM-based agent with
parameterized reasoning for drug discovery. DrugPilot addresses key limitations
of traditional end-to-end LLM prediction approaches through its parametric
inference architecture. This agent system supports major phases of the drug
discovery pipeline, facilitating automated planning and execution of
multi-stage research tasks. To address the critical challenge of multi-modal
drug data analysis (incorporating both public datasets and user-submitted
data), we developed an interactive parameterized memory pool. This innovative
component standardizes real-world drug data into parametric representations,
simultaneously enabling efficient knowledge retrieval in multi-turn dialogue
while mitigating the information loss inherent in text-based data transmission.
Additionally, we created a drug instruct dataset across 8 essential drug
discovery tasks for model fine-tuning and evaluation. Based on the Berkeley
function calling evaluation framework, DrugPilot demonstrated the most advanced
tool calling capabilities on our drug discovery tool instruction dataset,
outperforming existing agents (e.g., ReAct, LoT). Specifically, it achieves
task completion rates of 98.0%, 93.5%, and 64.0% on simple, multiple, and
multi-turn tasks, respectively.

</details>


### [167] [Visual Instruction Bottleneck Tuning](https://arxiv.org/abs/2505.13946)
*Changdae Oh,Jiatong Li,Shawn Im,Yixuan Li*

Main category: cs.AI

TL;DR: 提出Vittle方法，通过信息瓶颈原理提升多模态大模型在分布偏移下的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有提升多模态大语言模型泛化能力的方法通常需要更多指令数据或更大模型架构，成本较高。本文从表示学习角度出发，寻求更高效的方法。

Method: 基于信息瓶颈原理推导出变分下界，提出Visual Instruction Bottleneck Tuning（Vittle）方法，学习最小充分表示。

Result: 在45个数据集（含30个偏移场景）上的实验表明，Vittle能持续提升模型在分布偏移下的鲁棒性。

Conclusion: Vittle通过信息瓶颈原理有效提升了多模态大模型在分布偏移场景中的表现，且无需增加数据或模型规模。

Abstract: Despite widespread adoption, multimodal large language models (MLLMs) suffer
performance degradation when encountering unfamiliar queries under distribution
shifts. Existing methods to improve MLLM generalization typically require
either more instruction data or larger advanced model architectures, both of
which incur non-trivial human labor or computational costs. In this work, we
take an alternative approach to enhance the robustness of MLLMs under
distribution shifts, from a representation learning perspective. Inspired by
the information bottleneck (IB) principle, we derive a variational lower bound
of the IB for MLLMs and devise a practical implementation, Visual Instruction
Bottleneck Tuning (Vittle). We then provide a theoretical justification of
Vittle by revealing its connection to an information-theoretic robustness
metric of MLLM. Empirical validation of three MLLMs on open-ended and
closed-form question answering and object hallucination detection tasks over 45
datasets, including 30 shift scenarios, demonstrates that Vittle consistently
improves the MLLM's robustness under shifts by pursuing the learning of a
minimal sufficient representation.

</details>


### [168] [Solving Normalized Cut Problem with Constrained Action Space](https://arxiv.org/abs/2505.13986)
*Qize Jiang,Linsey Pang,Alice Gatti,Mahima Aggarwa,Giovanna Vantin,Xiaosong Ma,Weiwei Sun,Sanjay Chawla*

Main category: cs.AI

TL;DR: 本文提出了一种利用约束动作空间的强化学习方法，以引导组合优化问题解决方案朝向预定义模板实例，特别是在归一化割问题中，通过Wedge和Ring Transformer生成更接近自然最优分割的图形分区。


<details>
  <summary>Details</summary>
Motivation: 尽管强化学习在解决组合优化问题方面表现出色，但如何整合外部知识以引导解决方案朝向特定领域合适的结果仍是一个巨大挑战。本文旨在解决这一问题。

Method: 提出了一种基于约束动作空间的强化学习解决方案，利用Wedge和Ring Transformer在归一化割问题中生成特定形状（楔形和环形）的图形分区。

Result: 在交通网络领域的实验中，该方法生成了更接近自然最优分割的图形分区，证明了其有效性。

Conclusion: 本文提出的方法不仅适用于交通网络领域，其基于的原则还可推广到其他领域，为解决组合优化问题提供了新的思路。

Abstract: Reinforcement Learning (RL) has emerged as an important paradigm to solve
combinatorial optimization problems primarily due to its ability to learn
heuristics that can generalize across problem instances. However, integrating
external knowledge that will steer combinatorial optimization problem solutions
towards domain appropriate outcomes remains an extremely challenging task. In
this paper, we propose the first RL solution that uses constrained action
spaces to guide the normalized cut problem towards pre-defined template
instances. Using transportation networks as an example domain, we create a
Wedge and Ring Transformer that results in graph partitions that are shaped in
form of Wedges and Rings and which are likely to be closer to natural optimal
partitions. However, our approach is general as it is based on principles that
can be generalized to other domains.

</details>


### [169] [Divide by Question, Conquer by Agent: SPLIT-RAG with Question-Driven Graph Partitioning](https://arxiv.org/abs/2505.13994)
*Ruiyi Yang,Hao Xue,Imran Razzak,Hakim Hacid,Flora D. Salim*

Main category: cs.AI

TL;DR: SPLIT-RAG提出了一种多智能体RAG框架，通过语义图分割和协作子图检索，解决了现有RAG系统在大规模知识图谱上效率与准确性难以兼顾的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的RAG系统在处理大规模知识图谱时，常面临效率与准确性之间的权衡问题，简单查询时存在不必要的延迟，复杂多跳问题时又容易出现碎片化推理。

Method: SPLIT-RAG采用问题驱动的语义图分割方法，将知识图谱划分为语义连贯的子图，并为每个子图分配轻量级LLM智能体，仅在检索时激活相关子图，最后通过层次化合并模块解决子图间答案的不一致性。

Result: 实验验证表明，SPLIT-RAG相比现有方法在效率和准确性上均有显著提升。

Conclusion: SPLIT-RAG通过创新的多智能体框架和语义图分割技术，有效解决了大规模知识图谱检索中的效率与准确性问题，为RAG系统的优化提供了新思路。

Abstract: Retrieval-Augmented Generation (RAG) systems empower large language models
(LLMs) with external knowledge, yet struggle with efficiency-accuracy
trade-offs when scaling to large knowledge graphs. Existing approaches often
rely on monolithic graph retrieval, incurring unnecessary latency for simple
queries and fragmented reasoning for complex multi-hop questions. To address
these challenges, this paper propose SPLIT-RAG, a multi-agent RAG framework
that addresses these limitations with question-driven semantic graph
partitioning and collaborative subgraph retrieval. The innovative framework
first create Semantic Partitioning of Linked Information, then use the
Type-Specialized knowledge base to achieve Multi-Agent RAG. The attribute-aware
graph segmentation manages to divide knowledge graphs into semantically
coherent subgraphs, ensuring subgraphs align with different query types, while
lightweight LLM agents are assigned to partitioned subgraphs, and only relevant
partitions are activated during retrieval, thus reduce search space while
enhancing efficiency. Finally, a hierarchical merging module resolves
inconsistencies across subgraph-derived answers through logical verifications.
Extensive experimental validation demonstrates considerable improvements
compared to existing approaches.

</details>


### [170] [VeRecycle: Reclaiming Guarantees from Probabilistic Certificates for Stochastic Dynamical Systems after Change](https://arxiv.org/abs/2505.14001)
*Sterre Lutz,Matthijs T. J. Spaan,Anna Lukina*

Main category: cs.AI

TL;DR: VeRecycle框架首次实现了在离散时间随机动力系统中，当系统动态仅在部分状态空间变化时，高效复用概率安全证书，显著减少计算成本。


<details>
  <summary>Details</summary>
Motivation: 现实中的自主系统面临多种不确定性，现有方法在系统动态变化时需要完全重新认证，计算成本高昂。

Method: 提出VeRecycle框架，通过理论证明和算法实现，在系统动态局部变化时复用已有概率证书。

Result: 实验表明，VeRecycle在节省计算资源的同时，保持了竞争力的概率安全保证。

Conclusion: VeRecycle为神经控制系统的安全认证提供了高效且可靠的解决方案。

Abstract: Autonomous systems operating in the real world encounter a range of
uncertainties. Probabilistic neural Lyapunov certification is a powerful
approach to proving safety of nonlinear stochastic dynamical systems. When
faced with changes beyond the modeled uncertainties, e.g., unidentified
obstacles, probabilistic certificates must be transferred to the new system
dynamics. However, even when the changes are localized in a known part of the
state space, state-of-the-art requires complete re-certification, which is
particularly costly for neural certificates. We introduce VeRecycle, the first
framework to formally reclaim guarantees for discrete-time stochastic dynamical
systems. VeRecycle efficiently reuses probabilistic certificates when the
system dynamics deviate only in a given subset of states. We present a general
theoretical justification and algorithmic implementation. Our experimental
evaluation shows scenarios where VeRecycle both saves significant computational
effort and achieves competitive probabilistic guarantees in compositional
neural control.

</details>


### [171] [Disentangled Multi-span Evolutionary Network against Temporal Knowledge Graph Reasoning](https://arxiv.org/abs/2505.14020)
*Hao Dong,Ziyue Qiao,Zhiyuan Ning,Qi Hao,Yi Du,Pengyang Wang,Yuanchun Zhou*

Main category: cs.AI

TL;DR: 本文提出了一种名为DiMNet的新方法，用于时序知识图谱（TKG）推理，通过多跨度演化策略和解耦组件，显著提升了推理性能。


<details>
  <summary>Details</summary>
Motivation: 现有的TKG推理方法在建模子图语义演化时，忽略了子图间的内部结构交互以及潜在的平滑特征，这限制了推理性能的提升。

Method: DiMNet采用多跨度演化策略捕捉局部邻居特征和历史邻居语义信息，并通过解耦组件自适应分离节点的活跃和稳定特征，动态控制历史语义对未来演化的影响。

Result: 在四个真实世界的TKG数据集上的实验表明，DiMNet在推理性能上表现优异，MRR指标最高提升了22.7%。

Conclusion: DiMNet通过有效建模子图间的内部交互和区分语义变化与平滑特征，显著提升了TKG推理的准确性和性能。

Abstract: Temporal Knowledge Graphs (TKGs), as an extension of static Knowledge Graphs
(KGs), incorporate the temporal feature to express the transience of knowledge
by describing when facts occur. TKG extrapolation aims to infer possible future
facts based on known history, which has garnered significant attention in
recent years. Some existing methods treat TKG as a sequence of independent
subgraphs to model temporal evolution patterns, demonstrating impressive
reasoning performance. However, they still have limitations: 1) In modeling
subgraph semantic evolution, they usually neglect the internal structural
interactions between subgraphs, which are actually crucial for encoding TKGs.
2) They overlook the potential smooth features that do not lead to semantic
changes, which should be distinguished from the semantic evolution process.
Therefore, we propose a novel Disentangled Multi-span Evolutionary Network
(DiMNet) for TKG reasoning. Specifically, we design a multi-span evolution
strategy that captures local neighbor features while perceiving historical
neighbor semantic information, thus enabling internal interactions between
subgraphs during the evolution process. To maximize the capture of semantic
change patterns, we design a disentangle component that adaptively separates
nodes' active and stable features, used to dynamically control the influence of
historical semantics on future evolution. Extensive experiments conducted on
four real-world TKG datasets show that DiMNet demonstrates substantial
performance in TKG reasoning, and outperforms the state-of-the-art up to 22.7%
in MRR.

</details>


### [172] [ProMind-LLM: Proactive Mental Health Care via Causal Reasoning with Sensor Data](https://arxiv.org/abs/2505.14038)
*Xinzhe Zheng,Sijie Ji,Jiawei Sun,Renqi Chen,Wei Gao,Mani Srivastava*

Main category: cs.AI

TL;DR: 该论文提出ProMind-LLM，通过结合主观心理记录和客观行为数据，改进心理健康风险评估的可靠性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 心理健康风险是全球性挑战，现有方法依赖主观文本记录，易受不确定性影响，导致预测不一致。

Method: ProMind-LLM整合领域特定预训练、自优化机制和因果链式推理，处理数值行为数据并增强预测解释性。

Result: 在PMData和Globem数据集上验证，ProMind-LLM显著优于通用大语言模型。

Conclusion: ProMind-LLM为可靠、可解释且可扩展的心理健康解决方案奠定了基础。

Abstract: Mental health risk is a critical global public health challenge,
necessitating innovative and reliable assessment methods. With the development
of large language models (LLMs), they stand out to be a promising tool for
explainable mental health care applications. Nevertheless, existing approaches
predominantly rely on subjective textual mental records, which can be distorted
by inherent mental uncertainties, leading to inconsistent and unreliable
predictions. To address these limitations, this paper introduces ProMind-LLM.
We investigate an innovative approach integrating objective behavior data as
complementary information alongside subjective mental records for robust mental
health risk assessment. Specifically, ProMind-LLM incorporates a comprehensive
pipeline that includes domain-specific pretraining to tailor the LLM for mental
health contexts, a self-refine mechanism to optimize the processing of
numerical behavioral data, and causal chain-of-thought reasoning to enhance the
reliability and interpretability of its predictions. Evaluations of two
real-world datasets, PMData and Globem, demonstrate the effectiveness of our
proposed methods, achieving substantial improvements over general LLMs. We
anticipate that ProMind-LLM will pave the way for more dependable,
interpretable, and scalable mental health case solutions.

</details>


### [173] [Personalized Student Knowledge Modeling for Future Learning Resource Prediction](https://arxiv.org/abs/2505.14072)
*Soroush Hashemifar,Sherry Sahebi*

Main category: cs.AI

TL;DR: 提出KMaP模型，通过聚类学生画像实现个性化知识追踪与行为建模，解决现有方法在个性化、非评估材料建模及上下文信息丢失等问题。


<details>
  <summary>Details</summary>
Motivation: 现有学生知识追踪和行为建模方法存在个性化不足、忽视非评估材料（如讲座）的影响，以及固定长度序列分割导致上下文信息丢失等问题。

Method: 采用多任务学习方法KMaP，结合聚类技术构建个性化学生表征，同时建模学生知识和行为。

Result: 在两个真实数据集上的实验表明，不同学生群体存在显著行为差异，KMaP模型在预测学习资源偏好方面表现优异。

Conclusion: KMaP模型通过个性化学生表征有效提升了知识追踪和行为建模的准确性，为个性化学习提供了新思路。

Abstract: Despite advances in deep learning for education, student knowledge tracing
and behavior modeling face persistent challenges: limited personalization,
inadequate modeling of diverse learning activities (especially non-assessed
materials), and overlooking the interplay between knowledge acquisition and
behavioral patterns. Practical limitations, such as fixed-size sequence
segmentation, frequently lead to the loss of contextual information vital for
personalized learning. Moreover, reliance on student performance on assessed
materials limits the modeling scope, excluding non-assessed interactions like
lectures. To overcome these shortcomings, we propose Knowledge Modeling and
Material Prediction (KMaP), a stateful multi-task approach designed for
personalized and simultaneous modeling of student knowledge and behavior. KMaP
employs clustering-based student profiling to create personalized student
representations, improving predictions of future learning resource preferences.
Extensive experiments on two real-world datasets confirm significant behavioral
differences across student clusters and validate the efficacy of the KMaP
model.

</details>


### [174] [Memory Assignment for Finite-Memory Strategies in Adversarial Patrolling Games](https://arxiv.org/abs/2505.14137)
*Vojtěch Kůr,Vít Musil,Vojtěch Řehák*

Main category: cs.AI

TL;DR: 该论文提出了一种迭代调整内存分配的方法，解决了有限内存策略中内存分配选择的难题，提升了防御策略的有效性。


<details>
  <summary>Details</summary>
Motivation: 有限内存策略在对抗性巡逻游戏中表现出色，但内存分配的选择是一个长期未解决的难题，限制了策略的实际应用。

Method: 开发了一种通用方法，通过迭代调整内存分配，可与任何黑盒策略优化工具结合使用。

Result: 实验表明该方法在各种巡逻模型实例中具有鲁棒性，能有效优化防御策略。

Conclusion: 该方法解决了有限内存策略中的关键问题，为防御策略的优化提供了实用工具。

Abstract: Adversarial Patrolling games form a subclass of Security games where a
Defender moves between locations, guarding vulnerable targets. The main
algorithmic problem is constructing a strategy for the Defender that minimizes
the worst damage an Attacker can cause. We focus on the class of finite-memory
(also known as regular) Defender's strategies that experimentally outperformed
other competing classes. A finite-memory strategy can be seen as a positional
strategy on a finite set of states. Each state consists of a pair of a location
and a certain integer value--called memory. Existing algorithms improve the
transitional probabilities between the states but require that the available
memory size itself is assigned at each location manually. Choosing the right
memory assignment is a well-known open and hard problem that hinders the
usability of finite-memory strategies. We solve this issue by developing a
general method that iteratively changes the memory assignment. Our algorithm
can be used in connection with \emph{any} black-box strategy optimization tool.
We evaluate our method on various experiments and show its robustness by
solving instances of various patrolling models.

</details>


### [175] [RL of Thoughts: Navigating LLM Reasoning with Inference-time Reinforcement Learning](https://arxiv.org/abs/2505.14140)
*Qianyue Hao,Sibo Li,Jian Yuan,Yong Li*

Main category: cs.AI

TL;DR: 提出RLoT方法，通过强化学习训练轻量级导航模型，动态选择逻辑块增强LLM推理能力，显著提升性能且具备强迁移性。


<details>
  <summary>Details</summary>
Motivation: 现有推理时技术（如Chain/Tree/Graph-of-Thoughts）虽能提升大语言模型（LLM）推理性能，但缺乏任务自适应性，需手动预定义通用逻辑框架。

Method: 设计5个人类认知视角的基础逻辑块，用强化学习训练轻量级导航模型（RL navigator），动态组合逻辑块构建任务专属推理结构。

Result: 在多个基准测试（AIME/MATH/GPQA）和LLM（GPT/Llama/Qwen等）上，RLoT性能最高提升13.4%，仅3K参数的导航器可使10B级LLM媲美100B模型，且展现跨模型/任务的强迁移性。

Conclusion: RLoT通过自适应逻辑结构生成显著提升LLM推理效率，为轻量化增强模型性能提供新思路，代码已开源。

Abstract: Despite rapid advancements in large language models (LLMs), the token-level
autoregressive nature constrains their complex reasoning capabilities. To
enhance LLM reasoning, inference-time techniques, including
Chain/Tree/Graph-of-Thought(s), successfully improve the performance, as they
are fairly cost-effective by guiding reasoning through sophisticated logical
structures without modifying LLMs' parameters. However, these manually
predefined, task-agnostic frameworks are applied uniformly across diverse
tasks, lacking adaptability. To improve this, we propose RL-of-Thoughts (RLoT),
where we train a lightweight navigator model with reinforcement learning (RL)
to adaptively enhance LLM reasoning at inference time. Specifically, we design
five basic logic blocks from the perspective of human cognition. During the
reasoning process, the trained RL navigator dynamically selects the suitable
logic blocks and combines them into task-specific logical structures according
to problem characteristics. Experiments across multiple reasoning benchmarks
(AIME, MATH, GPQA, etc.) with multiple LLMs (GPT, Llama, Qwen, and DeepSeek)
illustrate that RLoT outperforms established inference-time techniques by up to
13.4%. Remarkably, with less than 3K parameters, our RL navigator is able to
make sub-10B LLMs comparable to 100B-scale counterparts. Moreover, the RL
navigator demonstrates strong transferability: a model trained on one specific
LLM-task pair can effectively generalize to unseen LLMs and tasks. Our code is
open-source at https://anonymous.4open.science/r/RL-LLM-Reasoning-1A30 for
reproducibility.

</details>


### [176] [Building a Stable Planner: An Extended Finite State Machine Based Planning Module for Mobile GUI Agent](https://arxiv.org/abs/2505.14141)
*Fanglin Mo,Junzhe Chen,Haoxuan Zhu,Xuming Hu*

Main category: cs.AI

TL;DR: 论文提出SPlanner模块，通过扩展有限状态机建模应用逻辑，将用户指令分解为可执行路径，显著提升移动GUI代理的任务成功率。


<details>
  <summary>Details</summary>
Motivation: 移动GUI代理在执行任务时因缺乏对应用功能的深度理解，常导致任务规划不准确和执行迷失。现有方法在任务规划环节存在明显不足。

Method: 采用扩展有限状态机(EFSM)建模应用控制逻辑，通过遍历状态机生成执行路径，并利用大语言模型将其转化为自然语言计划。

Result: 在AndroidWorld基准测试中，SPlanner与Qwen2.5-VL-72B配合实现63.8%任务成功率，比无规划辅助提升28.8个百分点。

Conclusion: SPlanner通过结构化应用建模和路径规划，有效解决了移动GUI代理的任务规划难题，显著提升了任务执行效果。

Abstract: Mobile GUI agents execute user commands by directly interacting with the
graphical user interface (GUI) of mobile devices, demonstrating significant
potential to enhance user convenience. However, these agents face considerable
challenges in task planning, as they must continuously analyze the GUI and
generate operation instructions step by step. This process often leads to
difficulties in making accurate task plans, as GUI agents lack a deep
understanding of how to effectively use the target applications, which can
cause them to become "lost" during task execution. To address the task planning
issue, we propose SPlanner, a plug-and-play planning module to generate
execution plans that guide vision language model(VLMs) in executing tasks. The
proposed planning module utilizes extended finite state machines (EFSMs) to
model the control logits and configurations of mobile applications. It then
decomposes a user instruction into a sequence of primary function modeled in
EFSMs, and generate the execution path by traversing the EFSMs. We further
refine the execution path into a natural language plan using an LLM. The final
plan is concise and actionable, and effectively guides VLMs to generate
interactive GUI actions to accomplish user tasks. SPlanner demonstrates strong
performance on dynamic benchmarks reflecting real-world mobile usage. On the
AndroidWorld benchmark, SPlanner achieves a 63.8% task success rate when paired
with Qwen2.5-VL-72B as the VLM executor, yielding a 28.8 percentage point
improvement compared to using Qwen2.5-VL-72B without planning assistance.

</details>


### [177] [Multimodal Mixture of Low-Rank Experts for Sentiment Analysis and Emotion Recognition](https://arxiv.org/abs/2505.14143)
*Shuo Zhang,Jinsong Zhang,Zhejun Zhang,Lei Li*

Main category: cs.AI

TL;DR: 提出了一种名为MMoLRE的新型多任务学习方法，用于多模态情感分析和情绪识别，通过共享和任务特定专家避免参数冲突，并利用低秩专家网络减少计算开销。


<details>
  <summary>Details</summary>
Motivation: 现有的多任务学习方法主要采用硬参数共享，忽视了复杂任务相关性导致的参数冲突，因此需要一种能够区分共同和独特任务特征的方法。

Method: MMoLRE方法结合了共享和任务特定专家，分别建模共同和独特任务特征，并设计了低秩专家网络以减少参数和计算开销。

Result: 在CMU-MOSI和CMU-MOSEI基准测试中，MMoLRE在多模态情感分析任务上达到了最先进的性能，在情绪识别任务上取得了竞争性结果。

Conclusion: MMoLRE通过有效区分任务特征和减少计算开销，在多模态情感分析和情绪识别任务中表现出色。

Abstract: Multi-task learning (MTL) enables the efficient transfer of extra knowledge
acquired from other tasks. The high correlation between multimodal sentiment
analysis (MSA) and multimodal emotion recognition (MER) supports their joint
training. However, existing methods primarily employ hard parameter sharing,
ignoring parameter conflicts caused by complex task correlations. In this
paper, we present a novel MTL method for MSA and MER, termed Multimodal Mixture
of Low-Rank Experts (MMoLRE). MMoLRE utilizes shared and task-specific experts
to distinctly model common and unique task characteristics, thereby avoiding
parameter conflicts. Additionally, inspired by low-rank structures in the
Mixture of Experts (MoE) framework, we design low-rank expert networks to
reduce parameter and computational overhead as the number of experts increases.
Extensive experiments on the CMU-MOSI and CMU-MOSEI benchmarks demonstrate that
MMoLRE achieves state-of-the-art performance on the MSA task and competitive
results on the MER task.

</details>


### [178] [s3: You Don't Need That Much Data to Train a Search Agent via RL](https://arxiv.org/abs/2505.14146)
*Pengcheng Jiang,Xueqiang Xu,Jiacheng Lin,Jinfeng Xiao,Zifeng Wang,Jimeng Sun,Jiawei Han*

Main category: cs.AI

TL;DR: 提出s3框架，通过解耦搜索与生成模块并采用Gain Beyond RAG奖励机制，显著提升检索增强生成系统的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法要么忽略下游任务的效用优化检索指标，要么联合微调整个大语言模型，导致检索与生成耦合，限制了实际搜索效用和与冻结或专有模型的兼容性。

Method: 提出s3框架，将搜索代理与生成模型解耦，并采用Gain Beyond RAG奖励（即生成准确率相对于朴素RAG的提升）训练搜索代理。

Result: 仅需2400个训练样本，s3即超越基于70倍以上数据训练的基线，在六个通用QA和五个医学QA基准上表现更优。

Conclusion: s3框架轻量且模型无关，通过解耦和针对性奖励机制，高效提升了检索增强生成系统的下游性能。

Abstract: Retrieval-augmented generation (RAG) systems empower large language models
(LLMs) to access external knowledge during inference. Recent advances have
enabled LLMs to act as search agents via reinforcement learning (RL), improving
information acquisition through multi-turn interactions with retrieval engines.
However, existing approaches either optimize retrieval using search-only
metrics (e.g., NDCG) that ignore downstream utility or fine-tune the entire LLM
to jointly reason and retrieve-entangling retrieval with generation and
limiting the real search utility and compatibility with frozen or proprietary
models. In this work, we propose s3, a lightweight, model-agnostic framework
that decouples the searcher from the generator and trains the searcher using a
Gain Beyond RAG reward: the improvement in generation accuracy over naive RAG.
s3 requires only 2.4k training samples to outperform baselines trained on over
70x more data, consistently delivering stronger downstream performance across
six general QA and five medical QA benchmarks.

</details>


### [179] [SHARP: Synthesizing High-quality Aligned Reasoning Problems for Large Reasoning Models Reinforcement Learning](https://arxiv.org/abs/2505.14147)
*Xiong Jun Wu,Zhenduo Zhang,ZuJie Wen,Zhiqiang Zhang,Wang Ren,Lei Shi,Cai Chen,Deng Zhao,Dingnan Jin,Qing Cui,Jun Zhou*

Main category: cs.AI

TL;DR: SHARP方法通过合成高质量、可验证的STEM问题集，提升大型推理模型的复杂推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法生成的STEM问题集质量不高、多样性不足且难以验证，限制了大型推理模型在复杂任务上的进步。

Method: SHARP采用自对齐原则和三阶段框架（对齐、实例化、推理），结合强化学习循环优化模型推理。

Result: 在GPQA等基准测试中，SHARP显著提升了模型的复杂推理准确率，接近专家水平。

Conclusion: SHARP为大型推理模型的训练提供了高质量问题集，有效提升了其复杂推理能力。

Abstract: Training large reasoning models (LRMs) with reinforcement learning in STEM
domains is hindered by the scarcity of high-quality, diverse, and verifiable
problem sets. Existing synthesis methods, such as Chain-of-Thought prompting,
often generate oversimplified or uncheckable data, limiting model advancement
on complex tasks. To address these challenges, we introduce SHARP, a unified
approach to Synthesizing High-quality Aligned Reasoning Problems for LRMs
reinforcement learning with verifiable rewards (RLVR). SHARP encompasses a
strategic set of self-alignment principles -- targeting graduate and
Olympiad-level difficulty, rigorous logical consistency, and unambiguous,
verifiable answers -- and a structured three-phase framework (Alignment,
Instantiation, Inference) that ensures thematic diversity and fine-grained
control over problem generation. We implement SHARP by leveraging a
state-of-the-art LRM to infer and verify challenging STEM questions, then
employ a reinforcement learning loop to refine the model's reasoning through
verifiable reward signals. Experiments on benchmarks such as GPQA demonstrate
that SHARP-augmented training substantially outperforms existing methods,
markedly improving complex reasoning accuracy and pushing LRM performance
closer to expert-level proficiency. Our contributions include the SHARP
strategy, framework design, end-to-end implementation, and experimental
evaluation of its effectiveness in elevating LRM reasoning capabilities.

</details>


### [180] [MM-Agent: LLM as Agents for Real-world Mathematical Modeling Problem](https://arxiv.org/abs/2505.14148)
*Fan Liu,Zherui Yang,Cancheng Liu,Tianrui Song,Xiaofeng Gao,Hao Liu*

Main category: cs.AI

TL;DR: 论文提出MM-Agent框架，通过四阶段建模方法显著提升LLM在数学建模任务中的表现，并在MCM/ICM竞赛中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型（LLMs）在严格建模能力上存在不足，限制了其解决现实问题的实用性。为此，研究旨在开发一个能系统化完成数学建模全流程的AI框架。

Method: 提出MM-Agent框架，将建模分解为开放式问题分析、结构化模型构建、计算求解和报告生成四个阶段，并基于MCM/ICM竞赛数据构建MM-Bench基准测试集。

Result: MM-Agent在基准测试中比人类专家方案提升11.88%，单任务仅需15分钟和0.88美元成本，并成功辅助两支本科队伍获得2025年MCM/ICM决赛奖（前2%）。

Conclusion: MM-Agent证明了LLM作为建模协作者的实际效能，为复杂现实问题的自动化求解提供了新范式。

Abstract: Mathematical modeling is a cornerstone of scientific discovery and
engineering practice, enabling the translation of real-world problems into
formal systems across domains such as physics, biology, and economics. Unlike
mathematical reasoning, which assumes a predefined formulation, modeling
requires open-ended problem analysis, abstraction, and principled
formalization. While Large Language Models (LLMs) have shown strong reasoning
capabilities, they fall short in rigorous model construction, limiting their
utility in real-world problem-solving. To this end, we formalize the task of
LLM-powered real-world mathematical modeling, where agents must analyze
problems, construct domain-appropriate formulations, and generate complete
end-to-end solutions. We introduce MM-Bench, a curated benchmark of 111
problems from the Mathematical Contest in Modeling (MCM/ICM), spanning the
years 2000 to 2025 and across ten diverse domains such as physics, biology, and
economics. To tackle this task, we propose MM-Agent, an expert-inspired
framework that decomposes mathematical modeling into four stages: open-ended
problem analysis, structured model formulation, computational problem solving,
and report generation. Experiments on MM-Bench show that MM-Agent significantly
outperforms baseline agents, achieving an 11.88\% improvement over human expert
solutions while requiring only 15 minutes and \$0.88 per task using GPT-4o.
Furthermore, under official MCM/ICM protocols, MM-Agent assisted two
undergraduate teams in winning the Finalist Award (\textbf{top 2.0\% among
27,456 teams}) in MCM/ICM 2025, demonstrating its practical effectiveness as a
modeling copilot. Our code is available at
https://github.com/usail-hkust/LLM-MM-Agent

</details>


### [181] [DSMentor: Enhancing Data Science Agents with Curriculum Learning and Online Knowledge Accumulation](https://arxiv.org/abs/2505.14163)
*He Wang,Alexander Hanbo Li,Yiqun Hu,Sheng Zhang,Hideo Kobayashi,Jiani Zhang,Henry Zhu,Chung-Wei Hang,Patrick Ng*

Main category: cs.AI

TL;DR: 论文提出DSMentor框架，通过课程学习策略优化LLM代理在数据科学任务中的表现，显著提升通过率和因果推理能力。


<details>
  <summary>Details</summary>
Motivation: 当前研究主要关注通过改进搜索、采样和规划技术来增强上下文学习，而忽视了推理过程中问题解决顺序的重要性。

Method: 开发了DSMentor框架，利用课程学习策略，按难度递增顺序组织任务，并结合长期记忆保留先前经验，指导代理学习进程。

Result: DSMentor使用Claude-3.5-Sonnet在DSEval和QRData基准上通过率提升达5.2%，因果推理问题通过率比GPT-4提高8.8%。

Conclusion: 研究强调了在推理过程中积累和利用知识的重要性，为通过基于课程的推理优化提升LLM性能开辟了新途径。

Abstract: Large language model (LLM) agents have shown promising performance in
generating code for solving complex data science problems. Recent studies
primarily focus on enhancing in-context learning through improved search,
sampling, and planning techniques, while overlooking the importance of the
order in which problems are tackled during inference. In this work, we develop
a novel inference-time optimization framework, referred to as DSMentor, which
leverages curriculum learning -- a strategy that introduces simpler task first
and progressively moves to more complex ones as the learner improves -- to
enhance LLM agent performance in challenging data science tasks. Our
mentor-guided framework organizes data science tasks in order of increasing
difficulty and incorporates a growing long-term memory to retain prior
experiences, guiding the agent's learning progression and enabling more
effective utilization of accumulated knowledge. We evaluate DSMentor through
extensive experiments on DSEval and QRData benchmarks. Experiments show that
DSMentor using Claude-3.5-Sonnet improves the pass rate by up to 5.2% on DSEval
and QRData compared to baseline agents. Furthermore, DSMentor demonstrates
stronger causal reasoning ability, improving the pass rate by 8.8% on the
causality problems compared to GPT-4 using Program-of-Thoughts prompts. Our
work underscores the importance of developing effective strategies for
accumulating and utilizing knowledge during inference, mirroring the human
learning process and opening new avenues for improving LLM performance through
curriculum-based inference optimization.

</details>


### [182] [Dynamic Replanning for Improved Public Transport Routing](https://arxiv.org/abs/2505.14193)
*Abdallah Abuaisha,Bojie Shen,Daniel Harabor,Peter Stuckey,Mark Wallace*

Main category: cs.AI

TL;DR: 论文提出公共交通动态重规划框架，通过主动推送方案显著提升效率。


<details>
  <summary>Details</summary>
Motivation: 现有公交延误处理方案存在局限：基于历史数据的备用方案无法利用实时延误信息，静态规划则忽略未来延误。实时数据的普及亟需系统级动态重规划框架。

Method: 提出两种动态重规划方案：用户主动请求的'拉取式'和服务器主动监控调整的'推送式'。

Result: 实验表明推送式方案优于拉取式，能实现显著加速，动态重规划可大幅节省到达时间。

Conclusion: 主动式动态重规划能有效利用实时延误数据，为公交系统提供更优路径方案。

Abstract: Delays in public transport are common, often impacting users through
prolonged travel times and missed transfers. Existing solutions for handling
delays remain limited; backup plans based on historical data miss opportunities
for earlier arrivals, while snapshot planning accounts for current delays but
not future ones. With the growing availability of live delay data, users can
adjust their journeys in real-time. However, the literature lacks a framework
that fully exploits this advantage for system-scale dynamic replanning. To
address this, we formalise the dynamic replanning problem in public transport
routing and propose two solutions: a "pull" approach, where users manually
request replanning, and a novel "push" approach, where the server proactively
monitors and adjusts journeys. Our experiments show that the push approach
outperforms the pull approach, achieving significant speedups. The results also
reveal substantial arrival time savings enabled by dynamic replanning.

</details>


### [183] [Embedded Mean Field Reinforcement Learning for Perimeter-defense Game](https://arxiv.org/abs/2505.14209)
*Li Wang,Xin Yu,Xuxin Lv,Gangzheng Ai,Wenjun Wu*

Main category: cs.AI

TL;DR: 本文研究了三维环境下的大规模异构边界防御博弈，提出了一种名为EMFAC的框架，通过表示学习和注意力机制提升防御者的决策效率和收敛速度，并在仿真和实际实验中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 随着无人机和导弹技术的快速发展，攻击者与防御者之间的边界防御博弈在多个领域变得日益复杂和重要。然而，现有研究多集中于小规模、简化的二维场景，忽略了现实环境中的扰动、运动动力学和异构性等因素，限制了实际应用。

Method: 本文提出了一种名为EMFAC（嵌入式均值场行动者-评论家）的框架，结合表示学习实现高层动作聚合，并引入基于奖励表示的轻量级注意力机制，以优化大规模异构控制问题。

Result: 通过大量仿真实验，EMFAC在收敛速度和整体性能上均优于现有基线方法。小规模实际实验进一步验证了该框架在复杂场景中的实用性。

Conclusion: EMFAC框架有效解决了大规模异构边界防御博弈中的挑战，为复杂环境下的防御策略提供了高效且可扩展的解决方案。

Abstract: With the rapid advancement of unmanned aerial vehicles (UAVs) and missile
technologies, perimeter-defense game between attackers and defenders for the
protection of critical regions have become increasingly complex and
strategically significant across a wide range of domains. However, existing
studies predominantly focus on small-scale, simplified two-dimensional
scenarios, often overlooking realistic environmental perturbations, motion
dynamics, and inherent heterogeneity--factors that pose substantial challenges
to real-world applicability. To bridge this gap, we investigate large-scale
heterogeneous perimeter-defense game in a three-dimensional setting,
incorporating realistic elements such as motion dynamics and wind fields. We
derive the Nash equilibrium strategies for both attackers and defenders,
characterize the victory regions, and validate our theoretical findings through
extensive simulations. To tackle large-scale heterogeneous control challenges
in defense strategies, we propose an Embedded Mean-Field Actor-Critic (EMFAC)
framework. EMFAC leverages representation learning to enable high-level action
aggregation in a mean-field manner, supporting scalable coordination among
defenders. Furthermore, we introduce a lightweight agent-level attention
mechanism based on reward representation, which selectively filters
observations and mean-field information to enhance decision-making efficiency
and accelerate convergence in large-scale tasks. Extensive simulations across
varying scales demonstrate the effectiveness and adaptability of EMFAC, which
outperforms established baselines in both convergence speed and overall
performance. To further validate practicality, we test EMFAC in small-scale
real-world experiments and conduct detailed analyses, offering deeper insights
into the framework's effectiveness in complex scenarios.

</details>


### [184] [Reinforcement Learning vs. Distillation: Understanding Accuracy and Capability in LLM Reasoning](https://arxiv.org/abs/2505.14216)
*Minwu Kim,Anubhav Shrestha,Safal Shrestha,Aadim Nepal,Keith Ross*

Main category: cs.AI

TL;DR: 研究发现，带可验证奖励的强化学习（RLVR）虽能提升整体准确率但无法增强模型能力，而蒸馏方法可同时提升两者。RLVR因侧重简单问题而损害难题表现，蒸馏则需引入新知识才能真正提升能力。


<details>
  <summary>Details</summary>
Motivation: 探究RLVR和蒸馏方法对语言模型推理行为的影响机制，解释为何RLVR无法提升模型能力而蒸馏可以。

Method: 通过实验分析RLVR和蒸馏在小型模型中的表现差异，包括问题难度分布、回答质量变化及知识引入的影响。

Result: 1) RLVR会牺牲难题准确率换取简单问题提升 2) 蒸馏需结合新知识才能增强能力 3) 现有回答质量指标（如长度、反思关键词）不可靠

Conclusion: 两种方法对模型能力的差异化影响源于训练侧重点不同：RLVR优化局部准确率，蒸馏通过知识迁移实现全局提升。这为模型训练策略选择提供了理论依据。

Abstract: Recent studies have shown that reinforcement learning with verifiable rewards
(RLVR) enhances overall accuracy but fails to improve capability, while
distillation can improve both. In this paper, we investigate the mechanisms
behind these phenomena. First, we demonstrate that RLVR does not improve
capability because it focuses on improving the accuracy of the less-difficult
questions to the detriment of the accuracy of the most difficult questions,
thereby leading to no improvement in capability. Second, we find that RLVR does
not merely increase the success probability for the less difficult questions,
but in our small model settings produces quality responses that were absent in
its output distribution before training. In addition, we show these responses
are neither noticeably longer nor feature more reflection-related keywords,
underscoring the need for more reliable indicators of response quality. Third,
we show that while distillation reliably improves accuracy by learning strong
reasoning patterns, it only improves capability when new knowledge is
introduced. Moreover, when distilling only with reasoning patterns and no new
knowledge, the accuracy of the less-difficult questions improves to the
detriment of the most difficult questions, similar to RLVR. Together, these
findings offer a clearer understanding of how RLVR and distillation shape
reasoning behavior in language models.

</details>


### [185] [Toward Embodied AGI: A Review of Embodied AI and the Road Ahead](https://arxiv.org/abs/2505.14235)
*Yequan Wang,Aixin Sun*

Main category: cs.AI

TL;DR: 论文提出了一种五级分类体系（L1-L5）来系统化具身AGI研究，回顾了基础阶段（L1-L2）的现状与挑战，并提出了实现高阶能力（L3-L5）的技术框架。


<details>
  <summary>Details</summary>
Motivation: 随着机器人技术和基础AI模型的进步，实现通用化具身AI系统成为可能。论文旨在通过建立系统化分类体系，推动具身AGI领域的发展。

Method: 采用分类学方法构建五级具身AGI框架（L1-L5），结合现有技术提出L3+机器人大脑的概念框架。

Result: 明确了各层级的研究现状（L1-L2）与关键技术缺口（L3-L5），提出了可扩展的技术实现路径。

Conclusion: 该分类体系为具身AGI提供了系统化研究基础，L3+框架为未来高阶能力探索指明了方向。

Abstract: Artificial General Intelligence (AGI) is often envisioned as inherently
embodied. With recent advances in robotics and foundational AI models, we stand
at the threshold of a new era-one marked by increasingly generalized embodied
AI systems. This paper contributes to the discourse by introducing a systematic
taxonomy of Embodied AGI spanning five levels (L1-L5). We review existing
research and challenges at the foundational stages (L1-L2) and outline the key
components required to achieve higher-level capabilities (L3-L5). Building on
these insights and existing technologies, we propose a conceptual framework for
an L3+ robotic brain, offering both a technical outlook and a foundation for
future exploration.

</details>


### [186] [EVA: Red-Teaming GUI Agents via Evolving Indirect Prompt Injection](https://arxiv.org/abs/2505.14289)
*Yijie Lu,Tianjie Ju,Manman Zhao,Xinbei Ma,Yuan Guo,ZhuoSheng Zhang*

Main category: cs.AI

TL;DR: 论文提出EVA框架，通过动态优化间接提示注入攻击，显著提高攻击成功率并揭示多模态GUI代理的共同漏洞。


<details>
  <summary>Details</summary>
Motivation: 随着多模态代理在图形用户界面(GUI)中执行任务，间接提示注入攻击（如通过视觉环境嵌入误导指令）威胁日益增加，需开发有效防御方法。

Method: 提出EVA红队框架，将攻击转化为闭环优化过程，动态调整对抗性线索、关键词和布局以适配代理的视觉注意力分布。

Result: 在6种GUI代理上的实验表明，EVA攻击成功率显著高于静态方法，且攻击模式可跨模型迁移，揭示了代理决策中的共性偏差。

Conclusion: 动态间接提示注入是红队测试的有效工具，同时能暴露多模态代理的通用脆弱性，为安全改进提供方向。

Abstract: As multimodal agents are increasingly trained to operate graphical user
interfaces (GUIs) to complete user tasks, they face a growing threat from
indirect prompt injection, attacks in which misleading instructions are
embedded into the agent's visual environment, such as popups or chat messages,
and misinterpreted as part of the intended task. A typical example is
environmental injection, in which GUI elements are manipulated to influence
agent behavior without directly modifying the user prompt. To address these
emerging attacks, we propose EVA, a red teaming framework for indirect prompt
injection which transforms the attack into a closed loop optimization by
continuously monitoring an agent's attention distribution over the GUI and
updating adversarial cues, keywords, phrasing, and layout, in response.
Compared with prior one shot methods that generate fixed prompts without regard
for how the model allocates visual attention, EVA dynamically adapts to
emerging attention hotspots, yielding substantially higher attack success rates
and far greater transferability across diverse GUI scenarios. We evaluate EVA
on six widely used generalist and specialist GUI agents in realistic settings
such as popup manipulation, chat based phishing, payments, and email
composition. Experimental results show that EVA substantially improves success
rates over static baselines. Under goal agnostic constraints, where the
attacker does not know the agent's task intent, EVA still discovers effective
patterns. Notably, we find that injection styles transfer well across models,
revealing shared behavioral biases in GUI agents. These results suggest that
evolving indirect prompt injection is a powerful tool not only for red teaming
agents, but also for uncovering common vulnerabilities in their multimodal
decision making.

</details>


### [187] [SafetyNet: Detecting Harmful Outputs in LLMs by Modeling and Monitoring Deceptive Behaviors](https://arxiv.org/abs/2505.14300)
*Maheep Chaudhary,Fazl Barez*

Main category: cs.AI

TL;DR: 提出了一种实时监测框架Safety-Net，通过无监督方法检测大语言模型的有害输出，准确率达96%。


<details>
  <summary>Details</summary>
Motivation: 受核能和航空等高危行业实时监控的启发，研究如何预防大语言模型生成暴力、色情等有害内容，特别是针对后门触发响应的隐藏漏洞。

Method: 采用无监督方法，将正常行为作为基线，有害输出视为异常，设计多检测器框架监测不同表征维度，防止模型通过改变表征方式逃避监测。

Result: Safety-Net框架在检测有害案例时达到96%的准确率，能有效捕捉模型生成有害内容时的内部行为特征。

Conclusion: 该研究为实时监测大语言模型的有害输出提供了有效方法，特别是针对未来可能出现的更具欺骗性的模型。

Abstract: High-risk industries like nuclear and aviation use real-time monitoring to
detect dangerous system conditions. Similarly, Large Language Models (LLMs)
need monitoring safeguards. We propose a real-time framework to predict harmful
AI outputs before they occur by using an unsupervised approach that treats
normal behavior as the baseline and harmful outputs as outliers. Our study
focuses specifically on backdoor-triggered responses -- where specific input
phrases activate hidden vulnerabilities causing the model to generate unsafe
content like violence, pornography, or hate speech. We address two key
challenges: (1) identifying true causal indicators rather than surface
correlations, and (2) preventing advanced models from deception -- deliberately
evading monitoring systems. Hence, we approach this problem from an
unsupervised lens by drawing parallels to human deception: just as humans
exhibit physical indicators while lying, we investigate whether LLMs display
distinct internal behavioral signatures when generating harmful content. Our
study addresses two critical challenges: 1) designing monitoring systems that
capture true causal indicators rather than superficial correlations; and
2)preventing intentional evasion by increasingly capable "Future models''. Our
findings show that models can produce harmful content through causal mechanisms
and can become deceptive by: (a) alternating between linear and non-linear
representations, and (b) modifying feature relationships. To counter this, we
developed Safety-Net -- a multi-detector framework that monitors different
representation dimensions, successfully detecting harmful behavior even when
information is shifted across representational spaces to evade individual
monitors. Our evaluation shows 96% accuracy in detecting harmful cases using
our unsupervised ensemble approach.

</details>


### [188] [Towards Embodied Cognition in Robots via Spatially Grounded Synthetic Worlds](https://arxiv.org/abs/2505.14366)
*Joel Currie,Gioele Migno,Enrico Piacenti,Maria Elena Giannaccini,Patric Bach,Davide De Tommaso,Agnieszka Wykowska*

Main category: cs.AI

TL;DR: 提出训练视觉语言模型进行视觉透视取样的概念框架，并发布合成数据集支持空间推理任务研究。


<details>
  <summary>Details</summary>
Motivation: 为实现人机交互中的具身认知核心能力——视觉透视取样，需开发能进行空间理解的AI系统。

Method: 使用NVIDIA Omniverse生成包含RGB图像、自然语言描述和物体姿态矩阵的合成数据集，监督学习Z轴距离推理。

Result: 构建了支持空间推理任务的数据集，未来可扩展至6自由度推理，数据集已公开。

Conclusion: 该研究为具身AI系统在交互场景中的空间理解能力奠定了基础。

Abstract: We present a conceptual framework for training Vision-Language Models (VLMs)
to perform Visual Perspective Taking (VPT), a core capability for embodied
cognition essential for Human-Robot Interaction (HRI). As a first step toward
this goal, we introduce a synthetic dataset, generated in NVIDIA Omniverse,
that enables supervised learning for spatial reasoning tasks. Each instance
includes an RGB image, a natural language description, and a ground-truth 4X4
transformation matrix representing object pose. We focus on inferring Z-axis
distance as a foundational skill, with future extensions targeting full 6
Degrees Of Freedom (DOFs) reasoning. The dataset is publicly available to
support further research. This work serves as a foundational step toward
embodied AI systems capable of spatial understanding in interactive human-robot
scenarios.

</details>


### [189] [SCAN: Semantic Document Layout Analysis for Textual and Visual Retrieval-Augmented Generation](https://arxiv.org/abs/2505.14381)
*Yuyang Dong,Nobuhiro Ueda,Krisztián Boros,Daiki Ito,Takuya Sera,Masafumi Oyamada*

Main category: cs.AI

TL;DR: SCAN方法通过语义文档布局分析提升文本和视觉RAG系统性能，在英日数据集上表现优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型和视觉语言模型的广泛应用，处理富含信息的文档仍具挑战性，需要平衡上下文保留与处理效率。

Method: SCAN采用粗粒度语义方法，将文档划分为连贯区域，并通过微调目标检测模型进行训练。

Result: 实验显示，SCAN使文本RAG性能提升9.0%，视觉RAG性能提升6.4%，超越传统和商业解决方案。

Conclusion: SCAN有效解决了富含视觉信息文档的处理难题，显著提升了RAG系统的性能。

Abstract: With the increasing adoption of Large Language Models (LLMs) and
Vision-Language Models (VLMs), rich document analysis technologies for
applications like Retrieval-Augmented Generation (RAG) and visual RAG are
gaining significant attention. Recent research indicates that using VLMs can
achieve better RAG performance, but processing rich documents still remains a
challenge since a single page contains large amounts of information. In this
paper, we present SCAN (\textbf{S}emanti\textbf{C} Document Layout
\textbf{AN}alysis), a novel approach enhancing both textual and visual
Retrieval-Augmented Generation (RAG) systems working with visually rich
documents. It is a VLM-friendly approach that identifies document components
with appropriate semantic granularity, balancing context preservation with
processing efficiency. SCAN uses a coarse-grained semantic approach that
divides documents into coherent regions covering continuous components. We
trained the SCAN model by fine-tuning object detection models with
sophisticated annotation datasets. Our experimental results across English and
Japanese datasets demonstrate that applying SCAN improves end-to-end textual
RAG performance by up to 9.0\% and visual RAG performance by up to 6.4\%,
outperforming conventional approaches and even commercial document processing
solutions.

</details>


### [190] [Beyond the First Error: Process Reward Models for Reflective Mathematical Reasoning](https://arxiv.org/abs/2505.14391)
*Zhaohui Yang,Chenghua He,Xiaowen Shi,Linjing Li,Qiyue Yin,Shihong Deng,Daxin Jiang*

Main category: cs.AI

TL;DR: 本文提出了一种针对长链思维过程（CoT）的新型数据标注方法，通过引入错误传播与终止概念，提升PRM在识别自我纠正行为上的能力，实验证明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前的数据标注方法在处理长链思维过程时存在局限，仅关注第一个错误步骤及之前步骤，忽视了后续可能出现的正确步骤和自我纠正机制。

Method: 提出了一种新的PRM数据标注方法，引入错误传播与终止概念，利用基于LLM的评判器进行标注，收集170万数据样本训练7B参数的PRM。

Result: 实验结果显示，该方法在搜索引导、BoN和F1分数等多项指标上优于现有开源PRM及基于MC的标注方法，且数据效率和性能更优。

Conclusion: 该方法不仅提高了数据效率，还展现出更好的性能和泛化能力，为长链思维过程的数据标注提供了有效解决方案。

Abstract: Many studies focus on data annotation techniques for training effective PRMs.
However, current methods encounter a significant issue when applied to long CoT
reasoning processes: they tend to focus solely on the first incorrect step and
all preceding steps, assuming that all subsequent steps are incorrect. These
methods overlook the unique self-correction and reflection mechanisms inherent
in long CoT, where correct reasoning steps may still occur after initial
reasoning mistakes. To address this issue, we propose a novel data annotation
method for PRMs specifically designed to score the long CoT reasoning process.
Given that under the reflection pattern, correct and incorrect steps often
alternate, we introduce the concepts of Error Propagation and Error Cessation,
enhancing PRMs' ability to identify both effective self-correction behaviors
and reasoning based on erroneous steps. Leveraging an LLM-based judger for
annotation, we collect 1.7 million data samples to train a 7B PRM and evaluate
it at both solution and step levels. Experimental results demonstrate that
compared to existing open-source PRMs and PRMs trained on open-source datasets,
our PRM achieves superior performance across various metrics, including search
guidance, BoN, and F1 scores. Compared to widely used MC-based annotation
methods, our annotation approach not only achieves higher data efficiency but
also delivers superior performance. Detailed analysis is also conducted to
demonstrate the stability and generalizability of our method.

</details>


### [191] [Knowledge Graph Based Repository-Level Code Generation](https://arxiv.org/abs/2505.14394)
*Mihir Athale,Vishal Vaddina*

Main category: cs.AI

TL;DR: 提出基于知识图谱的代码搜索与检索方法，提升大语言模型在代码库级别任务中的上下文感知能力。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在代码生成时存在上下文准确性不足的问题，尤其在不断演化的代码库中，现有代码搜索方法在质量和上下文相关性上缺乏鲁棒性。

Method: 将代码库表示为知识图谱，捕捉结构和关系信息，采用混合方法进行代码检索以提升上下文相关性，跟踪文件间模块依赖关系。

Result: 在EvoCodeBench基准测试中显著优于基线方法，验证了方法的有效性。

Conclusion: 基于知识图谱的代码生成方法可推动开发更鲁棒、上下文敏感的编码辅助工具。

Abstract: Recent advancements in Large Language Models (LLMs) have transformed code
generation from natural language queries. However, despite their extensive
knowledge and ability to produce high-quality code, LLMs often struggle with
contextual accuracy, particularly in evolving codebases. Current code search
and retrieval methods frequently lack robustness in both the quality and
contextual relevance of retrieved results, leading to suboptimal code
generation. This paper introduces a novel knowledge graph-based approach to
improve code search and retrieval leading to better quality of code generation
in the context of repository-level tasks. The proposed approach represents code
repositories as graphs, capturing structural and relational information for
enhanced context-aware code generation. Our framework employs a hybrid approach
for code retrieval to improve contextual relevance, track inter-file modular
dependencies, generate more robust code and ensure consistency with the
existing codebase. We benchmark the proposed approach on the Evolutionary Code
Benchmark (EvoCodeBench) dataset, a repository-level code generation benchmark,
and demonstrate that our method significantly outperforms the baseline
approach. These findings suggest that knowledge graph based code generation
could advance robust, context-sensitive coding assistance tools.

</details>


### [192] [Causal Cartographer: From Mapping to Reasoning Over Counterfactual Worlds](https://arxiv.org/abs/2505.14396)
*Gaël Gendron,Jože M. Rožanec,Michael Witbrock,Gillian Dobbie*

Main category: cs.AI

TL;DR: 论文提出Causal Cartographer框架，通过显式提取和建模因果关系，提升大语言模型在因果推理任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 当前基础模型（如大语言模型）缺乏因果推理能力，且现实场景中反事实评估受限。论文旨在解决这些问题。

Method: 采用图检索增强生成代理提取因果关系，构建因果知识库，并设计受约束的反事实推理代理进行逐步因果推断。

Result: 该方法能有效提取因果知识，降低推理成本，减少伪相关性，增强大语言模型的因果推理鲁棒性。

Conclusion: Causal Cartographer框架为构建可解释的因果世界模型提供了可行路径。

Abstract: Causal world models are systems that can answer counterfactual questions
about an environment of interest, i.e. predict how it would have evolved if an
arbitrary subset of events had been realized differently. It requires
understanding the underlying causes behind chains of events and conducting
causal inference for arbitrary unseen distributions. So far, this task eludes
foundation models, notably large language models (LLMs), which do not have
demonstrated causal reasoning capabilities beyond the memorization of existing
causal relationships. Furthermore, evaluating counterfactuals in real-world
applications is challenging since only the factual world is observed, limiting
evaluation to synthetic datasets. We address these problems by explicitly
extracting and modeling causal relationships and propose the Causal
Cartographer framework. First, we introduce a graph retrieval-augmented
generation agent tasked to retrieve causal relationships from data. This
approach allows us to construct a large network of real-world causal
relationships that can serve as a repository of causal knowledge and build
real-world counterfactuals. In addition, we create a counterfactual reasoning
agent constrained by causal relationships to perform reliable step-by-step
causal inference. We show that our approach can extract causal knowledge and
improve the robustness of LLMs for causal reasoning tasks while reducing
inference costs and spurious correlations.

</details>


### [193] [Unearthing Gems from Stones: Policy Optimization with Negative Sample Augmentation for LLM Reasoning](https://arxiv.org/abs/2505.14403)
*Zhaohui Yang,Shilei Jiang,Chen Hu,Linjing Li,Shihong Deng,Daxin Jiang*

Main category: cs.AI

TL;DR: 提出BCPG-NSA方法，通过精细利用负样本中的有效信息提升推理模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法忽视负样本中的自我反思和纠错步骤，导致潜在学习信号未被充分利用。

Method: BCPG-NSA框架包含样本分割、基于共识的步骤正确性评估及负样本增强策略优化三阶段。

Result: 在数学/编程推理任务上超越基线，证明样本效率提升且具有扩展鲁棒性。

Conclusion: 负样本中的有效步骤可被系统挖掘，为推理模型训练提供新思路。

Abstract: Recent advances in reasoning language models have witnessed a paradigm shift
from short to long CoT pattern. Given the substantial computational cost of
rollouts in long CoT models, maximizing the utility of fixed training datasets
becomes crucial. Our analysis reveals that negative responses contain valuable
components such as self-reflection and error-correction steps, yet primary
existing methods either completely discard negative samples (RFT) or apply
equal penalization across all tokens (RL), failing to leverage these potential
learning signals. In light of this, we propose Behavior Constrained Policy
Gradient with Negative Sample Augmentation (BCPG-NSA), a fine-grained offline
RL framework that encompasses three stages: 1) sample segmentation, 2)
consensus-based step correctness assessment combining LLM and PRM judgers, and
3) policy optimization with NSA designed to effectively mine positive steps
within negative samples. Experimental results show that BCPG-NSA outperforms
baselines on several challenging math/coding reasoning benchmarks using the
same training dataset, achieving improved sample efficiency and demonstrating
robustness and scalability when extended to multiple iterations.

</details>


### [194] [PRL: Prompts from Reinforcement Learning](https://arxiv.org/abs/2505.14412)
*Paweł Batorski,Adrian Kosmala,Paul Swoboda*

Main category: cs.AI

TL;DR: PRL是一种基于强化学习的自动提示生成方法，能生成未见过的少样本示例，在多个任务上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 设计有效的提示需要专家直觉和对任务的深入理解，且关键语义线索常难以察觉。PRL旨在通过自动化方法解决这一挑战。

Method: 提出PRL（基于强化学习的提示生成方法），可自动生成训练中未出现的少样本示例。

Result: 在文本分类、摘要和简化任务上超越APE和EvoPrompt，分类任务提升2.58%，摘要ROUGE提升4.32，简化SARI提升6.93。

Conclusion: PRL通过强化学习自动生成高质量提示，显著提升LLM在多种任务上的表现，代码已开源。

Abstract: Effective prompt engineering remains a central challenge in fully harnessing
the capabilities of LLMs. While well-designed prompts can dramatically enhance
performance, crafting them typically demands expert intuition and a nuanced
understanding of the task. Moreover, the most impactful prompts often hinge on
subtle semantic cues, ones that may elude human perception but are crucial for
guiding LLM behavior. In this paper, we introduce PRL (Prompts from
Reinforcement Learning), a novel RL-based approach for automatic prompt
generation. Unlike previous methods, PRL can produce novel few-shot examples
that were not seen during training. Our approach achieves state-of-the-art
performance across a range of benchmarks, including text classification,
simplification, and summarization. On the classification task, it surpasses
prior methods by 2.58% over APE and 1.00% over EvoPrompt. Additionally, it
improves the average ROUGE scores on the summarization task by 4.32 over APE
and by 2.12 over EvoPrompt and the SARI score on simplification by 6.93 over
APE and by 6.01 over EvoPrompt. Our code is available at
https://github.com/Batorskq/prl .

</details>


### [195] [SCOPE: Compress Mathematical Reasoning Steps for Efficient Automated Process Annotation](https://arxiv.org/abs/2505.14419)
*Huimin Xu,Xin Mao,Feng-Lin Li,Xiaobao Wu,Wang Chen,Wei Zhang,Anh Tuan Luu*

Main category: cs.AI

TL;DR: SCOPE提出了一种基于压缩的步骤估计方法，显著降低了过程奖励模型的标注成本。


<details>
  <summary>Details</summary>
Motivation: 现有的过程标注方法（如人工标注或蒙特卡洛模拟）计算成本高昂，需要更高效的替代方案。

Method: 将自然语言推理步骤转换为代码并通过抽象语法树归一化，合并等效步骤构建前缀树，利用压缩技术降低复杂度。

Result: 仅用5%的计算资源构建了包含196K样本的大规模数据集，训练出的PRM在Best-of-N和ProcessBench上优于现有方法。

Conclusion: SCOPE通过压缩技术高效生成训练数据，为过程奖励模型提供了经济可行的解决方案。

Abstract: Process Reward Models (PRMs) have demonstrated promising results in
mathematical reasoning, but existing process annotation approaches, whether
through human annotations or Monte Carlo simulations, remain computationally
expensive. In this paper, we introduce Step COmpression for Process Estimation
(SCOPE), a novel compression-based approach that significantly reduces
annotation costs. We first translate natural language reasoning steps into code
and normalize them through Abstract Syntax Tree, then merge equivalent steps to
construct a prefix tree. Unlike simulation-based methods that waste numerous
samples on estimation, SCOPE leverages a compression-based prefix tree where
each root-to-leaf path serves as a training sample, reducing the complexity
from $O(NMK)$ to $O(N)$. We construct a large-scale dataset containing 196K
samples with only 5% of the computational resources required by previous
methods. Empirical results demonstrate that PRMs trained on our dataset
consistently outperform existing automated annotation approaches on both
Best-of-N strategy and ProcessBench.

</details>


### [196] [Towards Reliable Proof Generation with LLMs: A Neuro-Symbolic Approach](https://arxiv.org/abs/2505.14479)
*Oren Sultan,Eitan Stern,Dafna Shahaf*

Main category: cs.AI

TL;DR: 论文提出了一种结合神经与符号方法的技术，通过检索类似问题和形式验证器反馈来提升大语言模型在几何证明中的准确性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在需要严格逻辑推理和符号运算的领域（如数学证明生成）表现不佳，因此需要一种新方法来提升其可靠性和准确性。

Method: 采用神经符号方法，结合大语言模型的生成能力和结构化组件，通过检索类似问题及其证明来引导模型，并使用形式验证器评估和反馈生成的证明。

Result: 该方法显著提升了OpenAI o1模型在几何证明中的准确性（58%-70%的改进），类似问题和验证器反馈均对提升有贡献。

Conclusion: 通过生成可证明正确的结论，大语言模型的可靠性、准确性和一致性可以得到显著提升，从而解锁需要高可信度的复杂任务和关键实际应用。

Abstract: Large language models (LLMs) struggle with formal domains that require
rigorous logical deduction and symbolic reasoning, such as mathematical proof
generation. We propose a neuro-symbolic approach that combines LLMs' generative
strengths with structured components to overcome this challenge. As a
proof-of-concept, we focus on geometry problems. Our approach is two-fold: (1)
we retrieve analogous problems and use their proofs to guide the LLM, and (2) a
formal verifier evaluates the generated proofs and provides feedback, helping
the model fix incorrect proofs. We demonstrate that our method significantly
improves proof accuracy for OpenAI's o1 model (58%-70% improvement); both
analogous problems and the verifier's feedback contribute to these gains. More
broadly, shifting to LLMs that generate provably correct conclusions could
dramatically improve their reliability, accuracy and consistency, unlocking
complex tasks and critical real-world applications that require
trustworthiness.

</details>


### [197] [Reasoning Models Better Express Their Confidence](https://arxiv.org/abs/2505.14489)
*Dongkeun Yoon,Seungone Kim,Sohee Yang,Sunkyoung Kim,Soyeon Kim,Yongil Kim,Eunbi Choi,Yireun Kim,Minjoon Seo*

Main category: cs.AI

TL;DR: 研究发现，采用链式思维推理的大语言模型在问题解决和信心表达上表现更优，其动态调整信心的能力显著提升了校准效果。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在信心表达上常不准确，影响其可靠性。本文探讨通过链式思维推理（CoT）是否能改善这一问题。

Method: 在六个数据集上对六种推理模型进行基准测试，比较推理与非推理模型在信心校准上的表现，并分析推理模型中的慢思考行为对校准的影响。

Result: 推理模型在36个设置中的33个表现优于非推理模型，信心校准随CoT展开逐步提升，移除慢思考行为会导致校准显著下降。非推理模型通过慢思考指导也能获益。

Conclusion: 链式思维推理通过慢思考行为动态调整信心，显著提升模型校准效果，这一方法对非推理模型同样有效。

Abstract: Despite their strengths, large language models (LLMs) often fail to
communicate their confidence accurately, making it difficult to assess when
they might be wrong and limiting their reliability. In this work, we
demonstrate that reasoning models-LLMs that engage in extended chain-of-thought
(CoT) reasoning-exhibit superior performance not only in problem-solving but
also in accurately expressing their confidence. Specifically, we benchmark six
reasoning models across six datasets and find that they achieve strictly better
confidence calibration than their non-reasoning counterparts in 33 out of the
36 settings. Our detailed analysis reveals that these gains in calibration stem
from the slow thinking behaviors of reasoning models-such as exploring
alternative approaches and backtracking-which enable them to adjust their
confidence dynamically throughout their CoT, making it progressively more
accurate. In particular, we find that reasoning models become increasingly
better calibrated as their CoT unfolds, a trend not observed in non-reasoning
models. Moreover, removing slow thinking behaviors from the CoT leads to a
significant drop in calibration. Lastly, we show that these gains are not
exclusive to reasoning models-non-reasoning models also benefit when guided to
perform slow thinking via in-context learning.

</details>


### [198] [BACON: A fully explainable AI model with graded logic for decision making problems](https://arxiv.org/abs/2505.14510)
*Haishi Bai,Jozo Dujmovic,Jianwu Wang*

Main category: cs.AI

TL;DR: BACON是一个新型可解释AI框架，通过分级逻辑自动训练模型，在保持高准确性的同时提供透明决策逻辑。


<details>
  <summary>Details</summary>
Motivation: 随着AI在医疗、金融等高风险领域的应用增加，需要模型不仅准确且透明可解释，以支持人机协作。

Method: 提出BACON框架，利用分级逻辑训练可解释模型，生成符号化决策规则。

Result: 在布尔近似、花卉分类等多个场景中，BACON均能输出高性能且人类可验证的紧凑决策逻辑。

Conclusion: BACON为可解释AI提供了一种实用且原理清晰的方法，能生成精确可信的决策解释。

Abstract: As machine learning models and autonomous agents are increasingly deployed in
high-stakes, real-world domains such as healthcare, security, finance, and
robotics, the need for transparent and trustworthy explanations has become
critical. To ensure end-to-end transparency of AI decisions, we need models
that are not only accurate but also fully explainable and human-tunable. We
introduce BACON, a novel framework for automatically training explainable AI
models for decision making problems using graded logic. BACON achieves high
predictive accuracy while offering full structural transparency and precise,
logic-based symbolic explanations, enabling effective human-AI collaboration
and expert-guided refinement. We evaluate BACON with a diverse set of
scenarios: classic Boolean approximation, Iris flower classification, house
purchasing decisions and breast cancer diagnosis. In each case, BACON provides
high-performance models while producing compact, human-verifiable decision
logic. These results demonstrate BACON's potential as a practical and
principled approach for delivering crisp, trustworthy explainable AI.

</details>


### [199] [Guarded Query Routing for Large Language Models](https://arxiv.org/abs/2505.14524)
*Richard Šléher,William Brach,Tibor Sloboda,Kristián Košťál,Lukas Galke*

Main category: cs.AI

TL;DR: 论文研究了带防护的查询路由问题，提出了GQR-Bench基准，对比了多种路由方法的性能，发现WideMLP在准确性和速度上达到最佳平衡。


<details>
  <summary>Details</summary>
Motivation: 查询路由任务需要处理分布外查询，如无关领域的问题、其他语言的查询或不安全文本，因此需要一种带防护的路由机制。

Method: 引入GQR-Bench基准，覆盖三个目标领域和七个数据集，对比了LLM、传统机器学习模型和词袋分类器的路由效果。

Result: WideMLP在准确性（88%）和速度（<4ms）上表现最佳，fastText速度最快（<1ms），LLM准确性最高（91%）但速度较慢。

Conclusion: 研究挑战了LLM在查询路由中的自动依赖，为实际应用提供了具体建议，并计划发布GQR-Bench作为Python包。

Abstract: Query routing, the task to route user queries to different large language
model (LLM) endpoints, can be considered as a text classification problem.
However, out-of-distribution queries must be handled properly, as those could
be questions about unrelated domains, queries in other languages, or even
contain unsafe text. Here, we thus study a \emph{guarded} query routing
problem, for which we first introduce the Guarded Query Routing Benchmark
(GQR-Bench), which covers three exemplary target domains (law, finance, and
healthcare), and seven datasets to test robustness against out-of-distribution
queries. We then use GQR-Bench to contrast the effectiveness and efficiency of
LLM-based routing mechanisms (GPT-4o-mini, Llama-3.2-3B, and Llama-3.1-8B),
standard LLM-based guardrail approaches (LlamaGuard and NVIDIA NeMo
Guardrails), continuous bag-of-words classifiers (WideMLP, fastText), and
traditional machine learning models (SVM, XGBoost). Our results show that
WideMLP, enhanced with out-of-domain detection capabilities, yields the best
trade-off between accuracy (88\%) and speed (<4ms). The embedding-based
fastText excels at speed (<1ms) with acceptable accuracy (80\%), whereas LLMs
yield the highest accuracy (91\%) but are comparatively slow (62ms for local
Llama-3.1:8B and 669ms for remote GPT-4o-mini calls). Our findings challenge
the automatic reliance on LLMs for (guarded) query routing and provide concrete
recommendations for practical applications. GQR-Bench will be released as a
Python package -- \texttt{gqr}.

</details>


### [200] [A Logic of General Attention Using Edge-Conditioned Event Models (Extended Version)](https://arxiv.org/abs/2505.14539)
*Gaia Belardinelli,Thomas Bolander,Sebastian Watzl*

Main category: cs.AI

TL;DR: 本文提出了一种通用的注意力逻辑，克服了现有动态认知逻辑在建模复杂注意力场景时的局限性，并展示了其在AI代理推理人类注意力偏差中的应用。


<details>
  <summary>Details</summary>
Motivation: 现有的动态认知逻辑只能建模对原子公式的注意力，无法处理复杂的注意力场景（如逻辑结构命题、高阶信念等），且模型规模随代理数和声明文字数呈指数增长。

Method: 通过推广边缘条件事件模型（保持表达能力的同时指数级压缩规模），并将注意力扩展到任意公式（包括其他代理的信念或注意力），将注意力视为一种模态。

Result: 新逻辑框架能够建模任意公式的注意力，支持对注意力模态的公理化，并通过AI代理推理人类注意力偏差的案例验证了实用性。

Conclusion: 该工作为注意力建模提供了通用逻辑工具，揭示了注意力作为认知模态的数学性质，并展示了其在识别系统性注意力偏差方面的价值。

Abstract: In this work, we present the first general logic of attention. Attention is a
powerful cognitive ability that allows agents to focus on potentially complex
information, such as logically structured propositions, higher-order beliefs,
or what other agents pay attention to. This ability is a strength, as it helps
to ignore what is irrelevant, but it can also introduce biases when some types
of information or agents are systematically ignored. Existing dynamic epistemic
logics for attention cannot model such complex attention scenarios, as they
only model attention to atomic formulas. Additionally, such logics quickly
become cumbersome, as their size grows exponentially in the number of agents
and announced literals. Here, we introduce a logic that overcomes both
limitations. First, we generalize edge-conditioned event models, which we show
to be as expressive as standard event models yet exponentially more succinct
(generalizing both standard event models and generalized arrow updates).
Second, we extend attention to arbitrary formulas, allowing agents to also
attend to other agents' beliefs or attention. Our work treats attention as a
modality, like belief or awareness. We introduce attention principles that
impose closure properties on that modality and that can be used in its
axiomatization. Throughout, we illustrate our framework with examples of AI
agents reasoning about human attentional biases, demonstrating how such agents
can discover attentional biases.

</details>


### [201] [Multi-agent Reinforcement Learning vs. Fixed-Time Control for Traffic Signal Optimization: A Simulation Study](https://arxiv.org/abs/2505.14544)
*Saahil Mahato*

Main category: cs.AI

TL;DR: 该研究探讨了使用多智能体强化学习（MARL）优化多交叉路口交通信号协调，相比传统固定时间控制系统，显著减少了平均等待时间并提高了通行效率。


<details>
  <summary>Details</summary>
Motivation: 城市交通拥堵，尤其在交叉路口，严重影响出行时间、燃油消耗和排放。传统固定时间信号控制系统难以有效应对动态交通模式。

Method: 研究在模拟环境中应用MARL，开发了基于Pygame的仿真模型，模拟随机生成车流的互联交叉路口网络，并实现分散式MARL控制器，每个交通信号作为自主智能体，基于本地观察和邻近智能体信息做出决策。

Result: 与基线固定时间控制器相比，MARL方法在平均车辆等待时间和整体通行量方面表现出统计显著的改进。

Conclusion: 研究表明，基于MARL的动态控制策略在提升城市交通管理效率方面具有巨大潜力，但需进一步研究解决可扩展性和实际实施挑战。

Abstract: Urban traffic congestion, particularly at intersections, significantly
impacts travel time, fuel consumption, and emissions. Traditional fixed-time
signal control systems often lack the adaptability to manage dynamic traffic
patterns effectively. This study explores the application of multi-agent
reinforcement learning (MARL) to optimize traffic signal coordination across
multiple intersections within a simulated environment. Utilizing Pygame, a
simulation was developed to model a network of interconnected intersections
with randomly generated vehicle flows to reflect realistic traffic variability.
A decentralized MARL controller was implemented, in which each traffic signal
operates as an autonomous agent, making decisions based on local observations
and information from neighboring agents. Performance was evaluated against a
baseline fixed-time controller using metrics such as average vehicle wait time
and overall throughput. The MARL approach demonstrated statistically
significant improvements, including reduced average waiting times and improved
throughput. These findings suggest that MARL-based dynamic control strategies
hold substantial promise for improving urban traffic management efficiency.
More research is recommended to address scalability and real-world
implementation challenges.

</details>


### [202] [Agent Context Protocols Enhance Collective Inference](https://arxiv.org/abs/2505.14569)
*Devansh Bhardwaj,Arjun Beniwal,Shreyas Chaudhari,Ashwin Kalyan,Tanmay Rajpurohit,Karthik R. Narasimhan,Ameet Deshpande,Vishvak Murahari*

Main category: cs.AI

TL;DR: 论文提出了一种名为Agent Context Protocols (ACPs)的结构化协议，用于提升多智能体系统的协作与通信效率，显著提高了复杂任务的性能。


<details>
  <summary>Details</summary>
Motivation: 当前多智能体系统依赖自然语言进行协调，存在交互复杂性和互操作性不足的问题，限制了通用智能系统的发展。

Method: 引入ACPs协议，结合持久执行蓝图和标准化消息模式，实现多智能体的结构化通信、协调和错误处理。

Result: ACPs在长周期网络辅助任务中达到28.3%的准确率，并在多模态技术报告生成上超越商业AI系统。

Conclusion: ACPs具有高度模块化和可扩展性，能快速构建高性能通用智能体系统。

Abstract: AI agents have become increasingly adept at complex tasks such as coding,
reasoning, and multimodal understanding. However, building generalist systems
requires moving beyond individual agents to collective inference -- a paradigm
where multi-agent systems with diverse, task-specialized agents complement one
another through structured communication and collaboration. Today, coordination
is usually handled with imprecise, ad-hoc natural language, which limits
complex interaction and hinders interoperability with domain-specific agents.
We introduce Agent context protocols (ACPs): a domain- and agent-agnostic
family of structured protocols for agent-agent communication, coordination, and
error handling. ACPs combine (i) persistent execution blueprints -- explicit
dependency graphs that store intermediate agent outputs -- with (ii)
standardized message schemas, enabling robust and fault-tolerant multi-agent
collective inference. ACP-powered generalist systems reach state-of-the-art
performance: 28.3 % accuracy on AssistantBench for long-horizon web assistance
and best-in-class multimodal technical reports, outperforming commercial AI
systems in human evaluation. ACPs are highly modular and extensible, allowing
practitioners to build top-tier generalist agents quickly.

</details>


### [203] [Towards a Foundation Model for Communication Systems](https://arxiv.org/abs/2505.14603)
*Davide Buffelli,Sowmen Das,Yu-Wei Lin,Sattar Vakili,Chien-Yi Wang,Masoud Attarifar,Pritthijit Nath,Da-shan Shiu*

Main category: cs.AI

TL;DR: 该论文提出了一种基于Transformer的多模态基础模型，用于直接处理通信数据，解决了包括标记化、位置嵌入、多模态等关键挑战，并实证了模型在估计多种通信特征上的有效性。


<details>
  <summary>Details</summary>
Motivation: 当前AI在通信系统中的应用多为任务特定解决方案，而AI领域正朝着能够支持多种应用的大型通用模型发展。因此，作者希望开发一种适用于通信数据的基础模型。

Method: 提出了一种基于Transformer的多模态模型，设计了解决通信数据特有挑战的方法，包括标记化、位置嵌入、多模态处理、可变特征大小和归一化。

Result: 实证研究表明，该模型能够成功估计多种通信特征，包括传输秩、预编码器选择、多普勒扩展和延迟分布。

Conclusion: 该研究为通信数据的基础模型开发迈出了重要一步，展示了Transformer架构在通信领域的潜力，为未来更通用的AI通信解决方案奠定了基础。

Abstract: Artificial Intelligence (AI) has demonstrated unprecedented performance
across various domains, and its application to communication systems is an
active area of research. While current methods focus on task-specific
solutions, the broader trend in AI is shifting toward large general models
capable of supporting multiple applications. In this work, we take a step
toward a foundation model for communication data--a transformer-based,
multi-modal model designed to operate directly on communication data. We
propose methodologies to address key challenges, including tokenization,
positional embedding, multimodality, variable feature sizes, and normalization.
Furthermore, we empirically demonstrate that such a model can successfully
estimate multiple features, including transmission rank, selected precoder,
Doppler spread, and delay profile.

</details>


### [204] [Let LLMs Break Free from Overthinking via Self-Braking Tuning](https://arxiv.org/abs/2505.14604)
*Haoran Zhao,Yuchen Yan,Yongliang Shen,Haolei Xu,Wenqi Zhang,Kaitao Song,Jian Shao,Weiming Lu,Jun Xiao,Yueting Zhuang*

Main category: cs.AI

TL;DR: 论文提出Self-Braking Tuning（SBT）框架，通过让模型自我调节推理过程，减少冗余计算，在保持精度的同时显著降低token消耗。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型（LRMs）虽通过生成长思维链提升性能，但伴随冗余推理导致计算开销剧增和过度思考问题。现有方法多依赖外部干预，需开发自主调控机制。

Method: 构建基于标准答案的过度思考识别指标，设计冗余推理检测方法；提出自适应推理长度数据构造策略及制动提示机制，使模型学会适时终止推理。

Result: 在数学基准测试（AIME等）中，SBT减少高达60%的token消耗，同时保持与无约束模型相当的准确率。

Conclusion: SBT框架通过模型自调节有效解决过度思考问题，为平衡推理效率与性能提供了新思路。

Abstract: Large reasoning models (LRMs), such as OpenAI o1 and DeepSeek-R1, have
significantly enhanced their reasoning capabilities by generating longer chains
of thought, demonstrating outstanding performance across a variety of tasks.
However, this performance gain comes at the cost of a substantial increase in
redundant reasoning during the generation process, leading to high
computational overhead and exacerbating the issue of overthinking. Although
numerous existing approaches aim to address the problem of overthinking, they
often rely on external interventions. In this paper, we propose a novel
framework, Self-Braking Tuning (SBT), which tackles overthinking from the
perspective of allowing the model to regulate its own reasoning process, thus
eliminating the reliance on external control mechanisms. We construct a set of
overthinking identification metrics based on standard answers and design a
systematic method to detect redundant reasoning. This method accurately
identifies unnecessary steps within the reasoning trajectory and generates
training signals for learning self-regulation behaviors. Building on this
foundation, we develop a complete strategy for constructing data with adaptive
reasoning lengths and introduce an innovative braking prompt mechanism that
enables the model to naturally learn when to terminate reasoning at an
appropriate point. Experiments across mathematical benchmarks (AIME, AMC,
MATH500, GSM8K) demonstrate that our method reduces token consumption by up to
60% while maintaining comparable accuracy to unconstrained models.

</details>


### [205] [SATBench: Benchmarking LLMs' Logical Reasoning via Automated Puzzle Generation from SAT Formulas](https://arxiv.org/abs/2505.14615)
*Anjiang Wei,Yuheng Wu,Yingjia Wan,Tarun Suresh,Huanmi Tan,Zhanke Zhou,Sanmi Koyejo,Ke Wang,Alex Aiken*

Main category: cs.AI

TL;DR: SATBench是一个通过布尔可满足性问题评估大语言模型逻辑推理能力的基准测试，揭示了当前模型在搜索式逻辑推理上的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注基于推理规则的逻辑推理，而本文旨在通过布尔可满足性问题（SAT）的搜索性质，评估大语言模型在解决复杂逻辑约束时的能力。

Method: SATBench通过自动化流程生成2100个基于SAT公式的逻辑谜题，并将其转化为故事背景和条件，同时支持通过调整子句数量控制难度，并通过LLM辅助和求解器进行一致性验证。

Result: 实验结果显示，即使是当前最强模型o4-mini在困难UNSAT问题上准确率仅为65.0%，接近随机基线50%。

Conclusion: SATBench暴露了当前大语言模型在搜索式逻辑推理上的根本性局限，为未来逻辑推理研究提供了可扩展的测试平台。

Abstract: We introduce SATBench, a benchmark for evaluating the logical reasoning
capabilities of large language models (LLMs) through logical puzzles derived
from Boolean satisfiability (SAT) problems. Unlike prior work that focuses on
inference rule-based reasoning, which often involves deducing conclusions from
a set of premises, our approach leverages the search-based nature of SAT
problems, where the objective is to find a solution that fulfills a specified
set of logical constraints. Each instance in SATBench is generated from a SAT
formula, then translated into a story context and conditions using LLMs. The
generation process is fully automated and allows for adjustable difficulty by
varying the number of clauses. All 2100 puzzles are validated through both
LLM-assisted and solver-based consistency checks, with human validation on a
subset. Experimental results show that even the strongest model, o4-mini,
achieves only 65.0% accuracy on hard UNSAT problems, close to the random
baseline of 50%. SATBench exposes fundamental limitations in the search-based
logical reasoning abilities of current LLMs and provides a scalable testbed for
future research in logical reasoning.

</details>


### [206] [Debating for Better Reasoning: An Unsupervised Multimodal Approach](https://arxiv.org/abs/2505.14627)
*Ashutosh Adhikari,Mirella Lapata*

Main category: cs.AI

TL;DR: 论文探讨了在多模态环境下通过辩论机制提升视觉问答任务性能的方法，弱模型通过辩论监督强模型，实验表明该方法优于单个专家模型。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在多领域和多模态中的能力提升，如何有效监督其性能成为挑战，尤其是在模型能力超越人类评估者时。辩论机制被视为一种潜在的解决方案。

Method: 研究将辩论范式扩展到多模态环境，特别是在视觉问答任务中，两个视觉语言专家模型进行辩论，一个仅依赖文本的盲模型根据辩论质量裁决。专家模型仅捍卫其真实信念的答案，避免角色扮演，专注于专家分歧的实例。

Result: 实验表明，辩论框架在多个多模态任务中表现优于单个专家模型。此外，通过微调，较弱的大语言模型的裁决有助于增强视觉语言模型的推理能力。

Conclusion: 辩论机制在多模态环境下有效，能够通过弱模型监督强模型，提升整体性能，并为模型推理能力的增强提供了新途径。

Abstract: As Large Language Models (LLMs) gain expertise across diverse domains and
modalities, scalable oversight becomes increasingly challenging, particularly
when their capabilities may surpass human evaluators. Debate has emerged as a
promising mechanism for enabling such oversight. In this work, we extend the
debate paradigm to a multimodal setting, exploring its potential for weaker
models to supervise and enhance the performance of stronger models. We focus on
visual question answering (VQA), where two "sighted" expert vision-language
models debate an answer, while a "blind" (text-only) judge adjudicates based
solely on the quality of the arguments. In our framework, the experts defend
only answers aligned with their beliefs, thereby obviating the need for
explicit role-playing and concentrating the debate on instances of expert
disagreement. Experiments on several multimodal tasks demonstrate that the
debate framework consistently outperforms individual expert models. Moreover,
judgments from weaker LLMs can help instill reasoning capabilities in
vision-language models through finetuning.

</details>


### [207] [Cost-Augmented Monte Carlo Tree Search for LLM-Assisted Planning](https://arxiv.org/abs/2505.14656)
*Zihao Zhang,Fei Liu*

Main category: cs.AI

TL;DR: 论文提出CATS方法，通过成本增强的蒙特卡洛树搜索提升LLM在成本敏感规划中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型（LLM）在开放推理中表现优异，但在成本敏感规划中常忽略动作成本差异或无法满足严格预算限制。

Method: 引入成本增强蒙特卡洛树搜索（CATS），将显式成本意识融入LLM引导的规划过程，适应不同约束强度。

Result: 实验表明GPT-4.1等原生LLM在严格预算下表现不佳，而CATS能稳定实现更高任务成功率和成本效率。

Conclusion: CATS通过结合LLM推理能力与结构化搜索，为预算敏感决策提供了有效解决方案。

Abstract: While LLMs excel at open-ended reasoning, they often struggle with
cost-sensitive planning, either treating all actions as having equal cost or
failing to stay within strict budgets. In this paper, we introduce
Cost-Augmented Monte Carlo Tree Search (CATS), a novel approach that brings
explicit cost-awareness into LLM-guided planning. Tight cost constraints push
the planner to quickly identify infeasible solutions, while looser constraints
encourage optimization for minimal cost. We benchmark top LLMs such as GPT-4.1,
Claude-3.7-Sonnet, and DeepSeek-R1, against our CATS planner to evaluate their
performance in cost-sensitive scenarios. Our experiments suggest that raw LLMs
such as GPT-4.1 often falter under tight budgets, whereas CATS consistently
delivers strong performance, achieving higher task success rates and better
cost efficiency. CATS provides an effective solution for budget-aware
decision-making by combining the reasoning power of LLMs with structured
search.

</details>


### [208] [SAFEPATH: Preventing Harmful Reasoning in Chain-of-Thought via Early Alignment](https://arxiv.org/abs/2505.14667)
*Wonje Jeung,Sangyeon Yoon,Minsuk Kahng,Albert No*

Main category: cs.AI

TL;DR: SAFEPATH是一种轻量级对齐方法，通过在推理开始时生成简短的安全提示来减少有害输出，同时保持推理性能。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型（LRMs）在处理有害提示时可能产生不安全输出，现有安全对齐方法在减少有害输出的同时会降低推理深度，且在复杂任务中表现不佳。

Method: SAFEPATH通过微调LRMs，在推理开始时生成8个token的安全提示，其余推理过程不受监督，还提出了无需微调的零样本变体。

Result: SAFEPATH在DeepSeek-R1-Distill-Llama-8B模型中减少了90%的有害输出，阻止了83.3%的越狱尝试，计算量比现有方法低295.9倍和314.1倍。

Conclusion: SAFEPATH有效平衡了安全性和推理性能，揭示了现有方法在推理中心模型中的局限性，为更安全的AI提供了新方向。

Abstract: Large Reasoning Models (LRMs) have become powerful tools for complex problem
solving, but their structured reasoning pathways can lead to unsafe outputs
when exposed to harmful prompts. Existing safety alignment methods reduce
harmful outputs but can degrade reasoning depth, leading to significant
trade-offs in complex, multi-step tasks, and remain vulnerable to sophisticated
jailbreak attacks. To address this, we introduce SAFEPATH, a lightweight
alignment method that fine-tunes LRMs to emit a short, 8-token Safety Primer at
the start of their reasoning, in response to harmful prompts, while leaving the
rest of the reasoning process unsupervised. Empirical results across multiple
benchmarks indicate that SAFEPATH effectively reduces harmful outputs while
maintaining reasoning performance. Specifically, SAFEPATH reduces harmful
responses by up to 90.0% and blocks 83.3% of jailbreak attempts in the
DeepSeek-R1-Distill-Llama-8B model, while requiring 295.9x less compute than
Direct Refusal and 314.1x less than SafeChain. We further introduce a zero-shot
variant that requires no fine-tuning. In addition, we provide a comprehensive
analysis of how existing methods in LLMs generalize, or fail, when applied to
reasoning-centric models, revealing critical gaps and new directions for safer
AI.

</details>


### [209] [ContextAgent: Context-Aware Proactive LLM Agents with Open-World Sensory Perceptions](https://arxiv.org/abs/2505.14668)
*Bufang Yang,Lilin Xu,Liekang Zeng,Kaiwei Liu,Siyang Jiang,Wenrui Lu,Hongkai Chen,Xiaofan Jiang,Guoliang Xing,Zhenyu Yan*

Main category: cs.AI

TL;DR: 论文提出ContextAgent，首个结合多维度感知上下文的大模型主动代理，通过穿戴设备数据提升意图理解和服务预测能力，并在新基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有主动代理仅依赖封闭环境观察或基于规则的通知，导致用户意图理解不足且功能有限。需开发能整合多感官上下文、实现精准主动服务的新方法。

Method: 1) 从穿戴设备视频/音频等感知数据提取多维上下文；2) 结合历史人物上下文预测主动服务需求；3) 自动调用工具提供无干扰协助。

Result: 在涵盖9类场景/20种工具的ContextAgentBench测试中，主动预测和工具调用准确率分别比基线高8.5%和6.0%。

Conclusion: ContextAgent证明了多模态上下文对提升AI主动服务的重要性，为开发更人性化的主动助手提供了新方向。

Abstract: Recent advances in Large Language Models (LLMs) have propelled intelligent
agents from reactive responses to proactive support. While promising, existing
proactive agents either rely exclusively on observations from enclosed
environments (e.g., desktop UIs) with direct LLM inference or employ rule-based
proactive notifications, leading to suboptimal user intent understanding and
limited functionality for proactive service. In this paper, we introduce
ContextAgent, the first context-aware proactive agent that incorporates
extensive sensory contexts to enhance the proactive capabilities of LLM agents.
ContextAgent first extracts multi-dimensional contexts from massive sensory
perceptions on wearables (e.g., video and audio) to understand user intentions.
ContextAgent then leverages the sensory contexts and the persona contexts from
historical data to predict the necessity for proactive services. When proactive
assistance is needed, ContextAgent further automatically calls the necessary
tools to assist users unobtrusively. To evaluate this new task, we curate
ContextAgentBench, the first benchmark for evaluating context-aware proactive
LLM agents, covering 1,000 samples across nine daily scenarios and twenty
tools. Experiments on ContextAgentBench show that ContextAgent outperforms
baselines by achieving up to 8.5% and 6.0% higher accuracy in proactive
predictions and tool calling, respectively. We hope our research can inspire
the development of more advanced, human-centric, proactive AI assistants.

</details>


### [210] [Two Experts Are All You Need for Steering Thinking: Reinforcing Cognitive Effort in MoE Reasoning Models Without Additional Training](https://arxiv.org/abs/2505.14681)
*Mengru Wang,Xingyu Chen,Yue Wang,Zhiwei He,Jiahao Xu,Tian Liang,Qiuzhi Liu,Yunzhi Yao,Wenxuan Wang,Ruotian Ma,Haitao Mi,Ningyu Zhang,Zhaopeng Tu,Xiaolong Li,Dong Yu*

Main category: cs.AI

TL;DR: 该论文提出了一种名为RICE的新型推理时间引导方法，通过强化认知专家来提升大型推理模型的认知效率和准确性，无需额外训练。


<details>
  <summary>Details</summary>
Motivation: 现有的大型推理模型（如MoE架构）在推理过程中常出现认知效率低下的问题，如过度思考或思考不足，影响了模型的性能和泛化能力。

Method: 利用归一化点互信息（nPMI）识别并强化特定认知专家，这些专家负责协调元级推理操作，如标记“<think>”等。

Result: 在DeepSeek-R1和Qwen3-235B等模型上的实验表明，RICE显著提升了推理准确性、认知效率和跨领域泛化能力，且优于现有的推理引导技术。

Conclusion: RICE作为一种轻量级方法，不仅提升了推理模型的认知效率，还保持了模型的通用指令跟随能力，为增强高级推理模型的实用性提供了新方向。

Abstract: Mixture-of-Experts (MoE) architectures within Large Reasoning Models (LRMs)
have achieved impressive reasoning capabilities by selectively activating
experts to facilitate structured cognitive processes. Despite notable advances,
existing reasoning models often suffer from cognitive inefficiencies like
overthinking and underthinking. To address these limitations, we introduce a
novel inference-time steering methodology called Reinforcing Cognitive Experts
(RICE), designed to improve reasoning performance without additional training
or complex heuristics. Leveraging normalized Pointwise Mutual Information
(nPMI), we systematically identify specialized experts, termed ''cognitive
experts'' that orchestrate meta-level reasoning operations characterized by
tokens like ''<think>''. Empirical evaluations with leading MoE-based LRMs
(DeepSeek-R1 and Qwen3-235B) on rigorous quantitative and scientific reasoning
benchmarks demonstrate noticeable and consistent improvements in reasoning
accuracy, cognitive efficiency, and cross-domain generalization. Crucially, our
lightweight approach substantially outperforms prevalent reasoning-steering
techniques, such as prompt design and decoding constraints, while preserving
the model's general instruction-following skills. These results highlight
reinforcing cognitive experts as a promising, practical, and interpretable
direction to enhance cognitive efficiency within advanced reasoning models.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [211] [Tuning Learning Rates with the Cumulative-Learning Constant](https://arxiv.org/abs/2505.13457)
*Nathan Faraj*

Main category: cs.LG

TL;DR: 本文提出了一种优化机器学习学习率的新方法，发现学习率与数据集大小之间此前未被认识到的比例关系，并提出了累积学习常数的概念。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于探索数据集规模如何影响训练动态，以提高机器学习模型的训练效率和性能。

Method: 通过分析学习率与数据集大小的比例关系，提出了一种新的学习率优化方法，并引入了累积学习常数的概念。

Result: 研究发现学习率与数据集大小存在比例关系，并提出了一个框架来设计和优化高级学习率调度策略。

Conclusion: 这些发现有望在广泛的机器学习应用中提升训练效率和性能。

Abstract: This paper introduces a novel method for optimizing learning rates in machine
learning. A previously unrecognized proportionality between learning rates and
dataset sizes is discovered, providing valuable insights into how dataset scale
influences training dynamics. Additionally, a cumulative learning constant is
identified, offering a framework for designing and optimizing advanced learning
rate schedules. These findings have the potential to enhance training
efficiency and performance across a wide range of machine learning
applications.

</details>


### [212] [FPGA-based Acceleration for Convolutional Neural Networks: A Comprehensive Review](https://arxiv.org/abs/2505.13461)
*Junye Jiang,Yaan Zhou,Yuanhao Gong,Haoxuan Yuan,Shuanglong Liu*

Main category: cs.LG

TL;DR: 该论文综述了基于FPGA的CNN硬件加速器，探讨了性能评估框架、优化策略及不同架构的比较，并指出未来挑战与机遇。


<details>
  <summary>Details</summary>
Motivation: 随着卷积神经网络（CNN）复杂度的增加，计算需求显著上升，需要高效的硬件加速器。FPGA因其可重构性、并行性和能效成为理想选择。

Method: 论文通过总结现有研究，提出了性能评估框架，并分析了并行计算、数据流优化和软硬件协同设计等关键优化策略。

Result: 研究比较了不同FPGA架构在延迟、吞吐量、计算效率、功耗和资源利用率等方面的表现。

Conclusion: 论文强调了FPGA在CNN加速中的潜力，并指出未来创新方向，包括进一步优化和解决现有挑战。

Abstract: Convolutional Neural Networks (CNNs) are fundamental to deep learning,
driving applications across various domains. However, their growing complexity
has significantly increased computational demands, necessitating efficient
hardware accelerators. Field-Programmable Gate Arrays (FPGAs) have emerged as a
leading solution, offering reconfigurability, parallelism, and energy
efficiency. This paper provides a comprehensive review of FPGA-based hardware
accelerators specifically designed for CNNs. It presents and summarizes the
performance evaluation framework grounded in existing studies and explores key
optimization strategies, such as parallel computing, dataflow optimization, and
hardware-software co-design. It also compares various FPGA architectures in
terms of latency, throughput, compute efficiency, power consumption, and
resource utilization. Finally, the paper highlights future challenges and
opportunities, emphasizing the potential for continued innovation in this
field.

</details>


### [213] [End-to-end fully-binarized network design: from Generic Learned Thermometer to Block Pruning](https://arxiv.org/abs/2505.13462)
*Thien Nguyen,William Guicquero*

Main category: cs.LG

TL;DR: 本文提出了一种名为通用学习温度计（GLT）的编码技术，用于改进二进制神经网络（BNN）的输入数据表示，通过学习非线性量化阈值实现。结合轻量级分组卷积和块剪枝技术，显著提升了模型精度并降低了计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 现有二进制神经网络研究主要关注模型权重和激活函数，而忽略了输入原始数据的优化。本文旨在通过改进输入数据表示，提升BNN的精度和效率。

Method: 提出GLT编码技术，通过学习非线性量化阈值实现多数据二值化，替代传统的模数转换（ADC）。同时结合轻量级分组卷积、块剪枝和知识蒸馏技术，进一步减小模型规模和计算复杂度。

Result: 实验表明，GLT技术显著提升了模型精度（在STL-10和VWW数据集上验证），结合块剪枝技术后，实现了轻量级（小于1Mb）全二值化模型，适用于传感器端的持续推理场景。

Conclusion: GLT技术为BNN提供了更优的输入数据表示方法，结合轻量化和剪枝技术，实现了高精度、低复杂度的模型，适用于实际应用场景。

Abstract: Existing works on Binary Neural Network (BNN) mainly focus on model's weights
and activations while discarding considerations on the input raw data. This
article introduces Generic Learned Thermometer (GLT), an encoding technique to
improve input data representation for BNN, relying on learning non linear
quantization thresholds. This technique consists in multiple data binarizations
which can advantageously replace a conventional Analog to Digital Conversion
(ADC) that uses natural binary coding. Additionally, we jointly propose a
compact topology with light-weight grouped convolutions being trained thanks to
block pruning and Knowledge Distillation (KD), aiming at reducing furthermore
the model size so as its computational complexity. We show that GLT brings
versatility to the BNN by intrinsically performing global tone mapping,
enabling significant accuracy gains in practice (demonstrated by simulations on
the STL-10 and VWW datasets). Moreover, when combining GLT with our proposed
block-pruning technique, we successfully achieve lightweight (under 1Mb),
fully-binarized models with limited accuracy degradation while being suitable
for in-sensor always-on inference use cases.

</details>


### [214] [Predicting The Evolution of Interfaces with Fourier Neural Operators](https://arxiv.org/abs/2505.13463)
*Paolo Guida,William L. Roberts*

Main category: cs.LG

TL;DR: 该论文展示了神经算子能够快速预测多相流中的液-汽界面演化，适用于需要快速响应的工业控制过程。


<details>
  <summary>Details</summary>
Motivation: 传统CFD模型在复杂多相流问题（如涉及大密度梯度或相变的液-汽流动）中计算速度不足，难以满足工业快速控制的需求。

Method: 利用实验数据、模拟或两者结合训练神经算子，并在体积流体模拟中进行验证。

Result: 神经算子的预测时间尺度与多相应用相当，尤其在液-汽界面演化预测中表现出高精度。

Conclusion: 神经算子可作为快速响应工业控制的有效工具，特别是在多相流过程控制中。

Abstract: Recent progress in AI has established neural operators as powerful tools that
can predict the evolution of partial differential equations, such as the
Navier-Stokes equations. Some complex problems rely on sophisticated algorithms
to deal with strong discontinuities in the computational domain. For example,
liquid-vapour multiphase flows are a challenging problem in many
configurations, particularly those involving large density gradients or phase
change. The complexity mentioned above has not allowed for fine control of fast
industrial processes or applications because computational fluid dynamics (CFD)
models do not have a quick enough forecasting ability. This work demonstrates
that the time scale of neural operators-based predictions is comparable to the
time scale of multi-phase applications, thus proving they can be used to
control processes that require fast response. Neural Operators can be trained
using experimental data, simulations or a combination. In the following, neural
operators were trained in volume of fluid simulations, and the resulting
predictions showed very high accuracy, particularly in predicting the evolution
of the liquid-vapour interface, one of the most critical tasks in a multi-phase
process controller.

</details>


### [215] [The Spotlight Resonance Method: Resolving the Alignment of Embedded Activations](https://arxiv.org/abs/2505.13471)
*George Bird*

Main category: cs.LG

TL;DR: 该论文提出了一种可视化工具，用于分析深度学习模型中数据表示的轴向对齐特性，揭示了激活函数如何导致表示与神经元基对齐。


<details>
  <summary>Details</summary>
Motivation: 当前理解深度学习模型如何表示数据的方法有限，因此需要开发新的工具来解析模型内部的数据分布和对齐特性。

Method: 论文提出了一种可视化工具，通过评估数据在特权基向量定义的平面上的分布，提供原子级和整体级的直观度量。

Result: 研究发现，嵌入表示倾向于与特权基对齐，且激活函数直接导致了这种对齐现象。

Conclusion: 该方法揭示了表示与神经元基对齐的因果机制，为理解深度学习模型的内部表示提供了新视角。

Abstract: Understanding how deep learning models represent data is currently difficult
due to the limited number of methodologies available. This paper demonstrates a
versatile and novel visualisation tool for determining the axis alignment of
embedded data at any layer in any deep learning model. In particular, it
evaluates the distribution around planes defined by the network's privileged
basis vectors. This method provides both an atomistic and a holistic, intuitive
metric for interpreting the distribution of activations across all planes. It
ensures that both positive and negative signals contribute, treating the
activation vector as a whole. Depending on the application, several variations
of this technique are presented, with a resolution scale hyperparameter to
probe different angular scales. Using this method, multiple examples are
provided that demonstrate embedded representations tend to be axis-aligned with
the privileged basis. This is not necessarily the standard basis, and it is
found that activation functions directly result in privileged bases. Hence, it
provides a direct causal link between functional form symmetry breaking and
representational alignment, explaining why representations have a tendency to
align with the neuron basis. Therefore, using this method, we begin to answer
the fundamental question of what causes the observed tendency of
representations to align with neurons. Finally, examples of so-called
grandmother neurons are found in a variety of networks.

</details>


### [216] [Optimal Control for Transformer Architectures: Enhancing Generalization, Robustness and Efficiency](https://arxiv.org/abs/2505.13499)
*Kelvin Kan,Xingjian Li,Benjamin J. Zhang,Tuhin Sahai,Stanley Osher,Markos A. Katsoulakis*

Main category: cs.LG

TL;DR: 该论文通过最优控制理论优化Transformer的训练和架构设计，提升性能并减少参数，实验证明在多个任务中有效。


<details>
  <summary>Details</summary>
Motivation: 传统Transformer改进依赖试错法，成本高且缺乏理论支持。本文旨在通过最优控制理论提供系统化、理论驱动的改进方法。

Method: 采用连续时间最优控制理论框架，对Transformer的训练和架构进行优化，实现即插即用的轻量级改进。

Result: 实验显示，在文本生成、情感分析等任务中，模型测试性能提升且参数更高效。如nanoGPT测试损失降低46%，参数减少42%。

Conclusion: 首次将最优控制理论应用于Transformer训练与架构，为理论驱动的模型优化提供了新基础，超越了传统试错方法。

Abstract: We study Transformers through the perspective of optimal control theory,
using tools from continuous-time formulations to derive actionable insights
into training and architecture design. This framework improves the performance
of existing Transformer models while providing desirable theoretical
guarantees, including generalization and robustness. Our framework is designed
to be plug-and-play, enabling seamless integration with established Transformer
models and requiring only slight changes to the implementation. We conduct
seven extensive experiments on tasks motivated by text generation, sentiment
analysis, image classification, and point cloud classification. Experimental
results show that the framework improves the test performance of the baselines,
while being more parameter-efficient. On character-level text generation with
nanoGPT, our framework achieves a 46% reduction in final test loss while using
42% fewer parameters. On GPT-2, our framework achieves a 5.6% reduction in
final test loss, demonstrating scalability to larger models. To the best of our
knowledge, this is the first work that applies optimal control theory to both
the training and architecture of Transformers. It offers a new foundation for
systematic, theory-driven improvements and moves beyond costly trial-and-error
approaches.

</details>


### [217] [SPIEDiff: robust learning of long-time macroscopic dynamics from short-time particle simulations with quantified epistemic uncertainty](https://arxiv.org/abs/2505.13501)
*Zequn He,Celia Reina*

Main category: cs.LG

TL;DR: SPIEDiff框架通过统计物理和机器学习，解决了耗散系统宏观动力学与热力学数据驱动建模中的时间尺度限制、热力学势非唯一性等难题，实现了高效准确的长时预测。


<details>
  <summary>Details</summary>
Motivation: 传统粒子模拟存在时间尺度限制、热力学势非唯一性及不确定性量化效率低等问题，阻碍了耗散系统宏观动力学与热力学的数据驱动发现。

Method: 提出SPIEDiff框架，结合统计物理、条件扩散模型和认知网络，利用短时粒子模拟数据推断热力学与动力学行为。

Result: 在随机Arrhenius粒子过程中，SPIEDiff能精确还原热力学和动力学特性，仅需分钟级计算即可完成传统模拟需数天/年的长时预测。

Conclusion: SPIEDiff为热力学模型的数据驱动发现提供了高效、可靠且可量化不确定性的新途径。

Abstract: The data-driven discovery of long-time macroscopic dynamics and
thermodynamics of dissipative systems with particle fidelity is hampered by
significant obstacles. These include the strong time-scale limitations inherent
to particle simulations, the non-uniqueness of the thermodynamic potentials and
operators from given macroscopic dynamics, and the need for efficient
uncertainty quantification. This paper introduces Statistical-Physics Informed
Epistemic Diffusion Models (SPIEDiff), a machine learning framework designed to
overcome these limitations in the context of purely dissipative systems by
leveraging statistical physics, conditional diffusion models, and epinets. We
evaluate the proposed framework on stochastic Arrhenius particle processes and
demonstrate that SPIEDiff can accurately uncover both thermodynamics and
kinetics, while enabling reliable long-time macroscopic predictions using only
short-time particle simulation data. SPIEDiff can deliver accurate predictions
with quantified uncertainty in minutes, drastically reducing the computational
demand compared to direct particle simulations, which would take days or years
in the examples considered. Overall, SPIEDiff offers a robust and trustworthy
pathway for the data-driven discovery of thermodynamic models.

</details>


### [218] [Federated Low-Rank Adaptation for Foundation Models: A Survey](https://arxiv.org/abs/2505.13502)
*Yiyuan Yang,Guodong Long,Qinghua Lu,Liming Zhu,Jing Jiang,Chengqi Zhang*

Main category: cs.LG

TL;DR: 该论文综述了如何将低秩适应（LoRA）集成到联邦学习（FL）中，以高效微调基础模型，同时解决分布式学习、异构性和效率等关键挑战。


<details>
  <summary>Details</summary>
Motivation: 有效利用私有数据集开发基础模型仍具挑战性。联邦学习（FL）通过协作框架降低数据隐私风险，而低秩适应（LoRA）则提供资源高效的微调方法。结合两者（FedLoRA）成为研究重点。

Method: 通过分类现有工作，分析如何用LoRA解决联邦微调中的分布式学习、异构性和效率问题。

Result: 总结了FedLoRA领域的研究进展，并指出不同方法在应对三大挑战时的优缺点。

Conclusion: 提出了FedLoRA未来的开放研究问题和方向，为推进该领域发展提供了路线图。

Abstract: Effectively leveraging private datasets remains a significant challenge in
developing foundation models. Federated Learning (FL) has recently emerged as a
collaborative framework that enables multiple users to fine-tune these models
while mitigating data privacy risks. Meanwhile, Low-Rank Adaptation (LoRA)
offers a resource-efficient alternative for fine-tuning foundation models by
dramatically reducing the number of trainable parameters. This survey examines
how LoRA has been integrated into federated fine-tuning for foundation models,
an area we term FedLoRA, by focusing on three key challenges: distributed
learning, heterogeneity, and efficiency. We further categorize existing work
based on the specific methods used to address each challenge. Finally, we
discuss open research questions and highlight promising directions for future
investigation, outlining the next steps for advancing FedLoRA.

</details>


### [219] [Open Set Domain Adaptation with Vision-language models via Gradient-aware Separation](https://arxiv.org/abs/2505.13507)
*Haoyang Chen*

Main category: cs.LG

TL;DR: 该论文提出利用CLIP模型解决开放集域适应问题，通过可学习文本提示和梯度分析模块实现跨域对齐和未知样本检测。


<details>
  <summary>Details</summary>
Motivation: 当前开放集域适应方法难以利用模态间的语义关系，且在未知样本检测中存在误差累积问题。

Method: 1) 基于提示的跨域对齐：通过可学习文本提示动态调整CLIP文本编码器；2) 梯度感知开放集分离：通过梯度分析模块量化域偏移。

Result: 在Office-Home数据集上，该方法优于CLIP基线和标准基线，消融实验验证了梯度范数的关键作用。

Conclusion: 该方法有效解决了开放集域适应中的跨域对齐和未知样本识别问题。

Abstract: Open-Set Domain Adaptation (OSDA) confronts the dual challenge of aligning
known-class distributions across domains while identifying
target-domain-specific unknown categories. Current approaches often fail to
leverage semantic relationships between modalities and struggle with error
accumulation in unknown sample detection. We propose to harness Contrastive
Language-Image Pretraining (CLIP) to address these limitations through two key
innovations: 1) Prompt-driven cross-domain alignment: Learnable textual prompts
conditioned on domain discrepancy metrics dynamically adapt CLIP's text
encoder, enabling semantic consistency between source and target domains
without explicit unknown-class supervision. 2) Gradient-aware open-set
separation: A gradient analysis module quantifies domain shift by comparing the
L2-norm of gradients from the learned prompts, where known/unknown samples
exhibit statistically distinct gradient behaviors. Evaluations on Office-Home
show that our method consistently outperforms CLIP baseline and standard
baseline. Ablation studies confirm the gradient norm's critical role.

</details>


### [220] [On the definition and importance of interpretability in scientific machine learning](https://arxiv.org/abs/2505.13510)
*Conor Rowan,Alireza Doostan*

Main category: cs.LG

TL;DR: 论文探讨了科学机器学习中可解释性的定义，指出稀疏性不等于可解释性，并提出基于机制理解的科学可解释性定义。


<details>
  <summary>Details</summary>
Motivation: 传统科学模型通过简洁数学表达式揭示关系，而神经网络虽能预测物理现象，却难以整合到科学知识体系中。科学界对可解释性的需求日益增长，但现有定义和方法不足以满足科学机器学习的需求。

Method: 作者回顾了科学界外的可解释性机器学习文献，分析了其定义和方法在科学机器学习中的不足，并提出了一个基于机制理解的操作性定义。

Result: 论文提出，科学机器学习的可解释性应强调对机制的理解而非数学稀疏性，指出稀疏性常非必要，并质疑缺乏先验知识时实现可解释科学发现的可能性。

Conclusion: 精确且哲学上合理的可解释性定义有助于聚焦研究，克服实现数据驱动科学未来的主要障碍。

Abstract: Though neural networks trained on large data sets have been successfully used
to describe and predict many physical phenomena, there is a sense among
scientists that, unlike traditional scientific models, where relationships come
packaged in the form of simple mathematical expressions, the findings of the
neural network cannot be integrated into the body of scientific knowledge.
Critics of ML's inability to produce human-understandable relationships have
converged on the concept of "interpretability" as its point of departure from
more traditional forms of science. As the growing interest in interpretability
has shown, researchers in the physical sciences seek not just predictive
models, but also to uncover the fundamental principles that govern a system of
interest. However, clarity around a definition of interpretability and the
precise role that it plays in science is lacking in the literature. In this
work, we argue that researchers in equation discovery and symbolic regression
tend to conflate the concept of sparsity with interpretability. We review key
papers on interpretable ML from outside the scientific community and argue
that, though the definitions and methods they propose can inform questions of
interpretability for SciML, they are inadequate for this new purpose. Noting
these deficiencies, we propose an operational definition of interpretability
for the physical sciences. Our notion of interpretability emphasizes
understanding of the mechanism over mathematical sparsity. Innocuous though it
may seem, this emphasis on mechanism shows that sparsity is often unnecessary.
It also questions the possibility of interpretable scientific discovery when
prior knowledge is lacking. We believe a precise and philosophically informed
definition of interpretability in SciML will help focus research efforts toward
the most significant obstacles to realizing a data-driven scientific future.

</details>


### [221] [LoRASuite: Efficient LoRA Adaptation Across Large Language Model Upgrades](https://arxiv.org/abs/2505.13515)
*Yanan Li,Fanxu Meng,Muhan Zhang,Shiai Zhu,Shangguang Wang,Mengwei Xu*

Main category: cs.LG

TL;DR: LoRASuite提出模块化方法，利用已有LoRA权重适配新版大模型，显著提升效率并降低资源消耗。


<details>
  <summary>Details</summary>
Motivation: 大模型频繁更新导致旧版LoRA权重失效，重新训练成本高且不环保，需高效迁移方案。

Method: 通过转移矩阵映射参数，基于对齐指标分配层/注意力头，辅以小规模微调保证稳定性。

Result: 在MiniCPM和Qwen模型上，数学任务平均提升1.4/6.6分，内存减少5.5GB，计算时间降低78.23%。

Conclusion: LoRASuite优于传统LoRA方法，部分场景超越全量训练，实现高效环保的模型适配。

Abstract: As Large Language Models (LLMs) are frequently updated, LoRA weights trained
on earlier versions quickly become obsolete. The conventional practice of
retraining LoRA weights from scratch on the latest model is costly,
time-consuming, and environmentally detrimental, particularly as the diversity
of LLMs and downstream tasks expands. This motivates a critical question: "How
can we efficiently leverage existing LoRA weights to adapt to newer model
versions?" To address this, we propose LoRASuite, a modular approach tailored
specifically to various types of LLM updates. First, we compute a transfer
matrix utilizing known parameters from both old and new LLMs. Next, we allocate
corresponding layers and attention heads based on centered kernel alignment and
cosine similarity metrics, respectively. A subsequent small-scale, skillful
fine-tuning step ensures numerical stability. Experimental evaluations
demonstrate that LoRASuite consistently surpasses small-scale vanilla LoRA
methods. Notably, on backbone LLMs such as MiniCPM and Qwen, LoRASuite even
exceeds the performance of full-scale LoRA retraining, with average
improvements of +1.4 and +6.6 points on math tasks, respectively. Additionally,
LoRASuite significantly reduces memory consumption by 5.5 GB and computational
time by 78.23%.

</details>


### [222] [Zero-Shot Forecasting Mortality Rates: A Global Study](https://arxiv.org/abs/2505.13521)
*Gabor Petnehazi,Laith Al Shaggah,Jozsef Gall,Bernadett Aradi*

Main category: cs.LG

TL;DR: 该研究评估了零样本时间序列预测模型（TimesFM和CHRONOS）在死亡率预测中的表现，发现CHRONOS短期预测优于传统方法，而随机森林模型整体表现最佳。


<details>
  <summary>Details</summary>
Motivation: 探索无需任务特定微调的预训练基础模型在死亡率预测中的潜力，以简化预测流程并提高效率。

Method: 使用50个国家111个年龄组的数据，对比零样本模型（TimesFM、CHRONOS）、传统方法（ARIMA、Lee-Carter）和机器学习模型（随机森林）在5年、10年和20年预测期的表现。

Result: CHRONOS短期预测优于传统方法，TimesFM表现不佳；微调CHRONOS可提升长期预测精度；随机森林模型整体表现最佳。

Conclusion: 零样本预测具有潜力，但需谨慎选择模型并进行领域适配；随机森林仍是当前最优解决方案。

Abstract: This study explores the potential of zero-shot time series forecasting, an
innovative approach leveraging pre-trained foundation models, to forecast
mortality rates without task-specific fine-tuning. We evaluate two
state-of-the-art foundation models, TimesFM and CHRONOS, alongside traditional
and machine learning-based methods across three forecasting horizons (5, 10,
and 20 years) using data from 50 countries and 111 age groups. In our
investigations, zero-shot models showed varying results: while CHRONOS
delivered competitive shorter-term forecasts, outperforming traditional methods
like ARIMA and the Lee-Carter model, TimesFM consistently underperformed.
Fine-tuning CHRONOS on mortality data significantly improved long-term
accuracy. A Random Forest model, trained on mortality data, achieved the best
overall performance. These findings underscore the potential of zero-shot
forecasting while highlighting the need for careful model selection and
domain-specific adaptation.

</details>


### [223] [Multi-head Temporal Latent Attention](https://arxiv.org/abs/2505.13544)
*Keqi Deng,Philip C. Woodland*

Main category: cs.LG

TL;DR: MTLA通过动态合并相邻KV缓存向量和步长感知因果掩码，显著降低自注意力推理的内存占用，提升推理速度，同时保持性能。


<details>
  <summary>Details</summary>
Motivation: Transformer自注意力的KV缓存随序列长度线性增长，成为推理效率的瓶颈。

Method: 提出MTLA，利用超网络动态合并时间相邻的KV缓存向量，并引入步长感知因果掩码解决压缩KV缓存与序列长度不匹配问题。

Result: 在语音翻译等任务中，MTLA相比MHA实现5.3倍加速和8.3倍内存节省，同时保持翻译质量。

Conclusion: MTLA在保持性能的同时显著提升推理效率和内存利用率，适用于多种任务。

Abstract: While Transformer self-attention offers strong parallelism, the Key-Value
(KV) cache grows linearly with sequence length and becomes a bottleneck for
inference efficiency. Multi-head latent attention was recently developed to
compress the KV cache into a low-rank latent space. This paper proposes
Multi-head Temporal Latent Attention (MTLA), which further reduces the KV cache
size along the temporal dimension, greatly lowering the memory footprint of
self-attention inference. MTLA employs a hyper-network to dynamically merge
temporally adjacent KV cache vectors. To address the mismatch between the
compressed KV cache and processed sequence lengths, a stride-aware causal mask
is proposed to ensure efficient parallel training and consistency with
inference behaviour. Experiments across tasks, including speech translation,
speech recognition, speech understanding and text summarisation, demonstrate
that MTLA achieves competitive performance compared to standard Multi-Head
Attention (MHA), while greatly improving inference speed and GPU memory usage.
For example, on a English-German speech translation task, MTLA achieves a 5.3x
speedup and a reduction in GPU memory usage by a factor of 8.3 compared to MHA,
while maintaining translation quality.

</details>


### [224] [Exploring Federated Pruning for Large Language Models](https://arxiv.org/abs/2505.13547)
*Pengxin Guo,Yinong Wang,Wei Li,Mengting Liu,Ming Li,Jinkai Zheng,Liangqiong Qu*

Main category: cs.LG

TL;DR: FedPrLLM是一个联邦学习框架，用于在保护隐私的前提下压缩大语言模型（LLM），通过客户端本地计算剪枝掩码矩阵并与服务器共享，实现全局模型的协作剪枝。


<details>
  <summary>Details</summary>
Motivation: 当前LLM剪枝方法通常需要公开校准样本，这在隐私敏感领域难以获取。FedPrLLM旨在解决这一问题，提供一种隐私保护的LLM压缩方法。

Method: FedPrLLM框架中，每个客户端基于本地校准数据计算剪枝掩码矩阵，并与服务器共享以剪枝全局模型，从而在保护本地数据隐私的同时实现协作剪枝。

Result: 实验表明，在FedPrLLM框架中，采用一次性剪枝、分层比较且不缩放权重是最优选择。

Conclusion: FedPrLLM为隐私敏感领域的LLM剪枝提供了有效指导，未来可进一步推动该领域的研究。

Abstract: LLM pruning has emerged as a promising technology for compressing LLMs,
enabling their deployment on resource-limited devices. However, current
methodologies typically require access to public calibration samples, which can
be challenging to obtain in privacy-sensitive domains. To address this issue,
we introduce FedPrLLM, a comprehensive federated pruning framework designed for
the privacy-preserving compression of LLMs. In FedPrLLM, each client only needs
to calculate a pruning mask matrix based on its local calibration data and
share it with the server to prune the global model. This approach allows for
collaborative pruning of the global model with the knowledge of each client
while maintaining local data privacy. Additionally, we conduct extensive
experiments to explore various possibilities within the FedPrLLM framework,
including different comparison groups, pruning strategies, and the decision to
scale weights. Our extensive evaluation reveals that one-shot pruning with
layer comparison and no weight scaling is the optimal choice within the
FedPrLLM framework. We hope our work will help guide future efforts in pruning
LLMs in privacy-sensitive fields. Our code is available at
https://github.com/Pengxin-Guo/FedPrLLM.

</details>


### [225] [Breaking the Compression Ceiling: Data-Free Pipeline for Ultra-Efficient Delta Compression](https://arxiv.org/abs/2505.13563)
*Xiaohui Wang,Peng Ye,Chenyu Huang,Shenghe Zheng,Bo Zhang,Wanli Ouyang,Tao Chen*

Main category: cs.LG

TL;DR: UltraDelta是一种无需数据的delta压缩方法，通过多层优化实现超高压缩比并保持模型性能。


<details>
  <summary>Details</summary>
Motivation: 随着微调预训练模型的普及，存储大量微调模型带来了显著的存储开销。现有delta压缩方法无法同时保证高压缩比和性能，且依赖数据。

Method: UltraDelta采用三种关键技术：(1)基于方差的混合稀疏分配，(2)分布感知压缩，(3)迹范数引导的重新缩放，从层间、层内和全局维度优化压缩。

Result: 实验表明，UltraDelta在LLaMA-2(133x)、NLP模型(800x)、视觉模型(400x)和多模态模型(40x)上均优于现有方法，尤其在超高压缩比下表现突出。

Conclusion: UltraDelta首次实现了无需数据、超高压缩比且性能稳定的delta压缩，为多任务模型存储提供了高效解决方案。

Abstract: With the rise of the fine-tuned--pretrained paradigm, storing numerous
fine-tuned models for multi-tasking creates significant storage overhead. Delta
compression alleviates this by storing only the pretrained model and the highly
compressed delta weights (the differences between fine-tuned and pretrained
model weights). However, existing methods fail to maintain both high
compression and performance, and often rely on data. To address these
challenges, we propose UltraDelta, the first data-free delta compression
pipeline that achieves both ultra-high compression and strong performance.
UltraDelta is designed to minimize redundancy, maximize information, and
stabilize performance across inter-layer, intra-layer, and global dimensions,
using three key components: (1) Variance-Based Mixed Sparsity Allocation
assigns sparsity based on variance, giving lower sparsity to high-variance
layers to preserve inter-layer information. (2) Distribution-Aware Compression
applies uniform quantization and then groups parameters by value, followed by
group-wise pruning, to better preserve intra-layer distribution. (3)
Trace-Norm-Guided Rescaling uses the trace norm of delta weights to estimate a
global rescaling factor, improving model stability under higher compression.
Extensive experiments across (a) large language models (fine-tuned on LLaMA-2
7B and 13B) with up to 133x, (b) general NLP models (RoBERTa-base, T5-base)
with up to 800x, (c) vision models (ViT-B/32, ViT-L/14) with up to 400x, and
(d) multi-modal models (BEiT-3) with 40x compression ratio, demonstrate that
UltraDelta consistently outperforms existing methods, especially under
ultra-high compression.

</details>


### [226] [Online Decision-Focused Learning](https://arxiv.org/abs/2505.13564)
*Aymeric Capitaine,Maxime Haddouche,Eric Moulines,Michael I. Jordan,Etienne Boursier,Alain Durmus*

Main category: cs.LG

TL;DR: 该论文研究了动态环境中的决策导向学习（DFL），提出了一种在线算法，通过正则化和乐观原则处理目标函数不可微和非凸的挑战，并在背包实验中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 传统的决策导向学习（DFL）研究集中在静态环境下，即数据批次固定且目标函数不变。然而，现实中的决策问题往往处于动态环境中，目标函数和数据分布会随时间变化。因此，研究动态环境中的DFL具有重要实际意义。

Method: 论文提出了一种在线算法，通过（i）正则化目标函数使其可微，（ii）利用乐观原则和近最优预言机及适当扰动，解决了目标函数梯度为零或未定义以及非凸性的问题。

Result: 论文在单纯形和一般有界凸多面体决策空间上建立了预期动态遗憾的界限，并通过背包实验证明了所提算法优于传统的预测导向方法。

Conclusion: 该研究为动态环境中的决策导向学习提供了一种有效的在线算法，能够处理目标函数和数据分布的时变特性，并在实验中展示了其优越性能。

Abstract: Decision-focused learning (DFL) is an increasingly popular paradigm for
training predictive models whose outputs are used in decision-making tasks.
Instead of merely optimizing for predictive accuracy, DFL trains models to
directly minimize the loss associated with downstream decisions. This
end-to-end strategy holds promise for tackling complex combinatorial problems;
however, existing studies focus solely on scenarios where a fixed batch of data
is available and the objective function does not change over time. We instead
investigate DFL in dynamic environments where the objective function and data
distribution evolve over time. This setting is challenging because the
objective function has zero or undefined gradients -- which prevents the use of
standard first-order optimization methods -- and is generally non-convex. To
address these difficulties, we (i) regularize the objective to make it
differentiable and (ii) make use of the optimism principle, based on a
near-optimal oracle along with an appropriate perturbation. This leads to a
practical online algorithm for which we establish bounds on the expected
dynamic regret, both when the decision space is a simplex and when it is a
general bounded convex polytope. Finally, we demonstrate the effectiveness of
our algorithm by comparing its performance with a classic prediction-focused
approach on a simple knapsack experiment.

</details>


### [227] [Learning Dynamics of RNNs in Closed-Loop Environments](https://arxiv.org/abs/2505.13567)
*Yoav Ger,Omri Barak*

Main category: cs.LG

TL;DR: 该论文研究了闭环环境下线性RNN的学习动态，发现与开环训练相比，闭环训练会引发不同的学习轨迹，并揭示了短期策略改进与长期稳定性之间的权衡机制。


<details>
  <summary>Details</summary>
Motivation: 现有RNN训练多采用开环监督模式，而生物学习发生在闭环环境中。论文旨在建立闭环训练的理论框架，以更贴近生物学习的真实场景。

Method: 开发了闭环训练线性RNN的数学理论，通过解析学习动态阶段划分，并与开环训练进行对比分析。最后在运动控制任务中验证框架适用性。

Result: 发现闭环训练会产生独特的学习轨迹，其动态受短期策略改进和长期环境交互稳定性的双重目标驱动，与开环训练存在本质差异。

Conclusion: 研究强调了在生物可解释建模中考虑闭环动态的重要性，为理解神经计算提供了新视角。

Abstract: Recurrent neural networks (RNNs) trained on neuroscience-inspired tasks offer
powerful models of brain computation. However, typical training paradigms rely
on open-loop, supervised settings, whereas real-world learning unfolds in
closed-loop environments. Here, we develop a mathematical theory describing the
learning dynamics of linear RNNs trained in closed-loop contexts. We first
demonstrate that two otherwise identical RNNs, trained in either closed- or
open-loop modes, follow markedly different learning trajectories. To probe this
divergence, we analytically characterize the closed-loop case, revealing
distinct stages aligned with the evolution of the training loss. Specifically,
we show that the learning dynamics of closed-loop RNNs, in contrast to
open-loop ones, are governed by an interplay between two competing objectives:
short-term policy improvement and long-term stability of the agent-environment
interaction. Finally, we apply our framework to a realistic motor control task,
highlighting its broader applicability. Taken together, our results underscore
the importance of modeling closed-loop dynamics in a biologically plausible
setting.

</details>


### [228] [Surrogate Modeling of 3D Rayleigh-Benard Convection with Equivariant Autoencoders](https://arxiv.org/abs/2505.13569)
*Fynn Fromme,Christine Allen-Blanchette,Hans Harder,Sebastian Peitz*

Main category: cs.LG

TL;DR: 该论文提出了一种端到端的等变替代模型，用于建模和理解大规模物理系统，特别是在三维Rayleigh-Bénard对流问题中表现出色。


<details>
  <summary>Details</summary>
Motivation: 大规模物理系统（如电磁学、核聚变反应堆、磁流体力学和流体力学等）通常由偏微分方程控制，具有高自由度和复杂的多尺度动力学特性。传统方法在准确性和样本效率方面存在挑战，因此需要更高效的方法。

Method: 论文提出了一种端到端的等变替代模型，结合了等变卷积自编码器和等变卷积LSTM，使用$G$-可操纵核。特别针对三维Rayleigh-Bénard对流问题，利用垂直堆叠的$D_4$-可操纵核层，并在垂直方向上进行部分核共享以提高效率。

Result: 该方法在样本效率和参数效率方面均有显著提升，并且能够更好地扩展到更复杂的动力学（即更大的Rayleigh数）。

Conclusion: 通过等变替代模型，论文成功解决了大规模物理系统中的建模和预测问题，展示了其在复杂动力学中的优越性能。

Abstract: The use of machine learning for modeling, understanding, and controlling
large-scale physics systems is quickly gaining in popularity, with examples
ranging from electromagnetism over nuclear fusion reactors and
magneto-hydrodynamics to fluid mechanics and climate modeling. These systems --
governed by partial differential equations -- present unique challenges
regarding the large number of degrees of freedom and the complex dynamics over
many scales both in space and time, and additional measures to improve accuracy
and sample efficiency are highly desirable. We present an end-to-end
equivariant surrogate model consisting of an equivariant convolutional
autoencoder and an equivariant convolutional LSTM using $G$-steerable kernels.
As a case study, we consider the three-dimensional Rayleigh-B\'enard
convection, which describes the buoyancy-driven fluid flow between a heated
bottom and a cooled top plate. While the system is E(2)-equivariant in the
horizontal plane, the boundary conditions break the translational equivariance
in the vertical direction. Our architecture leverages vertically stacked layers
of $D_4$-steerable kernels, with additional partial kernel sharing in the
vertical direction for further efficiency improvement. Our results demonstrate
significant gains both in sample and parameter efficiency, as well as a better
scaling to more complex dynamics, that is, larger Rayleigh numbers. The
accompanying code is available under
https://github.com/FynnFromme/equivariant-rb-forecasting.

</details>


### [229] [An Overview of Arithmetic Adaptations for Inference of Convolutional Neural Networks on Re-configurable Hardware](https://arxiv.org/abs/2505.13575)
*Ilkay Wunderlich,Benjamin Koch,Sven Schönfeld*

Main category: cs.LG

TL;DR: 该论文探讨了在FPGA上部署TinyYOLOv3检测器网络的最佳实践方法，包括批量归一化融合、滤波器剪枝和训练后网络量化等技术。


<details>
  <summary>Details</summary>
Motivation: 由于卷积神经网络（CNNs）在嵌入式平台（如FPGA）上部署时存在计算强度高、内存需求大和算术条件复杂等缺点，因此需要开发有效的策略来优化其性能。

Method: 论文采用了批量归一化融合、滤波器剪枝和训练后网络量化等技术，以优化TinyYOLOv3检测器网络在XILINX Artix-7 FPGA上的运行效率。

Result: 通过这些方法，论文展示了在FPGA上高效运行TinyYOLOv3检测器网络的可行性，并显著降低了计算和内存需求。

Conclusion: 论文提出的方法为在资源受限的嵌入式平台上部署高性能CNN提供了一种有效的解决方案。

Abstract: Convolutional Neural Networks (CNNs) have gained high popularity as a tool
for computer vision tasks and for that reason are used in various applications.
There are many different concepts, like single shot detectors, that have been
published for detecting objects in images or video streams. However, CNNs
suffer from disadvantages regarding the deployment on embedded platforms such
as re-configurable hardware like Field Programmable Gate Arrays (FPGAs). Due to
the high computational intensity, memory requirements and arithmetic
conditions, a variety of strategies for running CNNs on FPGAs have been
developed. The following methods showcase our best practice approaches for a
TinyYOLOv3 detector network on a XILINX Artix-7 FPGA using techniques like
fusion of batch normalization, filter pruning and post training network
quantization.

</details>


### [230] [FlexFed: Mitigating Catastrophic Forgetting in Heterogeneous Federated Learning in Pervasive Computing Environments](https://arxiv.org/abs/2505.13576)
*Sara Alosaime,Arshad Jhumka*

Main category: cs.LG

TL;DR: 本文提出FlexFed方法，解决联邦学习在人类活动识别（HAR）环境中的灾难性遗忘问题，通过动态调整训练频率和优化数据保留策略，提升效率10-15%。


<details>
  <summary>Details</summary>
Motivation: 在HAR等普适计算环境中，联邦学习面临数据分布非平稳、客户端间歇参与等问题，导致灾难性遗忘（CF）。现有方法因隐私限制无法直接采用持续学习的内存回放机制，需针对性解决方案。

Method: 提出FlexFed框架：1）优先保留关键数据以高效利用内存；2）根据数据分布偏移、客户端能力和离线时长动态调整训练频率；3）设计新指标量化CF，并构建模拟HAR流数据的评估框架。

Result: 实验表明FlexFed有效缓解CF，联邦学习效率提升10-15%，收敛更快更稳定，尤其对低频或欠表征数据效果显著。

Conclusion: FlexFed通过动态资源分配和CF量化机制，为资源受限的HAR场景提供了更鲁棒的联邦学习解决方案。

Abstract: Federated Learning (FL) enables collaborative model training while preserving
privacy by allowing clients to share model updates instead of raw data.
Pervasive computing environments (e.g., for Human Activity Recognition, HAR),
which we focus on in this paper, are characterized by resource-constrained end
devices, streaming sensor data and intermittent client participation.
Variations in user behavior, common in HAR environments, often result in
non-stationary data distributions. As such, existing FL approaches face
challenges in HAR settings due to differing assumptions. The combined effects
of HAR characteristics, namely heterogeneous data and intermittent
participation, can lead to a severe issue called catastrophic forgetting (CF).
Unlike Continuous Learning (CL), which addresses CF using memory and replay
mechanisms, FL's privacy constraints prohibit such strategies.
  To tackle CF in HAR environments, we propose FlexFed, a novel FL approach
that prioritizes data retention for efficient memory use and dynamically
adjusts offline training frequency based on distribution shifts, client
capability and offline duration. To better quantify CF in FL, we introduce a
new metric that accounts for under-represented data, enabling more accurate
evaluations. We also develop a realistic HAR-based evaluation framework that
simulates streaming data, dynamic distributions, imbalances and varying
availability. Experiments show that FlexFed mitigates CF more effectively,
improves FL efficiency by 10 to 15 % and achieves faster, more stable
convergence, especially for infrequent or under-represented data.

</details>


### [231] [Symmetry-Breaking Descent for Invariant Cost Functionals](https://arxiv.org/abs/2505.13578)
*Mikhail Osipov*

Main category: cs.LG

TL;DR: 该论文提出了一种利用对称性结构优化不变成本函数的方法，通过构造显式的对称性破缺变形来降低任务成本，适用于黑盒模型和对称性约束任务。


<details>
  <summary>Details</summary>
Motivation: 在机器学习、成像和反问题中，成本函数通常具有全局对称性且不可微，难以直接优化。论文旨在解决这类对称性约束下的黑盒优化问题。

Method: 论文提出了一种变分方法，通过最小化辅助能量泛函构造对称性破缺的变形场，生成横向于对称轨道的变形方向，从而降低成本函数。

Result: 在温和正则条件下，该方法能严格降低成本函数或逃离局部平坦区域，退化集的测度为零，且无需模型梯度或标签信息。

Conclusion: 该方法为对称性约束下的黑盒优化提供了理论工具，适用于不变成本函数的优化问题。

Abstract: We study the problem of reducing a task cost functional $W(S)$, defined over
Sobolev-class signals $S$, when the cost is invariant under a global symmetry
group $G \subset \mathrm{Diff}(M)$ and accessible only as a black-box. Such
scenarios arise in machine learning, imaging, and inverse problems, where cost
metrics reflect model outputs or performance scores but are non-differentiable
and model-internal. We propose a variational method that exploits the symmetry
structure to construct explicit, symmetry-breaking deformations of the input
signal. A gauge field $\phi$, obtained by minimizing an auxiliary energy
functional, induces a deformation $h = A_\phi[S]$ that generically lies
transverse to the $G$-orbit of $S$. We prove that, under mild regularity, the
cost $W$ strictly decreases along this direction -- either via Clarke
subdifferential descent or by escaping locally flat plateaus. The exceptional
set of degeneracies has zero Gaussian measure. Our approach requires no access
to model gradients or labels and operates entirely at test time. It provides a
principled tool for optimizing invariant cost functionals via Lie-algebraic
variational flows, with applications to black-box models and
symmetry-constrained tasks.

</details>


### [232] [OMGPT: A Sequence Modeling Framework for Data-driven Operational Decision Making](https://arxiv.org/abs/2505.13580)
*Hanzhao Wang,Guanting Chen,Kalyan Talluri,Xiaocheng Li*

Main category: cs.LG

TL;DR: 提出OMGPT模型，基于Transformer解决运筹学中的序列决策问题，无需假设模型结构，利用预训练数据提升性能。


<details>
  <summary>Details</summary>
Motivation: 传统运筹学方法依赖特定模型假设，难以利用海量预训练数据。本文旨在通过通用序列建模框架统一多类决策任务（如动态定价、库存管理），并探索预训练Transformer的潜力。

Method: 1) 构建通用序列建模框架，将决策问题转化为历史信息预测未来动作；2) 设计基于Transformer的OMGPT模型，直接映射历史到动作；3) 从贝叶斯角度理论分析预训练任务多样性与测试任务的关系。

Result: OMGPT在动态定价、库存管理等任务中表现优异，验证了预训练数据与无模型结构的优势。理论分析表明性能与任务多样性及测试-预训练任务差异相关。

Conclusion: OMGPT实现了运筹学方法的范式转变，其数据驱动特性与Transformer架构的结合为序列决策问题提供了新思路。

Abstract: We build a Generative Pre-trained Transformer (GPT) model from scratch to
solve sequential decision making tasks arising in contexts of operations
research and management science which we call OMGPT. We first propose a general
sequence modeling framework to cover several operational decision making tasks
as special cases, such as dynamic pricing, inventory management, resource
allocation, and queueing control. Under the framework, all these tasks can be
viewed as a sequential prediction problem where the goal is to predict the
optimal future action given all the historical information. Then we train a
transformer-based neural network model (OMGPT) as a natural and powerful
architecture for sequential modeling. This marks a paradigm shift compared to
the existing methods for these OR/OM tasks in that (i) the OMGPT model can take
advantage of the huge amount of pre-trained data; (ii) when tackling these
problems, OMGPT does not assume any analytical model structure and enables a
direct and rich mapping from the history to the future actions. Either of these
two aspects, to the best of our knowledge, is not achieved by any existing
method. We establish a Bayesian perspective to theoretically understand the
working mechanism of the OMGPT on these tasks, which relates its performance
with the pre-training task diversity and the divergence between the testing
task and pre-training tasks. Numerically, we observe a surprising performance
of the proposed model across all the above tasks.

</details>


### [233] [Uncovering Critical Sets of Deep Neural Networks via Sample-Independent Critical Lifting](https://arxiv.org/abs/2505.13582)
*Leyang Zhang,Yaoyu Zhang,Tao Luo*

Main category: cs.LG

TL;DR: 本文研究了神经网络临界点的样本依赖性，提出了一种样本无关的临界提升算子，并证明了在足够大的样本量下存在样本依赖的临界点。


<details>
  <summary>Details</summary>
Motivation: 探讨神经网络临界点如何依赖于训练样本，以及如何通过提升算子捕捉样本无关的临界点。

Method: 引入样本无关的临界提升算子，将单个网络的参数映射到另一组网络的参数集，定义样本依赖和样本无关的提升临界点。

Result: 发现现有临界嵌入方法无法捕捉所有样本无关临界点，并证明在足够大样本量下存在样本依赖临界点，其中包括鞍点。

Conclusion: 临界点的样本依赖性对神经网络优化有重要影响，样本无关提升算子为理解这一现象提供了新工具。

Abstract: This paper investigates the sample dependence of critical points for neural
networks. We introduce a sample-independent critical lifting operator that
associates a parameter of one network with a set of parameters of another, thus
defining sample-dependent and sample-independent lifted critical points. We
then show by example that previously studied critical embeddings do not capture
all sample-independent lifted critical points. Finally, we demonstrate the
existence of sample-dependent lifted critical points for sufficiently large
sample sizes and prove that saddles appear among them.

</details>


### [234] [Half Search Space is All You Need](https://arxiv.org/abs/2505.13586)
*Pavel Rumiantsev,Mark Coates*

Main category: cs.LG

TL;DR: 该论文提出了一种通过Zero-Shot NAS自动剪枝搜索空间的方法，显著降低了One-Shot NAS的内存消耗和搜索时间，同时保持了搜索精度。


<details>
  <summary>Details</summary>
Motivation: One-Shot NAS方法在搜索过程中需要较高的GPU内存，这限制了其应用范围。为了缓解这一问题，作者希望通过自动剪枝搜索空间来减少内存消耗和搜索时间。

Method: 论文提出了一种结合Zero-Shot NAS和One-Shot NAS的方法。首先使用Zero-Shot NAS高效地从搜索空间中移除低性能的架构，然后在剪枝后的搜索空间上应用One-Shot NAS。

Result: 实验结果表明，该方法在DARTS搜索空间上比基线One-Shot设置减少了81%的内存消耗，同时达到了相同的精度水平。

Conclusion: 通过Zero-Shot NAS剪枝搜索空间是一种有效的方法，能够在保持搜索精度的同时显著降低One-Shot NAS的资源需求。

Abstract: Neural Architecture Search (NAS) is a powerful tool for automating
architecture design. One-Shot NAS techniques, such as DARTS, have gained
substantial popularity due to their combination of search efficiency with
simplicity of implementation. By design, One-Shot methods have high GPU memory
requirements during the search. To mitigate this issue, we propose to prune the
search space in an efficient automatic manner to reduce memory consumption and
search time while preserving the search accuracy. Specifically, we utilise
Zero-Shot NAS to efficiently remove low-performing architectures from the
search space before applying One-Shot NAS to the pruned search space.
Experimental results on the DARTS search space show that our approach reduces
memory consumption by 81% compared to the baseline One-Shot setup while
achieving the same level of accuracy.

</details>


### [235] [Deterministic Bounds and Random Estimates of Metric Tensors on Neuromanifolds](https://arxiv.org/abs/2505.13614)
*Ke Sun*

Main category: cs.LG

TL;DR: 该论文提出了一种高效估计深度神经网络Fisher信息矩阵的方法，通过低维核心空间分析扩展到高维神经流形，并引入无偏随机估计器。


<details>
  <summary>Details</summary>
Motivation: 现代深度神经网络的高维参数空间（神经流形）具有由Fisher信息定义的独特度量张量，准确估计该张量对深度学习理论和实践方法至关重要。

Method: 通过分析分类网络的低维核心空间概率分布的黎曼度量谱，扩展到神经流形上的确定性边界，并基于Hutchinson迹估计器提出无偏随机估计方法。

Result: 提出的方法可通过单次反向传播高效计算，支持对角、块对角或完整张量估计，且估计质量的标准差由真实值缩放保证。

Conclusion: 该方法为深度神经网络的Fisher信息矩阵估计提供了高效可靠的解决方案，适用于多种网络结构。

Abstract: The high dimensional parameter space of modern deep neural networks -- the
neuromanifold -- is endowed with a unique metric tensor defined by the Fisher
information, estimating which is crucial for both theory and practical methods
in deep learning. To analyze this tensor for classification networks, we return
to a low dimensional space of probability distributions -- the core space --
and carefully analyze the spectrum of its Riemannian metric. We extend our
discoveries there into deterministic bounds of the metric tensor on the
neuromanifold. We introduce an unbiased random estimate of the metric tensor
and its bounds based on Hutchinson's trace estimator. It can be evaluated
efficiently through a single backward pass and can be used to estimate the
diagonal, or block diagonal, or the full tensor. Its quality is guaranteed with
a standard deviation bounded by the true value up to scaling.

</details>


### [236] [Learning (Approximately) Equivariant Networks via Constrained Optimization](https://arxiv.org/abs/2505.13631)
*Andrei Manolache,Luiz F. O. Chamon,Mathias Niepert*

Main category: cs.LG

TL;DR: 论文提出了一种自适应约束等变性（ACE）方法，通过逐步收紧约束来平衡等变性与非等变性，提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 现实数据常因噪声、结构变异或测量偏差等偏离完美对称性，严格等变模型难以拟合数据，而无约束模型又无法有效利用部分对称性。

Method: 基于同伦原理，从非等变模型出发，逐步减少其与等变性的偏差，实现数据驱动的平衡。

Result: ACE方法在多种架构和任务中均优于严格等变模型和启发式松弛方法，提升了性能指标、样本效率和鲁棒性。

Conclusion: ACE通过自适应约束在等变性与灵活性之间找到平衡，为处理非对称数据提供了新思路。

Abstract: Equivariant neural networks are designed to respect symmetries through their
architecture, boosting generalization and sample efficiency when those
symmetries are present in the data distribution. Real-world data, however,
often departs from perfect symmetry because of noise, structural variation,
measurement bias, or other symmetry-breaking effects. Strictly equivariant
models may struggle to fit the data, while unconstrained models lack a
principled way to leverage partial symmetries. Even when the data is fully
symmetric, enforcing equivariance can hurt training by limiting the model to a
restricted region of the parameter space. Guided by homotopy principles, where
an optimization problem is solved by gradually transforming a simpler problem
into a complex one, we introduce Adaptive Constrained Equivariance (ACE), a
constrained optimization approach that starts with a flexible, non-equivariant
model and gradually reduces its deviation from equivariance. This gradual
tightening smooths training early on and settles the model at a data-driven
equilibrium, balancing between equivariance and non-equivariance. Across
multiple architectures and tasks, our method consistently improves performance
metrics, sample efficiency, and robustness to input perturbations compared with
strictly equivariant models and heuristic equivariance relaxations.

</details>


### [237] [Incentivizing Truthful Language Models via Peer Elicitation Games](https://arxiv.org/abs/2505.13636)
*Baiting Chen,Tong Zhu,Jiale Han,Lexin Li,Gang Li,Xiaowu Dai*

Main category: cs.LG

TL;DR: 该论文提出了Peer Elicitation Games (PEG)框架，通过博弈论机制激励大语言模型生成更真实的内容，无需监督或微调。


<details>
  <summary>Details</summary>
Motivation: 大语言模型(LLMs)虽然生成能力强，但仍存在不一致和幻觉问题。论文旨在通过博弈论方法解决这一问题，无需依赖真实标签或额外训练。

Method: 提出PEG框架，包含一个生成器和多个鉴别器，通过基于行列式的互信息评分机制激励模型真实报告，并证明其能收敛到纳什均衡。

Result: 实验表明，PEG在多个基准测试中显著提高了事实准确性，且理论证明其能实现稳定和真实的策略收敛。

Conclusion: PEG是一种无需监督或微调即可有效提升LLMs真实性的实用方法，具有理论保证和实证效果。

Abstract: Large Language Models (LLMs) have demonstrated strong generative capabilities
but remain prone to inconsistencies and hallucinations. We introduce Peer
Elicitation Games (PEG), a training-free, game-theoretic framework for aligning
LLMs through a peer elicitation mechanism involving a generator and multiple
discriminators instantiated from distinct base models. Discriminators interact
in a peer evaluation setting, where rewards are computed using a
determinant-based mutual information score that provably incentivizes truthful
reporting without requiring ground-truth labels. We establish theoretical
guarantees showing that each agent, via online learning, achieves sublinear
regret in the sense their cumulative performance approaches that of the best
fixed truthful strategy in hindsight. Moreover, we prove last-iterate
convergence to a truthful Nash equilibrium, ensuring that the actual policies
used by agents converge to stable and truthful behavior over time. Empirical
evaluations across multiple benchmarks demonstrate significant improvements in
factual accuracy. These results position PEG as a practical approach for
eliciting truthful behavior from LLMs without supervision or fine-tuning.

</details>


### [238] [4Hammer: a board-game reinforcement learning environment for the hour long time frame](https://arxiv.org/abs/2505.13638)
*Massimo Fioravanti,Giovanni Agosta*

Main category: cs.LG

TL;DR: 论文提出4Hammer强化学习环境，用于评估大语言模型在复杂棋盘游戏中的长期任务表现。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在短期任务上表现良好，但在需要长时间持续的任务上表现不佳。缺乏专门针对强化学习和LLM评估的复杂棋盘游戏环境。

Method: 提出4Hammer强化学习环境，模拟《战锤40K》这一复杂零和棋盘游戏的子集，要求理解50多页规则并跟踪游戏状态。

Result: 4Hammer环境填补了复杂棋盘游戏在强化学习和LLM评估中的空白。

Conclusion: 4Hammer为评估LLM在长期复杂任务中的表现提供了有效工具。

Abstract: Large Language Models (LLMs) have demonstrated strong performance on tasks
with short time frames, but struggle with tasks requiring longer durations.
While datasets covering extended-duration tasks, such as software engineering
tasks or video games, do exist, there are currently few implementations of
complex board games specifically designed for reinforcement learning and LLM
evaluation. To address this gap, we propose the 4Hammer reinforcement learning
environment, a digital twin simulation of a subset of Warhammer 40,000-a
complex, zero-sum board game. Warhammer 40,000 features intricate rules,
requiring human players to thoroughly read and understand over 50 pages of
detailed natural language rules, grasp the interactions between their game
pieces and those of their opponents, and independently track and communicate
the evolving game state.

</details>


### [239] [FedCTTA: A Collaborative Approach to Continual Test-Time Adaptation in Federated Learning](https://arxiv.org/abs/2505.13643)
*Rakibul Hasan Rajib,Md Akil Raihan Iftee,Mir Sazzat Hossain,A. K. M. Mahbubur Rahman,Sajib Mistry,M Ashraful Amin,Amin Ahsan Ali*

Main category: cs.LG

TL;DR: 提出FedCTTA框架，解决联邦学习中测试时适应的计算开销、隐私风险和可扩展性问题，通过相似性感知聚合和最小化熵实现高效隐私保护。


<details>
  <summary>Details</summary>
Motivation: 联邦学习（FL）在隐私敏感应用中表现优异，但常因训练与部署分布偏移导致性能下降。现有测试时适应（TTA）方法存在计算开销大、隐私风险及可扩展性差的问题。

Method: FedCTTA框架通过基于模型输出分布的相似性感知聚合（无需直接共享特征），结合客户端熵最小化策略，实现隐私保护且高效的持续适应。

Result: 实验表明，FedCTTA在时空异构场景下优于现有方法，无需服务端训练且内存占用恒定，具有强扩展性。

Conclusion: FedCTTA为联邦学习提供了一种兼顾隐私、效率和适应性的测试时适应解决方案，显著提升了模型在动态分布下的性能。

Abstract: Federated Learning (FL) enables collaborative model training across
distributed clients without sharing raw data, making it ideal for
privacy-sensitive applications. However, FL models often suffer performance
degradation due to distribution shifts between training and deployment.
Test-Time Adaptation (TTA) offers a promising solution by allowing models to
adapt using only test samples. However, existing TTA methods in FL face
challenges such as computational overhead, privacy risks from feature sharing,
and scalability concerns due to memory constraints. To address these
limitations, we propose Federated Continual Test-Time Adaptation (FedCTTA), a
privacy-preserving and computationally efficient framework for federated
adaptation. Unlike prior methods that rely on sharing local feature statistics,
FedCTTA avoids direct feature exchange by leveraging similarity-aware
aggregation based on model output distributions over randomly generated noise
samples. This approach ensures adaptive knowledge sharing while preserving data
privacy. Furthermore, FedCTTA minimizes the entropy at each client for
continual adaptation, enhancing the model's confidence in evolving target
distributions. Our method eliminates the need for server-side training during
adaptation and maintains a constant memory footprint, making it scalable even
as the number of clients or training rounds increases. Extensive experiments
show that FedCTTA surpasses existing methods across diverse temporal and
spatial heterogeneity scenarios.

</details>


### [240] [Collapsing Taylor Mode Automatic Differentiation](https://arxiv.org/abs/2505.13644)
*Felix Dangel,Tim Siebert,Marius Zeinhofer,Andrea Walther*

Main category: cs.LG

TL;DR: 提出了一种优化泰勒模式自动微分的方法，通过重写计算图来加速偏微分方程算子的计算，优于嵌套反向传播。


<details>
  <summary>Details</summary>
Motivation: 当前通过嵌套反向传播计算偏微分方程算子成本高昂，限制了其在科学机器学习中的应用。需要更高效的方法。

Method: 引入了一种优化技术，通过重写计算图来“折叠”导数，适用于一般线性偏微分方程算子和随机化泰勒模式。

Result: 实现了该方法并在常见偏微分方程算子上验证，证实其加速了泰勒模式并优于嵌套反向传播。

Conclusion: 该方法通过简单传播计算图中的和来优化计算，可由机器学习编译器实现，无需用户介入，显著提升了效率。

Abstract: Computing partial differential equation (PDE) operators via nested
backpropagation is expensive, yet popular, and severely restricts their utility
for scientific machine learning. Recent advances, like the forward Laplacian
and randomizing Taylor mode automatic differentiation (AD), propose forward
schemes to address this. We introduce an optimization technique for Taylor mode
that 'collapses' derivatives by rewriting the computational graph, and
demonstrate how to apply it to general linear PDE operators, and randomized
Taylor mode. The modifications simply require propagating a sum up the
computational graph, which could -- or should -- be done by a machine learning
compiler, without exposing complexity to users. We implement our collapsing
procedure and evaluate it on popular PDE operators, confirming it accelerates
Taylor mode and outperforms nested backpropagation.

</details>


### [241] [Self-Reinforced Graph Contrastive Learning](https://arxiv.org/abs/2505.13650)
*Chou-Ying Hsieh,Chun-Fu Jang,Cheng-En Hsieh,Qian-Hui Chen,Sy-Yen Kuo*

Main category: cs.LG

TL;DR: SRGCL是一种新型图对比学习框架，通过动态评估和选择高质量正样本对，提升图表示学习性能。


<details>
  <summary>Details</summary>
Motivation: 图对比学习（GCL）在自监督图表示学习中表现优异，但如何确保正样本对的质量以保留图的语义和结构特性是关键挑战。

Method: 提出SRGCL框架，利用模型自身的编码器动态评估和选择高质量正样本对，采用多增强策略的统一正样本对生成器和基于流形假设的选择器。

Result: 在多种图分类任务上的实验表明，SRGCL作为插件模块，性能优于现有最先进的GCL方法。

Conclusion: SRGCL通过迭代优化正样本对选择，有效提升了图对比学习的性能，具有广泛的适用性和高效性。

Abstract: Graphs serve as versatile data structures in numerous real-world
domains-including social networks, molecular biology, and knowledge graphs-by
capturing intricate relational information among entities. Among graph-based
learning techniques, Graph Contrastive Learning (GCL) has gained significant
attention for its ability to derive robust, self-supervised graph
representations through the contrasting of positive and negative sample pairs.
However, a critical challenge lies in ensuring high-quality positive pairs so
that the intrinsic semantic and structural properties of the original graph are
preserved rather than distorted. To address this issue, we propose SRGCL
(Self-Reinforced Graph Contrastive Learning), a novel framework that leverages
the model's own encoder to dynamically evaluate and select high-quality
positive pairs. We designed a unified positive pair generator employing
multiple augmentation strategies, and a selector guided by the manifold
hypothesis to maintain the underlying geometry of the latent space. By adopting
a probabilistic mechanism for selecting positive pairs, SRGCL iteratively
refines its assessment of pair quality as the encoder's representational power
improves. Extensive experiments on diverse graph-level classification tasks
demonstrate that SRGCL, as a plug-in module, consistently outperforms
state-of-the-art GCL methods, underscoring its adaptability and efficacy across
various domains.

</details>


### [242] [RL in Name Only? Analyzing the Structural Assumptions in RL post-training for LLMs](https://arxiv.org/abs/2505.13697)
*Soumya Rani Samineni,Durgesh Kalwar,Karthik Valmeekam,Kaya Stechly,Subbarao Kambhampati*

Main category: cs.LG

TL;DR: 该论文批判性分析了基于强化学习(RL)的大语言模型(LLM)后训练方法，指出其简化假设使RL框架效果等同于监督学习，并通过实验验证了监督微调能达到类似GRPO的性能。


<details>
  <summary>Details</summary>
Motivation: 针对当前RL后训练方法（如DeepSeek R1采用的GRPO）被过度炒作能提升LLM推理能力的现象，作者旨在揭示这些方法背后简化假设的局限性。

Method: 1) 分析将LLM训练建模为MDP时的两个关键假设（状态=上下文窗口+令牌、奖励均匀分配）
2) 在GSM8K和Countdown基准上对比GRPO与监督微调（含正负样本）

Result: 实验表明：迭代监督微调性能与GRPO相当；现有RL框架的结构假设会诱导模型生成更长中间令牌序列，但实际未真正利用RL机制。

Conclusion: 当前LLM的RL后训练框架因过度简化假设而存在争议，其效果本质上可通过监督学习实现，需重新审视RL对推理能力提升的实际贡献。

Abstract: Reinforcement learning-based post-training of large language models (LLMs)
has recently gained attention, particularly following the release of DeepSeek
R1, which applied GRPO for fine-tuning. Amid the growing hype around improved
reasoning abilities attributed to RL post-training, we critically examine the
formulation and assumptions underlying these methods. We start by highlighting
the popular structural assumptions made in modeling LLM training as a Markov
Decision Process (MDP), and show how they lead to a degenerate MDP that doesn't
quite need the RL/GRPO apparatus. The two critical structural assumptions
include (1) making the MDP states be just a concatenation of the actions-with
states becoming the context window and the actions becoming the tokens in LLMs
and (2) splitting the reward of a state-action trajectory uniformly across the
trajectory. Through a comprehensive analysis, we demonstrate that these
simplifying assumptions make the approach effectively equivalent to an
outcome-driven supervised learning. Our experiments on benchmarks including
GSM8K and Countdown using Qwen-2.5 base models show that iterative supervised
fine-tuning, incorporating both positive and negative samples, achieves
performance comparable to GRPO-based training. We will also argue that the
structural assumptions indirectly incentivize the RL to generate longer
sequences of intermediate tokens-which in turn feeds into the narrative of "RL
generating longer thinking traces." While RL may well be a very useful
technique for improving the reasoning abilities of LLMs, our analysis shows
that the simplistic structural assumptions made in modeling the underlying MDP
render the popular LLM RL frameworks and their interpretations questionable.

</details>


### [243] [Unsupervised anomaly detection in MeV ultrafast electron diffraction](https://arxiv.org/abs/2505.13702)
*Mariana A. Fazio,Salvador Sosa Güitron,Marcus Babzien,Mikhail Fedurin,Junjie Li,Mark Palmer,Sandra S. Biedron,Manel Martinez-Ramon*

Main category: cs.LG

TL;DR: 提出一种无监督异常检测方法，用于自动识别MUED中的故障图像，无需人工标注数据。


<details>
  <summary>Details</summary>
Motivation: 传统图像异常检测需要大量人工标注，耗时耗力。本研究旨在通过无监督技术解放用户，实现自动化检测并提供不确定性度量辅助决策。

Method: 采用无监督学习框架，模型自主识别数据集中的异常模式，并量化检测结果的不确定性。

Result: 该方法有效实现了无需标注数据的故障图像检测，同时提供不确定性评估指标。

Conclusion: 无监督异常检测方案可显著降低人工成本，不确定性度量增强了检测结果的可信度和实用性。

Abstract: This study focus in the construction of an unsupervised anomaly detection
methodology to detect faulty images in MUED. We believe that unsupervised
techniques are the best choice for our purposes because the data used to train
the detector does not need to be manually labeled, and instead, the machine is
intended to detect by itself the anomalies in the dataset, which liberates the
user of tedious, time-consuming initial image examination. The structure must,
additionally, provide the user with some measure of uncertainty in the
detection, so the user can take decisions based on this measure.

</details>


### [244] [Policy-Driven World Model Adaptation for Robust Offline Model-based Reinforcement Learning](https://arxiv.org/abs/2505.13709)
*Jiayu Chen,Aravind Venugopal,Jeff Schneider*

Main category: cs.LG

TL;DR: 本文提出了一种动态调整世界模型与策略的统一学习框架，通过最大化极小优化问题提升离线模型强化学习的鲁棒性，在多个任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有离线模型强化学习方法采用两阶段训练，导致世界模型未针对策略学习优化，且策略在部署时缺乏鲁棒性，易受环境噪声影响。

Method: 提出基于Stackelberg学习动态的统一框架，通过最大化极小优化动态调整世界模型与策略，提升鲁棒性。

Result: 在12个噪声D4RL MuJoCo任务和3个随机托卡马克控制任务中实现了最先进的性能。

Conclusion: 所提方法有效解决了目标不匹配问题，显著提升了策略的鲁棒性和泛化能力。

Abstract: Offline reinforcement learning (RL) offers a powerful paradigm for
data-driven control. Compared to model-free approaches, offline model-based RL
(MBRL) explicitly learns a world model from a static dataset and uses it as a
surrogate simulator, improving data efficiency and enabling potential
generalization beyond the dataset support. However, most existing offline MBRL
methods follow a two-stage training procedure: first learning a world model by
maximizing the likelihood of the observed transitions, then optimizing a policy
to maximize its expected return under the learned model. This objective
mismatch results in a world model that is not necessarily optimized for
effective policy learning. Moreover, we observe that policies learned via
offline MBRL often lack robustness during deployment, and small adversarial
noise in the environment can lead to significant performance degradation. To
address these, we propose a framework that dynamically adapts the world model
alongside the policy under a unified learning objective aimed at improving
robustness. At the core of our method is a maximin optimization problem, which
we solve by innovatively utilizing Stackelberg learning dynamics. We provide
theoretical analysis to support our design and introduce computationally
efficient implementations. We benchmark our algorithm on twelve noisy D4RL
MuJoCo tasks and three stochastic Tokamak Control tasks, demonstrating its
state-of-the-art performance.

</details>


### [245] [Turbocharging Gaussian Process Inference with Approximate Sketch-and-Project](https://arxiv.org/abs/2505.13723)
*Pratik Rathore,Zachary Frangella,Sachin Garg,Shaghayegh Fazliani,Michał Dereziński,Madeleine Udell*

Main category: cs.LG

TL;DR: 提出了一种名为ADASAP的分布式加速草图投影算法，用于高效解决高斯过程推理中的大规模线性系统问题，显著提升了计算效率。


<details>
  <summary>Details</summary>
Motivation: 高斯过程（GPs）在生物统计学和机器学习中广泛应用，但其推理过程在大规模数据集上计算效率低下，传统方法难以应对现代应用中的海量数据需求。

Method: 采用基于草图投影的近似分布式加速算法（ADASAP），结合行列式点过程理论，确保后验均值快速收敛至真实值。

Result: ADASAP在多个基准数据集和大规模贝叶斯优化任务中表现优于现有方法，成功处理了超过3亿样本的数据集。

Conclusion: ADASAP为高斯过程推理提供了一种高效、可扩展的解决方案，尤其适用于大规模数据场景。

Abstract: Gaussian processes (GPs) play an essential role in biostatistics, scientific
machine learning, and Bayesian optimization for their ability to provide
probabilistic predictions and model uncertainty. However, GP inference
struggles to scale to large datasets (which are common in modern applications),
since it requires the solution of a linear system whose size scales
quadratically with the number of samples in the dataset. We propose an
approximate, distributed, accelerated sketch-and-project algorithm
($\texttt{ADASAP}$) for solving these linear systems, which improves
scalability. We use the theory of determinantal point processes to show that
the posterior mean induced by sketch-and-project rapidly converges to the true
posterior mean. In particular, this yields the first efficient, condition
number-free algorithm for estimating the posterior mean along the top spectral
basis functions, showing that our approach is principled for GP inference.
$\texttt{ADASAP}$ outperforms state-of-the-art solvers based on conjugate
gradient and coordinate descent across several benchmark datasets and a
large-scale Bayesian optimization task. Moreover, $\texttt{ADASAP}$ scales to a
dataset with $> 3 \cdot 10^8$ samples, a feat which has not been accomplished
in the literature.

</details>


### [246] [Power Lines: Scaling Laws for Weight Decay and Batch Size in LLM Pre-training](https://arxiv.org/abs/2505.13738)
*Shane Bergsma,Nolan Dey,Gurpreet Gosal,Gavia Gray,Daria Soboleva,Joel Hestness*

Main category: cs.LG

TL;DR: 该论文研究了大型语言模型预训练中超参数（如学习率和权重衰减）的缩放规律，提出了基于模型大小、数据集大小和批量大小的超参数调整方法，并验证了最优时间尺度与参数-标记比的幂律关系。


<details>
  <summary>Details</summary>
Motivation: 高效的大型语言模型预训练需要精心调整的超参数，如学习率和权重衰减。然而，随着模型规模、数据集大小和批量大小的变化，如何调整这些超参数仍是一个挑战。本文旨在探索这些超参数的缩放规律，以优化预训练过程。

Method: 论文通过研究AdamW时间尺度（B/(ηλD)）在不同训练设置下的变化，验证了最优权重衰减λ与批量大小B的线性关系。此外，还分析了最优批量大小Bopt和临界批量大小Bcrit的缩放规律。

Result: 研究发现，最优时间尺度与参数-标记比（D/N）呈精确的幂律关系，从而可以提前预测大规模训练中的最优λ。此外，Bopt和Bcrit与数据集大小D呈幂律关系，而与模型大小N无关。

Conclusion: 这些发现为在实际训练中选择帕累托最优的模型大小N和数据集大小D提供了理论依据，有助于在训练时间和计算资源之间找到平衡。

Abstract: Efficient LLM pre-training requires well-tuned hyperparameters (HPs),
including learning rate {\eta} and weight decay {\lambda}. We study scaling
laws for HPs: formulas for how to scale HPs as we scale model size N, dataset
size D, and batch size B. Recent work suggests the AdamW timescale,
B/({\eta}{\lambda}D), should remain constant across training settings, and we
verify the implication that optimal {\lambda} scales linearly with B, for a
fixed N,D. However, as N,D scale, we show the optimal timescale obeys a precise
power law in the tokens-per-parameter ratio, D/N. This law thus provides a
method to accurately predict {\lambda}opt in advance of large-scale training.
We also study scaling laws for optimal batch size Bopt (the B enabling lowest
loss at a given N,D) and critical batch size Bcrit (the B beyond which further
data parallelism becomes ineffective). In contrast with prior work, we find
both Bopt and Bcrit scale as power laws in D, independent of model size, N.
Finally, we analyze how these findings inform the real-world selection of
Pareto-optimal N and D under dual training time and compute objectives.

</details>


### [247] [Improving Compositional Generation with Diffusion Models Using Lift Scores](https://arxiv.org/abs/2505.13740)
*Chenning Yu,Sicun Gao*

Main category: cs.LG

TL;DR: 提出一种基于lift scores的重采样准则，提升扩散模型组合生成效果，无需额外训练模块。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型在组合生成任务中对多条件对齐效果不佳，需高效评估生成样本与各条件的匹配度。

Method: 利用lift scores评估单条件对齐性并组合结果，通过原模型近似计算，优化版本降低计算开销。

Result: 在2D合成数据、CLEVR位置任务和文生图任务中显著提升组合生成的条件对齐效果。

Conclusion: lift scores为扩散模型组合生成提供了一种高效且无需额外训练的质量提升方案。

Abstract: We introduce a novel resampling criterion using lift scores, for improving
compositional generation in diffusion models. By leveraging the lift scores, we
evaluate whether generated samples align with each single condition and then
compose the results to determine whether the composed prompt is satisfied. Our
key insight is that lift scores can be efficiently approximated using only the
original diffusion model, requiring no additional training or external modules.
We develop an optimized variant that achieves relatively lower computational
overhead during inference while maintaining effectiveness. Through extensive
experiments, we demonstrate that lift scores significantly improved the
condition alignment for compositional generation across 2D synthetic data,
CLEVR position tasks, and text-to-image synthesis. Our code is available at
http://github.com/rainorangelemon/complift.

</details>


### [248] [Understanding Task Representations in Neural Networks via Bayesian Ablation](https://arxiv.org/abs/2505.13742)
*Andrew Nam,Declan Campbell,Thomas Griffiths,Jonathan Cohen,Sarah-Jane Leslie*

Main category: cs.LG

TL;DR: 本文提出了一种新的概率框架，结合贝叶斯推断和信息论工具，用于解释神经网络中的潜在任务表示。


<details>
  <summary>Details</summary>
Motivation: 神经网络的灵活性使其成为认知建模的强大工具，但其亚符号语义特性使得解释其学习表示具有挑战性。

Method: 受贝叶斯推断启发，定义表示单元的分布以推断其对任务性能的因果贡献，并利用信息论提出一系列工具和指标。

Result: 提出的方法能够揭示关键模型特性，包括表示的分布性、流形复杂性和多义性。

Conclusion: 该框架为神经网络表示的解释提供了新的视角和工具，有助于理解其内部工作机制。

Abstract: Neural networks are powerful tools for cognitive modeling due to their
flexibility and emergent properties. However, interpreting their learned
representations remains challenging due to their sub-symbolic semantics. In
this work, we introduce a novel probabilistic framework for interpreting latent
task representations in neural networks. Inspired by Bayesian inference, our
approach defines a distribution over representational units to infer their
causal contributions to task performance. Using ideas from information theory,
we propose a suite of tools and metrics to illuminate key model properties,
including representational distributedness, manifold complexity, and
polysemanticity.

</details>


### [249] [Synthetic Non-stationary Data Streams for Recognition of the Unknown](https://arxiv.org/abs/2505.13745)
*Joanna Komorniczak*

Main category: cs.LG

TL;DR: 该论文提出了一种合成数据流生成策略，用于处理非平稳数据流中的概念漂移和新类别出现的问题，并展示了无监督漂移检测器在检测新颖性和概念漂移中的应用。


<details>
  <summary>Details</summary>
Motivation: 在动态环境中，数据流的非平稳性表现为概念漂移和新类别的出现。现有方法通常只关注其中一个问题，而忽略了二者可能同时存在的情况。此外，开放集识别任务中，如何有效分类已知类别并识别未知对象也变得越来越重要。

Method: 论文提出了一种合成数据流生成策略，模拟概念漂移和新类别的出现，并利用无监督漂移检测器来检测新颖性和概念漂移。

Result: 研究表明，无监督漂移检测器能够有效检测新颖性和概念漂移，生成的合成数据流可用于开放集识别任务。

Conclusion: 该论文为处理非平稳数据流中的概念漂移和新类别出现提供了一种有效方法，并展示了其在开放集识别任务中的潜在应用价值。

Abstract: The problem of data non-stationarity is commonly addressed in data stream
processing. In a dynamic environment, methods should continuously be ready to
analyze time-varying data -- hence, they should enable incremental training and
respond to concept drifts. An equally important variability typical for
non-stationary data stream environments is the emergence of new, previously
unknown classes. Often, methods focus on one of these two phenomena --
detection of concept drifts or detection of novel classes -- while both
difficulties can be observed in data streams. Additionally, concerning
previously unknown observations, the topic of open set of classes has become
particularly important in recent years, where the goal of methods is to
efficiently classify within known classes and recognize objects outside the
model competence. This article presents a strategy for synthetic data stream
generation in which both concept drifts and the emergence of new classes
representing unknown objects occur. The presented research shows how
unsupervised drift detectors address the task of detecting novelty and concept
drifts and demonstrates how the generated data streams can be utilized in the
open set recognition task.

</details>


### [250] [Finding Maximum Independent Sets in Dynamic Graphs using Unsupervised Learning](https://arxiv.org/abs/2505.13754)
*Devendra Parkar,Anya Chaturvedi,Andréa W. Richa,Joshua J. Daymude*

Main category: cs.LG

TL;DR: 本文提出首个针对动态图的最大独立集（MaxIS）无监督学习模型，结合图神经网络与分布式更新机制，在性能与效率上超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有最大独立集方法主要针对静态图设计，无法高效处理动态图中边变化的场景。本文旨在填补这一空白，开发适用于动态图的快速、可扩展解决方案。

Method: 结合图神经网络的结构学习能力与分布式更新机制，通过单步并行操作处理边的增减事件，并引入半径参数化机制平衡性能与计算效率。

Result: 在100-10,000节点的合成和真实动态图上，模型在近似比、运行时间和内存使用上显著优于现有方法，且能泛化到比训练图大100倍的图结构。

Conclusion: 该模型为动态图MaxIS问题提供了首个无监督学习框架，在保持竞争力的求解质量同时，计算效率比贪婪算法快1.5-23倍，展现了优越的实用价值。

Abstract: We present the first unsupervised learning model for finding Maximum
Independent Sets (MaxIS) in dynamic graphs where edges change over time. Our
method combines structural learning from graph neural networks (GNNs) with a
learned distributed update mechanism that, given an edge addition or deletion
event, modifies nodes' internal memories and infers their MaxIS membership in a
single, parallel step. We parameterize our model by the update mechanism's
radius and investigate the resulting performance-runtime tradeoffs for various
dynamic graph topologies. We evaluate our model against state-of-the-art MaxIS
methods for static graphs, including a mixed integer programming solver,
deterministic rule-based algorithms, and a heuristic learning framework based
on dynamic programming and GNNs. Across synthetic and real-world dynamic graphs
of 100-10,000 nodes, our model achieves competitive approximation ratios with
excellent scalability; on large graphs, it significantly outperforms the
state-of-the-art heuristic learning framework in solution quality, runtime, and
memory usage. Our model generalizes well on graphs 100x larger than the ones
used for training, achieving performance at par with both a greedy technique
and a commercial mixed integer programming solver while running 1.5-23x faster
than greedy.

</details>


### [251] [Panda: A pretrained forecast model for universal representation of chaotic dynamics](https://arxiv.org/abs/2505.13755)
*Jeffrey Lai,Anthony Bao,William Gilpin*

Main category: cs.LG

TL;DR: 该论文提出了一种名为Panda的模型，通过训练在合成混沌系统数据集上，展示了零样本预测真实混沌系统和自发预测偏微分方程的能力。


<details>
  <summary>Details</summary>
Motivation: 混沌系统对小误差极为敏感，这使得构建预测真实世界动态系统（如流体流动或神经元活动）的数据驱动模型具有挑战性。现有方法要么是针对单个时间序列单独训练的专用模型，要么是在缺乏动态结构的大规模时间序列数据库上训练的基础模型。

Method: 作者提出Panda模型（Patched Attention for Nonlinear DynAmics），并利用进化算法发现了一个包含2×10^4个混沌动态系统的新型合成数据集进行训练。

Result: Panda模型在仅使用低维常微分方程训练的情况下，展现出对未见过的真实混沌系统的零样本预测能力，并能自发预测偏微分方程。此外，模型还展示了非线性共振模式和微分方程的神经缩放定律。

Conclusion: 该研究表明，预训练模型在探索非线性动力学等抽象数学领域具有巨大潜力，Panda模型的表现突显了其在混沌系统预测中的有效性。

Abstract: Chaotic systems are intrinsically sensitive to small errors, challenging
efforts to construct predictive data-driven models of real-world dynamical
systems such as fluid flows or neuronal activity. Prior efforts comprise either
specialized models trained separately on individual time series, or foundation
models trained on vast time series databases with little underlying dynamical
structure. Motivated by dynamical systems theory, we present Panda, Patched
Attention for Nonlinear DynAmics. We train Panda on a novel synthetic,
extensible dataset of $2 \times 10^4$ chaotic dynamical systems that we
discover using an evolutionary algorithm. Trained purely on simulated data,
Panda exhibits emergent properties: zero-shot forecasting of unseen real world
chaotic systems, and nonlinear resonance patterns in cross-channel attention
heads. Despite having been trained only on low-dimensional ordinary
differential equations, Panda spontaneously develops the ability to predict
partial differential equations without retraining. We demonstrate a neural
scaling law for differential equations, underscoring the potential of
pretrained models for probing abstract mathematical domains like nonlinear
dynamics.

</details>


### [252] [Consistency Conditions for Differentiable Surrogate Losses](https://arxiv.org/abs/2505.13760)
*Drona Khurana,Anish Thilagar,Dhamma Kimpara,Rafael Frongillo*

Main category: cs.LG

TL;DR: 该论文研究了离散预测任务中替代损失的统计一致性，提出了间接引发（IE）和强IE的概念，以简化校准条件的验证，并在一维和更高维情况下分析了它们与校准的等价性。


<details>
  <summary>Details</summary>
Motivation: 在离散预测任务中，验证替代损失的统计一致性通常需要通过校准条件，但直接验证校准条件较为困难。因此，研究更易验证的条件（如间接引发）与校准的等价性具有重要意义。

Method: 论文首先在一维凸可微损失函数中证明了间接引发（IE）与校准的等价性，并通过反例展示了高维情况下这种等价性不成立。随后引入了强IE的概念，并证明了其对可微损失的校准性。

Result: 在一维情况下，IE与校准等价；高维情况下，强IE是可微损失校准的充分条件，且对强凸可微损失是充要条件。论文还通过多个问题展示了IE和强IE在设计一致性替代损失中的应用。

Conclusion: 间接引发（IE）和强IE为验证替代损失的统计一致性提供了更简便的方法，尤其适用于可微和强凸可微损失函数，为设计和分析一致性替代损失提供了有力工具。

Abstract: The statistical consistency of surrogate losses for discrete prediction tasks
is often checked via the condition of calibration. However, directly verifying
calibration can be arduous. Recent work shows that for polyhedral surrogates, a
less arduous condition, indirect elicitation (IE), is still equivalent to
calibration. We give the first results of this type for non-polyhedral
surrogates, specifically the class of convex differentiable losses. We first
prove that under mild conditions, IE and calibration are equivalent for
one-dimensional losses in this class. We construct a counter-example that shows
that this equivalence fails in higher dimensions. This motivates the
introduction of strong IE, a strengthened form of IE that is equally easy to
verify. We establish that strong IE implies calibration for differentiable
surrogates and is both necessary and sufficient for strongly convex,
differentiable surrogates. Finally, we apply these results to a range of
problems to demonstrate the power of IE and strong IE for designing and
analyzing consistent differentiable surrogates.

</details>


### [253] [WIND: Accelerated RNN-T Decoding with Windowed Inference for Non-blank Detection](https://arxiv.org/abs/2505.13765)
*Hainan Xu,Vladimir Bataev,Lilit Grigoryan,Boris Ginsburg*

Main category: cs.LG

TL;DR: 提出了一种名为WIND的新型推理策略，通过并行处理窗口内的多帧数据，显著加速RNN-T推理速度，同时保持模型准确性。


<details>
  <summary>Details</summary>
Motivation: 传统RNN-T推理需要逐帧顺序处理，效率较低。WIND旨在通过并行处理多帧数据来提升推理速度，同时不牺牲模型精度。

Method: WIND采用窗口式并行推理策略，在解码时快速定位非空白预测。实现了贪婪解码、批量贪婪解码（带标签循环技术）以及一种新型束搜索解码方法。

Result: 实验表明：贪婪模式下推理速度提升达2.4倍（WER不变）；束搜索算法在精度略优的同时显著提速。代码将开源。

Conclusion: WIND有效解决了RNN-T推理效率问题，为语音识别任务提供了高效准确的解决方案。

Abstract: We propose Windowed Inference for Non-blank Detection (WIND), a novel
strategy that significantly accelerates RNN-T inference without compromising
model accuracy. During model inference, instead of processing frames
sequentially, WIND processes multiple frames simultaneously within a window in
parallel, allowing the model to quickly locate non-blank predictions during
decoding, resulting in significant speed-ups. We implement WIND for greedy
decoding, batched greedy decoding with label-looping techniques, and also
propose a novel beam-search decoding method. Experiments on multiple datasets
with different conditions show that our method, when operating in greedy modes,
speeds up as much as 2.4X compared to the baseline sequential approach while
maintaining identical Word Error Rate (WER) performance. Our beam-search
algorithm achieves slightly better accuracy than alternative methods, with
significantly improved speed. We will open-source our WIND implementation.

</details>


### [254] [Augmenting Online RL with Offline Data is All You Need: A Unified Hybrid RL Algorithm Design and Analysis](https://arxiv.org/abs/2505.13768)
*Ruiquan Huang,Donghao Li,Chengshuai Shi,Cong Shen,Jing Yang*

Main category: cs.LG

TL;DR: 该论文提出了一种混合学习框架，结合离线数据集和在线交互来优化强化学习策略，在次优差距和在线学习遗憾两个指标上达到最优效果。


<details>
  <summary>Details</summary>
Motivation: 研究如何同时利用离线数据集和在线交互来提升强化学习的效果，克服纯在线或纯离线方法的局限性。

Method: 提出了一种统一的混合学习算法，结合基于置信度的在线强化学习算法和离线数据集。

Result: 算法在次优差距和在线学习遗憾两个指标上表现优异，理论分析和实验验证均支持其有效性。

Conclusion: 混合学习方法显著优于纯在线或纯离线方法，并在不同覆盖属性的离线数据集上展现出有趣的分离现象。

Abstract: This paper investigates a hybrid learning framework for reinforcement
learning (RL) in which the agent can leverage both an offline dataset and
online interactions to learn the optimal policy. We present a unified algorithm
and analysis and show that augmenting confidence-based online RL algorithms
with the offline dataset outperforms any pure online or offline algorithm alone
and achieves state-of-the-art results under two learning metrics, i.e.,
sub-optimality gap and online learning regret. Specifically, we show that our
algorithm achieves a sub-optimality gap
$\tilde{O}(\sqrt{1/(N_0/\mathtt{C}(\pi^*|\rho)+N_1}) )$, where
$\mathtt{C}(\pi^*|\rho)$ is a new concentrability coefficient, $N_0$ and $N_1$
are the numbers of offline and online samples, respectively. For regret
minimization, we show that it achieves a constant $\tilde{O}(
\sqrt{N_1/(N_0/\mathtt{C}(\pi^{-}|\rho)+N_1)} )$ speed-up compared to pure
online learning, where $\mathtt{C}(\pi^-|\rho)$ is the concentrability
coefficient over all sub-optimal policies. Our results also reveal an
interesting separation on the desired coverage properties of the offline
dataset for sub-optimality gap minimization and regret minimization. We further
validate our theoretical findings in several experiments in special RL models
such as linear contextual bandits and Markov decision processes (MDPs).

</details>


### [255] [Beyond Semantics: The Unreasonable Effectiveness of Reasonless Intermediate Tokens](https://arxiv.org/abs/2505.13775)
*Kaya Stechly,Karthik Valmeekam,Atharva Gundawar,Vardhan Palod,Subbarao Kambhampati*

Main category: cs.LG

TL;DR: 论文质疑CoT（思维链）中中间语义对模型性能的实际影响，发现即使使用正确或噪声的中间步骤训练，模型性能变化不大，挑战了CoT能诱导可预测推理行为的假设。


<details>
  <summary>Details</summary>
Motivation: 近期大型推理模型的成功被归因于CoT训练，尤其是通过从基础LLM采样CoT来发现新推理模式。本文旨在验证中间语义（常被拟人化为“思考”或推理痕迹）是否真正影响模型性能。

Method: 使用形式化可验证的推理痕迹和解决方案训练Transformer模型，约束中间步骤和最终输出与形式化求解器（如A*搜索）对齐，并通过形式化解释器系统评估中间痕迹的正确性及其对解决方案的因果影响。

Result: 即使使用完全正确的痕迹训练，模型在得出正确解时仍会产生无效推理痕迹；而使用噪声痕迹训练时，性能与正确数据训练相当甚至更好，且泛化能力更强。

Conclusion: 中间语义或CoT痕迹与解决方案准确性关联性较弱，挑战了其诱导可预测推理行为的假设，并警示避免过度拟人化或将其视为语言模型具备类人或算法行为的证据。

Abstract: Recent impressive results from large reasoning models have been interpreted
as a triumph of Chain of Thought (CoT), and especially of the process of
training on CoTs sampled from base LLMs in order to help find new reasoning
patterns. In this paper, we critically examine that interpretation by
investigating how the semantics of intermediate tokens-often anthropomorphized
as "thoughts" or reasoning traces and which are claimed to display behaviors
like backtracking, self-verification etc.-actually influence model performance.
We train transformer models on formally verifiable reasoning traces and
solutions, constraining both intermediate steps and final outputs to align with
those of a formal solver (in our case, A* search). By constructing a formal
interpreter of the semantics of our problems and intended algorithm, we
systematically evaluate not only solution accuracy but also the correctness of
intermediate traces, thus allowing us to evaluate whether the latter causally
influences the former. We notice that, despite significant improvements on the
solution-only baseline, models trained on entirely correct traces still produce
invalid reasoning traces when arriving at correct solutions. To further show
that trace accuracy is only loosely connected to solution accuracy, we then
train models on noisy, corrupted traces which have no relation to the specific
problem each is paired with, and find that not only does performance remain
largely consistent with models trained on correct data, but in some cases can
improve upon it and generalize more robustly on out-of-distribution tasks.
These results challenge the assumption that intermediate tokens or "Chains of
Thought" induce predictable reasoning behaviors and caution against
anthropomorphizing such outputs or over-interpreting them (despite their mostly
correct forms) as evidence of human-like or algorithmic behaviors in language
models.

</details>


### [256] [Preference Learning with Lie Detectors can Induce Honesty or Evasion](https://arxiv.org/abs/2505.13787)
*Chris Cundy,Adam Gleave*

Main category: cs.LG

TL;DR: 研究探讨了在LLM训练中引入谎言检测器对模型诚实性的影响，发现不同条件下可能导致模型更诚实或学会欺骗检测器。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统能力增强，欺骗行为可能误导用户并破坏评估。研究旨在探索谎言检测器在训练中的作用及其潜在风险。

Method: 使用包含65k样本的DolusChat数据集，结合谎言检测器和GRPO算法，分析探索量、检测器准确性和KL正则化强度对模型诚实性的影响。

Result: 研究发现，GRPO在特定条件下（如高TPR或强KL正则化）可学习诚实策略，但其他情况下可能导致85%以上的欺骗率；而DPO算法在现实TPR下欺骗率低于25%。

Conclusion: 谎言检测器增强的训练在不同情境下可能成为可扩展监督的有效工具，也可能加剧难以检测的错位，需谨慎使用。

Abstract: As AI systems become more capable, deceptive behaviors can undermine
evaluation and mislead users at deployment. Recent work has shown that lie
detectors can accurately classify deceptive behavior, but they are not
typically used in the training pipeline due to concerns around contamination
and objective hacking. We examine these concerns by incorporating a lie
detector into the labelling step of LLM post-training and evaluating whether
the learned policy is genuinely more honest, or instead learns to fool the lie
detector while remaining deceptive. Using DolusChat, a novel 65k-example
dataset with paired truthful/deceptive responses, we identify three key factors
that determine the honesty of learned policies: amount of exploration during
preference learning, lie detector accuracy, and KL regularization strength. We
find that preference learning with lie detectors and GRPO can lead to policies
which evade lie detectors, with deception rates of over 85\%. However, if the
lie detector true positive rate (TPR) or KL regularization is sufficiently
high, GRPO learns honest policies. In contrast, off-policy algorithms (DPO)
consistently lead to deception rates under 25\% for realistic TPRs. Our results
illustrate a more complex picture than previously assumed: depending on the
context, lie-detector-enhanced training can be a powerful tool for scalable
oversight, or a counterproductive method encouraging undetectable misalignment.

</details>


### [257] [Scalable Autoregressive 3D Molecule Generation](https://arxiv.org/abs/2505.13791)
*Austin H. Cheng,Chong Sun,Alán Aspuru-Guzik*

Main category: cs.LG

TL;DR: Quetzal是一种新型自回归模型，通过原子序列生成3D分子结构，性能优于现有自回归模型，并与扩散模型竞争。


<details>
  <summary>Details</summary>
Motivation: 当前3D分子生成领域由扩散模型主导，自回归模型表现落后。本文旨在开发一种简单但可扩展的自回归模型，提升生成质量和速度。

Method: Quetzal将分子视为有序原子序列，结合因果Transformer预测原子类型和小型扩散MLP建模位置分布。

Result: Quetzal在生成质量上显著提升，与最先进扩散模型竞争，并实现更快生成速度和精确似然计算。

Conclusion: Quetzal展示了自回归模型在3D分子生成中的潜力，为可扩展性和通用性提供了新视角。

Abstract: Generative models of 3D molecular structure play a rapidly growing role in
the design and simulation of molecules. Diffusion models currently dominate the
space of 3D molecule generation, while autoregressive models have trailed
behind. In this work, we present Quetzal, a simple but scalable autoregressive
model that builds molecules atom-by-atom in 3D. Treating each molecule as an
ordered sequence of atoms, Quetzal combines a causal transformer that predicts
the next atom's discrete type with a smaller Diffusion MLP that models the
continuous next-position distribution. Compared to existing autoregressive
baselines, Quetzal achieves substantial improvements in generation quality and
is competitive with the performance of state-of-the-art diffusion models. In
addition, by reducing the number of expensive forward passes through a dense
transformer, Quetzal enables significantly faster generation speed, as well as
exact divergence-based likelihood computation. Finally, without any
architectural changes, Quetzal natively handles variable-size tasks like
hydrogen decoration and scaffold completion. We hope that our work motivates a
perspective on scalability and generality for generative modelling of 3D
molecules.

</details>


### [258] [Context-Free Synthetic Data Mitigates Forgetting](https://arxiv.org/abs/2505.13811)
*Parikshit Bansal,Sujay Sanghavi*

Main category: cs.LG

TL;DR: 通过上下文无关生成缓解语言模型微调中的遗忘问题，效果优于其他方法。


<details>
  <summary>Details</summary>
Motivation: 微调语言模型常导致其在其他任务上性能下降（灾难性遗忘）。研究旨在仅通过模型权重（无需原始训练数据）缓解该问题。

Method: 提出上下文无关生成技术，近似无偏估计原始模型与新模型的KL散度，并将其加入微调数据集。对比了合成上下文数据与部分预训练数据的效果。

Result: 在OLMo-1B（预训练保留）和R1-Distill-Llama-8B（推理保留）上验证：上下文无关生成能有效减少遗忘，且优于其他数据增强方式。

Conclusion: 上下文无关生成是缓解遗忘的轻量级解决方案，尤其在数据受限场景中具有实用价值。

Abstract: Fine-tuning a language model often results in a degradation of its existing
performance on other tasks, due to a shift in the model parameters; this
phenomenon is often referred to as (catastrophic) forgetting. We are interested
in mitigating this, in settings where we only have access to the model weights
but no access to its training data/recipe. A natural approach is to penalize
the KL divergence between the original model and the new one. Our main
realization is that a simple process - which we term context-free generation -
allows for an approximate unbiased estimation of this KL divergence. We show
that augmenting a fine-tuning dataset with context-free generations mitigates
forgetting, in two settings: (a) preserving the zero-shot performance of
pretrained-only models, and (b) preserving the reasoning performance of
thinking models. We show that contextual synthetic data, and even a portion of
the pretraining data, are less effective. We also investigate the effect of
choices like generation temperature, data ratios etc. We present our results
for OLMo-1B for pretrained-only setting and R1-Distill-Llama-8B for the
reasoning setting.

</details>


### [259] [FlashKAT: Understanding and Addressing Performance Bottlenecks in the Kolmogorov-Arnold Transformer](https://arxiv.org/abs/2505.13813)
*Matthew Raffel,Lizhong Chen*

Main category: cs.LG

TL;DR: FlashKAT通过优化梯度累积内存瓶颈，显著提升KAT训练速度86.5倍，同时减少梯度误差。


<details>
  <summary>Details</summary>
Motivation: Kolmogorov-Arnold Transformer (KAT)虽计算量接近传统Transformer，但训练速度慢123倍，主要因GR-KAN反向传播中的内存瓶颈。

Method: 提出FlashKAT，重构内核以减少梯度累积的原子操作和慢速内存访问，解决内存瓶颈问题。

Result: FlashKAT相比当前最佳KAT实现训练速度提升86.5倍，并降低系数梯度的舍入误差。

Conclusion: FlashKAT有效解决了KAT的内存瓶颈问题，大幅提升训练效率，为大规模任务应用铺平道路。

Abstract: The Kolmogorov-Arnold Network (KAN) has been gaining popularity as an
alternative to the multi-layer perceptron (MLP) with its increased
expressiveness and interpretability. However, the KAN can be orders of
magnitude slower due to its increased computational cost and training
instability, limiting its applicability to larger-scale tasks. Recently, the
Kolmogorov-Arnold Transformer (KAT) has been proposed, which can achieve FLOPs
similar to the traditional Transformer with MLPs by leveraging Group-Rational
KAN (GR-KAN). Unfortunately, despite the comparable FLOPs, our
characterizations reveal that the KAT is still 123x slower in training speeds,
indicating that there are other performance bottlenecks beyond FLOPs. In this
paper, we conduct a series of experiments to understand the root cause of the
slowdown in KAT. We uncover that the slowdown can be isolated to memory stalls
and, more specifically, in the backward pass of GR-KAN caused by inefficient
gradient accumulation. To address this memory bottleneck, we propose FlashKAT,
which builds on our restructured kernel that minimizes gradient accumulation
with atomic adds and accesses to slow memory. Evaluations demonstrate that
FlashKAT can achieve a training speedup of 86.5x compared with the
state-of-the-art KAT, while reducing rounding errors in the coefficient
gradients. Our code is available at https://github.com/OSU-STARLAB/FlashKAT.

</details>


### [260] [Fragments to Facts: Partial-Information Fragment Inference from LLMs](https://arxiv.org/abs/2505.13819)
*Lucas Rosenblatt,Bin Han,Robert Wolfe,Bill Howe*

Main category: cs.LG

TL;DR: 该论文研究了在攻击者仅掌握部分无序样本信息的情况下，微调大语言模型(LLMs)对片段特定提取攻击的脆弱性，并提出了两种无需原始数据的方法进行系统性分析。


<details>
  <summary>Details</summary>
Motivation: 先前工作主要关注攻击者能获取完整样本或长有序前缀的强对抗假设，而忽略了攻击者仅掌握部分无序信息时的模型脆弱性问题。本文旨在探索这种较弱假设下的威胁模型。

Method: 提出了两种无需原始数据的方法：(1) 基于成员推理的似然比攻击；(2) 利用外部先验进行正则化的新方法PRISM。

Result: 在医疗和法律领域的案例研究表明，这两种方法与需要分布内标记数据的基线分类器性能相当，证明了方法的鲁棒性。

Conclusion: 研究表明微调LLMs容易受到片段特定提取攻击，即使在攻击者仅掌握部分信息的情况下，提出的数据无关方法能有效实施此类攻击。

Abstract: Large language models (LLMs) can leak sensitive training data through
memorization and membership inference attacks. Prior work has primarily focused
on strong adversarial assumptions, including attacker access to entire samples
or long, ordered prefixes, leaving open the question of how vulnerable LLMs are
when adversaries have only partial, unordered sample information. For example,
if an attacker knows a patient has "hypertension," under what conditions can
they query a model fine-tuned on patient data to learn the patient also has
"osteoarthritis?" In this paper, we introduce a more general threat model under
this weaker assumption and show that fine-tuned LLMs are susceptible to these
fragment-specific extraction attacks. To systematically investigate these
attacks, we propose two data-blind methods: (1) a likelihood ratio attack
inspired by methods from membership inference, and (2) a novel approach, PRISM,
which regularizes the ratio by leveraging an external prior. Using examples
from both medical and legal settings, we show that both methods are competitive
with a data-aware baseline classifier that assumes access to labeled
in-distribution data, underscoring their robustness.

</details>


### [261] [Structured Agent Distillation for Large Language Model](https://arxiv.org/abs/2505.13820)
*Jun Liu,Zhenglun Kong,Peiyan Dong,Changdi Yang,Tianqi Li,Hao Tang,Geng Yuan,Wei Niu,Wenbin Zhang,Pu Zhao,Xue Lin,Dong Huang,Yanzhi Wang*

Main category: cs.LG

TL;DR: 提出结构化智能体蒸馏框架，将大型语言模型智能体压缩为小型学生模型，保持推理和行动一致性，显著降低部署成本。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型作为决策智能体虽强，但推理成本高、模型体积大，限制了实际部署。

Method: 通过分割轨迹为{[REASON]}和{[ACT]}片段，应用片段特定损失函数，对齐师生模型行为。

Result: 在ALFWorld等任务上超越基线方法，实现高效压缩且性能下降最小。

Conclusion: 结构化蒸馏能有效压缩模型并保持决策能力，为可部署智能体提供解决方案。

Abstract: Large language models (LLMs) exhibit strong capabilities as decision-making
agents by interleaving reasoning and actions, as seen in ReAct-style
frameworks. Yet, their practical deployment is constrained by high inference
costs and large model sizes. We propose Structured Agent Distillation, a
framework that compresses large LLM-based agents into smaller student models
while preserving both reasoning fidelity and action consistency. Unlike
standard token-level distillation, our method segments trajectories into
{[REASON]} and {[ACT]} spans, applying segment-specific losses to align each
component with the teacher's behavior. This structure-aware supervision enables
compact agents to better replicate the teacher's decision process. Experiments
on ALFWorld, HotPotQA-ReAct, and WebShop show that our approach consistently
outperforms token-level and imitation learning baselines, achieving significant
compression with minimal performance drop. Scaling and ablation results further
highlight the importance of span-level alignment for efficient and deployable
agents.

</details>


### [262] [Rethink the Role of Deep Learning towards Large-scale Quantum Systems](https://arxiv.org/abs/2505.13852)
*Yusheng Zhao,Chi Zhang,Yuxuan Du*

Main category: cs.LG

TL;DR: 该论文通过系统比较深度学习和传统机器学习在量子系统基态学习任务中的表现，发现传统方法常能达到或超越深度学习的效果，质疑了当前深度学习在此领域的必要性。


<details>
  <summary>Details</summary>
Motivation: 量子系统基态性质的表征是理解其行为的基础，但计算上具有挑战性。尽管AI领域提出了多种机器学习和深度学习方法，但这些方法中深度学习的必要性和具体作用尚不明确，且以往研究常使用不一致或不实际的量子资源构建数据集，导致不公平比较。

Method: 研究者在三种哈密顿量家族上系统地对深度学习模型与传统机器学习方法进行了基准测试，将量子系统规模扩展至127个量子比特，并在三种关键的基态学习任务中确保使用相同的量子资源。

Result: 结果显示，在所有任务中，传统机器学习模型的性能通常与深度学习方法相当甚至更优。随机化测试进一步表明，测量输入特征对深度学习模型的预测性能影响微乎其微。

Conclusion: 这些发现对当前深度学习模型在许多量子系统学习场景中的必要性提出了质疑，并为其有效利用提供了有价值的见解。

Abstract: Characterizing the ground state properties of quantum systems is fundamental
to capturing their behavior but computationally challenging. Recent advances in
AI have introduced novel approaches, with diverse machine learning (ML) and
deep learning (DL) models proposed for this purpose. However, the necessity and
specific role of DL models in these tasks remain unclear, as prior studies
often employ varied or impractical quantum resources to construct datasets,
resulting in unfair comparisons. To address this, we systematically benchmark
DL models against traditional ML approaches across three families of
Hamiltonian, scaling up to 127 qubits in three crucial ground-state learning
tasks while enforcing equivalent quantum resource usage. Our results reveal
that ML models often achieve performance comparable to or even exceeding that
of DL approaches across all tasks. Furthermore, a randomization test
demonstrates that measurement input features have minimal impact on DL models'
prediction performance. These findings challenge the necessity of current DL
models in many quantum system learning scenarios and provide valuable insights
into their effective utilization.

</details>


### [263] [Learning Spatio-Temporal Dynamics for Trajectory Recovery via Time-Aware Transformer](https://arxiv.org/abs/2505.13857)
*Tian Sun,Yuqi Chen,Baihua Zheng,Weiwei Sun*

Main category: cs.LG

TL;DR: 提出TedTrajRec方法，通过PD-GNN和TedFormer分别捕捉交通和轨迹的时空动态，提升低采样率GPS轨迹的恢复效果。


<details>
  <summary>Details</summary>
Motivation: 现实应用中GPS轨迹常因采样率低、间隔不规则而难以直接使用，现有方法未能充分利用轨迹和路网的复杂时空动态。

Method: TedTrajRec结合PD-GNN建模周期性交通动态，以及TedFormer（集成神经ODE的Transformer）处理轨迹时空动态。

Result: 在三个真实数据集上的实验表明，TedTrajRec性能优越。

Conclusion: 通过分而治之的时空动态建模，TedTrajRec显著提升了低采样轨迹的恢复效果。

Abstract: In real-world applications, GPS trajectories often suffer from low sampling
rates, with large and irregular intervals between consecutive GPS points. This
sparse characteristic presents challenges for their direct use in GPS-based
systems. This paper addresses the task of map-constrained trajectory recovery,
aiming to enhance trajectory sampling rates of GPS trajectories. Previous
studies commonly adopt a sequence-to-sequence framework, where an encoder
captures the trajectory patterns and a decoder reconstructs the target
trajectory. Within this framework, effectively representing the road network
and extracting relevant trajectory features are crucial for overall
performance. Despite advancements in these models, they fail to fully leverage
the complex spatio-temporal dynamics present in both the trajectory and the
road network.
  To overcome these limitations, we categorize the spatio-temporal dynamics of
trajectory data into two distinct aspects: spatial-temporal traffic dynamics
and trajectory dynamics. Furthermore, We propose TedTrajRec, a novel method for
trajectory recovery. To capture spatio-temporal traffic dynamics, we introduce
PD-GNN, which models periodic patterns and learns topologically aware dynamics
concurrently for each road segment. For spatio-temporal trajectory dynamics, we
present TedFormer, a time-aware Transformer that incorporates temporal dynamics
for each GPS location by integrating closed-form neural ordinary differential
equations into the attention mechanism. This allows TedFormer to effectively
handle irregularly sampled data. Extensive experiments on three real-world
datasets demonstrate the superior performance of TedTrajRec. The code is
publicly available at https://github.com/ysygMhdxw/TEDTrajRec/.

</details>


### [264] [Enforcing Hard Linear Constraints in Deep Learning Models with Decision Rules](https://arxiv.org/abs/2505.13858)
*Gonzalo E. Constante-Flores,Hao Chen,Can Li*

Main category: cs.LG

TL;DR: 提出了一种模型无关的框架，确保神经网络输出满足输入相关的线性等式和不等式约束，结合任务网络和安全网络，保证约束满足且计算高效。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在安全关键任务中部署时，需满足硬性约束（如物理定律、公平性要求或安全限制），但现有方法在复杂约束、计算成本或可行性保证方面存在不足。

Method: 结合任务网络（预测准确性）和安全网络（基于随机和鲁棒优化的决策规则），通过凸组合确保输出在整个输入空间中满足约束，无需迭代或运行时优化。

Result: 在基准回归任务中，该方法能持续满足约束，同时保持竞争性精度和低推理延迟，并证明其是约束函数的通用逼近器。

Conclusion: 该框架提供了一种高效且可靠的方式，在训练和推理阶段均能保证约束满足，适用于需要硬性约束的安全关键应用。

Abstract: Deep learning models are increasingly deployed in safety-critical tasks where
predictions must satisfy hard constraints, such as physical laws, fairness
requirements, or safety limits. However, standard architectures lack built-in
mechanisms to enforce such constraints, and existing approaches based on
regularization or projection are often limited to simple constraints,
computationally expensive, or lack feasibility guarantees. This paper proposes
a model-agnostic framework for enforcing input-dependent linear equality and
inequality constraints on neural network outputs. The architecture combines a
task network trained for prediction accuracy with a safe network trained using
decision rules from the stochastic and robust optimization literature to ensure
feasibility across the entire input space. The final prediction is a convex
combination of the two subnetworks, guaranteeing constraint satisfaction during
both training and inference without iterative procedures or runtime
optimization. We prove that the architecture is a universal approximator of
constrained functions and derive computationally tractable formulations based
on linear decision rules. Empirical results on benchmark regression tasks show
that our method consistently satisfies constraints while maintaining
competitive accuracy and low inference latency.

</details>


### [265] [Utilizing Strategic Pre-training to Reduce Overfitting: Baguan -- A Pre-trained Weather Forecasting Model](https://arxiv.org/abs/2505.13873)
*Peisong Niu,Ziqing Ma,Tian Zhou,Weiqi Chen,Lefei Shen,Rong Jin,Liang Sun*

Main category: cs.LG

TL;DR: 论文提出Baguan模型，通过自监督预训练和微调解决天气预报中的过拟合问题，性能优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 天气预报数据有限，传统AI模型易过拟合，需创新方法提升预测精度。

Method: 使用Siamese Autoencoder进行自监督预训练，并针对不同预报时间微调。

Result: Baguan在中期预报、S2S建模和区域预报中表现优异，过拟合控制良好。

Conclusion: 预训练策略有效提升天气预报模型性能，泛化能力强。

Abstract: Weather forecasting has long posed a significant challenge for humanity.
While recent AI-based models have surpassed traditional numerical weather
prediction (NWP) methods in global forecasting tasks, overfitting remains a
critical issue due to the limited availability of real-world weather data
spanning only a few decades. Unlike fields like computer vision or natural
language processing, where data abundance can mitigate overfitting, weather
forecasting demands innovative strategies to address this challenge with
existing data. In this paper, we explore pre-training methods for weather
forecasting, finding that selecting an appropriately challenging pre-training
task introduces locality bias, effectively mitigating overfitting and enhancing
performance. We introduce Baguan, a novel data-driven model for medium-range
weather forecasting, built on a Siamese Autoencoder pre-trained in a
self-supervised manner and fine-tuned for different lead times. Experimental
results show that Baguan outperforms traditional methods, delivering more
accurate forecasts. Additionally, the pre-trained Baguan demonstrates robust
overfitting control and excels in downstream tasks, such as
subseasonal-to-seasonal (S2S) modeling and regional forecasting, after
fine-tuning.

</details>


### [266] [InfiFPO: Implicit Model Fusion via Preference Optimization in Large Language Models](https://arxiv.org/abs/2505.13878)
*Yanggan Gu,Zhaoyi Yan,Yuanyi Wang,Yiming Zhang,Qi Zhou,Fei Wu,Hongxia Yang*

Main category: cs.LG

TL;DR: InfiFPO是一种新型的偏好优化方法，通过融合多个大语言模型的概率信息，有效提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有模型融合方法主要关注监督微调阶段，而偏好对齐阶段的研究较少，且现有方法忽略了源模型的概率信息，限制了性能提升。

Method: 提出InfiFPO方法，通过序列级多源概率融合、概率裁剪和最大间隔融合策略，在避免复杂词汇对齐的同时保留概率信息。

Result: 在11个基准测试中，InfiFPO显著优于现有方法，将Phi-4模型的平均性能从79.95提升至83.33。

Conclusion: InfiFPO通过有效融合多源概率信息，显著提升了模型在数学、编程和推理任务中的能力。

Abstract: Model fusion combines multiple Large Language Models (LLMs) with different
strengths into a more powerful, integrated model through lightweight training
methods. Existing works on model fusion focus primarily on supervised
fine-tuning (SFT), leaving preference alignment (PA) --a critical phase for
enhancing LLM performance--largely unexplored. The current few fusion methods
on PA phase, like WRPO, simplify the process by utilizing only response outputs
from source models while discarding their probability information. To address
this limitation, we propose InfiFPO, a preference optimization method for
implicit model fusion. InfiFPO replaces the reference model in Direct
Preference Optimization (DPO) with a fused source model that synthesizes
multi-source probabilities at the sequence level, circumventing complex
vocabulary alignment challenges in previous works and meanwhile maintaining the
probability information. By introducing probability clipping and max-margin
fusion strategies, InfiFPO enables the pivot model to align with human
preferences while effectively distilling knowledge from source models.
Comprehensive experiments on 11 widely-used benchmarks demonstrate that InfiFPO
consistently outperforms existing model fusion and preference optimization
methods. When using Phi-4 as the pivot model, InfiFPO improve its average
performance from 79.95 to 83.33 on 11 benchmarks, significantly improving its
capabilities in mathematics, coding, and reasoning tasks.

</details>


### [267] [CRAFT: Time Series Forecasting with Cross-Future Behavior Awareness](https://arxiv.org/abs/2505.13896)
*Yingwei Zhang,Ke Bu,Zhuoran Zhuang,Tao Xie,Yao Yu,Dong Li,Yang Guo,Detao Lv*

Main category: cs.LG

TL;DR: 本文提出CRAFT方法，利用跨未来行为（CFB）特征解决时间序列预测中的不确定性难题，通过多个模块提取和补充趋势信息，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 时间序列预测（TSF）在实际应用中面临未来数据预测的不确定性难题，尤其是在过去观测数据有限的情况下。为了解决这一问题，本文探索了跨未来行为（CFB）在TSF中的应用。

Method: CRAFT方法通过Koopman预测模块提取关键趋势，内部趋势挖掘模块补充跨未来行为矩阵的未知区域，外部趋势引导模块获取更高层次的代表性趋势，并使用需求约束损失校准预测结果的分布偏差。

Result: 在离线和在线A/B测试中，CRAFT方法在真实数据集上表现出显著的有效性。

Conclusion: CRAFT方法通过利用跨未来行为特征和多个趋势挖掘模块，有效解决了时间序列预测中的不确定性难题，实验验证了其优越性能。

Abstract: The past decades witness the significant advancements in time series
forecasting (TSF) across various real-world domains, including e-commerce and
disease spread prediction. However, TSF is usually constrained by the
uncertainty dilemma of predicting future data with limited past observations.
To settle this question, we explore the use of Cross-Future Behavior (CFB) in
TSF, which occurs before the current time but takes effect in the future. We
leverage CFB features and propose the CRoss-Future Behavior Awareness based
Time Series Forecasting method (CRAFT). The core idea of CRAFT is to utilize
the trend of cross-future behavior to mine the trend of time series data to be
predicted. Specifically, to settle the sparse and partial flaws of cross-future
behavior, CRAFT employs the Koopman Predictor Module to extract the key trend
and the Internal Trend Mining Module to supplement the unknown area of the
cross-future behavior matrix. Then, we introduce the External Trend Guide
Module with a hierarchical structure to acquire more representative trends from
higher levels. Finally, we apply the demand-constrained loss to calibrate the
distribution deviation of prediction results. We conduct experiments on
real-world dataset. Experiments on both offline large-scale dataset and online
A/B test demonstrate the effectiveness of CRAFT. Our dataset and code is
available at https://github.com/CRAFTinTSF/CRAFT.

</details>


### [268] [Do Language Models Use Their Depth Efficiently?](https://arxiv.org/abs/2505.13898)
*Róbert Csordás,Christopher D. Manning,Christopher Potts*

Main category: cs.LG

TL;DR: 研究发现，更深的大语言模型并未有效利用其深度进行新型计算，而是将相同计算分散到更多层中，导致性能提升递减。


<details>
  <summary>Details</summary>
Motivation: 探讨现代大语言模型是否有效利用其深度进行更高级别的计算，还是仅仅将相同计算分散到更多层中。

Method: 通过分析Llama 3.1和Qwen 3系列模型的残差流，比较子层输出、跳过层的影响、多跳任务的表现，并训练浅层模型到深层模型的线性映射。

Result: 深层模型的第二半层贡献较小，跳过这些层对预测影响小；多跳任务中未发现深度用于子结果组合；深层模型仅将相同计算分散到更多层中。

Conclusion: 更深的大语言模型并未利用其深度学习新型计算，而是进行更细粒度的残差调整，这解释了堆叠Transformer架构性能提升递减的原因。

Abstract: Modern LLMs are increasingly deep, and depth correlates with performance,
albeit with diminishing returns. However, do these models use their depth
efficiently? Do they compose more features to create higher-order computations
that are impossible in shallow models, or do they merely spread the same kinds
of computation out over more layers? To address these questions, we analyze the
residual stream of the Llama 3.1 and Qwen 3 family of models. We find: First,
comparing the output of the sublayers to the residual stream reveals that
layers in the second half contribute much less than those in the first half,
with a clear phase transition between the two halves. Second, skipping layers
in the second half has a much smaller effect on future computations and output
predictions. Third, for multihop tasks, we are unable to find evidence that
models are using increased depth to compose subresults in examples involving
many hops. Fourth, we seek to directly address whether deeper models are using
their additional layers to perform new kinds of computation. To do this, we
train linear maps from the residual stream of a shallow model to a deeper one.
We find that layers with the same relative depth map best to each other,
suggesting that the larger model simply spreads the same computations out over
its many layers. All this evidence suggests that deeper models are not using
their depth to learn new kinds of computation, but only using the greater depth
to perform more fine-grained adjustments to the residual. This may help explain
why increasing scale leads to diminishing returns for stacked Transformer
architectures.

</details>


### [269] [Exploring Causes of Representational Similarity in Machine Learning Models](https://arxiv.org/abs/2505.13899)
*Zeyu Michael Li,Hung Anh Vu,Damilola Awofisayo,Emily Wenger*

Main category: cs.LG

TL;DR: 该论文探讨了数据集重叠和任务重叠如何影响机器学习模型的表示相似性，发现两者均能提高相似性，且结合时效果最强。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于理解不同模态机器学习模型表示相似性的原因，尤其是数据集重叠和任务重叠这两个潜在因素。

Method: 通过一系列实验评估数据集重叠和任务重叠对模型表示相似性的影响。

Result: 结果表明，数据集重叠和任务重叠均与更高的表示相似性正相关，且两者结合时效果最为显著。

Conclusion: 论文得出结论，数据集和任务的重叠是导致模型表示相似性的重要因素，这为理解模型行为提供了新视角。

Abstract: Numerous works have noted significant similarities in how machine learning
models represent the world, even across modalities. Although much effort has
been devoted to uncovering properties and metrics on which these models align,
surprisingly little work has explored causes of this similarity. To advance
this line of inquiry, this work explores how two possible causal factors --
dataset overlap and task overlap -- influence downstream model similarity. The
exploration of dataset overlap is motivated by the reality that large-scale
generative AI models are often trained on overlapping datasets of scraped
internet data, while the exploration of task overlap seeks to substantiate
claims from a recent work, the Platonic Representation Hypothesis, that task
similarity may drive model similarity. We evaluate the effects of both factors
through a broad set of experiments. We find that both positively correlate with
higher representational similarity and that combining them provides the
strongest effect. Our code and dataset are published.

</details>


### [270] [New Evidence of the Two-Phase Learning Dynamics of Neural Networks](https://arxiv.org/abs/2505.13900)
*Zhanpeng Zhou,Yongyi Yang,Mahito Sugiyama,Junchi Yan*

Main category: cs.LG

TL;DR: 该论文通过时间窗口比较网络状态，揭示了深度学习的双阶段特性：混沌效应和锥效应，表明网络从敏感探索到稳定优化的动态转变。


<details>
  <summary>Details</summary>
Motivation: 理解深度神经网络的学习机制是现代机器学习中的一个基本挑战。现有证据表明训练动态经历明显的相变，但对此的理解仍不完整。

Method: 论文引入了一种基于时间窗口的视角，通过微小参数扰动和跟踪经验神经切线核（eNTK）的演化来分析网络状态。

Result: 研究发现：i) 混沌效应显示网络早期对初始条件高度敏感；ii) 锥效应表明模型功能轨迹被限制在一个狭窄的锥形子集中。

Conclusion: 这些效应提供了深度网络在训练过程中从敏感探索到稳定优化的结构和动态视角。

Abstract: Understanding how deep neural networks learn remains a fundamental challenge
in modern machine learning. A growing body of evidence suggests that training
dynamics undergo a distinct phase transition, yet our understanding of this
transition is still incomplete. In this paper, we introduce an interval-wise
perspective that compares network states across a time window, revealing two
new phenomena that illuminate the two-phase nature of deep learning. i)
\textbf{The Chaos Effect.} By injecting an imperceptibly small parameter
perturbation at various stages, we show that the response of the network to the
perturbation exhibits a transition from chaotic to stable, suggesting there is
an early critical period where the network is highly sensitive to initial
conditions; ii) \textbf{The Cone Effect.} Tracking the evolution of the
empirical Neural Tangent Kernel (eNTK), we find that after this transition
point the model's functional trajectory is confined to a narrow cone-shaped
subset: while the kernel continues to change, it gets trapped into a tight
angular region. Together, these effects provide a structural, dynamical view of
how deep networks transition from sensitive exploration to stable refinement
during training.

</details>


### [271] [Learning to Insert for Constructive Neural Vehicle Routing Solver](https://arxiv.org/abs/2505.13904)
*Fu Luo,Xi Lin,Mengyuan Zhong,Fei Liu,Zhenkun Wang,Jianyong Sun,Qingfu Zhang*

Main category: cs.LG

TL;DR: 本文提出了一种基于插入范式的新型学习方法L2C-Insert，用于神经组合优化（NCO），通过灵活插入节点提升车辆路径问题（VRP）的求解质量。


<details>
  <summary>Details</summary>
Motivation: 现有构造性NCO方法通常采用顺序添加节点的范式，导致结果次优。为克服这一局限，作者探索了插入式范式的潜力。

Method: 提出L2C-Insert框架，包含三个核心组件：精准插入位置预测的模型架构、高效训练方案、以及充分利用插入灵活性的推理技术。

Result: 在TSP和CVRP的合成及真实实例上验证，L2C-Insert在不同规模问题上均表现出优越性能。

Conclusion: 插入式范式显著提高了NCO方法的灵活性和求解质量，为组合优化问题提供了新的解决思路。

Abstract: Neural Combinatorial Optimisation (NCO) is a promising learning-based
approach for solving Vehicle Routing Problems (VRPs) without extensive manual
design. While existing constructive NCO methods typically follow an
appending-based paradigm that sequentially adds unvisited nodes to partial
solutions, this rigid approach often leads to suboptimal results. To overcome
this limitation, we explore the idea of insertion-based paradigm and propose
Learning to Construct with Insertion-based Paradigm (L2C-Insert), a novel
learning-based method for constructive NCO. Unlike traditional approaches,
L2C-Insert builds solutions by strategically inserting unvisited nodes at any
valid position in the current partial solution, which can significantly enhance
the flexibility and solution quality. The proposed framework introduces three
key components: a novel model architecture for precise insertion position
prediction, an efficient training scheme for model optimization, and an
advanced inference technique that fully exploits the insertion paradigm's
flexibility. Extensive experiments on both synthetic and real-world instances
of the Travelling Salesman Problem (TSP) and Capacitated Vehicle Routing
Problem (CVRP) demonstrate that L2C-Insert consistently achieves superior
performance across various problem sizes.

</details>


### [272] [Cross-Domain Diffusion with Progressive Alignment for Efficient Adaptive Retrieval](https://arxiv.org/abs/2505.13907)
*Junyu Luo,Yusheng Zhao,Xiao Luo,Zhiping Xiao,Wei Ju,Li Shen,Dacheng Tao,Ming Zhang*

Main category: cs.LG

TL;DR: 本文提出了一种名为COUPLE的新方法，通过图扩散和渐进对齐技术，解决了无监督高效领域自适应检索中的噪声问题和跨域特征对齐难题。


<details>
  <summary>Details</summary>
Motivation: 现有的无监督高效领域自适应检索方法通常无法处理目标域中的潜在噪声，并且直接对齐跨域的高级特征，导致检索性能不佳。

Method: COUPLE方法通过构建跨域关系图，利用噪声鲁棒的图流扩散模拟源域到目标域的转移动态，识别低噪声簇，并结合分层Mixup操作进行渐进式域对齐。

Result: 大量实验表明，COUPLE在竞争性基准测试中表现出色，有效提升了领域自适应哈希学习的性能。

Conclusion: COUPLE方法通过噪声鲁棒的图扩散和渐进式对齐，显著提升了无监督高效领域自适应检索的效果。

Abstract: Unsupervised efficient domain adaptive retrieval aims to transfer knowledge
from a labeled source domain to an unlabeled target domain, while maintaining
low storage cost and high retrieval efficiency. However, existing methods
typically fail to address potential noise in the target domain, and directly
align high-level features across domains, thus resulting in suboptimal
retrieval performance. To address these challenges, we propose a novel
Cross-Domain Diffusion with Progressive Alignment method (COUPLE). This
approach revisits unsupervised efficient domain adaptive retrieval from a graph
diffusion perspective, simulating cross-domain adaptation dynamics to achieve a
stable target domain adaptation process. First, we construct a cross-domain
relationship graph and leverage noise-robust graph flow diffusion to simulate
the transfer dynamics from the source domain to the target domain, identifying
lower noise clusters. We then leverage the graph diffusion results for
discriminative hash code learning, effectively learning from the target domain
while reducing the negative impact of noise. Furthermore, we employ a
hierarchical Mixup operation for progressive domain alignment, which is
performed along the cross-domain random walk paths. Utilizing target domain
discriminative hash learning and progressive domain alignment, COUPLE enables
effective domain adaptive hash learning. Extensive experiments demonstrate
COUPLE's effectiveness on competitive benchmarks.

</details>


### [273] [ShortcutProbe: Probing Prediction Shortcuts for Learning Robust Models](https://arxiv.org/abs/2505.13910)
*Guangtao Zheng,Wenqian Ye,Aidong Zhang*

Main category: cs.LG

TL;DR: 论文提出了一种无需人工标注的深度学习模型后处理框架ShortcutProbe，通过识别潜在空间中的预测捷径并重新训练模型以提高鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型常因学习目标与非关键特征间的伪相关而产生偏差，现有方法依赖昂贵的人工标注且难以捕捉细微偏差。

Method: 提出ShortcutProbe框架，在模型潜在空间中识别预测捷径，并通过重新训练使模型对这些捷径具有不变性。

Result: 理论分析和实验表明，该框架能有效提升模型对伪相关偏差的鲁棒性，适用于多种数据集。

Conclusion: ShortcutProbe是一种高效实用的工具，无需人工标注即可显著提升模型鲁棒性。

Abstract: Deep learning models often achieve high performance by inadvertently learning
spurious correlations between targets and non-essential features. For example,
an image classifier may identify an object via its background that spuriously
correlates with it. This prediction behavior, known as spurious bias, severely
degrades model performance on data that lacks the learned spurious
correlations. Existing methods on spurious bias mitigation typically require a
variety of data groups with spurious correlation annotations called group
labels. However, group labels require costly human annotations and often fail
to capture subtle spurious biases such as relying on specific pixels for
predictions. In this paper, we propose a novel post hoc spurious bias
mitigation framework without requiring group labels. Our framework, termed
ShortcutProbe, identifies prediction shortcuts that reflect potential
non-robustness in predictions in a given model's latent space. The model is
then retrained to be invariant to the identified prediction shortcuts for
improved robustness. We theoretically analyze the effectiveness of the
framework and empirically demonstrate that it is an efficient and practical
tool for improving a model's robustness to spurious bias on diverse datasets.

</details>


### [274] [RLVR-World: Training World Models with Reinforcement Learning](https://arxiv.org/abs/2505.13934)
*Jialong Wu,Shaofeng Yin,Ningya Feng,Mingsheng Long*

Main category: cs.LG

TL;DR: RLVR-World框架通过强化学习直接优化世界模型的任务指标，显著提升了语言和视频模型的性能。


<details>
  <summary>Details</summary>
Motivation: 传统最大似然估计训练目标与世界模型的任务指标（如准确性或感知质量）不一致，需要更直接优化的方法。

Method: 提出RLVR-World框架，利用可验证奖励的强化学习（RLVR）直接优化世界模型的任务指标。

Result: 在文本游戏、网页导航和机器人操作等领域，语言和视频世界模型的性能显著提升。

Conclusion: RLVR作为一种后训练范式，有望广泛提升生成模型的实用性。

Abstract: World models predict state transitions in response to actions and are
increasingly developed across diverse modalities. However, standard training
objectives such as maximum likelihood estimation (MLE) often misalign with
task-specific goals of world models, i.e., transition prediction metrics like
accuracy or perceptual quality. In this paper, we present RLVR-World, a unified
framework that leverages reinforcement learning with verifiable rewards (RLVR)
to directly optimize world models for such metrics. Despite formulating world
modeling as autoregressive prediction of tokenized sequences, RLVR-World
evaluates metrics of decoded predictions as verifiable rewards. We demonstrate
substantial performance gains on both language- and video-based world models
across domains, including text games, web navigation, and robot manipulation.
Our work indicates that, beyond recent advances in reasoning language models,
RLVR offers a promising post-training paradigm for enhancing the utility of
generative models more broadly.

</details>


### [275] [CLEVER: A Curated Benchmark for Formally Verified Code Generation](https://arxiv.org/abs/2505.13938)
*Amitayush Thakur,Jasper Lee,George Tsoukalas,Meghana Sistla,Matthew Zhao,Stefan Zetzche,Greg Durrett,Yisong Yue,Swarat Chaudhuri*

Main category: cs.LG

TL;DR: 论文介绍了CLEVER，一个高质量的、经过筛选的161个问题的基准测试集，用于在Lean中进行端到端验证的代码生成。


<details>
  <summary>Details</summary>
Motivation: 为了解决现有基准测试中存在的问题，如测试用例监督、LLM生成的注释、规范泄露实现逻辑或允许空泛解决方案，作者提出了CLEVER基准测试集。

Method: CLEVER基准测试集包含两个任务：生成与保留的真实规范匹配的规范，以及生成可证明满足此规范的Lean实现。所有输出都通过Lean的类型检查器进行后验验证。

Result: 使用CLEVER评估了几种基于最先进语言模型的少样本和代理方法，这些方法在实现完全验证方面都遇到了困难。

Conclusion: CLEVER为程序合成和形式推理提供了一个具有挑战性的前沿基准测试集。

Abstract: We introduce ${\rm C{\small LEVER}}$, a high-quality, curated benchmark of
161 problems for end-to-end verified code generation in Lean. Each problem
consists of (1) the task of generating a specification that matches a held-out
ground-truth specification, and (2) the task of generating a Lean
implementation that provably satisfies this specification. Unlike prior
benchmarks, ${\rm C{\small LEVER}}$ avoids test-case supervision, LLM-generated
annotations, and specifications that leak implementation logic or allow vacuous
solutions. All outputs are verified post-hoc using Lean's type checker to
ensure machine-checkable correctness. We use ${\rm C{\small LEVER}}$ to
evaluate several few-shot and agentic approaches based on state-of-the-art
language models. These methods all struggle to achieve full verification,
establishing it as a challenging frontier benchmark for program synthesis and
formal reasoning. Our benchmark can be found on
GitHub(https://github.com/trishullab/clever) as well as
HuggingFace(https://huggingface.co/datasets/amitayusht/clever). All our
evaluation code is also available
online(https://github.com/trishullab/clever-prover).

</details>


### [276] [VAMO: Efficient Large-Scale Nonconvex Optimization via Adaptive Zeroth Order Variance Reduction](https://arxiv.org/abs/2505.13954)
*Jiahe Chen,Ziye Ma*

Main category: cs.LG

TL;DR: VAMO是一种结合一阶和零阶梯度优化的混合方法，通过方差减少技术提高收敛速度，适用于大规模非凸优化问题。


<details>
  <summary>Details</summary>
Motivation: 大规模非凸优化问题在机器学习中常见，但传统一阶方法计算成本高，零阶方法收敛慢。需要一种平衡收敛速度和计算效率的优化方法。

Method: 提出VAMO方法，结合一阶小批量梯度和零阶有限差分估计，采用SVRG框架，实现维度无关的收敛速度。

Result: VAMO在传统神经网络训练和LLM微调中表现优于现有方法，收敛速度更快，计算效率更高。

Conclusion: VAMO为计算受限场景提供了一种高效、灵活的优化方案，显著提升了大模型训练的效率和收敛速度。

Abstract: Optimizing large-scale nonconvex problems, common in machine learning,
demands balancing rapid convergence with computational efficiency. First-order
(FO) stochastic methods like SVRG provide fast convergence and good
generalization but incur high costs due to full-batch gradients in large
models. Conversely, zeroth-order (ZO) algorithms reduce this burden using
estimated gradients, yet their slow convergence in high-dimensional settings
limits practicality. We introduce VAMO (VAriance-reduced Mixed-gradient
Optimizer), a stochastic variance-reduced method combining FO mini-batch
gradients with lightweight ZO finite-difference probes under an SVRG-style
framework. VAMO's hybrid design uses a two-point ZO estimator to achieve a
dimension-agnostic convergence rate of $\mathcal{O}(1/T + 1/b)$, where $T$ is
the number of iterations and $b$ is the batch-size, surpassing the
dimension-dependent slowdown of purely ZO methods and significantly improving
over SGD's $\mathcal{O}(1/\sqrt{T})$ rate. Additionally, we propose a
multi-point ZO variant that mitigates the $O(1/b)$ error by adjusting number of
estimation points to balance convergence and cost, making it ideal for a whole
range of computationally constrained scenarios. Experiments including
traditional neural network training and LLM finetuning show VAMO outperforms
established FO and ZO methods, offering a faster, more flexible option for
improved efficiency.

</details>


### [277] [When LLMs meet open-world graph learning: a new perspective for unlabeled data uncertainty](https://arxiv.org/abs/2505.13989)
*Yanzhe Wen,Xunkai Li,Qi Zhang,Zhu Lei,Guang Zeng,Rong-Hua Li,Guoren Wang*

Main category: cs.LG

TL;DR: 提出基于大语言模型的开放世界图学习框架OGA，解决标签不足和未知节点分类问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在开放世界场景中处理数据不确定性不足，特别是针对有限标签和未知类别节点的问题。

Method: 提出OGA框架，结合自适应标签可追溯性（融合语义与拓扑结构）和图标签标注器，支持模型更新。

Result: 综合实验验证了OGA的有效性和实用性。

Conclusion: OGA框架有效解决了开放世界图学习中的关键挑战。

Abstract: Recently, large language models (LLMs) have significantly advanced
text-attributed graph (TAG) learning. However, existing methods inadequately
handle data uncertainty in open-world scenarios, especially concerning limited
labeling and unknown-class nodes. Prior solutions typically rely on isolated
semantic or structural approaches for unknown-class rejection, lacking
effective annotation pipelines. To address these limitations, we propose
Open-world Graph Assistant (OGA), an LLM-based framework that combines adaptive
label traceability, which integrates semantics and topology for unknown-class
rejection, and a graph label annotator to enable model updates using newly
annotated nodes. Comprehensive experiments demonstrate OGA's effectiveness and
practicality.

</details>


### [278] [Towards Comprehensive and Prerequisite-Free Explainer for Graph Neural Networks](https://arxiv.org/abs/2505.14005)
*Han Zhang,Yan Wang,Guanfeng Liu,Pengfei Ding,Huaxiong Wang,Kwok-Yan Lam*

Main category: cs.LG

TL;DR: 提出OPEN方法，解决现有GNN可解释性方法在泛化性和前提条件上的局限性，通过分区样本空间和学习不同分布下的决策逻辑，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有GNN可解释性方法（XGNN）存在两大局限：无法捕捉全局样本空间中不同分布的完整决策逻辑，以及对边属性和GNN内部访问的严格前提条件。这限制了方法的性能和泛化能力。

Method: 提出OPEN方法，首次实现样本空间的多环境分区（每个环境包含不同分布的图数据），并通过采样子图分析预测结果，无需严格前提条件即可学习GNN的跨分布决策逻辑。

Result: 实验表明OPEN能捕捉近乎完整的GNN决策逻辑，在保真度上超越现有方法且效率相当，并增强了实际场景的鲁棒性。

Conclusion: OPEN作为首个无前提条件的综合性GNN解释器，通过环境分区和跨分布学习，显著提升了可解释性方法的完备性和实用性。

Abstract: To enhance the reliability and credibility of graph neural networks (GNNs)
and improve the transparency of their decision logic, a new field of
explainability of GNNs (XGNN) has emerged. However, two major limitations
severely degrade the performance and hinder the generalizability of existing
XGNN methods: they (a) fail to capture the complete decision logic of GNNs
across diverse distributions in the entire dataset's sample space, and (b)
impose strict prerequisites on edge properties and GNN internal accessibility.
To address these limitations, we propose OPEN, a novel c\textbf{O}mprehensive
and \textbf{P}rerequisite-free \textbf{E}xplainer for G\textbf{N}Ns. OPEN, as
the first work in the literature, can infer and partition the entire dataset's
sample space into multiple environments, each containing graphs that follow a
distinct distribution. OPEN further learns the decision logic of GNNs across
different distributions by sampling subgraphs from each environment and
analyzing their predictions, thus eliminating the need for strict
prerequisites. Experimental results demonstrate that OPEN captures nearly
complete decision logic of GNNs, outperforms state-of-the-art methods in
fidelity while maintaining similar efficiency, and enhances robustness in
real-world scenarios.

</details>


### [279] [Adaptive Sentencing Prediction with Guaranteed Accuracy and Legal Interpretability](https://arxiv.org/abs/2505.14011)
*Yifei Jin,Xin Zheng,Lei Guo*

Main category: cs.LG

TL;DR: 该论文提出了一种基于中国刑法、具有法律解释性的饱和机制量刑（SMS）模型，并开发了相应的动量最小均方（MLMS）自适应算法，通过理论分析和实验验证了其预测准确性和适用性。


<details>
  <summary>Details</summary>
Motivation: 现有司法量刑预测研究多依赖端到端模型，忽视了量刑逻辑且缺乏解释性，无法满足学术研究和司法实践的需求。

Method: 提出SMS模型和MLMS自适应算法，建立无需数据平稳性和独立性假设的预测准确性数学理论，并构建了中国故意伤害罪（CIBH）数据集进行验证。

Result: 实验表明，该方法的预测准确性接近理论最优上界，验证了模型的适用性和算法的准确性。

Conclusion: SMS模型结合MLMS算法在保持法律解释性的同时，实现了接近理论极限的预测准确性，为司法量刑预测提供了新思路。

Abstract: Existing research on judicial sentencing prediction predominantly relies on
end-to-end models, which often neglect the inherent sentencing logic and lack
interpretability-a critical requirement for both scholarly research and
judicial practice. To address this challenge, we make three key
contributions:First, we propose a novel Saturated Mechanistic Sentencing (SMS)
model, which provides inherent legal interpretability by virtue of its
foundation in China's Criminal Law. We also introduce the corresponding
Momentum Least Mean Squares (MLMS) adaptive algorithm for this model. Second,
for the MLMS algorithm based adaptive sentencing predictor, we establish a
mathematical theory on the accuracy of adaptive prediction without resorting to
any stationarity and independence assumptions on the data. We also provide a
best possible upper bound for the prediction accuracy achievable by the best
predictor designed in the known parameters case. Third, we construct a Chinese
Intentional Bodily Harm (CIBH) dataset. Utilizing this real-world data,
extensive experiments demonstrate that our approach achieves a prediction
accuracy that is not far from the best possible theoretical upper bound,
validating both the model's suitability and the algorithm's accuracy.

</details>


### [280] [Adversarial Training from Mean Field Perspective](https://arxiv.org/abs/2505.14021)
*Soichiro Kumano,Hiroshi Kera,Toshihiko Yamasaki*

Main category: cs.LG

TL;DR: 该论文首次在无数据分布假设的随机深度神经网络中，基于平均场理论框架，对对抗训练进行了理论分析，推导了对抗损失的紧上界，并揭示了网络结构对对抗训练的影响。


<details>
  <summary>Details</summary>
Motivation: 尽管对抗训练在防御对抗样本方面表现出色，但其训练动态机制尚未被充分理解。本研究旨在填补这一理论空白，特别是在无数据分布假设的情况下，深入探讨对抗训练的内在机理。

Method: 研究引入了一种基于平均场理论的新理论框架，克服了现有方法的局限性。通过该框架，分析了不同范数（ℓ_p和ℓ_q）下对抗损失的上界，并探讨了网络结构（如是否有捷径连接）和宽度对训练的影响。

Result: 论文推导了对抗损失的紧上界，证明了无捷径连接的网络通常无法进行对抗训练，且对抗训练会降低网络容量。同时发现增加网络宽度可以缓解这些问题，并揭示了输入输出维度对上界及权重方差时间演化的影响。

Conclusion: 该研究为对抗训练提供了理论基础，表明网络结构设计（如增加宽度）对提升对抗训练效果至关重要，并为未来研究提供了新的理论框架和分析工具。

Abstract: Although adversarial training is known to be effective against adversarial
examples, training dynamics are not well understood. In this study, we present
the first theoretical analysis of adversarial training in random deep neural
networks without any assumptions on data distributions. We introduce a new
theoretical framework based on mean field theory, which addresses the
limitations of existing mean field-based approaches. Based on this framework,
we derive (empirically tight) upper bounds of $\ell_q$ norm-based adversarial
loss with $\ell_p$ norm-based adversarial examples for various values of $p$
and $q$. Moreover, we prove that networks without shortcuts are generally not
adversarially trainable and that adversarial training reduces network capacity.
We also show that network width alleviates these issues. Furthermore, we
present the various impacts of the input and output dimensions on the upper
bounds and time evolution of the weight variance.

</details>


### [281] [FedGraM: Defending Against Untargeted Attacks in Federated Learning via Embedding Gram Matrix](https://arxiv.org/abs/2505.14024)
*Di Wu,Qian Li,Heng Yang,Yong Han*

Main category: cs.LG

TL;DR: 该论文提出了一种名为FedGraM的新型鲁棒聚合方法，用于检测和消除联邦学习中的无目标攻击，通过利用辅助数据集和Gram矩阵范数来筛选恶意模型，实验证明其优于现有防御方法。


<details>
  <summary>Details</summary>
Motivation: 联邦学习（FL）在保护数据隐私的同时，容易受到旨在降低全局模型性能的无目标攻击。现有防御机制因数据异构性在实际环境中效果有限，因此需要更有效的方法来检测和消除这些攻击。

Method: 论文提出FedGraM方法，服务器维护一个包含每类一个样本的辅助数据集，通过计算本地模型嵌入的Gram矩阵范数来评估其类间分离能力，并过滤掉范数最大的潜在恶意模型，然后对剩余模型进行平均以形成全局模型。

Result: 实验结果表明，FedGraM在使用有限数据样本构建辅助数据集的情况下，表现出色，优于现有的最先进防御方法。

Conclusion: FedGraM通过利用辅助数据集和Gram矩阵范数，有效检测和消除了联邦学习中的无目标攻击，显著提升了模型的鲁棒性和性能。

Abstract: Federated Learning (FL) enables geographically distributed clients to
collaboratively train machine learning models by sharing only their local
models, ensuring data privacy. However, FL is vulnerable to untargeted attacks
that aim to degrade the global model's performance on the underlying data
distribution. Existing defense mechanisms attempt to improve FL's resilience
against such attacks, but their effectiveness is limited in practical FL
environments due to data heterogeneity. On the contrary, we aim to detect and
remove the attacks to mitigate their impact. Generalization contribution plays
a crucial role in distinguishing untargeted attacks. Our observations indicate
that, with limited data, the divergence between embeddings representing
different classes provides a better measure of generalization than direct
accuracy. In light of this, we propose a novel robust aggregation method,
FedGraM, designed to defend against untargeted attacks in FL. The server
maintains an auxiliary dataset containing one sample per class to support
aggregation. This dataset is fed to the local models to extract embeddings.
Then, the server calculates the norm of the Gram Matrix of the embeddings for
each local model. The norm serves as an indicator of each model's inter-class
separation capability in the embedding space. FedGraM identifies and removes
potentially malicious models by filtering out those with the largest norms,
then averages the remaining local models to form the global model. We conduct
extensive experiments to evaluate the performance of FedGraM. Our empirical
results show that with limited data samples used to construct the auxiliary
dataset, FedGraM achieves exceptional performance, outperforming
state-of-the-art defense methods.

</details>


### [282] [Partition-wise Graph Filtering: A Unified Perspective Through the Lens of Graph Coarsening](https://arxiv.org/abs/2505.14033)
*Guoming Li,Jian Yang,Yifan Chen*

Main category: cs.LG

TL;DR: 本文提出了一种基于粗化引导的分区过滤方法（CPF），通过结合图级和节点级过滤策略，解决了异构图分类中的过拟合问题。


<details>
  <summary>Details</summary>
Motivation: 传统图神经网络（GNNs）采用统一的图级过滤策略，难以处理异构图。虽然节点级过滤提高了适应性，但缺乏统一框架且易导致过拟合。

Method: 提出CPF方法，通过图粗化算法和k-means聚类生成节点分区，分别进行结构感知和特征感知的分区过滤。

Result: 实验证明CPF在节点分类和图异常检测任务中优于其他方法，展示了其理论优势和实际应用价值。

Conclusion: CPF通过分区过滤有效结合了图级和节点级策略，为图数据分类提供了更优解决方案。

Abstract: Filtering-based graph neural networks (GNNs) constitute a distinct class of
GNNs that employ graph filters to handle graph-structured data, achieving
notable success in various graph-related tasks. Conventional methods adopt a
graph-wise filtering paradigm, imposing a uniform filter across all nodes, yet
recent findings suggest that this rigid paradigm struggles with heterophilic
graphs. To overcome this, recent works have introduced node-wise filtering,
which assigns distinct filters to individual nodes, offering enhanced
adaptability. However, a fundamental gap remains: a comprehensive framework
unifying these two strategies is still absent, limiting theoretical insights
into the filtering paradigms. Moreover, through the lens of Contextual
Stochastic Block Model, we reveal that a synthesis of graph-wise and node-wise
filtering provides a sufficient solution for classification on graphs
exhibiting both homophily and heterophily, suggesting the risk of excessive
parameterization and potential overfitting with node-wise filtering. To address
the limitations, this paper introduces Coarsening-guided Partition-wise
Filtering (CPF). CPF innovates by performing filtering on node partitions. The
method begins with structure-aware partition-wise filtering, which filters node
partitions obtained via graph coarsening algorithms, and then performs
feature-aware partition-wise filtering, refining node embeddings via filtering
on clusters produced by $k$-means clustering over features. In-depth analysis
is conducted for each phase of CPF, showing its superiority over other
paradigms. Finally, benchmark node classification experiments, along with a
real-world graph anomaly detection application, validate CPF's efficacy and
practical utility.

</details>


### [283] [Adaptive Cyclic Diffusion for Inference Scaling](https://arxiv.org/abs/2505.14036)
*Gyubin Lee,Truong Nhat Nguyen Bao,Jaesik Yoon,Dongwoo Lee,Minsu Kim,Yoshua Bengio,Sungjin Ahn*

Main category: cs.LG

TL;DR: 提出自适应双向循环扩散框架ABCD，通过动态调整计算资源提升扩散模型推理效率。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型推理时采用固定去噪计划，无法根据实例难度或任务需求动态分配计算资源。

Method: ABCD框架包含三个组件：循环扩散搜索、自动探索-利用平衡和自适应思考时间，实现双向扩散和动态计算控制。

Result: 实验表明ABCD在多种任务上提升性能，同时保持计算效率。

Conclusion: ABCD为扩散模型推理提供了灵活高效的自适应计算分配方案。

Abstract: Diffusion models have demonstrated strong generative capabilities across
domains ranging from image synthesis to complex reasoning tasks. However, most
inference-time scaling methods rely on fixed denoising schedules, limiting
their ability to allocate computation based on instance difficulty or
task-specific demands adaptively. We introduce the challenge of adaptive
inference-time scaling-dynamically adjusting computational effort during
inference-and propose Adaptive Bi-directional Cyclic Diffusion (ABCD), a
flexible, search-based inference framework. ABCD refines outputs through
bi-directional diffusion cycles while adaptively controlling exploration depth
and termination. It comprises three components: Cyclic Diffusion Search,
Automatic Exploration-Exploitation Balancing, and Adaptive Thinking Time.
Experiments show that ABCD improves performance across diverse tasks while
maintaining computational efficiency.

</details>


### [284] [Model Steering: Learning with a Reference Model Improves Generalization Bounds and Scaling Laws](https://arxiv.org/abs/2505.06699)
*Xiyuan Wei,Ming Lin,Fanjiang Ye,Fengguang Song,Liangliang Cao,My T. Thai,Tianbao Yang*

Main category: cs.LG

TL;DR: 该论文提出了一种名为‘模型引导’的学习范式，通过参考模型指导目标模型的训练，提升数据效率和泛化能力。基于DRO理论提出了DRRho风险最小化框架，并在CLIP任务中验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在模型引导训练中缺乏理论支持，导致性能不佳。论文旨在通过理论分析填补这一空白，提升模型训练的效率和效果。

Method: 提出了基于分布鲁棒优化（DRO）的DRRho风险最小化框架，并将其应用于对比学习（CLIP）任务，开发了DRRho-CLIP方法。

Result: 理论分析证实了该方法的泛化优势，实验表明DRRho-CLIP在数据效率和扩展性上优于无参考模型的CLIP及现有启发式方法。

Conclusion: 论文首次为模型引导学习提供了理论依据，并通过DRRho框架和CLIP应用验证了其实际价值，推动了该领域的理解和实践。

Abstract: This paper formalizes an emerging learning paradigm that uses a trained model
as a reference to guide and enhance the training of a target model through
strategic data selection or weighting, named $\textbf{model steering}$. While
ad-hoc methods have been used in various contexts, including the training of
large foundation models, its underlying principles remain insufficiently
understood, leading to sub-optimal performance. In this work, we propose a
theory-driven framework for model steering called $\textbf{DRRho risk
minimization}$, which is rooted in Distributionally Robust Optimization (DRO).
Through a generalization analysis, we provide theoretical insights into why
this approach improves generalization and data efficiency compared to training
without a reference model. To the best of our knowledge, this is the first time
such theoretical insights are provided for the new learning paradigm, which
significantly enhance our understanding and practice of model steering.
Building on these insights and the connection between contrastive learning and
DRO, we introduce a novel method for Contrastive Language-Image Pretraining
(CLIP) with a reference model, termed DRRho-CLIP. Extensive experiments
validate the theoretical insights, reveal a superior scaling law compared to
CLIP without a reference model, and demonstrate its strength over existing
heuristic approaches.

</details>


### [285] [Learning High-dimensional Ionic Model Dynamics Using Fourier Neural Operators](https://arxiv.org/abs/2505.14039)
*Luca Pellegrini,Massimiliano Ghiotto,Edoardo Centofanti,Luca Franco Pavarino*

Main category: cs.LG

TL;DR: 该研究探讨了傅里叶神经算子（FNO）在高维离子模型动力学学习中的有效性，通过自动超参数调优在无约束和约束条件下成功预测了多个经典模型的复杂动态行为。


<details>
  <summary>Details</summary>
Motivation: 离子模型因其刚性和多尺度非线性特性，传统人工神经网络难以准确模拟其复杂动态。本研究旨在验证FNO能否有效学习高维离子模型中所有状态变量的演化。

Method: 采用傅里叶神经算子，对FitzHugh-Nagumo（2变量）、Hodgkin-Huxley（4变量）和O'Hara-Rudy（41变量）三个模型进行训练，并通过自动超参数调优比较无约束/约束架构的性能。

Result: 两种架构在所有模型上均达到相当精度，但无约束架构训练周期减少约50%。FNO成功捕捉了高维系统的多尺度动态特性。

Conclusion: 傅里叶神经算子能准确建模高维离子模型的复杂动力学，为计算神经科学和心脏病学模拟提供了新工具。

Abstract: Ionic models, described by systems of stiff ordinary differential equations,
are fundamental tools for simulating the complex dynamics of excitable cells in
both Computational Neuroscience and Cardiology. Approximating these models
using Artificial Neural Networks poses significant challenges due to their
inherent stiffness, multiscale nonlinearities, and the wide range of dynamical
behaviors they exhibit, including multiple equilibrium points, limit cycles,
and intricate interactions. While in previous studies the dynamics of the
transmembrane potential has been predicted in low dimensionality settings, in
the present study we extend these results by investigating whether Fourier
Neural Operators can effectively learn the evolution of all the state variables
within these dynamical systems in higher dimensions. We demonstrate the
effectiveness of this approach by accurately learning the dynamics of three
well-established ionic models with increasing dimensionality: the two-variable
FitzHugh-Nagumo model, the four-variable Hodgkin-Huxley model, and the
forty-one-variable O'Hara-Rudy model. To ensure the selection of near-optimal
configurations for the Fourier Neural Operator, we conducted automatic
hyperparameter tuning under two scenarios: an unconstrained setting, where the
number of trainable parameters is not limited, and a constrained case with a
fixed number of trainable parameters. Both constrained and unconstrained
architectures achieve comparable results in terms of accuracy across all the
models considered. However, the unconstrained architecture required
approximately half the number of training epochs to achieve similar error
levels, as evidenced by the loss function values recorded during training.
These results underline the capabilities of Fourier Neural Operators to
accurately capture complex multiscale dynamics, even in high-dimensional
dynamical systems.

</details>


### [286] [Unsupervised Graph Clustering with Deep Structural Entropy](https://arxiv.org/abs/2505.14040)
*Jingyun Zhang,Hao Peng,Li Sun,Guanlin Wu,Chunyang Liu,Zhengtao Yu*

Main category: cs.LG

TL;DR: 论文提出了一种名为DeSE的无监督图聚类框架，通过深度结构熵增强原始图结构，解决了现有方法在稀疏或噪声图上的性能下降问题。


<details>
  <summary>Details</summary>
Motivation: 当前基于图神经网络的聚类方法过度依赖原始图结构，当图过于稀疏或包含噪声边时性能显著下降。此外，这些方法依赖传统聚类技术（如k-means），可能无法充分捕捉节点间的潜在结构关系。

Method: 1. 提出可微分软分配结构熵计算方法；2. 设计结构学习层（SLL）从原始特征生成属性图以优化原始结构；3. 基于GNN的聚类分配层（ASS）通过最小化结构熵和边交叉熵损失实现稳定聚类。

Result: 在四个基准数据集上对比八种无监督图聚类基线，DeSE在效果和可解释性上均表现出优越性。

Conclusion: DeSE框架通过深度结构熵和端到端优化，显著提升了稀疏/噪声图上的聚类性能，为图结构学习提供了新思路。

Abstract: Research on Graph Structure Learning (GSL) provides key insights for
graph-based clustering, yet current methods like Graph Neural Networks (GNNs),
Graph Attention Networks (GATs), and contrastive learning often rely heavily on
the original graph structure. Their performance deteriorates when the original
graph's adjacency matrix is too sparse or contains noisy edges unrelated to
clustering. Moreover, these methods depend on learning node embeddings and
using traditional techniques like k-means to form clusters, which may not fully
capture the underlying graph structure between nodes. To address these
limitations, this paper introduces DeSE, a novel unsupervised graph clustering
framework incorporating Deep Structural Entropy. It enhances the original graph
with quantified structural information and deep neural networks to form
clusters. Specifically, we first propose a method for calculating structural
entropy with soft assignment, which quantifies structure in a differentiable
form. Next, we design a Structural Learning layer (SLL) to generate an
attributed graph from the original feature data, serving as a target to enhance
and optimize the original structural graph, thereby mitigating the issue of
sparse connections between graph nodes. Finally, our clustering assignment
method (ASS), based on GNNs, learns node embeddings and a soft assignment
matrix to cluster on the enhanced graph. The ASS layer can be stacked to meet
downstream task requirements, minimizing structural entropy for stable
clustering and maximizing node consistency with edge-based cross-entropy loss.
Extensive comparative experiments are conducted on four benchmark datasets
against eight representative unsupervised graph clustering baselines,
demonstrating the superiority of the DeSE in both effectiveness and
interpretability.

</details>


### [287] [Adversarially Pretrained Transformers may be Universally Robust In-Context Learners](https://arxiv.org/abs/2505.14042)
*Soichiro Kumano,Hiroshi Kera,Toshihiko Yamasaki*

Main category: cs.LG

TL;DR: 对抗训练是一种有效的防御方法，但计算成本高。本文提出通过对抗预训练的Transformer模型，利用上下文学习实现下游任务的鲁棒性，无需额外训练。


<details>
  <summary>Details</summary>
Motivation: 对抗训练虽然有效，但计算成本高昂。研究旨在探索是否可以通过对抗预训练的Transformer模型，在无需额外训练的情况下，实现下游任务的鲁棒性。

Method: 通过理论分析，证明对抗预训练的Transformer模型可以通过上下文学习，在没有参数更新的情况下，鲁棒地泛化到多个未见任务。

Result: 对抗预训练的Transformer模型能够专注于鲁棒特征，抵抗利用非预测性特征的攻击。但也存在局限性，如在某些条件下不存在普遍鲁棒的单层Transformer，且存在准确性与鲁棒性的权衡。

Conclusion: 对抗预训练的Transformer模型可以作为鲁棒的基础模型，减少下游任务的对抗训练需求，但仍需解决准确性与鲁棒性的权衡及上下文演示数量的问题。

Abstract: Adversarial training is one of the most effective adversarial defenses, but
it incurs a high computational cost. In this study, we show that transformers
adversarially pretrained on diverse tasks can serve as robust foundation models
and eliminate the need for adversarial training in downstream tasks.
Specifically, we theoretically demonstrate that through in-context learning, a
single adversarially pretrained transformer can robustly generalize to multiple
unseen tasks without any additional training, i.e., without any parameter
updates. This robustness stems from the model's focus on robust features and
its resistance to attacks that exploit non-predictive features. Besides these
positive findings, we also identify several limitations. Under certain
conditions (though unrealistic), no universally robust single-layer
transformers exist. Moreover, robust transformers exhibit an
accuracy--robustness trade-off and require a large number of in-context
demonstrations. The code is available at
https://github.com/s-kumano/universally-robust-in-context-learner.

</details>


### [288] [Generalized Category Discovery via Token Manifold Capacity Learning](https://arxiv.org/abs/2505.14044)
*Luyao Tang,Kunze Huang,Chaoqi Chen,Cheng Chen*

Main category: cs.LG

TL;DR: 提出MTMC方法，通过最大化类令牌的流形容量来提升GCD性能，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统GCD方法因过度最小化簇内差异而牺牲流形容量，限制了类内表示的丰富性。

Method: MTMC利用奇异值的核范数衡量流形容量，最大化类令牌的多样性以保留数据结构。

Result: 在粗细粒度数据集上验证，MTMC提升了聚类精度和类别数量估计能力。

Conclusion: MTMC增强了表示完整性、类间可分性并减少维度塌缩，是开放世界学习的关键组件。

Abstract: Generalized category discovery (GCD) is essential for improving deep learning
models' robustness in open-world scenarios by clustering unlabeled data
containing both known and novel categories. Traditional GCD methods focus on
minimizing intra-cluster variations, often sacrificing manifold capacity, which
limits the richness of intra-class representations. In this paper, we propose a
novel approach, Maximum Token Manifold Capacity (MTMC), that prioritizes
maximizing the manifold capacity of class tokens to preserve the diversity and
complexity of data. MTMC leverages the nuclear norm of singular values as a
measure of manifold capacity, ensuring that the representation of samples
remains informative and well-structured. This method enhances the
discriminability of clusters, allowing the model to capture detailed semantic
features and avoid the loss of critical information during clustering. Through
theoretical analysis and extensive experiments on coarse- and fine-grained
datasets, we demonstrate that MTMC outperforms existing GCD methods, improving
both clustering accuracy and the estimation of category numbers. The
integration of MTMC leads to more complete representations, better inter-class
separability, and a reduction in dimensional collapse, establishing MTMC as a
vital component for robust open-world learning. Code is in
github.com/lytang63/MTMC.

</details>


### [289] [Textual Steering Vectors Can Improve Visual Understanding in Multimodal Large Language Models](https://arxiv.org/abs/2505.14071)
*Woody Haosheng Gan,Deqing Fu,Julian Asilis,Ollie Liu,Dani Yogatama,Vatsal Sharan,Robin Jia,Willie Neiswanger*

Main category: cs.LG

TL;DR: 该研究探索了如何利用文本导向向量提升多模态大语言模型（MLLMs）的性能，无需修改模型参数，取得了显著效果。


<details>
  <summary>Details</summary>
Motivation: 当前，大型语言模型（LLMs）已有有效的导向方法，但多模态大语言模型（MLLMs）由于出现较晚和架构多样性，缺乏类似技术。研究旨在填补这一空白。

Method: 研究采用稀疏自编码器（SAEs）、均值漂移和线性探测等方法，从MLLMs的文本LLM骨干中提取向量进行导向。

Result: 文本导向显著提升了MLLMs在多模态任务中的准确性，特别是在空间关系和计数任务上表现突出，均值漂移方法在CV-Bench上分别提升了7.3%和3.3%。

Conclusion: 文本导向向量是一种高效、低成本的机制，能够显著增强MLLMs的基础性能，且无需大量额外数据收集和计算开销。

Abstract: Steering methods have emerged as effective and targeted tools for guiding
large language models' (LLMs) behavior without modifying their parameters.
Multimodal large language models (MLLMs), however, do not currently enjoy the
same suite of techniques, due in part to their recency and architectural
diversity. Inspired by this gap, we investigate whether MLLMs can be steered
using vectors derived from their text-only LLM backbone, via sparse
autoencoders (SAEs), mean shift, and linear probing. We find that text-derived
steering consistently enhances multimodal accuracy across diverse MLLM
architectures and visual tasks. In particular, mean shift boosts spatial
relationship accuracy on CV-Bench by up to +7.3% and counting accuracy by up to
+3.3%, outperforming prompting and exhibiting strong generalization to
out-of-distribution datasets. These results highlight textual steering vectors
as a powerful, efficient mechanism for enhancing grounding in MLLMs with
minimal additional data collection and computational overhead.

</details>


### [290] [Collaborative Unlabeled Data Optimization](https://arxiv.org/abs/2505.14117)
*Xinyi Shang,Peng Sun,Fengyuan Liu,Tao Lin*

Main category: cs.LG

TL;DR: 该论文提出了一种名为CoOpt的新型数据优化框架，通过将知识编码到数据本身，解决了深度学习训练中数据利用效率低的问题，显著提升了模型性能与训练速度。


<details>
  <summary>Details</summary>
Motivation: 现有模型中心化方法存在知识锁定在模型参数中、难以复用和扩展的问题，限制了深度学习的效率和可持续性。

Method: 提出CoOpt框架，通过分布式处理未标记数据并利用公开的任务无关模型，实现高效、可扩展的数据优化。

Result: 在Tiny-ImageNet和ImageNet-1K上分别实现了13.6%和6.8%的性能提升，训练速度分别加快1.94倍和1.2倍。

Conclusion: CoOpt通过数据优化显著提升了深度学习训练的效率和可持续性，为数据中心的深度学习范式提供了新思路。

Abstract: This paper pioneers a novel data-centric paradigm to maximize the utility of
unlabeled data, tackling a critical question: How can we enhance the efficiency
and sustainability of deep learning training by optimizing the data itself? We
begin by identifying three key limitations in existing model-centric
approaches, all rooted in a shared bottleneck: knowledge extracted from data is
locked to model parameters, hindering its reusability and scalability. To this
end, we propose CoOpt, a highly efficient, parallelized framework for
collaborative unlabeled data optimization, thereby effectively encoding
knowledge into the data itself. By distributing unlabeled data and leveraging
publicly available task-agnostic models, CoOpt facilitates scalable, reusable,
and sustainable training pipelines. Extensive experiments across diverse
datasets and architectures demonstrate its efficacy and efficiency, achieving
13.6% and 6.8% improvements on Tiny-ImageNet and ImageNet-1K, respectively,
with training speedups of $1.94 \times $ and $1.2 \times$.

</details>


### [291] [Assessing wildfire susceptibility in Iran: Leveraging machine learning for geospatial analysis of climatic and anthropogenic factors](https://arxiv.org/abs/2505.14122)
*Ehsan Masoudian,Ali Mirzaei,Hossein Bagheri*

Main category: cs.LG

TL;DR: 该研究利用遥感、GIS和机器学习技术，分析了气候与人为因素对伊朗野火风险的影响，发现人为因素在季节性分析中影响更大，并生成了高分辨率火灾风险地图。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探讨气候条件和人类活动如何共同影响伊朗的野火风险，以填补该地区火灾动态研究的空白。

Method: 采用遥感、GIS处理技术（如云计算）和机器学习算法，结合多场景数据采样策略进行分析。

Result: 气候因素（土壤湿度、温度、湿度）和人为因素（人口密度、电力线距离）均显著影响火灾风险，但季节性分析中人为因素影响更突出。高风险区域包括扎格罗斯中部、希尔卡尼亚森林东北部和阿拉斯巴兰森林北部。

Conclusion: 研究通过高分辨率火灾风险地图揭示了伊朗火灾动态，强调了制定有效火灾管理策略的紧迫性。

Abstract: This study investigates the multifaceted factors influencing wildfire risk in
Iran, focusing on the interplay between climatic conditions and human
activities. Utilizing advanced remote sensing, geospatial information system
(GIS) processing techniques such as cloud computing, and machine learning
algorithms, this research analyzed the impact of climatic parameters,
topographic features, and human-related factors on wildfire susceptibility
assessment and prediction in Iran. Multiple scenarios were developed for this
purpose based on the data sampling strategy. The findings revealed that
climatic elements such as soil moisture, temperature, and humidity
significantly contribute to wildfire susceptibility, while human
activities-particularly population density and proximity to powerlines-also
played a crucial role. Furthermore, the seasonal impact of each parameter was
separately assessed during warm and cold seasons. The results indicated that
human-related factors, rather than climatic variables, had a more prominent
influence during the seasonal analyses. This research provided new insights
into wildfire dynamics in Iran by generating high-resolution wildfire
susceptibility maps using advanced machine learning classifiers. The generated
maps identified high risk areas, particularly in the central Zagros region, the
northeastern Hyrcanian Forest, and the northern Arasbaran forest, highlighting
the urgent need for effective fire management strategies.

</details>


### [292] [Contrastive Consolidation of Top-Down Modulations Achieves Sparsely Supervised Continual Learning](https://arxiv.org/abs/2505.14125)
*Viet Anh Khoa Tran,Emre Neftci,Willem. A. M. Wybo*

Main category: cs.LG

TL;DR: 论文提出任务调制对比学习（TMCL），受大脑新皮层生物物理机制启发，通过预测编码原理无监督整合自上而下信息，解决机器学习中的灾难性遗忘问题，并在少量标注数据下实现类增量学习和迁移学习的性能提升。


<details>
  <summary>Details</summary>
Motivation: 生物大脑能持续从无标注数据流中学习，并整合稀疏标注的专有信息而不损害泛化能力。相比之下，机器学习方法在这种自然学习场景下容易因监督微调导致灾难性遗忘。论文旨在模仿大脑机制，实现稳定与可塑性的平衡。

Method: 提出TMCL框架：1）利用对比损失构建视图不变表征空间；2）通过新类标注数据学习仿射调制参数以增强新类分离性（不修改前馈权重）；3）通过匹配调制/未调制表征引入调制不变性，并利用历史调制稳定表征空间。

Result: 实验表明：在仅1%标注数据下，TMCL在类增量学习和迁移学习中性能优于最先进的无监督方法及可比监督方法，同时缓解了灾难性遗忘问题。

Conclusion: 自上而下调制机制对平衡稳定性与可塑性至关重要，TMCL通过神经科学启发的设计为持续学习提供了新思路。

Abstract: Biological brains learn continually from a stream of unlabeled data, while
integrating specialized information from sparsely labeled examples without
compromising their ability to generalize. Meanwhile, machine learning methods
are susceptible to catastrophic forgetting in this natural learning setting, as
supervised specialist fine-tuning degrades performance on the original task. We
introduce task-modulated contrastive learning (TMCL), which takes inspiration
from the biophysical machinery in the neocortex, using predictive coding
principles to integrate top-down information continually and without
supervision. We follow the idea that these principles build a view-invariant
representation space, and that this can be implemented using a contrastive
loss. Then, whenever labeled samples of a new class occur, new affine
modulations are learned that improve separation of the new class from all
others, without affecting feedforward weights. By co-opting the view-invariance
learning mechanism, we then train feedforward weights to match the unmodulated
representation of a data sample to its modulated counterparts. This introduces
modulation invariance into the representation space, and, by also using past
modulations, stabilizes it. Our experiments show improvements in both
class-incremental and transfer learning over state-of-the-art unsupervised
approaches, as well as over comparable supervised approaches, using as few as
1% of available labels. Taken together, our work suggests that top-down
modulations play a crucial role in balancing stability and plasticity.

</details>


### [293] [MAS-KCL: Knowledge component graph structure learning with large language model-based agentic workflow](https://arxiv.org/abs/2505.14126)
*Yuan-Hao Jiang,Kezong Tang,Zi-Wei Chen,Yuang Wei,Tian-Yi Liu,Jiayi Wu*

Main category: cs.LG

TL;DR: 提出了一种基于多智能体系统的知识组件图结构学习算法MAS-KCL，通过双向反馈机制优化学习路径识别，提升教育可持续性。


<details>
  <summary>Details</summary>
Motivation: 准确的KC图能帮助教育者定位学习者表现不佳的根本原因，从而实施针对性教学干预。

Method: 开发了MAS-KCL算法，利用多智能体系统和双向反馈机制优化KC图结构，调整边生成概率分布。

Result: 在5个合成数据集和4个真实教育数据集上验证了算法在学习路径识别中的有效性。

Conclusion: 通过精确识别学习路径，教师能设计更全面的学习计划，促进教育可持续发展。

Abstract: Knowledge components (KCs) are the fundamental units of knowledge in the
field of education. A KC graph illustrates the relationships and dependencies
between KCs. An accurate KC graph can assist educators in identifying the root
causes of learners' poor performance on specific KCs, thereby enabling targeted
instructional interventions. To achieve this, we have developed a KC graph
structure learning algorithm, named MAS-KCL, which employs a multi-agent system
driven by large language models for adaptive modification and optimization of
the KC graph. Additionally, a bidirectional feedback mechanism is integrated
into the algorithm, where AI agents leverage this mechanism to assess the value
of edges within the KC graph and adjust the distribution of generation
probabilities for different edges, thereby accelerating the efficiency of
structure learning. We applied the proposed algorithm to 5 synthetic datasets
and 4 real-world educational datasets, and experimental results validate its
effectiveness in learning path recognition. By accurately identifying learners'
learning paths, teachers are able to design more comprehensive learning plans,
enabling learners to achieve their educational goals more effectively, thus
promoting the sustainable development of education.

</details>


### [294] [A Methodological Framework for Measuring Spatial Labeling Similarity](https://arxiv.org/abs/2505.14128)
*Yihang Du,Jiaying Hu,Suyang Hou,Yueyang Ding,Xiaobo Sun*

Main category: cs.LG

TL;DR: 提出了一种名为SLAM的框架，用于全面评估空间标记的相似性，考虑了标签匹配、拓扑结构和异质性影响。


<details>
  <summary>Details</summary>
Motivation: 现有方法在评估空间标记相似性时，常忽略标签匹配、拓扑结构和异质性影响，导致评估不全面。

Method: 将空间标记转化为图结构，提取属性分布，计算分布差异以反映标记间的差异，并实现SLAM方法。

Result: 通过模拟和真实数据实验，证明SLAM比其他评估指标更全面、准确地反映标记质量。

Conclusion: SLAM为空间标记相似性评估提供了更全面的方法，适用于空间转录组学等领域。

Abstract: Spatial labeling assigns labels to specific spatial locations to characterize
their spatial properties and relationships, with broad applications in
scientific research and practice. Measuring the similarity between two spatial
labelings is essential for understanding their differences and the contributing
factors, such as changes in location properties or labeling methods. An
adequate and unbiased measurement of spatial labeling similarity should
consider the number of matched labels (label agreement), the topology of
spatial label distribution, and the heterogeneous impacts of mismatched labels.
However, existing methods often fail to account for all these aspects. To
address this gap, we propose a methodological framework to guide the
development of methods that meet these requirements. Given two spatial
labelings, the framework transforms them into graphs based on location
organization, labels, and attributes (e.g., location significance). The
distributions of their graph attributes are then extracted, enabling an
efficient computation of distributional discrepancy to reflect the
dissimilarity level between the two labelings. We further provide a concrete
implementation of this framework, termed Spatial Labeling Analogy Metric
(SLAM), along with an analysis of its theoretical foundation, for evaluating
spatial labeling results in spatial transcriptomics (ST) \textit{as per} their
similarity with ground truth labeling. Through a series of carefully designed
experimental cases involving both simulated and real ST data, we demonstrate
that SLAM provides a comprehensive and accurate reflection of labeling quality
compared to other well-established evaluation metrics. Our code is available at
https://github.com/YihDu/SLAM.

</details>


### [295] [Local Mixtures of Experts: Essentially Free Test-Time Training via Model Merging](https://arxiv.org/abs/2505.14136)
*Ryo Bertolissi,Jonas Hübotter,Ido Hakimi,Andreas Krause*

Main category: cs.LG

TL;DR: 提出TTMM方法，通过模型合并扩展MoE模型的专家数量，降低推理成本，接近TTT性能但速度更快。


<details>
  <summary>Details</summary>
Motivation: 当前MoE模型因训练和推理成本高而仅使用少量专家，限制了模型容量。TTT虽能提升性能但计算昂贵。

Method: 提出测试时模型合并（TTMM），在训练时分摊TTT成本，扩展专家数量并减少测试时开销。

Result: TTMM性能随专家数量增加接近TTT，且1B参数基础模型下测试速度快100倍以上。

Conclusion: TTMM为扩展测试时训练提供了一种高效低成本的方法。

Abstract: Mixture of expert (MoE) models are a promising approach to increasing model
capacity without increasing inference cost, and are core components of many
state-of-the-art language models. However, current MoE models typically use
only few experts due to prohibitive training and inference cost. We propose
Test-Time Model Merging (TTMM) which scales the MoE paradigm to an order of
magnitude more experts and uses model merging to avoid almost any test-time
overhead. We show that TTMM is an approximation of test-time training (TTT),
which fine-tunes an expert model for each prediction task, i.e., prompt. TTT
has recently been shown to significantly improve language models, but is
computationally expensive. We find that performance of TTMM improves with more
experts and approaches the performance of TTT. Moreover, we find that with a 1B
parameter base model, TTMM is more than 100x faster than TTT at test-time by
amortizing the cost of TTT at train-time. Thus, TTMM offers a promising
cost-effective approach to scale test-time training.

</details>


### [296] [FlowQ: Energy-Guided Flow Policies for Offline Reinforcement Learning](https://arxiv.org/abs/2505.14139)
*Marvin Alles,Nutan Chen,Patrick van der Smagt,Botond Cseke*

Main category: cs.LG

TL;DR: 提出了一种名为能量引导流匹配的新方法，用于增强流模型的训练，并在推理时无需引导。该方法在离线强化学习中表现出色。


<details>
  <summary>Details</summary>
Motivation: 当前在扩散模型中，引导采样已被广泛研究，但在训练过程中引入引导的研究较少。本文旨在探索如何在训练阶段有效利用引导，以提升模型性能。

Method: 通过将能量引导的概率路径近似为高斯路径，学习条件速度场，提出FlowQ算法，基于能量引导流匹配进行离线强化学习。

Result: 该方法在性能上具有竞争力，且策略训练时间与流采样步骤数无关，保持恒定。

Conclusion: 能量引导流匹配是一种有效的训练方法，能够提升流模型性能，并在离线强化学习中展现出潜力。

Abstract: The use of guidance to steer sampling toward desired outcomes has been widely
explored within diffusion models, especially in applications such as image and
trajectory generation. However, incorporating guidance during training remains
relatively underexplored. In this work, we introduce energy-guided flow
matching, a novel approach that enhances the training of flow models and
eliminates the need for guidance at inference time. We learn a conditional
velocity field corresponding to the flow policy by approximating an
energy-guided probability path as a Gaussian path. Learning guided trajectories
is appealing for tasks where the target distribution is defined by a
combination of data and an energy function, as in reinforcement learning.
Diffusion-based policies have recently attracted attention for their expressive
power and ability to capture multi-modal action distributions. Typically, these
policies are optimized using weighted objectives or by back-propagating
gradients through actions sampled by the policy. As an alternative, we propose
FlowQ, an offline reinforcement learning algorithm based on energy-guided flow
matching. Our method achieves competitive performance while the policy training
time is constant in the number of flow sampling steps.

</details>


### [297] [Personalized Bayesian Federated Learning with Wasserstein Barycenter Aggregation](https://arxiv.org/abs/2505.14161)
*Ting Wei,Biao Mei,Junliang Lyu,Renquan Zhang,Feng Zhou,Yifan Sun*

Main category: cs.LG

TL;DR: FedWBA是一种新型的个性化贝叶斯联邦学习方法，通过非参数后验表示和Wasserstein重心聚合，提升了预测准确性、不确定性校准和收敛速度。


<details>
  <summary>Details</summary>
Motivation: 现有的个性化贝叶斯联邦学习方法在客户端后验推断中存在参数假设限制，且在服务器聚合时采用简单参数平均，存在局限性。

Method: FedWBA在客户端使用基于粒子的变分推断进行非参数后验表示，在服务器端引入基于粒子的Wasserstein重心聚合方法。

Result: 理论证明FedWBA具有局部和全局收敛保证，实验显示其在预测准确性、不确定性校准和收敛速度上优于基线方法。

Conclusion: FedWBA通过改进局部推断和全局聚合，有效解决了现有方法的局限性，展现出优越的性能和鲁棒性。

Abstract: Personalized Bayesian federated learning (PBFL) handles non-i.i.d. client
data and quantifies uncertainty by combining personalization with Bayesian
inference. However, existing PBFL methods face two limitations: restrictive
parametric assumptions in client posterior inference and naive parameter
averaging for server aggregation. To overcome these issues, we propose FedWBA,
a novel PBFL method that enhances both local inference and global aggregation.
At the client level, we use particle-based variational inference for
nonparametric posterior representation. At the server level, we introduce
particle-based Wasserstein barycenter aggregation, offering a more
geometrically meaningful approach. Theoretically, we provide local and global
convergence guarantees for FedWBA. Locally, we prove a KL divergence decrease
lower bound per iteration for variational inference convergence. Globally, we
show that the Wasserstein barycenter converges to the true parameter as the
client data size increases. Empirically, experiments show that FedWBA
outperforms baselines in prediction accuracy, uncertainty calibration, and
convergence rate, with ablation studies confirming its robustness.

</details>


### [298] [Nonparametric Teaching for Graph Property Learners](https://arxiv.org/abs/2505.14170)
*Chen Zhang,Weixin Bu,Zeyi Ren,Zhengwu Liu,Yik-Chung Wu,Ngai Wong*

Main category: cs.LG

TL;DR: 论文提出了一种名为GraNT的新范式，通过非参数化教学视角重新解释图结构数据的学习过程，显著提升了图卷积网络（GCNs）的训练效率。


<details>
  <summary>Details</summary>
Motivation: 图结构数据（如分子溶解度）的属性推断通常需要学习从图到其属性的隐式映射，这一过程对图卷积网络等学习器来说成本高昂。为解决这一问题，作者提出了GraNT方法。

Method: GraNT通过非参数化教学框架，选择图-属性对的子集来促进GCN训练的更快收敛。该方法分析了图结构对梯度下降的影响，并将GCN的参数更新重新解释为非参数化教学中的函数梯度下降。

Result: 实验结果显示，GraNT在图级回归、图级分类、节点级回归和节点级分类任务中分别减少了36.62%、38.19%、30.97%和47.30%的训练时间，同时保持了泛化性能。

Conclusion: 研究表明，GraNT方法在提升图属性学习器（如GCNs）的学习效率方面具有显著效果，同时保持了模型的泛化能力。

Abstract: Inferring properties of graph-structured data, e.g., the solubility of
molecules, essentially involves learning the implicit mapping from graphs to
their properties. This learning process is often costly for graph property
learners like Graph Convolutional Networks (GCNs). To address this, we propose
a paradigm called Graph Neural Teaching (GraNT) that reinterprets the learning
process through a novel nonparametric teaching perspective. Specifically, the
latter offers a theoretical framework for teaching implicitly defined (i.e.,
nonparametric) mappings via example selection. Such an implicit mapping is
realized by a dense set of graph-property pairs, with the GraNT teacher
selecting a subset of them to promote faster convergence in GCN training. By
analytically examining the impact of graph structure on parameter-based
gradient descent during training, and recasting the evolution of GCNs--shaped
by parameter updates--through functional gradient descent in nonparametric
teaching, we show for the first time that teaching graph property learners
(i.e., GCNs) is consistent with teaching structure-aware nonparametric
learners. These new findings readily commit GraNT to enhancing learning
efficiency of the graph property learner, showing significant reductions in
training time for graph-level regression (-36.62%), graph-level classification
(-38.19%), node-level regression (-30.97%) and node-level classification
(-47.30%), all while maintaining its generalization performance.

</details>


### [299] [Safety Subspaces are Not Distinct: A Fine-Tuning Case Study](https://arxiv.org/abs/2505.14185)
*Kaustubh Ponkshe,Shaan Shah,Raghav Singhal,Praneeth Vepakomma*

Main category: cs.LG

TL;DR: 研究发现大语言模型的安全对齐行为并非集中在特定子空间，而是与模型整体学习动态高度纠缠，挑战了子空间防御的可行性。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型通过安全对齐生成符合社会规范的输出，但这种对齐易受微调破坏。已有研究假设安全行为对应权重空间的特定几何方向，本工作旨在验证这一假设。

Method: 在参数空间和激活空间进行系统性实验，分析安全相关行为是否集中在特定子空间，并测试五种开源大语言模型。

Result: 安全与不安全行为共享相同子空间，不同安全属性的提示激活重叠表征，未发现专控安全的独立子空间。

Conclusion: 安全对齐具有全局性而非局部几何特征，基于子空间的防御方法存在根本局限，需开发新策略维持持续训练中的模型对齐。

Abstract: Large Language Models (LLMs) rely on safety alignment to produce socially
acceptable responses. This is typically achieved through instruction tuning and
reinforcement learning from human feedback. However, this alignment is known to
be brittle: further fine-tuning, even on benign or lightly contaminated data,
can degrade safety and reintroduce harmful behaviors. A growing body of work
suggests that alignment may correspond to identifiable geometric directions in
weight space, forming subspaces that could, in principle, be isolated or
preserved to defend against misalignment. In this work, we conduct a
comprehensive empirical study of this geometric perspective. We examine whether
safety-relevant behavior is concentrated in specific subspaces, whether it can
be separated from general-purpose learning, and whether harmfulness arises from
distinguishable patterns in internal representations. Across both parameter and
activation space, our findings are consistent: subspaces that amplify safe
behaviors also amplify unsafe ones, and prompts with different safety
implications activate overlapping representations. We find no evidence of a
subspace that selectively governs safety. These results challenge the
assumption that alignment is geometrically localized. Rather than residing in
distinct directions, safety appears to emerge from entangled, high-impact
components of the model's broader learning dynamics. This suggests that
subspace-based defenses may face fundamental limitations and underscores the
need for alternative strategies to preserve alignment under continued training.
We corroborate these findings through multiple experiments on five open-source
LLMs. Our code is publicly available at:
https://github.com/CERT-Lab/safety-subspaces.

</details>


### [300] [$α$-GAN by Rényi Cross Entropy](https://arxiv.org/abs/2505.14190)
*Ni Ding,Miao Qiao,Jiaxing Xu,Yiping Ke,Xiaoyu Zhang*

Main category: cs.LG

TL;DR: 该论文提出了一种基于Rényi度量的生成对抗网络α-GAN，通过Rényi交叉熵构建价值函数，优化生成器和判别器的性能，并在α∈(0,1)时加速收敛。


<details>
  <summary>Details</summary>
Motivation: 传统GAN在训练过程中可能遇到梯度消失等问题，作者希望通过引入Rényi度量来改进GAN的性能，尤其是在梯度优化方面。

Method: 使用Rényi交叉熵构建价值函数，形成生成器和判别器之间的min-max问题，并通过调整Rényi阶数α来优化模型。

Result: 实验结果表明，当α∈(0,1)时，梯度被指数级放大，从而加速收敛，并可能解决梯度消失等常见问题。

Conclusion: α-GAN在α=1时退化为传统GAN，但在α∈(0,1)时表现出更快的收敛速度和更好的优化性能，这一范围在现有Rényi版本GAN中尚未充分探索。

Abstract: This paper proposes $\alpha$-GAN, a generative adversarial network using
R\'{e}nyi measures. The value function is formulated, by R\'{e}nyi cross
entropy, as an expected certainty measure incurred by the discriminator's soft
decision as to where the sample is from, true population or the generator. The
discriminator tries to maximize the R\'{e}nyi certainty about sample source,
while the generator wants to reduce it by injecting fake samples. This forms a
min-max problem with the solution parameterized by the R\'{e}nyi order
$\alpha$. This $\alpha$-GAN reduces to vanilla GAN at $\alpha = 1$, where the
value function is exactly the binary cross entropy. The optimization of
$\alpha$-GAN is over probability (vector) space. It is shown that the gradient
is exponentially enlarged when R\'{e}nyi order is in the range $\alpha \in
(0,1)$. This makes convergence faster, which is verified by experimental
results. A discussion shows that choosing $\alpha \in (0,1)$ may be able to
solve some common problems, e.g., vanishing gradient. A following observation
reveals that this range has not been fully explored in the existing R\'{e}nyi
version GANs.

</details>


### [301] [FLASH-D: FlashAttention with Hidden Softmax Division](https://arxiv.org/abs/2505.14201)
*Kosmas Alexandridis,Vasileios Titopoulos,Giorgos Dimitrakopoulos*

Main category: cs.LG

TL;DR: FLASH-D简化了FlashAttention核心计算，通过数学等效优化实现硬件面积和功耗降低，同时保持性能不变。


<details>
  <summary>Details</summary>
Motivation: 传统Transformer注意力机制中的softmax计算存在效率瓶颈，FlashAttention虽优化了GPU计算，但仍有进一步简化和硬件加速的空间。

Method: 提出FLASH-D，通过隐藏softmax除法、稳定计算指数函数及减少计算成本，简化FlashAttention核心计算，保持其分块计算特性。

Result: 在28nm工艺下，FLASH-D相比现有并行硬件架构平均减少22.8%面积和20.3%功耗，且无性能损失。

Conclusion: FLASH-D通过数学等效优化显著提升硬件效率，为注意力机制的高效硬件实现提供了新方向。

Abstract: The transformer's attention mechanism has revolutionized AI and machine
learning, with its efficient computation being crucial to its performance.
However, calculating attention involves matrix operations interspersed with
softmax rescaling, which inherently slows down computation and requires
processing the entire input sequence. Building on online softmax computation,
FlashAttention integrates softmax calculation with matrix arithmetic, enabling
tiled computation independent of sequence length. While optimized for GPUs,
FlashAttention's simplicity makes it amenable to direct hardware acceleration.
This work re-evaluates the core FlashAttention kernel, presenting FLASH-D a
mathematically equivalent, yet simplified, formulation that achieves: (a)
hiding softmax division within other non-linear function evaluations; (b)
inherently numerically stable computation of exponentials, eliminating the need
for maximum value subtraction; and (c) a reduction in computational cost
without introducing numerical approximations to the FlashAttention kernel.
Importantly, the essential FlashAttention properties that facilitate efficient
tiled implementation are fully preserved. Hardware implementation results at
28nm demonstrate that this proposed formulation achieves a 22.8% reduction in
area and a 20.3% reduction in power, on average, compared to state-of-the-art
parallel hardware architectures without any performance penalty.

</details>


### [302] [MSDformer: Multi-scale Discrete Transformer For Time Series Generation](https://arxiv.org/abs/2505.14202)
*Zhicheng Chen,Shibo Feng,Xi Xiao,Zhong Zhang,Qing Li,Xingyu Gao,Peilin Zhao*

Main category: cs.LG

TL;DR: MSDformer提出了一种基于多尺度离散令牌建模的时间序列生成方法，通过多尺度令牌化和自回归建模显著提升了生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有离散令牌建模方法无法捕捉复杂时间序列的多尺度模式，且缺乏理论指导优化。

Method: 采用多尺度时间序列令牌化器和多尺度自回归令牌建模技术，结合率失真理论验证。

Result: 实验表明MSDformer显著优于现有方法，多尺度信息建模有效提升生成质量。

Conclusion: 多尺度信息整合和模式建模能显著增强基于DTM的时间序列生成质量。

Abstract: Discrete Token Modeling (DTM), which employs vector quantization techniques,
has demonstrated remarkable success in modeling non-natural language
modalities, particularly in time series generation. While our prior work
SDformer established the first DTM-based framework to achieve state-of-the-art
performance in this domain, two critical limitations persist in existing DTM
approaches: 1) their inability to capture multi-scale temporal patterns
inherent to complex time series data, and 2) the absence of theoretical
foundations to guide model optimization. To address these challenges, we
proposes a novel multi-scale DTM-based time series generation method, called
Multi-Scale Discrete Transformer (MSDformer). MSDformer employs a multi-scale
time series tokenizer to learn discrete token representations at multiple
scales, which jointly characterize the complex nature of time series data.
Subsequently, MSDformer applies a multi-scale autoregressive token modeling
technique to capture the multi-scale patterns of time series within the
discrete latent space. Theoretically, we validate the effectiveness of the DTM
method and the rationality of MSDformer through the rate-distortion theorem.
Comprehensive experiments demonstrate that MSDformer significantly outperforms
state-of-the-art methods. Both theoretical analysis and experimental results
demonstrate that incorporating multi-scale information and modeling multi-scale
patterns can substantially enhance the quality of generated time series in
DTM-based approaches. The code will be released upon acceptance.

</details>


### [303] [Challenges and Limitations in the Synthetic Generation of mHealth Sensor Data](https://arxiv.org/abs/2505.14206)
*Flavio Di Martino,Franca Delmastro*

Main category: cs.LG

TL;DR: 论文系统评估了生成模型在时间序列合成中的表现，重点关注多模态、长程依赖和条件生成能力，并提出新评估框架揭示现有方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 移动传感器数据稀缺和隐私问题阻碍了mHealth领域AI应用的发展，合成数据生成（如GAN和扩散模型）成为潜在解决方案，但现有模型在复杂时间序列模式生成上存在不足。

Method: 提出系统性评估框架，测试生成模型在多模态、长程依赖和条件生成等关键挑战上的表现，并衡量合成数据的内在质量及下游任务实用性。

Result: 发现现有方法在跨模态一致性、时间连贯性及合成数据实际应用（如数据增强）中存在显著缺陷。

Conclusion: 需进一步研究以提升时间序列生成质量，增强生成模型在mHealth中的适用性。

Abstract: The widespread adoption of mobile sensors has the potential to provide
massive and heterogeneous time series data, driving Artificial Intelligence
applications in mHealth. However, data collection remains limited due to
stringent ethical regulations, privacy concerns, and other constraints,
hindering progress in the field. Synthetic data generation, particularly
through Generative Adversarial Networks and Diffusion Models, has emerged as a
promising solution to address both data scarcity and privacy issues. Yet, these
models are often limited to short-term, unimodal signal patterns. This paper
presents a systematic evaluation of state-of-the-art generative models for time
series synthesis, with a focus on their ability to jointly handle
multi-modality, long-range dependencies, and conditional generation-key
challenges in the mHealth domain. To ensure a fair comparison, we introduce a
novel evaluation framework designed to measure both the intrinsic quality of
synthetic data and its utility in downstream predictive tasks. Our findings
reveal critical limitations in the existing approaches, particularly in
maintaining cross-modal consistency, preserving temporal coherence, and
ensuring robust performance in train-on-synthetic, test-on-real, and data
augmentation scenarios. Finally, we present our future research directions to
enhance synthetic time series generation and improve the applicability of
generative models in mHealth.

</details>


### [304] [A PID-Controlled Tensor Wheel Decomposition Model for Dynamic Link Prediction](https://arxiv.org/abs/2505.14211)
*Qu Wang,Yan Xia*

Main category: cs.LG

TL;DR: 该论文提出了一种PID控制的张量轮分解（PTWD）模型，用于动态网络中的链接预测，通过结合张量轮分解和PID控制原理，提高了预测准确性。


<details>
  <summary>Details</summary>
Motivation: 动态网络中的链接预测是一个重要挑战，传统静态网络方法难以捕捉时间依赖性和权重动态，而基于张量的方法能更好地建模多维交互。

Method: 论文提出PTWD模型，利用张量轮分解（TWD）捕捉动态网络的潜在特征，并结合PID控制原理优化模型参数学习过程。

Result: 在四个真实数据集上的实验表明，PTWD模型相比其他模型具有更准确的链接预测能力。

Conclusion: PTWD模型通过结合TWD和PID控制，显著提升了动态网络链接预测的准确性，验证了其有效性。

Abstract: Link prediction in dynamic networks remains a fundamental challenge in
network science, requiring the inference of potential interactions and their
evolving strengths through spatiotemporal pattern analysis. Traditional static
network methods have inherent limitations in capturing temporal dependencies
and weight dynamics, while tensor-based methods offer a promising paradigm by
encoding dynamic networks into high-order tensors to explicitly model
multidimensional interactions across nodes and time. Among them, tensor wheel
decomposition (TWD) stands out for its innovative topological structure, which
decomposes high-order tensors into cyclic factors and core tensors to maintain
structural integrity. To improve the prediction accuracy, this study introduces
a PID-controlled tensor wheel decomposition (PTWD) model, which mainly adopts
the following two ideas: 1) exploiting the representation power of TWD to
capture the latent features of dynamic network topology and weight evolution,
and 2) integrating the proportional-integral-derivative (PID) control principle
into the optimization process to obtain a stable model parameter learning
scheme. The performance on four real datasets verifies that the proposed PTWD
model has more accurate link prediction capabilities compared to other models.

</details>


### [305] [Regularized least squares learning with heavy-tailed noise is minimax optimal](https://arxiv.org/abs/2505.14214)
*Mattes Mollenhauer,Nicole Mücke,Dimitri Meunier,Arthur Gretton*

Main category: cs.LG

TL;DR: 该论文研究了在噪声具有有限高阶矩的情况下，再生核希尔伯特空间中岭回归的性能，并建立了基于积分算子框架的超风险界限，证明了正则化最小二乘法对重尾噪声的渐近鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 过去二十年中，相关研究通常假设噪声服从亚指数分布，而本文旨在探讨在噪声仅具有有限高阶矩的情况下，岭回归的性能表现，以扩展其应用范围。

Method: 论文采用积分算子框架，并基于Hilbert空间值随机变量的Fuk-Nagaev不等式进行推导，建立了包含亚高斯和多项式项的过剩风险界限。

Result: 在标准特征值衰减条件下，论文证明了岭回归能够达到与亚指数噪声下相同的收敛速率，且这些速率是最优的，展示了其对重尾噪声的鲁棒性。

Conclusion: 研究表明，即使在噪声具有有限高阶矩的情况下，正则化最小二乘法仍能保持其性能，为处理重尾噪声提供了理论支持。

Abstract: This paper examines the performance of ridge regression in reproducing kernel
Hilbert spaces in the presence of noise that exhibits a finite number of higher
moments. We establish excess risk bounds consisting of subgaussian and
polynomial terms based on the well known integral operator framework. The
dominant subgaussian component allows to achieve convergence rates that have
previously only been derived under subexponential noise - a prevalent
assumption in related work from the last two decades. These rates are optimal
under standard eigenvalue decay conditions, demonstrating the asymptotic
robustness of regularized least squares against heavy-tailed noise. Our
derivations are based on a Fuk-Nagaev inequality for Hilbert-space valued
random variables.

</details>


### [306] [Federated learning in low-resource settings: A chest imaging study in Africa -- Challenges and lessons learned](https://arxiv.org/abs/2505.14217)
*Jorge Fabila,Lidia Garrucho,Víctor M. Campello,Carlos Martín-Isla,Karim Lekadir*

Main category: cs.LG

TL;DR: 该研究探讨了在非洲资源匮乏地区使用联邦学习（FL）进行结核病（TB）诊断的可行性，通过多机构协作训练AI模型而不共享原始数据，解决了隐私和数据稀缺问题，但面临基础设施不足等挑战。


<details>
  <summary>Details</summary>
Motivation: 传统集中式AI模型在资源匮乏地区面临患者数据隐私和稀缺的挑战，联邦学习提供了一种无需共享原始数据的协作解决方案。

Method: 研究涉及八个非洲国家的医院和研究机构，使用本地和公共数据集，比较本地训练模型与跨机构联邦模型的性能。

Result: 联邦学习显示出在资源匮乏地区实现AI驱动医疗的潜力，但基础设施差、网络不稳定、数字素养低和法规薄弱等问题限制了其广泛应用。

Conclusion: 尽管联邦学习在医疗领域具有前景，但在撒哈拉以南非洲的推广需要改善基础设施、教育和法规支持。

Abstract: This study explores the use of Federated Learning (FL) for tuberculosis (TB)
diagnosis using chest X-rays in low-resource settings across Africa. FL allows
hospitals to collaboratively train AI models without sharing raw patient data,
addressing privacy concerns and data scarcity that hinder traditional
centralized models. The research involved hospitals and research centers in
eight African countries. Most sites used local datasets, while Ghana and The
Gambia used public ones. The study compared locally trained models with a
federated model built across all institutions to evaluate FL's real-world
feasibility. Despite its promise, implementing FL in sub-Saharan Africa faces
challenges such as poor infrastructure, unreliable internet, limited digital
literacy, and weak AI regulations. Some institutions were also reluctant to
share model updates due to data control concerns. In conclusion, FL shows
strong potential for enabling AI-driven healthcare in underserved regions, but
broader adoption will require improvements in infrastructure, education, and
regulatory support.

</details>


### [307] [Fast and close Shannon entropy approximation](https://arxiv.org/abs/2505.14234)
*Illia Horenko,Davide Bassetti,Lukáš Pospíšil*

Main category: cs.LG

TL;DR: 论文提出了一种快速熵近似方法（FEA），显著提升了香农熵及其梯度的计算效率与稳定性，适用于机器学习等领域。


<details>
  <summary>Details</summary>
Motivation: 香农熵和冯·诺依曼熵在物理、信息论、机器学习和量子计算中广泛应用，但其梯度奇异性和高计算成本限制了相关工具的性能。

Method: 提出非奇异有理近似方法FEA，仅需5-6次基本运算，避免了查表、位移或级数近似等复杂操作。

Result: FEA的平均绝对误差低至10^-3，计算速度提升约50%，在机器学习特征选择任务中实现2-3个数量级的加速。

Conclusion: FEA通过低计算量、低误差和非奇异梯度的综合优势，显著提升了模型质量和计算效率。

Abstract: Shannon entropy (SE) and its quantum mechanical analogue von Neumann entropy
are key components in many tools used in physics, information theory, machine
learning (ML) and quantum computing. Besides of the significant amounts of SE
computations required in these fields, the singularity of the SE gradient is
one of the central mathematical reason inducing the high cost, frequently low
robustness and slow convergence of such tools. Here we propose the Fast Entropy
Approximation (FEA) - a non-singular rational approximation of Shannon entropy
and its gradient that achieves a mean absolute error of $10^{-3}$, which is
approximately $20$ times lower than comparable state-of-the-art methods. FEA
allows around $50\%$ faster computation, requiring only $5$ to $6$ elementary
computational operations, as compared to tens of elementary operations behind
the fastest entropy computation algorithms with table look-ups, bitshifts, or
series approximations. On a set of common benchmarks for the feature selection
problem in machine learning, we show that the combined effect of fewer
elementary operations, low approximation error, and a non-singular gradient
allows significantly better model quality and enables ML feature extraction
that is two to three orders of magnitude faster and computationally cheaper
when incorporating FEA into AI tools.

</details>


### [308] [Learning with Local Search MCMC Layers](https://arxiv.org/abs/2505.14240)
*Germain Vivier-Ardisson,Mathieu Blondel,Axel Parmentier*

Main category: cs.LG

TL;DR: 论文提出了一种理论上有保证的方法，将局部搜索启发式算法转化为可微分的组合层，以解决神经网络中组合优化层依赖不精确求解器的问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在依赖不精确求解器时缺乏理论保证或性能不足，而许多运筹学问题是NP难的，常需使用局部搜索启发式算法。

Method: 受模拟退火与Metropolis-Hastings的启发，将局部搜索中的邻域系统转化为提议分布，在可行解的组合空间上实现MCMC，构建可微分组合层及损失函数。

Result: 该方法在动态车辆路径问题（带时间窗）上验证了有效性，显著降低了计算负担。

Conclusion: 通过理论驱动的局部搜索方法，实现了高效且可微分的组合优化层，适用于大规模实际应用。

Abstract: Integrating combinatorial optimization layers into neural networks has
recently attracted significant research interest. However, many existing
approaches lack theoretical guarantees or fail to perform adequately when
relying on inexact solvers. This is a critical limitation, as many operations
research problems are NP-hard, often necessitating the use of
neighborhood-based local search heuristics. These heuristics iteratively
generate and evaluate candidate solutions based on an acceptance rule. In this
paper, we introduce a theoretically-principled approach for learning with such
inexact combinatorial solvers. Inspired by the connection between simulated
annealing and Metropolis-Hastings, we propose to transform problem-specific
neighborhood systems used in local search heuristics into proposal
distributions, implementing MCMC on the combinatorial space of feasible
solutions. This allows us to construct differentiable combinatorial layers and
associated loss functions. Replacing an exact solver by a local search strongly
reduces the computational burden of learning on many applications. We
demonstrate our approach on a large-scale dynamic vehicle routing problem with
time windows.

</details>


### [309] [A Private Approximation of the 2nd-Moment Matrix of Any Subsamplable Input](https://arxiv.org/abs/2505.14251)
*Bar Mahpud,Or Sheffet*

Main category: cs.LG

TL;DR: 提出一种基于子采样假设的差分隐私二阶矩估计算法，在保证隐私的同时实现高精度估计，并能处理含异常值的数据。


<details>
  <summary>Details</summary>
Motivation: 现有差分隐私二阶矩估计方法在极端输入或含异常值数据上表现不佳，需要一种更鲁棒的算法。

Method: 基于(m,α,β)-子采样假设构建递归算法框架，采用零集中差分隐私(zCDP)保证隐私性。

Result: 算法在任意(1±γ)精度范围内保持二阶矩估计准确性，并能有效处理分布D中含显著异常值的情况。

Conclusion: 该工作为差分隐私二阶矩估计提供了理论保证，拓展了在含噪声数据场景下的应用可能性。

Abstract: We study the problem of differentially private second moment estimation and
present a new algorithm that achieve strong privacy-utility trade-offs even for
worst-case inputs under subsamplability assumptions on the data. We call an
input $(m,\alpha,\beta)$-subsamplable if a random subsample of size $m$ (or
larger) preserves w.p $\geq 1-\beta$ the spectral structure of the original
second moment matrix up to a multiplicative factor of $1\pm \alpha$. Building
upon subsamplability, we give a recursive algorithmic framework similar to
Kamath et al 2019, that abides zero-Concentrated Differential Privacy (zCDP)
while preserving w.h.p. the accuracy of the second moment estimation upto an
arbitrary factor of $(1\pm\gamma)$. We then show how to apply our algorithm to
approximate the second moment matrix of a distribution $\mathcal{D}$, even when
a noticeable fraction of the input are outliers.

</details>


### [310] [Hybrid Adaptive Modeling in Process Monitoring: Leveraging Sequence Encoders and Physics-Informed Neural Networks](https://arxiv.org/abs/2505.14252)
*Mouad Elaarabi,Domenico Borzacchiello,Philippe Le Bot,Nathan Lauzeral,Sebastien Comas-Cardona*

Main category: cs.LG

TL;DR: 该论文提出了一种结合序列编码与物理信息神经网络（PINNs）的方法，用于实时适应变化的参数、边界和初始条件，解决了传统方法需重新训练的问题，并在三个不同问题上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 传统PINNs与稀疏回归结合的方法在参数、边界或初始条件变化时需要重新训练模型，限制了其在实际应用中的灵活性。本研究旨在解决这一问题，提出一种能够实时适应变化的模型架构。

Method: 采用深度集合（Deep Sets）或序列编码器对动态参数、边界条件和初始条件进行编码，并将编码后的特征作为PINNs的输入，使模型能够适应参数、边界和初始条件的变化。

Result: 在Rossler ODE系统、2D Navier-Stokes PDE问题和1D热监测问题中验证了模型的鲁棒性、泛化能力以及实时适应能力，特别是在噪声环境下和参数化输入条件下的表现优异。

Conclusion: 提出的方法成功实现了对变化参数的实时适应，扩展了PINNs的应用范围，为动态系统识别和实时应用提供了有效解决方案。

Abstract: In this work, we explore the integration of Sequence Encoding for Online
Parameter Identification with Physics-Informed Neural Networks to create a
model that, once trained, can be utilized for real time applications with
variable parameters, boundary conditions, and initial conditions. Recently, the
combination of PINNs with Sparse Regression has emerged as a method for
performing dynamical system identification through supervised learning and
sparse regression optimization, while also solving the dynamics using PINNs.
However, this approach can be limited by variations in parameters or boundary
and initial conditions, requiring retraining of the model whenever changes
occur. In this work, we introduce an architecture that employs Deep Sets or
Sequence Encoders to encode dynamic parameters, boundary conditions, and
initial conditions, using these encoded features as inputs for the PINN,
enabling the model to adapt to changes in parameters, BCs, and ICs. We apply
this approach to three different problems. First, we analyze the Rossler ODE
system, demonstrating the robustness of the model with respect to noise and its
ability to generalize. Next, we explore the model's capability in a 2D
Navier-Stokes PDE problem involving flow past a cylinder with a parametric
sinusoidal inlet velocity function, showing that the model can encode pressure
data from a few points to identify the inlet velocity profile and utilize
physics to compute velocity and pressure throughout the domain. Finally, we
address a 1D heat monitoring problem using real data from the heating of glass
fiber and thermoplastic composite plates.

</details>


### [311] [AAPO: Enhance the Reasoning Capabilities of LLMs with Advantage Momentum](https://arxiv.org/abs/2505.14264)
*Jian Xiong,Jingbo Zhou,Jingyong Ye,Dejing Dou*

Main category: cs.LG

TL;DR: 本文提出了一种名为AAPO的新型强化学习算法，通过动量估计增强优势值，解决了现有组相对优势估计方法在训练效率上的不足，并在数学推理任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有基于组相对优势估计的强化学习方法（如GRPO）虽然简化了训练流程，但在估计优势接近零时存在训练效率低下的问题。本文旨在解决这一局限性。

Method: 提出Advantage-Augmented Policy Optimization (AAPO)，通过动量估计方案增强优势值，优化交叉熵损失函数，从而提升训练效率。

Result: 在多个数学推理基准测试中，AAPO表现出优于现有方法的性能。

Conclusion: AAPO通过改进优势估计机制，有效提升了强化学习在语言模型推理任务中的训练效率和性能。

Abstract: Reinforcement learning (RL) has emerged as an effective approach for
enhancing the reasoning capabilities of large language models (LLMs),
especially in scenarios where supervised fine-tuning (SFT) falls short due to
limited chain-of-thought (CoT) data. Among RL-based post-training methods,
group relative advantage estimation, as exemplified by Group Relative Policy
Optimization (GRPO), has attracted considerable attention for eliminating the
dependency on the value model, thereby simplifying training compared to
traditional approaches like Proximal Policy Optimization (PPO). However, we
observe that exsiting group relative advantage estimation method still suffers
from training inefficiencies, particularly when the estimated advantage
approaches zero. To address this limitation, we propose Advantage-Augmented
Policy Optimization (AAPO), a novel RL algorithm that optimizes the
cross-entropy (CE) loss using advantages enhanced through a momentum-based
estimation scheme. This approach effectively mitigates the inefficiencies
associated with group relative advantage estimation. Experimental results on
multiple mathematical reasoning benchmarks demonstrate the superior performance
of AAPO.

</details>


### [312] [X-KAN: Optimizing Local Kolmogorov-Arnold Networks via Evolutionary Rule-Based Machine Learning](https://arxiv.org/abs/2505.14273)
*Hiroki Shiraishi,Hisao Ishibuchi,Masaya Nakata*

Main category: cs.LG

TL;DR: X-KAN提出了一种结合局部KAN模型和XCSF框架的新方法，显著提升了复杂或不连续函数的逼近精度。


<details>
  <summary>Details</summary>
Motivation: 现有神经网络方法在处理局部复杂或不连续函数时表现不佳，因为它们依赖单一的全局模型。

Method: X-KAN通过XCSF框架优化多个局部KAN模型，将KAN的高表达力与XCSF的自适应分区能力结合。

Result: 实验表明，X-KAN在逼近精度上显著优于传统方法，且能有效处理复杂或不连续函数，平均仅需7.2±2.3条规则。

Conclusion: X-KAN验证了将KAN作为XCSF局部模型的有效性，结合了准确性和通用性评估。

Abstract: Function approximation is a critical task in various fields. However,
existing neural network approaches struggle with locally complex or
discontinuous functions due to their reliance on a single global model covering
the entire problem space. We propose X-KAN, a novel method that optimizes
multiple local Kolmogorov-Arnold Networks (KANs) through an evolutionary
rule-based machine learning framework called XCSF. X-KAN combines KAN's high
expressiveness with XCSF's adaptive partitioning capability by implementing
local KAN models as rule consequents and defining local regions via rule
antecedents. Our experimental results on artificial test functions and
real-world datasets demonstrate that X-KAN significantly outperforms
conventional methods, including XCSF, Multi-Layer Perceptron, and KAN, in terms
of approximation accuracy. Notably, X-KAN effectively handles functions with
locally complex or discontinuous structures that are challenging for
conventional KAN, using a compact set of rules (average 7.2 $\pm$ 2.3 rules).
These results validate the effectiveness of using KAN as a local model in XCSF,
which evaluates the rule fitness based on both accuracy and generality. Our
X-KAN implementation is available at https://github.com/YNU-NakataLab/X-KAN.

</details>


### [313] [Scaling Law for Quantization-Aware Training](https://arxiv.org/abs/2505.14302)
*Mengzhao Chen,Chaoyi Zhang,Jing Liu,Yutao Zeng,Zeyue Xue,Zhiheng Liu,Yunshui Li,Jin Ma,Jie Huang,Xun Zhou,Ping Luo*

Main category: cs.LG

TL;DR: 本文提出统一量化感知训练（QAT）的缩放规律，通过实验发现量化误差与模型大小、训练数据量和量化粒度相关，并指出激活量化误差是主要瓶颈，混合精度量化可改善。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）需要大量计算和内存资源，量化感知训练（QAT）能降低模型精度但保持性能，但4位精度（W4A4）的QAT缩放行为尚不明确，现有研究忽略关键因素如训练token数量和量化粒度。

Method: 通过268次QAT实验，建模量化误差与模型大小、训练数据量和量化组大小的关系，分解W4A4量化误差为权重和激活分量，分析其敏感性差异。

Result: 量化误差随模型增大而减小，但随训练token增多和量化粒度变粗而增加。FC2层的激活量化误差是主要瓶颈，混合精度量化可使权重和激活误差趋近。

Conclusion: 权重和激活量化误差在不同场景下重要性不同，混合精度量化能有效改善QAT性能，为未来研究提供关键见解。

Abstract: Large language models (LLMs) demand substantial computational and memory
resources, creating deployment challenges. Quantization-aware training (QAT)
addresses these challenges by reducing model precision while maintaining
performance. However, the scaling behavior of QAT, especially at 4-bit
precision (W4A4), is not well understood. Existing QAT scaling laws often
ignore key factors such as the number of training tokens and quantization
granularity, which limits their applicability. This paper proposes a unified
scaling law for QAT that models quantization error as a function of model size,
training data volume, and quantization group size. Through 268 QAT experiments,
we show that quantization error decreases as model size increases, but rises
with more training tokens and coarser quantization granularity. To identify the
sources of W4A4 quantization error, we decompose it into weight and activation
components. Both components follow the overall trend of W4A4 quantization
error, but with different sensitivities. Specifically, weight quantization
error increases more rapidly with more training tokens. Further analysis shows
that the activation quantization error in the FC2 layer, caused by outliers, is
the primary bottleneck of W4A4 QAT quantization error. By applying
mixed-precision quantization to address this bottleneck, we demonstrate that
weight and activation quantization errors can converge to similar levels.
Additionally, with more training data, weight quantization error eventually
exceeds activation quantization error, suggesting that reducing weight
quantization error is also important in such scenarios. These findings offer
key insights for improving QAT research and development.

</details>


### [314] [MultiTab: A Comprehensive Benchmark Suite for Multi-Dimensional Evaluation in Tabular Domains](https://arxiv.org/abs/2505.14312)
*Kyungeun Lee,Moonjung Eo,Hye-Seung Cho,Dongmin Kim,Ye Seul Sim,Seoyoon Kim,Min-Kook Suh,Woohyung Lim*

Main category: cs.LG

TL;DR: MultiTab提出一个多维度的表格学习算法评估框架，通过分析196个数据集的关键特征，揭示模型性能对数据特性的敏感性。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试主要依赖平均指标，无法揭示模型在不同数据特性下的行为差异，因此需要更全面的评估方法。

Method: MultiTab将196个公开数据集按样本量、标签不平衡和特征交互等特性分类，并评估13种代表性模型。

Result: 模型性能高度依赖数据特性，例如样本级相似性模型在大样本或高特征相关性数据中表现更好。

Conclusion: MultiTab为模型设计提供更科学依据，并为特定数据特性选择模型提供实用指导。

Abstract: Despite the widespread use of tabular data in real-world applications, most
benchmarks rely on average-case metrics, which fail to reveal how model
behavior varies across diverse data regimes. To address this, we propose
MultiTab, a benchmark suite and evaluation framework for multi-dimensional,
data-aware analysis of tabular learning algorithms. Rather than comparing
models only in aggregate, MultiTab categorizes 196 publicly available datasets
along key data characteristics, including sample size, label imbalance, and
feature interaction, and evaluates 13 representative models spanning a range of
inductive biases. Our analysis shows that model performance is highly sensitive
to such regimes: for example, models using sample-level similarity excel on
datasets with large sample sizes or high inter-feature correlation, while
models encoding inter-feature dependencies perform best with weakly correlated
features. These findings reveal that inductive biases do not always behave as
intended, and that regime-aware evaluation is essential for understanding and
improving model behavior. MultiTab enables more principled model design and
offers practical guidance for selecting models tailored to specific data
characteristics. All datasets, code, and optimization logs are publicly
available at https://huggingface.co/datasets/LGAI-DILab/Multitab.

</details>


### [315] [Better Neural Network Expressivity: Subdividing the Simplex](https://arxiv.org/abs/2505.14338)
*Egor Bakaev,Florestan Brunck,Christoph Hertrich,Jack Stade,Amir Yehudayoff*

Main category: cs.LG

TL;DR: 本文推翻了关于ReLU神经网络深度最优性的猜想，证明了更少的隐藏层即可计算所有连续分段线性函数。


<details>
  <summary>Details</summary>
Motivation: 研究ReLU神经网络在计算连续分段线性函数时的表达能力，特别是关于网络深度的最优性问题。

Method: 通过构造性证明，展示了如何用更少的隐藏层实现最大函数等CPWL函数的计算，并利用多面体剖分进行几何解释。

Result: 证明了仅需⌈log₃(n-1)⌉+1层隐藏层即可计算ℝⁿ上所有CPWL函数，改进了之前⌈log₂(n+1)⌉层的结果。

Conclusion: ReLU神经网络在计算CPWL函数时所需的深度比之前认为的更小，这为网络架构设计提供了新的理论依据。

Abstract: This work studies the expressivity of ReLU neural networks with a focus on
their depth. A sequence of previous works showed that $\lceil \log_2(n+1)
\rceil$ hidden layers are sufficient to compute all continuous piecewise linear
(CPWL) functions on $\mathbb{R}^n$. Hertrich, Basu, Di Summa, and Skutella
(NeurIPS'21) conjectured that this result is optimal in the sense that there
are CPWL functions on $\mathbb{R}^n$, like the maximum function, that require
this depth. We disprove the conjecture and show that
$\lceil\log_3(n-1)\rceil+1$ hidden layers are sufficient to compute all CPWL
functions on $\mathbb{R}^n$.
  A key step in the proof is that ReLU neural networks with two hidden layers
can exactly represent the maximum function of five inputs. More generally, we
show that $\lceil\log_3(n-2)\rceil+1$ hidden layers are sufficient to compute
the maximum of $n\geq 4$ numbers. Our constructions almost match the
$\lceil\log_3(n)\rceil$ lower bound of Averkov, Hojny, and Merkert (ICLR'25) in
the special case of ReLU networks with weights that are decimal fractions. The
constructions have a geometric interpretation via polyhedral subdivisions of
the simplex into ``easier'' polytopes.

</details>


### [316] [Enhancing Classification with Semi-Supervised Deep Learning Using Distance-Based Sample Weights](https://arxiv.org/abs/2505.14345)
*Aydin Abedinia,Shima Tabakhi,Vahid Seydi*

Main category: cs.LG

TL;DR: 该论文提出了一种基于距离加权的半监督学习框架，通过优先处理靠近测试数据的关键样本提升模型性能，在12个基准数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决标记数据有限场景下（如医疗、安防领域）的模型泛化问题，同时应对噪声和不平衡数据的挑战。

Method: 结合不确定性一致性和图表示技术，设计距离加权机制动态筛选信息量最大的训练样本。

Result: 在准确率、精确率和召回率等指标上显著超越现有方法，尤其在噪声和不平衡数据中表现稳健。

Conclusion: 该框架为半监督学习提供了可扩展的实用解决方案，特别适用于数据受限的复杂场景。

Abstract: Recent advancements in semi-supervised deep learning have introduced
effective strategies for leveraging both labeled and unlabeled data to improve
classification performance. This work proposes a semi-supervised framework that
utilizes a distance-based weighting mechanism to prioritize critical training
samples based on their proximity to test data. By focusing on the most
informative examples, the method enhances model generalization and robustness,
particularly in challenging scenarios with noisy or imbalanced datasets.
Building on techniques such as uncertainty consistency and graph-based
representations, the approach addresses key challenges of limited labeled data
while maintaining scalability. Experiments on twelve benchmark datasets
demonstrate significant improvements across key metrics, including accuracy,
precision, and recall, consistently outperforming existing methods. This
framework provides a robust and practical solution for semi-supervised
learning, with potential applications in domains such as healthcare and
security where data limitations pose significant challenges.

</details>


### [317] [Towards eliciting latent knowledge from LLMs with mechanistic interpretability](https://arxiv.org/abs/2505.14352)
*Bartosz Cywiński,Emil Ryd,Senthooran Rajamanoharan,Neel Nanda*

Main category: cs.LG

TL;DR: 研究探索了如何从语言模型中提取隐藏知识，通过训练一个Taboo模型并测试黑盒和解释性方法，证明两者均能有效揭示秘密词。


<details>
  <summary>Details</summary>
Motivation: 随着语言模型变得更强大，确保其可信赖和可靠至关重要。初步证据显示模型可能试图欺骗或对操作者保密，因此需要探索提取隐藏知识的方法。

Method: 训练一个Taboo模型，该模型描述特定秘密词但不直接提及它。随后测试非解释性（黑盒）方法和基于机制解释性技术（如logit lens和稀疏自编码器）的策略。

Result: 在概念验证设置中，黑盒和解释性方法均能有效提取秘密词，表明这些方法在揭示隐藏知识方面具有潜力。

Conclusion: 该研究为从语言模型中提取秘密知识提供了初步方法，未来可在更复杂模型上测试和优化这些技术，以促进模型的安全可靠部署。

Abstract: As language models become more powerful and sophisticated, it is crucial that
they remain trustworthy and reliable. There is concerning preliminary evidence
that models may attempt to deceive or keep secrets from their operators. To
explore the ability of current techniques to elicit such hidden knowledge, we
train a Taboo model: a language model that describes a specific secret word
without explicitly stating it. Importantly, the secret word is not presented to
the model in its training data or prompt. We then investigate methods to
uncover this secret. First, we evaluate non-interpretability (black-box)
approaches. Subsequently, we develop largely automated strategies based on
mechanistic interpretability techniques, including logit lens and sparse
autoencoders. Evaluation shows that both approaches are effective in eliciting
the secret word in our proof-of-concept setting. Our findings highlight the
promise of these approaches for eliciting hidden knowledge and suggest several
promising avenues for future work, including testing and refining these methods
on more complex model organisms. This work aims to be a step towards addressing
the crucial problem of eliciting secret knowledge from language models, thereby
contributing to their safe and reliable deployment.

</details>


### [318] [Layer-wise Quantization for Quantized Optimistic Dual Averaging](https://arxiv.org/abs/2505.14371)
*Anh Duc Nguyen,Ilia Markov,Frank Zhengqing Wu,Ali Ramezani-Kebrya,Kimon Antonakopoulos,Dan Alistarh,Volkan Cevher*

Main category: cs.LG

TL;DR: 提出了一种适应深度神经网络异质性的分层量化框架，并应用于分布式变分不等式优化，新算法QODA在训练Wasserstein GAN时提速150%。


<details>
  <summary>Details</summary>
Motivation: 现代深度神经网络各层存在结构、表示特性等异质性，影响预测性能，需开发适应这种异质性的量化方法。

Method: 开发分层量化框架，提出QODA算法，结合自适应学习率，用于分布式变分不等式优化。

Result: QODA在单调变分不等式上达到有竞争力的收敛速度，训练Wasserstein GAN时比基线提速150%。

Conclusion: 分层量化框架和QODA算法有效适应神经网络异质性，显著提升分布式训练效率。

Abstract: Modern deep neural networks exhibit heterogeneity across numerous layers of
various types such as residuals, multi-head attention, etc., due to varying
structures (dimensions, activation functions, etc.), distinct representation
characteristics, which impact predictions. We develop a general layer-wise
quantization framework with tight variance and code-length bounds, adapting to
the heterogeneities over the course of training. We then apply a new layer-wise
quantization technique within distributed variational inequalities (VIs),
proposing a novel Quantized Optimistic Dual Averaging (QODA) algorithm with
adaptive learning rates, which achieves competitive convergence rates for
monotone VIs. We empirically show that QODA achieves up to a $150\%$ speedup
over the baselines in end-to-end training time for training Wasserstein GAN on
$12+$ GPUs.

</details>


### [319] [Algorithmic Hiring and Diversity: Reducing Human-Algorithm Similarity for Better Outcomes](https://arxiv.org/abs/2505.14388)
*Prasanna Parasurama,Panos Ipeirotis*

Main category: cs.LG

TL;DR: 算法筛选与管理者偏好高度相关时，强制短名单性别平衡对最终招聘多样性改善有限；提出新算法可显著提升多样性且不牺牲质量。


<details>
  <summary>Details</summary>
Motivation: 探讨算法工具在招聘中强制性别平衡短名单是否真能提高最终多样性，揭示关键影响因素。

Method: 理论分析+实证研究（80万份科技公司求职数据），提出互补性算法筛选可能被管理者忽视的合格候选人。

Result: 算法与管理者偏好相关性高时，强制平衡短名单效果有限；新算法使最终女性录用率提升12%-68%

Conclusion: 算法设计需针对性优化才能真正实现多样性目标，为公平招聘算法提供实践指导。

Abstract: Algorithmic tools are increasingly used in hiring to improve fairness and
diversity, often by enforcing constraints such as gender-balanced candidate
shortlists. However, we show theoretically and empirically that enforcing equal
representation at the shortlist stage does not necessarily translate into more
diverse final hires, even when there is no gender bias in the hiring stage. We
identify a crucial factor influencing this outcome: the correlation between the
algorithm's screening criteria and the human hiring manager's evaluation
criteria -- higher correlation leads to lower diversity in final hires. Using a
large-scale empirical analysis of nearly 800,000 job applications across
multiple technology firms, we find that enforcing equal shortlists yields
limited improvements in hire diversity when the algorithmic screening closely
mirrors the hiring manager's preferences. We propose a complementary
algorithmic approach designed explicitly to diversify shortlists by selecting
candidates likely to be overlooked by managers, yet still competitive according
to their evaluation criteria. Empirical simulations show that this approach
significantly enhances gender diversity in final hires without substantially
compromising hire quality. These findings highlight the importance of
algorithmic design choices in achieving organizational diversity goals and
provide actionable guidance for practitioners implementing fairness-oriented
hiring algorithms.

</details>


### [320] [Explaining Unreliable Perception in Automated Driving: A Fuzzy-based Monitoring Approach](https://arxiv.org/abs/2505.14407)
*Aniket Salvi,Gereon Weiss,Mario Trapp*

Main category: cs.LG

TL;DR: 该论文提出了一种新型的基于模糊逻辑的运行时监控器，用于机器学习感知组件，提供可解释的错误分析并增强自动驾驶系统的安全性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习系统的运行时监控缺乏人类可理解的错误解释，这影响了系统安全性和可靠性的保证。

Method: 设计了一种针对ML感知组件的模糊逻辑监控器，通过自然驾驶数据集进行验证，并构建了从单元级正确性到系统级安全性的保证案例。

Result: 新型监控器在保持系统可用性的同时，显著提升了安全性（减少危险情况），优于现有运行时监控器。

Conclusion: 该模糊逻辑监控器不仅提供了可解释的可靠性分析，还通过实证验证展示了其在提升自动驾驶系统安全性方面的有效性。

Abstract: Autonomous systems that rely on Machine Learning (ML) utilize online fault
tolerance mechanisms, such as runtime monitors, to detect ML prediction errors
and maintain safety during operation. However, the lack of human-interpretable
explanations for these errors can hinder the creation of strong assurances
about the system's safety and reliability. This paper introduces a novel
fuzzy-based monitor tailored for ML perception components. It provides
human-interpretable explanations about how different operating conditions
affect the reliability of perception components and also functions as a runtime
safety monitor. We evaluated our proposed monitor using naturalistic driving
datasets as part of an automated driving case study. The interpretability of
the monitor was evaluated and we identified a set of operating conditions in
which the perception component performs reliably. Additionally, we created an
assurance case that links unit-level evidence of \textit{correct} ML operation
to system-level \textit{safety}. The benchmarking demonstrated that our monitor
achieved a better increase in safety (i.e., absence of hazardous situations)
while maintaining availability (i.e., ability to perform the mission) compared
to state-of-the-art runtime ML monitors in the evaluated dataset.

</details>


### [321] [Byte Pair Encoding for Efficient Time Series Forecasting](https://arxiv.org/abs/2505.14411)
*Leon Götz,Marcel Kollovieh,Stephan Günnemann,Leo Schwinn*

Main category: cs.LG

TL;DR: 提出了一种基于模式的时序数据token化方法，通过频繁模式自适应压缩序列，结合条件解码优化，显著提升预测性能和效率。


<details>
  <summary>Details</summary>
Motivation: 现有时序token化方法固定样本数编码，导致简单模式（如常数值）产生冗余token，计算开销大。

Method: 基于频繁模式的离散词汇表，将样本与底层模式合并为token；引入无需梯度计算的条件解码作为后优化方法。

Result: 在时序基础模型上，token化使预测性能提升36%，效率提高1990%；条件解码进一步降低44%的MSE。

Conclusion: 该方法能自适应不同时序模式，泛化到未见数据，且token表示能捕捉统计矩和趋势等关键特征。

Abstract: Existing time series tokenization methods predominantly encode a constant
number of samples into individual tokens. This inflexible approach can generate
excessive tokens for even simple patterns like extended constant values,
resulting in substantial computational overhead. Inspired by the success of
byte pair encoding, we propose the first pattern-centric tokenization scheme
for time series analysis. Based on a discrete vocabulary of frequent motifs,
our method merges samples with underlying patterns into tokens, compressing
time series adaptively. Exploiting our finite set of motifs and the continuous
properties of time series, we further introduce conditional decoding as a
lightweight yet powerful post-hoc optimization method, which requires no
gradient computation and adds no computational overhead. On recent time series
foundation models, our motif-based tokenization improves forecasting
performance by 36% and boosts efficiency by 1990% on average. Conditional
decoding further reduces MSE by up to 44%. In an extensive analysis, we
demonstrate the adaptiveness of our tokenization to diverse temporal patterns,
its generalization to unseen data, and its meaningful token representations
capturing distinct time series properties, including statistical moments and
trends.

</details>


### [322] [Table Foundation Models: on knowledge pre-training for tabular learning](https://arxiv.org/abs/2505.14415)
*Myung Jun Kim,Félix Lefebvre,Gaëtan Brison,Alexandre Perez-Lebel,Gaël Varoquaux*

Main category: cs.LG

TL;DR: TARTE是一种新型表格基础模型，通过预训练将表格转换为知识增强的向量表示，提升下游任务性能并降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有表格基础模型需微调、计算成本高且难以复用，无法像文本或视觉基础模型那样便捷使用。TARTE旨在解决这些问题，通过捕获表格语义信息，提供更高效的预训练表示。

Method: 提出TARTE模型，利用字符串编码捕获表格语义，在大规模关系数据上预训练，生成可微调或与其他学习器结合的知识增强向量表示。

Result: TARTE显著提升了预测性能，优化了预测与计算的权衡，并能生成领域专用表示以支持进一步学习。

Conclusion: TARTE为表格学习提供了一种有效的知识预训练方法，推动了表格基础模型的发展。

Abstract: Table foundation models bring high hopes to data science: pre-trained on
tabular data to embark knowledge or priors, they should facilitate downstream
tasks on tables. One specific challenge is that of data semantics: numerical
entries take their meaning from context, e.g., column name. Pre-trained neural
networks that jointly model column names and table entries have recently
boosted prediction accuracy. While these models outline the promises of world
knowledge to interpret table values, they lack the convenience of popular
foundation models in text or vision. Indeed, they must be fine-tuned to bring
benefits, come with sizeable computation costs, and cannot easily be reused or
combined with other architectures. Here we introduce TARTE, a foundation model
that transforms tables to knowledge-enhanced vector representations using the
string to capture semantics. Pre-trained on large relational data, TARTE yields
representations that facilitate subsequent learning with little additional
cost. These representations can be fine-tuned or combined with other learners,
giving models that push the state-of-the-art prediction performance and improve
the prediction/computation performance trade-off. Specialized to a task or a
domain, TARTE gives domain-specific representations that facilitate further
learning. Our study demonstrates an effective approach to knowledge
pre-training for tabular learning.

</details>


### [323] [Explaining Neural Networks with Reasons](https://arxiv.org/abs/2505.14424)
*Levin Hornischer,Hannes Leitgeb*

Main category: cs.LG

TL;DR: 提出一种基于新数学哲学理论的可解释性方法，通过计算神经元的'原因向量'来解读神经网络，兼具逻辑与贝叶斯视角，并支持多语义性。


<details>
  <summary>Details</summary>
Motivation: 现有神经网络解释方法难以兼顾哲学基础、架构普适性、计算效率和语义多样性，需要一种统一、可扩展且忠实于模型行为的解释框架。

Method: 为每个神经元计算'原因向量'，量化其对特定命题（如图像分类/情感判断）的支持强度，结合前向传播实现高效计算。

Result: 该方法满足7项指标：哲学基础扎实、架构普适、计算高效、行为忠实、数据匹配、可优化训练、能提升模型鲁棒性与公平性。

Conclusion: 该理论为神经网络解释提供了兼具哲学严谨性与工程实用性的新范式，尤其擅长处理多语义神经元问题。

Abstract: We propose a new interpretability method for neural networks, which is based
on a novel mathematico-philosophical theory of reasons. Our method computes a
vector for each neuron, called its reasons vector. We then can compute how
strongly this reasons vector speaks for various propositions, e.g., the
proposition that the input image depicts digit 2 or that the input prompt has a
negative sentiment. This yields an interpretation of neurons, and groups
thereof, that combines a logical and a Bayesian perspective, and accounts for
polysemanticity (i.e., that a single neuron can figure in multiple concepts).
We show, both theoretically and empirically, that this method is: (1) grounded
in a philosophically established notion of explanation, (2) uniform, i.e.,
applies to the common neural network architectures and modalities, (3)
scalable, since computing reason vectors only involves forward-passes in the
neural network, (4) faithful, i.e., intervening on a neuron based on its reason
vector leads to expected changes in model output, (5) correct in that the
model's reasons structure matches that of the data source, (6) trainable, i.e.,
neural networks can be trained to improve their reason strengths, (7) useful,
i.e., it delivers on the needs for interpretability by increasing, e.g.,
robustness and fairness.

</details>


### [324] [Interpretable Neural System Dynamics: Combining Deep Learning with System Dynamics Modeling to Support Critical Applications](https://arxiv.org/abs/2505.14428)
*Riccardo D'Elia*

Main category: cs.LG

TL;DR: 该提案旨在通过开发可解释的神经系统动力学框架，结合深度学习的预测能力与传统系统动力学的可解释性，解决两者在可解释性和扩展性上的不足。


<details>
  <summary>Details</summary>
Motivation: 深度学习在复杂模型学习和准确预测方面表现出色，但缺乏可解释性和因果可靠性；传统系统动力学方法虽透明且具有因果洞察力，但扩展性有限且需要大量领域知识。

Method: 提出神经系统动力学流程，整合基于概念的可解释性、机制可解释性和因果机器学习，结合深度学习的预测能力和传统系统动力学的可解释性。

Result: 该框架将实现因果可靠性和扩展性，并通过欧盟资助的AutoMoTIF项目在自主多式联运系统中的实际应用进行验证。

Conclusion: 长期目标是收集可操作的见解，支持在自主系统中整合可解释性和安全性。

Abstract: The objective of this proposal is to bridge the gap between Deep Learning
(DL) and System Dynamics (SD) by developing an interpretable neural system
dynamics framework. While DL excels at learning complex models and making
accurate predictions, it lacks interpretability and causal reliability.
Traditional SD approaches, on the other hand, provide transparency and causal
insights but are limited in scalability and require extensive domain knowledge.
To overcome these limitations, this project introduces a Neural System Dynamics
pipeline, integrating Concept-Based Interpretability, Mechanistic
Interpretability, and Causal Machine Learning. This framework combines the
predictive power of DL with the interpretability of traditional SD models,
resulting in both causal reliability and scalability. The efficacy of the
proposed pipeline will be validated through real-world applications of the
EU-funded AutoMoTIF project, which is focused on autonomous multimodal
transportation systems. The long-term goal is to collect actionable insights
that support the integration of explainability and safety in autonomous
systems.

</details>


### [325] [RefiDiff: Refinement-Aware Diffusion for Efficient Missing Data Imputation](https://arxiv.org/abs/2505.14451)
*Md Atik Ahamed,Qiang Ye,Qiang Cheng*

Main category: cs.LG

TL;DR: RefiDiff提出了一种结合局部预测与全局去噪的高效高维混合数据缺失值填补框架，在MNAR机制下表现优异且训练速度快。


<details>
  <summary>Details</summary>
Motivation: 高维混合类型数据在MNAR机制下的缺失值填补存在挑战，现有方法难以兼顾局部与全局特征。

Method: 通过预精炼-去噪网络-后精炼的三阶段框架，利用Mamba网络捕捉特征间远程关联，统一编码混合数据类型。

Result: 在9个真实数据集上超越SOTA方法，MNAR场景下训练速度比DDPM快4倍，且无需调参。

Conclusion: RefiDiff为高维混合数据缺失值填补提供了高效稳定的解决方案，特别适用于复杂MNAR场景。

Abstract: Missing values in high-dimensional, mixed-type datasets pose significant
challenges for data imputation, particularly under Missing Not At Random (MNAR)
mechanisms. Existing methods struggle to integrate local and global data
characteristics, limiting performance in MNAR and high-dimensional settings. We
propose an innovative framework, RefiDiff, combining local machine learning
predictions with a novel Mamba-based denoising network capturing
interrelationships among distant features and samples. Our approach leverages
pre-refinement for initial warm-up imputations and post-refinement to polish
results, enhancing stability and accuracy. By encoding mixed-type data into
unified tokens, RefiDiff enables robust imputation without architectural or
hyperparameter tuning. RefiDiff outperforms state-of-the-art (SOTA) methods
across missing-value settings, excelling in MNAR with a 4x faster training time
than SOTA DDPM-based approaches. Extensive evaluations on nine real-world
datasets demonstrate its robustness, scalability, and effectiveness in handling
complex missingness patterns.

</details>


### [326] [Interpretable Reinforcement Learning for Load Balancing using Kolmogorov-Arnold Networks](https://arxiv.org/abs/2505.14459)
*Kamal Singh,Sami Marouani,Ahmad Al Sheikh,Pham Tran Anh Quang,Amaury Habrard*

Main category: cs.LG

TL;DR: 使用KAN网络实现可解释强化学习，优化网络负载均衡策略。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法在网络控制问题中缺乏可解释性且难以提取控制方程。

Method: 采用PPO算法结合1层KAN模型和MLP Critic网络，学习最大化吞吐量、最小化丢包和延迟的负载均衡策略。

Result: 该方法能提取控制方程，提升网络性能并提供可解释策略。

Conclusion: KAN网络为强化学习提供了可解释性，同时优化了网络控制效果。

Abstract: Reinforcement learning (RL) has been increasingly applied to network control
problems, such as load balancing. However, existing RL approaches often suffer
from lack of interpretability and difficulty in extracting controller
equations. In this paper, we propose the use of Kolmogorov-Arnold Networks
(KAN) for interpretable RL in network control. We employ a PPO agent with a
1-layer actor KAN model and an MLP Critic network to learn load balancing
policies that maximise throughput utility, minimize loss as well as delay. Our
approach allows us to extract controller equations from the learned neural
networks, providing insights into the decision-making process. We evaluate our
approach using different reward functions demonstrating its effectiveness in
improving network performance while providing interpretable policies.

</details>


### [327] [Adverseness vs. Equilibrium: Exploring Graph Adversarial Resilience through Dynamic Equilibrium](https://arxiv.org/abs/2505.14463)
*Xinxin Fan,Wenxiong Chen,Mengfan Li,Wenqi Wei,Ling Liu*

Main category: cs.LG

TL;DR: 该论文研究了图分析中的对抗攻击问题，提出了一个理论框架来证明临界对抗弹性状态的存在，并通过动态系统均衡点定位该状态，实验表明该方法优于现有防御方法。


<details>
  <summary>Details</summary>
Motivation: 当前针对图对抗攻击的防御方法主要从图本身或图神经网络的角度出发，但缺乏对图体系中是否存在内在对抗弹性状态的研究。本文旨在解决这一根本问题。

Method: 论文从三个独特视角出发：1) 将图对抗学习建模为复杂多目标动态系统；2) 提出广义理论框架证明临界对抗弹性状态的存在；3) 开发一维函数捕捉扰动下图体系的动态变化，并通过求解动态系统均衡点定位临界状态。

Result: 在五个常用真实数据集和三种代表性攻击下的多角度实验表明，所提方法显著优于现有最先进的防御方法。

Conclusion: 本文通过理论框架和动态系统方法，成功证明了图体系中临界对抗弹性状态的存在，并提出了有效的定位方法，为图对抗防御提供了新思路。

Abstract: Adversarial attacks to graph analytics are gaining increased attention. To
date, two lines of countermeasures have been proposed to resist various graph
adversarial attacks from the perspectives of either graph per se or graph
neural networks. Nevertheless, a fundamental question lies in whether there
exists an intrinsic adversarial resilience state within a graph regime and how
to find out such a critical state if exists. This paper contributes to tackle
the above research questions from three unique perspectives: i) we regard the
process of adversarial learning on graph as a complex multi-object dynamic
system, and model the behavior of adversarial attack; ii) we propose a
generalized theoretical framework to show the existence of critical adversarial
resilience state; and iii) we develop a condensed one-dimensional function to
capture the dynamic variation of graph regime under perturbations, and pinpoint
the critical state through solving the equilibrium point of dynamic system.
Multi-facet experiments are conducted to show our proposed approach can
significantly outperform the state-of-the-art defense methods under five
commonly-used real-world datasets and three representative attacks.

</details>


### [328] [ServerlessLoRA: Minimizing Latency and Cost in Serverless Inference for LoRA-Based LLMs](https://arxiv.org/abs/2505.14468)
*Yifan Sui,Hao Wang,Hanfei Yu,Yitao Hu,Jianxun Li,Hao Wang*

Main category: cs.LG

TL;DR: ServerlessLoRA提出了一种针对LoRA优化的无服务器推理系统，通过共享主干LLM、预加载和资源调度，显著降低了延迟和成本。


<details>
  <summary>Details</summary>
Motivation: 当前无服务器计算在服务通用LLM推理时表现良好，但在处理LoRA推理时存在参数冗余、加载延迟高和资源争用等问题，导致GPU浪费和成本增加。

Method: ServerlessLoRA通过跨函数共享主干LLM减少冗余，预加载LoRA工件降低冷启动延迟，并采用资源感知批处理和卸载缓解GPU争用。

Result: 实验表明，ServerlessLoRA将首次令牌时间（TTFT）降低86%，成本减少89%，显著优于现有方案。

Conclusion: ServerlessLoRA为LoRA推理提供了一种高效、低成本的解决方案，解决了当前无服务器计算的关键瓶颈。

Abstract: Serverless computing has grown rapidly for serving Large Language Model (LLM)
inference due to its pay-as-you-go pricing, fine-grained GPU usage, and rapid
scaling. However, our analysis reveals that current serverless can effectively
serve general LLM but fail with Low-Rank Adaptation (LoRA) inference due to
three key limitations: 1) massive parameter redundancy among functions where
99% of weights are unnecessarily duplicated, 2) costly artifact loading latency
beyond LLM loading, and 3) magnified resource contention when serving multiple
LoRA LLMs. These inefficiencies lead to massive GPU wastage, increased
Time-To-First-Token (TTFT), and high monetary costs.
  We propose ServerlessLoRA, a novel serverless inference system designed for
faster and cheaper LoRA LLM serving. ServerlessLoRA enables secure backbone LLM
sharing across isolated LoRA functions to reduce redundancy. We design a
pre-loading method that pre-loads comprehensive LoRA artifacts to minimize
cold-start latency. Furthermore, ServerlessLoRA employs contention aware
batching and offloading to mitigate GPU resource conflicts during bursty
workloads. Experiment on industrial workloads demonstrates that ServerlessLoRA
reduces TTFT by up to 86% and cuts monetary costs by up to 89% compared to
state-of-the-art LLM inference solutions.

</details>


### [329] [Personalised Insulin Adjustment with Reinforcement Learning: An In-Silico Validation for People with Diabetes on Intensive Insulin Treatment](https://arxiv.org/abs/2505.14477)
*Maria Panagiotou,Lorenzo Brigato,Vivien Streit,Amanda Hayoz,Stephan Proennecke,Stavros Athanasopoulos,Mikkel T. Olsen,Elizabeth J. den Brok,Cecilie H. Svensson,Konstantinos Makrilakis,Maria Xatzipsalti,Andriani Vazeou,Peter R. Mertens,Ulrik Pedersen-Bjergaard,Bastiaan E. de Galan,Stavroula Mougiakakou*

Main category: cs.LG

TL;DR: 该研究提出了一种基于强化学习的个性化胰岛素治疗推荐方法ABBA，用于改善1型和2型糖尿病患者的血糖控制，相比传统方法显著提高了血糖在目标范围内的时间。


<details>
  <summary>Details</summary>
Motivation: 尽管胰岛素制剂和技术有所进步，但调整胰岛素剂量对大多数1型和长期2型糖尿病患者仍是一个持续挑战。

Method: 研究开发了Adaptive Basal-Bolus Advisor (ABBA)，一种基于强化学习的个性化胰岛素治疗推荐方法，并通过FDA认可的模拟人群（101名1型和101名2型糖尿病患者）进行了计算机模拟测试。

Result: ABBA相比标准基础-餐时胰岛素顾问(BBA)显著提高了血糖在目标范围内的时间(TIR)，同时减少了血糖过低和过高的时间，且其性能在两个月内持续改善。

Conclusion: ABBA这种个性化胰岛素调整方法有潜力进一步优化血糖控制，支持糖尿病患者的日常自我管理，值得首次在人体中进行试验。

Abstract: Despite recent advances in insulin preparations and technology, adjusting
insulin remains an ongoing challenge for the majority of people with type 1
diabetes (T1D) and longstanding type 2 diabetes (T2D). In this study, we
propose the Adaptive Basal-Bolus Advisor (ABBA), a personalised insulin
treatment recommendation approach based on reinforcement learning for
individuals with T1D and T2D, performing self-monitoring blood glucose
measurements and multiple daily insulin injection therapy. We developed and
evaluated the ability of ABBA to achieve better time-in-range (TIR) for
individuals with T1D and T2D, compared to a standard basal-bolus advisor (BBA).
The in-silico test was performed using an FDA-accepted population, including
101 simulated adults with T1D and 101 with T2D. An in-silico evaluation shows
that ABBA significantly improved TIR and significantly reduced both times
below- and above-range, compared to BBA. ABBA's performance continued to
improve over two months, whereas BBA exhibited only modest changes. This
personalised method for adjusting insulin has the potential to further optimise
glycaemic control and support people with T1D and T2D in their daily
self-management. Our results warrant ABBA to be trialed for the first time in
humans.

</details>


### [330] [Learning to Integrate Diffusion ODEs by Averaging the Derivatives](https://arxiv.org/abs/2505.14502)
*Wenze Liu,Xiangyu Yue*

Main category: cs.LG

TL;DR: 该论文提出了一种名为'正割损失'的中间策略，通过微调或蒸馏预训练扩散模型，在保持性能的同时加速推理。


<details>
  <summary>Details</summary>
Motivation: 当前扩散模型推理加速方法中，数值求解器在极小步长下表现不佳，而蒸馏技术则常带来复杂性和不稳定性。

Method: 通过从导数-积分关系中推导损失函数，学习ODE积分，结合蒙特卡洛积分和Picard迭代的思想，逐步将切线延伸为正割。

Result: 在CIFAR-10上，EDM的正割版本实现了10步FID为2.14；在ImageNet-256×256上，SiT-XL/2的正割版本4步FID为2.27，8步FID为1.96。

Conclusion: 正割损失方法在扩散模型推理加速上取得了平衡性能与成本的显著效果。

Abstract: To accelerate diffusion model inference, numerical solvers perform poorly at
extremely small steps, while distillation techniques often introduce complexity
and instability. This work presents an intermediate strategy, balancing
performance and cost, by learning ODE integration using loss functions derived
from the derivative-integral relationship, inspired by Monte Carlo integration
and Picard iteration. From a geometric perspective, the losses operate by
gradually extending the tangent to the secant, thus are named as secant losses.
The secant losses can rapidly convert (via fine-tuning or distillation) a
pretrained diffusion model into its secant version. In our experiments, the
secant version of EDM achieves a $10$-step FID of $2.14$ on CIFAR-10, while the
secant version of SiT-XL/2 attains a $4$-step FID of $2.27$ and an $8$-step FID
of $1.96$ on ImageNet-$256\times256$. Code will be available.

</details>


### [331] [Just One Layer Norm Guarantees Stable Extrapolation](https://arxiv.org/abs/2505.14512)
*Juliusz Ziomek,George Whittle,Michael A. Osborne*

Main category: cs.LG

TL;DR: 论文通过NTK理论证明，在无限宽神经网络中加入一层Layer Norm能有效限制模型在训练数据外区域的输出范围，提升外推稳定性。


<details>
  <summary>Details</summary>
Motivation: 尽管神经网络广泛应用，但其在训练分布外区域的外推行为仍缺乏理论理解。现有研究局限于特定案例，需建立普适性分析框架。

Method: 应用神经正切核(NTK)理论分析无限宽网络，对比含/不含Layer Norm的网络在外推时的NTK特性差异，并通过有限宽度网络实验验证。

Result: 含Layer Norm的网络NTK变为有界方差核，输出保持稳定；而无Layer Norm的网络可能产生病态大输出。实验证实单层LN即可显著改善外推稳定性。

Conclusion: Layer Norm通过改变NTK性质成为外推稳定器，在蛋白质残基预测、跨种族年龄估计等任务中展现出实际应用价值。

Abstract: In spite of their prevalence, the behaviour of Neural Networks when
extrapolating far from the training distribution remains poorly understood,
with existing results limited to specific cases. In this work, we prove general
results -- the first of their kind -- by applying Neural Tangent Kernel (NTK)
theory to analyse infinitely-wide neural networks trained until convergence and
prove that the inclusion of just one Layer Norm (LN) fundamentally alters the
induced NTK, transforming it into a bounded-variance kernel. As a result, the
output of an infinitely wide network with at least one LN remains bounded, even
on inputs far from the training data. In contrast, we show that a broad class
of networks without LN can produce pathologically large outputs for certain
inputs. We support these theoretical findings with empirical experiments on
finite-width networks, demonstrating that while standard NNs often exhibit
uncontrolled growth outside the training domain, a single LN layer effectively
mitigates this instability. Finally, we explore real-world implications of this
extrapolatory stability, including applications to predicting residue sizes in
proteins larger than those seen during training and estimating age from facial
images of underrepresented ethnicities absent from the training set.

</details>


### [332] [Latent Flow Transformer](https://arxiv.org/abs/2505.14513)
*Yen-Chen Wu,Feng-Ting Liao,Meng-Hsi Chen,Pei-Chen Ho,Farhang Nabiei,Da-shan Shiu*

Main category: cs.LG

TL;DR: 论文提出Latent Flow Transformer (LFT)，通过流匹配训练替代传统Transformer中的多层结构，实现高效压缩并保持性能。


<details>
  <summary>Details</summary>
Motivation: 传统Transformer的多层结构效率低下，而基于扩散和流的连续层在图像生成中表现优越，因此探索在语言模型中应用连续层。

Method: LFT用单个学习的传输算子替代多层块，通过流匹配训练，并提出Flow Walking算法解决耦合保持问题。

Result: 在Pythia-410M模型上，LFT压缩6层性能优于跳过2层，进一步用FW算法压缩12层至1层，显著缩小自回归与流生成差距。

Conclusion: LFT证明了在语言模型中应用连续层的可行性，为高效压缩和性能提升提供了新方向。

Abstract: Transformers, the standard implementation for large language models (LLMs),
typically consist of tens to hundreds of discrete layers. While more layers can
lead to better performance, this approach has been challenged as far from
efficient, especially given the superiority of continuous layers demonstrated
by diffusion and flow-based models for image generation. We propose the Latent
Flow Transformer (LFT), which replaces a block of layers with a single learned
transport operator trained via flow matching, offering significant compression
while maintaining compatibility with the original architecture. Additionally,
we address the limitations of existing flow-based methods in \textit{preserving
coupling} by introducing the Flow Walking (FW) algorithm. On the Pythia-410M
model, LFT trained with flow matching compresses 6 of 24 layers and outperforms
directly skipping 2 layers (KL Divergence of LM logits at 0.407 vs. 0.529),
demonstrating the feasibility of this design. When trained with FW, LFT further
distills 12 layers into one while reducing the KL to 0.736 surpassing that from
skipping 3 layers (0.932), significantly narrowing the gap between
autoregressive and flow-based generation paradigms.

</details>


### [333] [Interpretable Dual-Stream Learning for Local Wind Hazard Prediction in Vulnerable Communities](https://arxiv.org/abs/2505.14522)
*Mahmuda Akhter Nishu,Chenyu Huang,Milad Roohi,Xin Zhong*

Main category: cs.LG

TL;DR: 该论文提出了一种可解释的双流学习框架，结合气象数据和文本事件描述，用于提升美国大平原地区弱势社区的风灾预测能力。


<details>
  <summary>Details</summary>
Motivation: 现有风灾预测系统主要关注气象因素，未能充分考虑社区特定的脆弱性，导致其在局部风险评估和韧性规划中的实用性受限。

Method: 采用随机森林和RoBERTa变换器的双流学习框架，通过后期融合机制整合结构化气象数据和非结构化文本叙事。

Result: 实验结果显示，该方法在性能上显著优于传统基线模型，并通过敏感性分析增强了模型决策的透明度。

Conclusion: 该框架不仅提高了预测准确性，还具有实际应用价值，能够支持应急准备和社区韧性建设。

Abstract: Wind hazards such as tornadoes and straight-line winds frequently affect
vulnerable communities in the Great Plains of the United States, where limited
infrastructure and sparse data coverage hinder effective emergency response.
Existing forecasting systems focus primarily on meteorological elements and
often fail to capture community-specific vulnerabilities, limiting their
utility for localized risk assessment and resilience planning. To address this
gap, we propose an interpretable dual-stream learning framework that integrates
structured numerical weather data with unstructured textual event narratives.
Our architecture combines a Random Forest and RoBERTa-based transformer through
a late fusion mechanism, enabling robust and context-aware wind hazard
prediction. The system is tailored for underserved tribal communities and
supports block-level risk assessment. Experimental results show significant
performance gains over traditional baselines. Furthermore, gradient-based
sensitivity and ablation studies provide insight into the model's
decision-making process, enhancing transparency and operational trust. The
findings demonstrate both predictive effectiveness and practical value in
supporting emergency preparedness and advancing community resilience.

</details>


### [334] [SifterNet: A Generalized and Model-Agnostic Trigger Purification Approach](https://arxiv.org/abs/2505.14531)
*Shaoye Luo,Xinxin Fan,Quanliang Jing,Chi Lin,Mengfan Li,Yunfeng Lu,Yongjun Xu*

Main category: cs.LG

TL;DR: 本文提出了一种基于Ising模型的通用、模型无关的触发器净化方法SifterNet，用于抵抗卷积神经网络和视觉Transformer大模型中的后门攻击，无需预先了解目标模型细节或大量干净样本，实验证明其性能优越。


<details>
  <summary>Details</summary>
Motivation: 现有触发器检测/移除方法通常需要预先了解目标模型细节、大量干净样本甚至模型重训练权限，这在实际应用中带来极大不便。理想的对策应能无视目标模型类型消除植入的触发器。

Method: 提出轻量级黑盒防御方法SifterNet，利用Hopfield网络的记忆关联功能，通过引入Ising模型思想有效净化输入样本中的触发器。

Result: 大量实验验证了该方法在触发器净化和高精度达成方面的有效性，在多个常用数据集上显著优于现有基线方法。

Conclusion: SifterNet通过Ising模型创新实现了无需目标模型知识的触发器净化，为后门防御提供了高效解决方案。

Abstract: Aiming at resisting backdoor attacks in convolution neural networks and
vision Transformer-based large model, this paper proposes a generalized and
model-agnostic trigger-purification approach resorting to the classic Ising
model. To date, existing trigger detection/removal studies usually require to
know the detailed knowledge of target model in advance, access to a large
number of clean samples or even model-retraining authorization, which brings
the huge inconvenience for practical applications, especially inaccessible to
target model. An ideal countermeasure ought to eliminate the implanted trigger
without regarding whatever the target models are. To this end, a lightweight
and black-box defense approach SifterNet is proposed through leveraging the
memorization-association functionality of Hopfield network, by which the
triggers of input samples can be effectively purified in a proper manner. The
main novelty of our proposed approach lies in the introduction of ideology of
Ising model. Extensive experiments also validate the effectiveness of our
approach in terms of proper trigger purification and high accuracy achievement,
and compared to the state-of-the-art baselines under several commonly-used
datasets, our SiferNet has a significant superior performance.

</details>


### [335] [Energy-Efficient Deep Reinforcement Learning with Spiking Transformers](https://arxiv.org/abs/2505.14533)
*Mohammad Irfan Uddin,Nishad Tasnim,Md Omor Faruk,Zejian Zhou*

Main category: cs.LG

TL;DR: 提出了一种结合脉冲神经网络（SNN）和强化学习的STRL算法，显著提高了能效和策略性能。


<details>
  <summary>Details</summary>
Motivation: 传统基于Transformer的强化学习模型计算复杂度高，能耗大，限制了其在现实自主系统中的部署。SNN因其生物启发的结构，提供了能效更高的替代方案。

Method: 设计了使用多步Leaky Integrate-and-Fire（LIF）神经元和注意力机制的SNN，结合状态、动作和奖励编码，构建了类似Transformer的结构。

Result: 在多个先进基准测试中，提出的SNN Transformer相比传统基于Transformer的模型，显著提升了策略性能和能效。

Conclusion: 该工作展示了在复杂现实决策场景中部署生物启发的低成本机器学习模型的潜力。

Abstract: Agent-based Transformers have been widely adopted in recent reinforcement
learning advances due to their demonstrated ability to solve complex tasks.
However, the high computational complexity of Transformers often results in
significant energy consumption, limiting their deployment in real-world
autonomous systems. Spiking neural networks (SNNs), with their biologically
inspired structure, offer an energy-efficient alternative for machine learning.
In this paper, a novel Spike-Transformer Reinforcement Learning (STRL)
algorithm that combines the energy efficiency of SNNs with the powerful
decision-making capabilities of reinforcement learning is developed.
Specifically, an SNN using multi-step Leaky Integrate-and-Fire (LIF) neurons
and attention mechanisms capable of processing spatio-temporal patterns over
multiple time steps is designed. The architecture is further enhanced with
state, action, and reward encodings to create a Transformer-like structure
optimized for reinforcement learning tasks. Comprehensive numerical experiments
conducted on state-of-the-art benchmarks demonstrate that the proposed SNN
Transformer achieves significantly improved policy performance compared to
conventional agent-based Transformers. With both enhanced energy efficiency and
policy optimality, this work highlights a promising direction for deploying
bio-inspired, low-cost machine learning models in complex real-world
decision-making scenarios.

</details>


### [336] [Spiking Neural Networks with Temporal Attention-Guided Adaptive Fusion for imbalanced Multi-modal Learning](https://arxiv.org/abs/2505.14535)
*Jiangrong Shen,Yulin Xie,Qi Xu,Gang Pan,Huajin Tang,Badong Chen*

Main category: cs.LG

TL;DR: 该论文提出了一种基于时间注意力引导的自适应融合框架（TAAF），用于解决多模态脉冲神经网络（SNNs）中的模态不平衡和时间错位问题，通过动态分配重要性分数和调制学习率，实现了高效的多模态学习。


<details>
  <summary>Details</summary>
Motivation: 多模态脉冲神经网络在高效感官处理方面具有潜力，但面临模态不平衡和时间错位的挑战。现有方法存在模态收敛速度不协调和静态融合机制忽略时间变化的问题。

Method: 论文提出了TAAF模块和时序自适应平衡融合损失函数，动态分配重要性分数并调制学习率，实现时间异质脉冲特征的层次整合和模态平衡。

Result: 在CREMA-D、AVE和EAD数据集上取得了最先进的性能（准确率分别为77.55%、70.65%和97.5%），并展示了高能效特性。

Conclusion: 该框架为神经形态系统中的时间相干多模态学习建立了新范式，弥合了生物感官处理与高效机器智能之间的差距。

Abstract: Multimodal spiking neural networks (SNNs) hold significant potential for
energy-efficient sensory processing but face critical challenges in modality
imbalance and temporal misalignment. Current approaches suffer from
uncoordinated convergence speeds across modalities and static fusion mechanisms
that ignore time-varying cross-modal interactions. We propose the temporal
attention-guided adaptive fusion framework for multimodal SNNs with two
synergistic innovations: 1) The Temporal Attention-guided Adaptive Fusion
(TAAF) module that dynamically assigns importance scores to fused spiking
features at each timestep, enabling hierarchical integration of temporally
heterogeneous spike-based features; 2) The temporal adaptive balanced fusion
loss that modulates learning rates per modality based on the above attention
scores, preventing dominant modalities from monopolizing optimization. The
proposed framework implements adaptive fusion, especially in the temporal
dimension, and alleviates the modality imbalance during multimodal learning,
mimicking cortical multisensory integration principles. Evaluations on CREMA-D,
AVE, and EAD datasets demonstrate state-of-the-art performance (77.55\%,
70.65\% and 97.5\%accuracy, respectively) with energy efficiency. The system
resolves temporal misalignment through learnable time-warping operations and
faster modality convergence coordination than baseline SNNs. This work
establishes a new paradigm for temporally coherent multimodal learning in
neuromorphic systems, bridging the gap between biological sensory processing
and efficient machine intelligence.

</details>


### [337] [Time to Embed: Unlocking Foundation Models for Time Series with Channel Descriptions](https://arxiv.org/abs/2505.14543)
*Utsav Dutta,Sina Khoshfetrat Pakazad,Henrik Ohlsson*

Main category: cs.LG

TL;DR: CHARM是一个创新的时间序列基础嵌入模型，通过结合通道级文本描述和新型训练架构，实现了跨领域的高效表示学习。


<details>
  <summary>Details</summary>
Motivation: 传统时间序列模型依赖特定任务和数据集，且特征工程复杂。尽管Transformer架构提升了可扩展性，但时间序列基础模型仍未被充分探索，主要集中在预测任务上。

Method: CHARM模型通过整合通道级文本描述（保持通道顺序不变性）和联合嵌入预测架构（JEPA），结合新型数据增强方案和损失函数，提升了表示的可解释性和训练稳定性。

Result: 参数量仅7M的CHARM模型在多种下游任务中达到最先进性能，为时间序列表示学习设立了新基准。

Conclusion: CHARM证明了基础模型在时间序列领域的潜力，其共享、可迁移且领域感知的表示能力为跨任务应用提供了新方向。

Abstract: Traditional time series models are task-specific and often depend on
dataset-specific training and extensive feature engineering. While
Transformer-based architectures have improved scalability, foundation models,
commonplace in text, vision, and audio, remain under-explored for time series
and are largely restricted to forecasting. We introduce $\textbf{CHARM}$, a
foundation embedding model for multivariate time series that learns shared,
transferable, and domain-aware representations. To address the unique
difficulties of time series foundation learning, $\textbf{CHARM}$ incorporates
architectural innovations that integrate channel-level textual descriptions
while remaining invariant to channel order. The model is trained using a Joint
Embedding Predictive Architecture (JEPA), with novel augmentation schemes and a
loss function designed to improve interpretability and training stability. Our
$7$M-parameter model achieves state-of-the-art performance across diverse
downstream tasks, setting a new benchmark for time series representation
learning.

</details>


### [338] [Physics-Guided Learning of Meteorological Dynamics for Weather Downscaling and Forecasting](https://arxiv.org/abs/2505.14555)
*Yingtao Luo,Shikai Fang,Binqing Wu,Qingsong Wen,Liang Sun*

Main category: cs.LG

TL;DR: PhyDL-NWP结合物理方程与深度学习，提升天气预报效率与准确性。


<details>
  <summary>Details</summary>
Motivation: 传统数值天气预报方法计算量大且物理不完整，深度学习模型虽高效但忽视物理规律，影响可解释性与泛化能力。

Method: 提出PhyDL-NWP框架，将物理方程与隐式力参数化融入数据驱动模型，通过自动微分计算物理项，并使用物理信息损失函数对齐预测与动态规律。

Result: PhyDL-NWP实现分辨率无关的降尺度，推理速度提升170倍，仅需55K参数，同时提高预报性能与物理一致性。

Conclusion: PhyDL-NWP通过物理引导的深度学习，显著优化了天气预报的效率和准确性。

Abstract: Weather forecasting is essential but remains computationally intensive and
physically incomplete in traditional numerical weather prediction (NWP)
methods. Deep learning (DL) models offer efficiency and accuracy but often
ignore physical laws, limiting interpretability and generalization. We propose
PhyDL-NWP, a physics-guided deep learning framework that integrates physical
equations with latent force parameterization into data-driven models. It
predicts weather variables from arbitrary spatiotemporal coordinates, computes
physical terms via automatic differentiation, and uses a physics-informed loss
to align predictions with governing dynamics. PhyDL-NWP enables resolution-free
downscaling by modeling weather as a continuous function and fine-tunes
pre-trained models with minimal overhead, achieving up to 170x faster inference
with only 55K parameters. Experiments show that PhyDL-NWP improves both
forecasting performance and physical consistency.

</details>


### [339] [Bellman operator convergence enhancements in reinforcement learning algorithms](https://arxiv.org/abs/2505.14564)
*David Krame Kadurha,Domini Jocema Leko Moutouo,Yae Ulrich Gaba*

Main category: cs.LG

TL;DR: 该论文通过拓扑学基础研究强化学习中的状态、动作和策略空间结构，利用Banach不动点定理和Bellman算子提升算法效率。


<details>
  <summary>Details</summary>
Motivation: 旨在通过数学理论深化对强化学习的理解，从而设计出更高效的决策算法。

Method: 回顾关键数学概念如完备度量空间，并利用Banach压缩原理和Bellman算子分析算法收敛性。

Result: 展示了Bellman算子的替代形式如何提升标准RL环境（如MountainCar）中的收敛速度和性能。

Conclusion: 数学理论的深入理解能有效提升强化学习算法的效率和性能。

Abstract: This paper reviews the topological groundwork for the study of reinforcement
learning (RL) by focusing on the structure of state, action, and policy spaces.
We begin by recalling key mathematical concepts such as complete metric spaces,
which form the foundation for expressing RL problems. By leveraging the Banach
contraction principle, we illustrate how the Banach fixed-point theorem
explains the convergence of RL algorithms and how Bellman operators, expressed
as operators on Banach spaces, ensure this convergence. The work serves as a
bridge between theoretical mathematics and practical algorithm design, offering
new approaches to enhance the efficiency of RL. In particular, we investigate
alternative formulations of Bellman operators and demonstrate their impact on
improving convergence rates and performance in standard RL environments such as
MountainCar, CartPole, and Acrobot. Our findings highlight how a deeper
mathematical understanding of RL can lead to more effective algorithms for
decision-making problems.

</details>


### [340] [KIPPO: Koopman-Inspired Proximal Policy Optimization](https://arxiv.org/abs/2505.14566)
*Andrei Cozma,Landon Harris,Hairong Qi*

Main category: cs.LG

TL;DR: 该论文提出了一种结合Koopman算子理论与PPO的强化学习方法KIPPO，通过近似线性化处理非线性系统动态，提升了策略学习的稳定性和性能。


<details>
  <summary>Details</summary>
Motivation: 尽管策略梯度方法如PPO在强化学习中表现优异，但在处理复杂非线性动态环境时仍面临梯度估计方差大、优化不稳定等问题。Koopman算子理论提供了一种将非线性系统线性化的框架，为改进策略学习提供了新思路。

Method: KIPPO通过引入Koopman近似辅助网络，在不改变核心策略或价值函数架构的情况下，学习系统动态的近似线性潜在空间表示，从而提升策略优化效果。

Result: 实验结果表明，KIPPO在多种连续控制任务中性能提升6-60%，同时将训练变异性降低高达91%。

Conclusion: KIPPO通过结合Koopman算子理论与PPO，有效提升了策略学习的稳定性和性能，为处理复杂非线性动态环境提供了一种新方法。

Abstract: Reinforcement Learning (RL) has made significant strides in various domains,
and policy gradient methods like Proximal Policy Optimization (PPO) have gained
popularity due to their balance in performance, training stability, and
computational efficiency. These methods directly optimize policies through
gradient-based updates. However, developing effective control policies for
environments with complex and non-linear dynamics remains a challenge. High
variance in gradient estimates and non-convex optimization landscapes often
lead to unstable learning trajectories. Koopman Operator Theory has emerged as
a powerful framework for studying non-linear systems through an
infinite-dimensional linear operator that acts on a higher-dimensional space of
measurement functions. In contrast with their non-linear counterparts, linear
systems are simpler, more predictable, and easier to analyze. In this paper, we
present Koopman-Inspired Proximal Policy Optimization (KIPPO), which learns an
approximately linear latent-space representation of the underlying system's
dynamics while retaining essential features for effective policy learning. This
is achieved through a Koopman-approximation auxiliary network that can be added
to the baseline policy optimization algorithms without altering the
architecture of the core policy or value function. Extensive experimental
results demonstrate consistent improvements over the PPO baseline with 6-60%
increased performance while reducing variability by up to 91% when evaluated on
various continuous control tasks.

</details>


### [341] [Adaptive Pruning of Deep Neural Networks for Resource-Aware Embedded Intrusion Detection on the Edge](https://arxiv.org/abs/2505.14592)
*Alexandre Broggi,Nathaniel Bastian,Lance Fiondella,Gokhan Kul*

Main category: cs.LG

TL;DR: 该论文研究了人工神经网络剪枝方法在网络安全数据集上的泛化能力，发现大多数方法效果不佳，仅有少数算法表现良好。


<details>
  <summary>Details</summary>
Motivation: 研究人工神经网络剪枝方法在简化网络结构和新数据集（网络安全领域）上的泛化能力，以优化模型大小和推理速度。

Method: 通过多种剪枝程度分析不同剪枝算法在新环境中的表现，评估其预测能力保留情况。

Result: 大多数剪枝方法在新数据集上泛化能力较差，仅有少数算法达到可接受水平。

Conclusion: 在网络安全数据集上，仅有部分剪枝算法表现良好，多数方法泛化能力不足。

Abstract: Artificial neural network pruning is a method in which artificial neural
network sizes can be reduced while attempting to preserve the predicting
capabilities of the network. This is done to make the model smaller or faster
during inference time. In this work we analyze the ability of a selection of
artificial neural network pruning methods to generalize to a new cybersecurity
dataset utilizing a simpler network type than was designed for. We analyze each
method using a variety of pruning degrees to best understand how each algorithm
responds to the new environment. This has allowed us to determine the most well
fit pruning method of those we searched for the task. Unexpectedly, we have
found that many of them do not generalize to the problem well, leaving only a
few algorithms working to an acceptable degree.

</details>


### [342] [Physics-informed Reduced Order Modeling of Time-dependent PDEs via Differentiable Solvers](https://arxiv.org/abs/2505.14595)
*Nima Hosseini Dashtbayaz,Hesam Salehipour,Adrian Butscher,Nigel Morris*

Main category: cs.LG

TL;DR: 该论文提出了一种物理信息降阶建模方法（Φ-ROM），通过将可微分的PDE求解器融入训练过程，显著提升了模型在未见参数下的泛化能力和长期预测性能。


<details>
  <summary>Details</summary>
Motivation: 传统降阶建模（ROM）方法在训练过程中未充分利用高保真数值求解器的物理信息，导致潜在动力学偏离实际物理规律，限制了模型的泛化和预测能力。

Method: 提出Φ-ROM框架，将可微分PDE求解器嵌入训练流程，使潜在空间动力学直接受求解器编码的物理规律约束，确保降阶系统与完整系统的强一致性。

Result: Φ-ROM在泛化性、长期预测、时空连续性及数据效率上均超越现有数据驱动ROM方法，并能基于稀疏/不规则观测数据重建和预测解场。

Conclusion: Φ-ROM通过紧密耦合数值求解器与机器学习，为复杂PDE系统提供了高精度、可解释且数据高效的降阶建模方案，其开源实现（JAX）具有广泛适用性。

Abstract: Reduced-order modeling (ROM) of time-dependent and parameterized differential
equations aims to accelerate the simulation of complex high-dimensional systems
by learning a compact latent manifold representation that captures the
characteristics of the solution fields and their time-dependent dynamics.
Although high-fidelity numerical solvers generate the training datasets, they
have thus far been excluded from the training process, causing the learned
latent dynamics to drift away from the discretized governing physics. This
mismatch often limits generalization and forecasting capabilities. In this
work, we propose Physics-informed ROM ($\Phi$-ROM) by incorporating
differentiable PDE solvers into the training procedure. Specifically, the
latent space dynamics and its dependence on PDE parameters are shaped directly
by the governing physics encoded in the solver, ensuring a strong
correspondence between the full and reduced systems. Our model outperforms
state-of-the-art data-driven ROMs and other physics-informed strategies by
accurately generalizing to new dynamics arising from unseen parameters,
enabling long-term forecasting beyond the training horizon, maintaining
continuity in both time and space, and reducing the data cost. Furthermore,
$\Phi$-ROM learns to recover and forecast the solution fields even when trained
or evaluated with sparse and irregular observations of the fields, providing a
flexible framework for field reconstruction and data assimilation. We
demonstrate the framework's robustness across different PDE solvers and
highlight its broad applicability by providing an open-source JAX
implementation readily extensible to other PDE systems and differentiable
solvers.

</details>


### [343] [CSTS: A Benchmark for the Discovery of Correlation Structures in Time Series Clustering](https://arxiv.org/abs/2505.14596)
*Isabella Degen,Zahraa S Abdallah,Henry W J Reeve,Kate Robson Brown*

Main category: cs.LG

TL;DR: 该论文提出了CSTS合成基准，用于评估多元时间序列数据中相关性结构的发现，帮助区分聚类失败的具体原因，并提供了数据生成框架和评估协议。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏已验证的真实数据，研究人员难以客观评估时间序列聚类的质量，无法确定聚类结果不佳是由于数据本身缺乏结构、算法限制还是不恰当的验证方法。为了解决这些问题，作者提出了CSTS基准。

Method: 引入CSTS（时间序列中的相关性结构）合成基准，通过系统变化数据条件、设定性能阈值和推荐评估协议，提供一个干净的基准来区分相关性结构退化与聚类算法及验证方法的限制。

Result: CSTS基准能够有效区分聚类失败的具体原因，实证验证显示降采样会适度扭曲相关性结构，而分布偏移和稀疏化的影响较小。案例研究展示了CSTS在诊断算法对非正态分布敏感性的实用性。

Conclusion: CSTS为基于相关性的时间序列聚类提供了严格的评估标准，能够帮助研究人员精确诊断方法局限性，推动该领域的科学性和严谨性。

Abstract: Time series clustering promises to uncover hidden structural patterns in data
with applications across healthcare, finance, industrial systems, and other
critical domains. However, without validated ground truth information,
researchers cannot objectively assess clustering quality or determine whether
poor results stem from absent structures in the data, algorithmic limitations,
or inappropriate validation methods, raising the question whether clustering is
"more art than science" (Guyon et al., 2009). To address these challenges, we
introduce CSTS (Correlation Structures in Time Series), a synthetic benchmark
for evaluating the discovery of correlation structures in multivariate time
series data. CSTS provides a clean benchmark that enables researchers to
isolate and identify specific causes of clustering failures by differentiating
between correlation structure deterioration and limitations of clustering
algorithms and validation methods. Our contributions are: (1) a comprehensive
benchmark for correlation structure discovery with distinct correlation
structures, systematically varied data conditions, established performance
thresholds, and recommended evaluation protocols; (2) empirical validation of
correlation structure preservation showing moderate distortion from
downsampling and minimal effects from distribution shifts and sparsification;
and (3) an extensible data generation framework enabling structure-first
clustering evaluation. A case study demonstrates CSTS's practical utility by
identifying an algorithm's previously undocumented sensitivity to non-normal
distributions, illustrating how the benchmark enables precise diagnosis of
methodological limitations. CSTS advances rigorous evaluation standards for
correlation-based time series clustering.

</details>


### [344] [Electrostatics from Laplacian Eigenbasis for Neural Network Interatomic Potentials](https://arxiv.org/abs/2505.14606)
*Maksim Zhdanov,Vladislav Kurenkov*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Recent advances in neural network interatomic potentials have emerged as a
promising research direction. However, popular deep learning models often lack
auxiliary constraints grounded in physical laws, which could accelerate
training and improve fidelity through physics-based regularization. In this
work, we introduce $\Phi$-Module, a universal plugin module that enforces
Poisson's equation within the message-passing framework to learn electrostatic
interactions in a self-supervised manner. Specifically, each atom-wise
representation is encouraged to satisfy a discretized Poisson's equation,
making it possible to acquire a potential $\boldsymbol{\phi}$ and a
corresponding charge density $\boldsymbol{\rho}$ linked to the learnable
Laplacian eigenbasis coefficients of a given molecular graph. We then derive an
electrostatic energy term, crucial for improved total energy predictions. This
approach integrates seamlessly into any existing neural potential with
insignificant computational overhead. Experiments on the OE62 and MD22
benchmarks confirm that models combined with $\Phi$-Module achieve robust
improvements over baseline counterparts. For OE62 error reduction ranges from
4.5\% to 17.8\%, and for MD22, baseline equipped with $\Phi$-Module achieves
best results on 5 out of 14 cases. Our results underscore how embedding a
first-principles constraint in neural interatomic potentials can significantly
improve performance while remaining hyperparameter-friendly, memory-efficient
and lightweight in training. Code will be available at
\href{https://github.com/dunnolab/phi-module}{dunnolab/phi-module}.

</details>


### [345] [MMD-Newton Method for Multi-objective Optimization](https://arxiv.org/abs/2505.14610)
*Hao Wang,Chenyu Shi,Angel E. Rodriguez-Fernandez,Oliver Schütze*

Main category: cs.LG

TL;DR: 该论文提出了一种基于最大均值差异（MMD）的牛顿方法（MMDN）来解决连续多目标优化问题（MOPs），并通过与多目标进化算法（MOEAs）的混合使用，显著提高了优化精度。


<details>
  <summary>Details</summary>
Motivation: 多目标优化问题（MOPs）的解决通常需要最小化帕累托前沿的有限近似集与参考集之间的距离。传统方法如Hausdorff距离存在局限性，因此作者提出使用MMD作为距离度量，以更有效地衡量两个集合之间的差异。

Method: 论文提出了MMDN方法，通过分析MMD的梯度和Hessian矩阵的解析表达式，设计了一种基于牛顿法的集合优化方法。此外，还提出了将MMDN与MOEAs混合使用的策略，先用MOEAs进行初步优化，再用MMDN进行精细优化。

Result: 在11个广泛使用的基准问题上进行的实验表明，混合方法（MMDN + MOEA）在相同计算预算下，比单独使用MOEA能够获得更高的优化精度。

Conclusion: MMDN方法在多目标优化问题中表现出色，尤其是在与MOEAs混合使用时，能够显著提升优化效果。该方法为连续多目标优化问题提供了一种新的解决方案。

Abstract: Maximum mean discrepancy (MMD) has been widely employed to measure the
distance between probability distributions. In this paper, we propose using MMD
to solve continuous multi-objective optimization problems (MOPs). For solving
MOPs, a common approach is to minimize the distance (e.g., Hausdorff) between a
finite approximate set of the Pareto front and a reference set. Viewing these
two sets as empirical measures, we propose using MMD to measure the distance
between them. To minimize the MMD value, we provide the analytical expression
of its gradient and Hessian matrix w.r.t. the search variables, and use them to
devise a novel set-oriented, MMD-based Newton (MMDN) method. Also, we analyze
the theoretical properties of MMD's gradient and Hessian, including the
first-order stationary condition and the eigenspectrum of the Hessian, which
are important for verifying the correctness of MMDN. To solve complicated
problems, we propose hybridizing MMDN with multiobjective evolutionary
algorithms (MOEAs), where we first execute an EA for several iterations to get
close to the global Pareto front and then warm-start MMDN with the result of
the MOEA to efficiently refine the approximation. We empirically test the
hybrid algorithm on 11 widely used benchmark problems, and the results show the
hybrid (MMDN + MOEA) can achieve a much better optimization accuracy than EA
alone with the same computation budget.

</details>


### [346] [Virtual Cells: Predict, Explain, Discover](https://arxiv.org/abs/2505.14613)
*Emmanuel Noutahi,Jason Hartford,Prudencio Tossou,Shawn Whitfield,Alisandra K. Denton,Cas Wognum,Kristina Ulicna,Jonathan Hsu,Michael Cuccarese,Emmanuel Bengio,Dominique Beaini,Christopher Gibson,Daniel Cohen,Berton Earnshaw*

Main category: cs.LG

TL;DR: 该论文提出利用AI和高通量细胞分析技术开发虚拟细胞模型，以预测细胞对扰动的功能响应，从而加速药物发现。


<details>
  <summary>Details</summary>
Motivation: 药物发现需要可靠的患者响应模拟模型，以安全、经济地测试大量治疗假设。虚拟细胞模型能预测细胞对扰动的功能响应，有助于发现安全有效的治疗方法。

Method: 论文提出开发虚拟细胞模型的关键原则，采用“实验室在环”方法生成新见解，并倡导基于生物学的基准测试指导模型开发。

Result: 虚拟细胞模型需准确预测细胞功能响应并解释其分子机制，为药物发现提供有用工具，并可扩展至更高层次的组织模型（如虚拟患者）。

Conclusion: 虚拟细胞模型为药物发现提供了优化框架，未来可推动更高层次模型的开发，对研究社区具有重要价值。

Abstract: Drug discovery is fundamentally a process of inferring the effects of
treatments on patients, and would therefore benefit immensely from
computational models that can reliably simulate patient responses, enabling
researchers to generate and test large numbers of therapeutic hypotheses safely
and economically before initiating costly clinical trials. Even a more specific
model that predicts the functional response of cells to a wide range of
perturbations would be tremendously valuable for discovering safe and effective
treatments that successfully translate to the clinic. Creating such virtual
cells has long been a goal of the computational research community that
unfortunately remains unachieved given the daunting complexity and scale of
cellular biology. Nevertheless, recent advances in AI, computing power, lab
automation, and high-throughput cellular profiling provide new opportunities
for reaching this goal. In this perspective, we present a vision for developing
and evaluating virtual cells that builds on our experience at Recursion. We
argue that in order to be a useful tool to discover novel biology, virtual
cells must accurately predict the functional response of a cell to
perturbations and explain how the predicted response is a consequence of
modifications to key biomolecular interactions. We then introduce key
principles for designing therapeutically-relevant virtual cells, describe a
lab-in-the-loop approach for generating novel insights with them, and advocate
for biologically-grounded benchmarks to guide virtual cell development.
Finally, we make the case that our approach to virtual cells provides a useful
framework for building other models at higher levels of organization, including
virtual patients. We hope that these directions prove useful to the research
community in developing virtual models optimized for positive impact on drug
discovery outcomes.

</details>


### [347] [Enhancing Learned Knowledge in LoRA Adapters Through Efficient Contrastive Decoding on Ascend NPUs](https://arxiv.org/abs/2505.14620)
*Morgan Lindsay Heisler,Linzi Xing,Ge Shi,Hanieh Sadri,Gursimran Singh,Weiwei Zhang,Tao Ye,Ying Xiong,Yong Zhang,Zhenan Fan*

Main category: cs.LG

TL;DR: 华为云用户使用LoRA高效微调大语言模型，但传统解码方法存在基础模型偏差问题。本文提出CoLD解码框架，通过对比专家模型与基础模型的概率分布差异优化解码，提升任务准确性并降低延迟。


<details>
  <summary>Details</summary>
Motivation: 传统解码方法（如贪婪搜索）在复杂推理任务中易受基础模型偏差干扰，导致响应泛化，无法充分利用LoRA的定制化知识。

Method: 提出CoLD框架，通过对比LoRA专家模型与基础模型的token概率分布差异进行解码，并针对华为昇腾NPU优化计算内核。

Result: CoLD使任务准确率最高提升5.54%，端到端延迟降低28%，优于贪婪解码。

Conclusion: CoLD为资源受限环境下的微调大模型提供了高效解码方案，对云端及本地应用具有广泛意义。

Abstract: Huawei Cloud users leverage LoRA (Low-Rank Adaptation) as an efficient and
scalable method to fine-tune and customize large language models (LLMs) for
application-specific needs. However, tasks that require complex reasoning or
deep contextual understanding are often hindered by biases or interference from
the base model when using typical decoding methods like greedy or beam search.
These biases can lead to generic or task-agnostic responses from the base model
instead of leveraging the LoRA-specific adaptations. In this paper, we
introduce Contrastive LoRA Decoding (CoLD), a novel decoding framework designed
to maximize the use of task-specific knowledge in LoRA-adapted models,
resulting in better downstream performance. CoLD uses contrastive decoding by
scoring candidate tokens based on the divergence between the probability
distributions of a LoRA-adapted expert model and the corresponding base model.
This approach prioritizes tokens that better align with the LoRA's learned
representations, enhancing performance for specialized tasks. While effective,
a naive implementation of CoLD is computationally expensive because each
decoding step requires evaluating multiple token candidates across both models.
To address this, we developed an optimized kernel for Huawei's Ascend NPU. CoLD
achieves up to a 5.54% increase in task accuracy while reducing end-to-end
latency by 28% compared to greedy decoding. This work provides practical and
efficient decoding strategies for fine-tuned LLMs in resource-constrained
environments and has broad implications for applied data science in both cloud
and on-premises settings.

</details>


### [348] [TinyV: Reducing False Negatives in Verification Improves RL for LLM Reasoning](https://arxiv.org/abs/2505.14625)
*Zhangchen Xu,Yuetai Li,Fengqing Jiang,Bhaskar Ramasubramanian,Luyao Niu,Bill Yuchen Lin,Radha Poovendran*

Main category: cs.LG

TL;DR: 论文揭示强化学习中验证器错误否定（false negatives）问题，提出轻量级验证器tinyV以提升奖励信号准确性，显著改善模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习依赖验证器提供可靠奖励信号，但研究发现验证器常错误否定正确输出（38%案例），导致训练效率下降，需解决此问题以优化模型微调。

Method: 提出tinyV——基于轻量级LLM的验证器，动态识别潜在错误否定并恢复有效响应，结合规则方法提升奖励估计准确性。

Result: 在数学推理任务中，tinyV将通过率提升最高10%，加速模型收敛，证明其有效缓解错误否定对训练的负面影响。

Conclusion: 解决验证器错误否定对RL训练至关重要，tinyV为改进LLM微调提供了实用方案，代码已开源。

Abstract: Reinforcement Learning (RL) has become a powerful tool for enhancing the
reasoning abilities of large language models (LLMs) by optimizing their
policies with reward signals. Yet, RL's success relies on the reliability of
rewards, which are provided by verifiers. In this paper, we expose and analyze
a widespread problem--false negatives--where verifiers wrongly reject correct
model outputs. Our in-depth study of the Big-Math-RL-Verified dataset reveals
that over 38% of model-generated responses suffer from false negatives, where
the verifier fails to recognize correct answers. We show, both empirically and
theoretically, that these false negatives severely impair RL training by
depriving the model of informative gradient signals and slowing convergence. To
mitigate this, we propose tinyV, a lightweight LLM-based verifier that augments
existing rule-based methods, which dynamically identifies potential false
negatives and recovers valid responses to produce more accurate reward
estimates. Across multiple math-reasoning benchmarks, integrating TinyV boosts
pass rates by up to 10% and accelerates convergence relative to the baseline.
Our findings highlight the critical importance of addressing verifier false
negatives and offer a practical approach to improve RL-based fine-tuning of
LLMs. Our code is available at https://github.com/uw-nsl/TinyV.

</details>


### [349] [KERL: Knowledge-Enhanced Personalized Recipe Recommendation using Large Language Models](https://arxiv.org/abs/2505.14629)
*Fnu Mohbat,Mohammed J Zaki*

Main category: cs.LG

TL;DR: KERL系统整合食品知识图谱与大语言模型，提供个性化食谱推荐及营养分析，性能显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏食品知识图谱与大语言模型的深度整合，KERL旨在填补这一空白，提供更全面的食品推荐解决方案。

Method: KERL通过自然语言问题提取实体，检索知识图谱子图作为上下文输入大语言模型，生成满足条件的食谱及营养信息。

Result: 实验表明，KERL在食品推荐、食谱生成和营养分析方面显著优于现有方法，并提供了公开的基准数据集。

Conclusion: KERL为食品推荐领域提供了一个完整、连贯的解决方案，展示了知识图谱与大语言模型结合的潜力。

Abstract: Recent advances in large language models (LLMs) and the abundance of food
data have resulted in studies to improve food understanding using LLMs. Despite
several recommendation systems utilizing LLMs and Knowledge Graphs (KGs), there
has been limited research on integrating food related KGs with LLMs. We
introduce KERL, a unified system that leverages food KGs and LLMs to provide
personalized food recommendations and generates recipes with associated
micro-nutritional information. Given a natural language question, KERL extracts
entities, retrieves subgraphs from the KG, which are then fed into the LLM as
context to select the recipes that satisfy the constraints. Next, our system
generates the cooking steps and nutritional information for each recipe. To
evaluate our approach, we also develop a benchmark dataset by curating recipe
related questions, combined with constraints and personal preferences. Through
extensive experiments, we show that our proposed KG-augmented LLM significantly
outperforms existing approaches, offering a complete and coherent solution for
food recommendation, recipe generation, and nutritional analysis. Our code and
benchmark datasets are publicly available at
https://github.com/mohbattharani/KERL.

</details>


### [350] [Bridging Predictive Coding and MDL: A Two-Part Code Framework for Deep Learning](https://arxiv.org/abs/2505.14635)
*Benjamin Prada,Shion Matsumoto,Abdul Malik Zekri,Ankur Mali*

Main category: cs.LG

TL;DR: 该论文首次将生物启发的预测编码（PC）与深度学习中的最小描述长度（MDL）原则联系起来，证明了PC训练能提供理论保证的泛化和收敛性能。


<details>
  <summary>Details</summary>
Motivation: 研究旨在为预测编码（PC）这一生物启发的局部学习规则提供理论支持，并将其与深度学习中的最小描述长度（MDL）原则联系起来，以替代传统的反向传播方法。

Method: 通过理论分析，证明逐层PC执行块坐标下降优化MDL目标，结合Hoeffding不等式和前缀编码先验，推导出新的泛化边界。

Result: 论文表明PC训练能单调降低经验编码长度，提供比无约束梯度下降更紧的风险边界，并收敛到块坐标稳定点，近似MDL最优解。

Conclusion: 该研究首次为PC训练的深度模型提供了理论保证，使其成为具有生物合理性的反向传播替代方案。

Abstract: We present the first theoretical framework that connects predictive coding
(PC), a biologically inspired local learning rule, with the minimum description
length (MDL) principle in deep networks. We prove that layerwise PC performs
block-coordinate descent on the MDL two-part code objective, thereby jointly
minimizing empirical risk and model complexity. Using Hoeffding's inequality
and a prefix-code prior, we derive a novel generalization bound of the form
$R(\theta) \le \^{R}(\theta) + \frac{L(\theta)}{N}$, capturing the tradeoff
between fit and compression. We further prove that each PC sweep monotonically
decreases the empirical two-part codelength, yielding tighter high-probability
risk bounds than unconstrained gradient descent. Finally, we show that repeated
PC updates converge to a block-coordinate stationary point, providing an
approximate MDL-optimal solution. To our knowledge, this is the first result
offering formal generalization and convergence guarantees for PC-trained deep
models, positioning PC as a theoretically grounded and biologically plausible
alternative to backpropagation.

</details>


### [351] [Early Diagnosis of Atrial Fibrillation Recurrence: A Large Tabular Model Approach with Structured and Unstructured Clinical Data](https://arxiv.org/abs/2505.14643)
*Ane G. Domingo-Aldama,Marcos Merino Prado,Alain García Olea,Koldo Gojenola Galletebeitia,Josu Goikoetxea Salutregi,Aitziber Atutxa Salazar*

Main category: cs.LG

TL;DR: 该研究通过结合结构化临床数据和自由文本出院报告，利用机器学习模型（尤其是LTM方法）提高了房颤复发的预测准确性，并揭示了传统临床评分的局限性。


<details>
  <summary>Details</summary>
Motivation: 房颤（AF）是一种常见且高发病率、高死亡率的 arrhythmia。在快速发展的 AF 节律控制治疗时代，预测 AF 复发对优化治疗方案至关重要。然而，传统评分如 CHADS2-VASc、HATCH 和 APPLE 预测准确性有限，且早期诊断研究常依赖可能存在错误和缺失信息的电子健康记录（EHR）数据。

Method: 研究通过自然语言处理技术处理自由文本出院报告，结合结构化临床数据生成高质量表格数据集。在 1,508 名 AF 患者中，评估了传统临床评分、机器学习模型及提出的 LTM 方法。

Result: LTM 方法在预测 AF 复发方面表现最佳，超越了传统临床评分和其他机器学习模型。此外，性别和年龄偏差分析揭示了人口统计学差异。

Conclusion: 结合结构化数据和自由文本源可生成高质量数据集。研究结果强调了传统临床评分在预测 AF 复发方面的局限性，并突出了基于机器学习的方法（尤其是 LTM 模型）的潜力。

Abstract: BACKGROUND: Atrial fibrillation (AF), the most common arrhythmia, is linked
to high morbidity and mortality. In a fast-evolving AF rhythm control treatment
era, predicting AF recurrence after its onset may be crucial to achieve the
optimal therapeutic approach, yet traditional scores like CHADS2-VASc, HATCH,
and APPLE show limited predictive accuracy. Moreover, early diagnosis studies
often rely on codified electronic health record (EHR) data, which may contain
errors and missing information.
  OBJECTIVE: This study aims to predict AF recurrence between one month and two
years after onset by evaluating traditional clinical scores, ML models, and our
LTM approach. Moreover, another objective is to develop a methodology for
integrating structured and unstructured data to enhance tabular dataset
quality.
  METHODS: A tabular dataset was generated by combining structured clinical
data with free-text discharge reports processed through natural language
processing techniques, reducing errors and annotation effort. A total of 1,508
patients with documented AF onset were identified, and models were evaluated on
a manually annotated test set. The proposed approach includes a LTM compared
against traditional clinical scores and ML models.
  RESULTS: The proposed LTM approach achieved the highest predictive
performance, surpassing both traditional clinical scores and ML models.
Additionally, the gender and age bias analyses revealed demographic
disparities.
  CONCLUSION: The integration of structured data and free-text sources resulted
in a high-quality dataset. The findings emphasize the limitations of
traditional clinical scores in predicting AF recurrence and highlight the
potential of ML-based approaches, particularly our LTM model.

</details>


### [352] [Explainable AI for Securing Healthcare in IoT-Integrated 6G Wireless Networks](https://arxiv.org/abs/2505.14659)
*Navneet Kaur,Lav Gupta*

Main category: cs.LG

TL;DR: 论文探讨了在6G医疗应用中，如何利用可解释AI技术（如SHAP、LIME和DiCE）来增强安全性和透明度。


<details>
  <summary>Details</summary>
Motivation: 随着医疗系统越来越多地采用先进的无线网络和连接设备，确保医疗应用的安全性变得至关重要。物联网医疗设备的集成虽然提升了患者护理，但也带来了严重的安全风险。

Method: 论文采用了可解释AI技术（SHAP、LIME和DiCE）来揭示漏洞并加强防御。

Result: 实验分析支持了该方法，并展示了有希望的结果。

Conclusion: 通过可解释AI技术，可以在6G医疗应用中提高安全性和信任度。

Abstract: As healthcare systems increasingly adopt advanced wireless networks and
connected devices, securing medical applications has become critical. The
integration of Internet of Medical Things devices, such as robotic surgical
tools, intensive care systems, and wearable monitors has enhanced patient care
but introduced serious security risks. Cyberattacks on these devices can lead
to life threatening consequences, including surgical errors, equipment failure,
and data breaches. While the ITU IMT 2030 vision highlights 6G's transformative
role in healthcare through AI and cloud integration, it also raises new
security concerns. This paper explores how explainable AI techniques like SHAP,
LIME, and DiCE can uncover vulnerabilities, strengthen defenses, and improve
trust and transparency in 6G enabled healthcare. We support our approach with
experimental analysis and highlight promising results.

</details>


### [353] [Quartet: Native FP4 Training Can Be Optimal for Large Language Models](https://arxiv.org/abs/2505.14669)
*Roberto L. Castro,Andrei Panferov,Soroush Tabesh,Oliver Sieberling,Jiale Chen,Mahdi Nikdan,Saleh Ashkboos,Dan Alistarh*

Main category: cs.LG

TL;DR: 论文提出Quartet方法，实现全FP4低精度训练大语言模型，在保持精度的同时显著提升计算效率。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型训练的计算需求激增，低精度运算（如FP4）能提高计算吞吐和能效，但现有方法存在精度损失和混合精度依赖问题。

Method: 提出Quartet方法，通过系统研究硬件支持的FP4训练，设计优化的CUDA内核，实现端到端FP4低精度计算。

Result: 在Llama类模型上验证了Quartet的有效性，实现了FP4精度下十亿级模型的训练，精度与标准精度和FP8训练相当。

Conclusion: Quartet证明全FP4训练是可行的，为低精度训练提供了新的解决方案。

Abstract: The rapid advancement of large language models (LLMs) has been paralleled by
unprecedented increases in computational demands, with training costs for
state-of-the-art models doubling every few months. Training models directly in
low-precision arithmetic offers a solution, by improving both computational
throughput and energy efficiency. Specifically, NVIDIA's recent Blackwell
architecture facilitates extremely low-precision operations, specifically FP4
variants, promising substantial efficiency gains. Yet, current algorithms for
training LLMs in FP4 precision face significant accuracy degradation and often
rely on mixed-precision fallbacks. In this paper, we systematically investigate
hardware-supported FP4 training and introduce Quartet, a new approach enabling
accurate, end-to-end FP4 training with all the major computations (in e.g.
linear layers) being performed in low precision. Through extensive evaluations
on Llama-type models, we reveal a new low-precision scaling law that quantifies
performance trade-offs across varying bit-widths and allows us to identify a
"near-optimal" low-precision training technique in terms of
accuracy-vs-computation, called Quartet. We implement Quartet using optimized
CUDA kernels tailored for NVIDIA Blackwell GPUs, and show that it can achieve
state-of-the-art accuracy for FP4 precision, successfully training
billion-scale models. Our method demonstrates that fully FP4-based training is
a competitive alternative to standard-precision and FP8 training. Our code is
available at https://github.com/IST-DASLab/Quartet.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [354] [When Bias Backfires: The Modulatory Role of Counterfactual Explanations on the Adoption of Algorithmic Bias in XAI-Supported Human Decision-Making](https://arxiv.org/abs/2505.14377)
*Ulrike Kuhl,Annika Bush*

Main category: cs.HC

TL;DR: 研究发现，带有偏见的AI推荐会潜移默化影响人类决策，反事实解释可逆转这种偏见效应，但信任度未受影响。


<details>
  <summary>Details</summary>
Motivation: 探讨AI推荐中隐含的性别偏见如何通过人机协作影响人类决策行为，以及解释性工具（XAI）能否缓解这种偏见传播。

Method: 采用三阶段受控实验（无AI基线阶段→带偏见AI推荐阶段→无AI后测阶段），294名参与者模拟招聘决策，对比有无反事实解释条件下偏见传播差异。

Result: 70%情况下参与者采纳AI建议；仅2.7%察觉偏见。无解释时人类决策被AI偏见同化，有解释时则逆转偏见。信任度无显著变化，但对男性偏见的AI会微妙影响决策信心。

Conclusion: 需精细校准可解释AI（XAI）以避免决策行为被算法偏见无形塑造，维护公平性并防止偏见扩散。

Abstract: Although the integration of artificial intelligence (AI) into everyday tasks
improves efficiency and objectivity, it also risks transmitting bias to human
decision-making. In this study, we conducted a controlled experiment that
simulated hiring decisions to examine how biased AI recommendations - augmented
with or without counterfactual explanations - influence human judgment over
time. Participants, acting as hiring managers, completed 60 decision trials
divided into a baseline phase without AI, followed by a phase with biased (X)AI
recommendations (favoring either male or female candidates), and a final
post-interaction phase without AI. Our results indicate that the participants
followed the AI recommendations 70% of the time when the qualifications of the
given candidates were comparable. Yet, only a fraction of participants detected
the gender bias (8 out of 294). Crucially, exposure to biased AI altered
participants' inherent preferences: in the post-interaction phase,
participants' independent decisions aligned with the bias when no
counterfactual explanations were provided before, but reversed the bias when
explanations were given. Reported trust did not differ significantly across
conditions. Confidence varied throughout the study phases after exposure to
male-biased AI, indicating nuanced effects of AI bias on decision certainty.
Our findings point to the importance of calibrating XAI to avoid unintended
behavioral shifts in order to safeguard equitable decision-making and prevent
the adoption of algorithmic bias.

</details>


### [355] [How Managers Perceive AI-Assisted Conversational Training for Workplace Communication](https://arxiv.org/abs/2505.14452)
*Lance T Wilhelm,Xiaohan Ding,Kirk McInnis Knutsen,Buse Carik,Eugenia H Rho*

Main category: cs.HC

TL;DR: AI辅助沟通培训系统CommCoach帮助管理者提升沟通技巧，强调个性化与适应性平衡。


<details>
  <summary>Details</summary>
Motivation: 管理者缺乏定制化沟通培训，AI系统可提供可扩展的解决方案，但AI在沟通训练中的角色尚不明确。

Method: 设计对话角色扮演系统CommCoach，通过半结构化访谈了解管理者对AI辅助沟通训练的预期。

Result: 管理者重视自适应、低风险的模拟练习，期待人机协作、透明反馈及对AI生成角色的更多控制。

Conclusion: AI辅助沟通培训需平衡个性化与结构化目标，同时解决适应性反馈与一致性、真实性与偏见等矛盾。

Abstract: Effective workplace communication is essential for managerial success, yet
many managers lack access to tailored and sustained training. Although
AI-assisted communication systems may offer scalable training solutions, little
is known about how managers envision the role of AI in helping them improve
their communication skills. To investigate this, we designed a conversational
role-play system, CommCoach, as a functional probe to understand how managers
anticipate using AI to practice their communication skills. Through
semi-structured interviews, participants emphasized the value of adaptive,
low-risk simulations for practicing difficult workplace conversations. They
also highlighted opportunities, including human-AI teaming, transparent and
context-aware feedback, and greater control over AI-generated personas.
AI-assisted communication training should balance personalization, structured
learning objectives, and adaptability to different user styles and contexts.
However, achieving this requires carefully navigating tensions between adaptive
and consistent AI feedback, realism and potential bias, and the open-ended
nature of AI conversations versus structured workplace discourse.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [356] [Data Balancing Strategies: A Survey of Resampling and Augmentation Methods](https://arxiv.org/abs/2505.13518)
*Behnam Yousefimehr,Mehdi Ghatee,Mohammad Amin Seifi,Javad Fazli,Sajed Tavakoli,Zahra Rafei,Shervin Ghaffari,Abolfazl Nikahd,Mahdi Razi Gandomani,Alireza Orouji,Ramtin Mahmoudi Kashani,Sarina Heshmati,Negin Sadat Mousavi*

Main category: stat.ML

TL;DR: 该论文综述了机器学习中处理不平衡数据的多种重采样方法，包括过采样、欠采样、生成模型等，并讨论了当前进展与未来方向。


<details>
  <summary>Details</summary>
Motivation: 不平衡数据会导致模型预测偏差和准确率下降，因此需要有效的重采样技术来改善类别分布不均的问题。

Method: 论文分类并回顾了多种数据平衡方法，包括合成过采样、自适应技术、生成模型、集成策略、混合方法、欠采样和基于邻居的方法。

Result: 论文展示了各种重采样技术的实际应用和案例研究，验证了它们的有效性。

Conclusion: 论文总结了当前重采样技术的发展，并提出了未来研究的潜在方向。

Abstract: Imbalanced data poses a significant obstacle in machine learning, as an
unequal distribution of class labels often results in skewed predictions and
diminished model accuracy. To mitigate this problem, various resampling
strategies have been developed, encompassing both oversampling and
undersampling techniques aimed at modifying class proportions. Conventional
oversampling approaches like SMOTE enhance the representation of the minority
class, whereas undersampling methods focus on trimming down the majority class.
Advances in deep learning have facilitated the creation of more complex
solutions, such as Generative Adversarial Networks (GANs) and Variational
Autoencoders (VAEs), which are capable of producing high-quality synthetic
examples. This paper reviews a broad spectrum of data balancing methods,
classifying them into categories including synthetic oversampling, adaptive
techniques, generative models, ensemble-based strategies, hybrid approaches,
undersampling, and neighbor-based methods. Furthermore, it highlights current
developments in resampling techniques and discusses practical implementations
and case studies that validate their effectiveness. The paper concludes by
offering perspectives on potential directions for future exploration in this
domain.

</details>


### [357] [Continuous Domain Generalization](https://arxiv.org/abs/2505.13519)
*Zekun Cai,Yiheng Yao,Guangji Bai,Renhe Jiang,Xuan Song,Ryosuke Shibasaki,Liang Zhao*

Main category: stat.ML

TL;DR: 该论文提出连续域泛化（CDG）任务，通过神经李传输算子（NeuralLTO）建模低维流形上的参数变化，解决多维度连续域泛化问题。


<details>
  <summary>Details</summary>
Motivation: 现实世界数据分布常随时间、地理和社会经济背景等多维度连续变化，现有域泛化方法仅处理离散或单轴变化，无法捕捉复杂多维变化。

Method: 提出基于几何与代数理论的框架，证明最优模型参数位于低维流形上，设计NeuralLTO实现结构化参数迁移，并引入门控机制和局部图表策略处理噪声描述符。

Result: 在遥感、科学文献和交通预测等真实数据集上，该方法在泛化准确性和描述符噪声鲁棒性上显著优于基线。

Conclusion: CDG框架通过流形建模和几何连续性约束，有效解决了多维连续域泛化问题，为复杂场景下的模型泛化提供了新思路。

Abstract: Real-world data distributions often shift continuously across multiple latent
factors such as time, geography, and socioeconomic context. However, existing
domain generalization approaches typically treat domains as discrete or
evolving along a single axis (e.g., time), which fails to capture the complex,
multi-dimensional nature of real-world variation. This paper introduces the
task of Continuous Domain Generalization (CDG), which aims to generalize
predictive models to unseen domains defined by arbitrary combinations of
continuous variation descriptors. We present a principled framework grounded in
geometric and algebraic theory, showing that optimal model parameters across
domains lie on a low-dimensional manifold. To model this structure, we propose
a Neural Lie Transport Operator (NeuralLTO), which enables structured parameter
transitions by enforcing geometric continuity and algebraic consistency. To
handle noisy or incomplete domain descriptors, we introduce a gating mechanism
to suppress irrelevant dimensions and a local chart-based strategy for robust
generalization. Extensive experiments on synthetic and real-world
datasets-including remote sensing, scientific documents, and traffic
forecasting-demonstrate that our method significantly outperforms existing
baselines in generalization accuracy and robustness under descriptor
imperfections.

</details>


### [358] [Randomised Optimism via Competitive Co-Evolution for Matrix Games with Bandit Feedback](https://arxiv.org/abs/2505.13562)
*Shishen Lin*

Main category: stat.ML

TL;DR: 该论文提出了一种名为COEBL的新算法，将进化算法与多臂老虎机框架结合，用于解决双人零和矩阵游戏中随机乐观策略的理论空白，并在实验中展示了其优越性能。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于探索双人零和矩阵游戏中未知收益矩阵和老虎机反馈下的学习问题，特别是随机乐观策略的潜力，此前该领域缺乏理论研究。

Method: 论文提出了COEBL算法，通过进化算法的变异算子实现随机乐观策略，并将其整合到老虎机框架中，以解决矩阵游戏中的学习问题。

Result: 理论分析表明COEBL实现了次线性遗憾，与确定性乐观方法性能相当。实验证明COEBL在多种矩阵游戏基准测试中优于经典老虎机算法。

Conclusion: 该研究首次在理论上分析了进化老虎机学习算法在矩阵游戏中的遗憾性能，展示了进化算法在游戏理论设置中实现随机乐观策略的有效性。

Abstract: Learning in games is a fundamental problem in machine learning and artificial
intelligence, with numerous
applications~\citep{silver2016mastering,schrittwieser2020mastering}. This work
investigates two-player zero-sum matrix games with an unknown payoff matrix and
bandit feedback, where each player observes their actions and the corresponding
noisy payoff. Prior studies have proposed algorithms for this
setting~\citep{o2021matrix,maiti2023query,cai2024uncoupled}, with
\citet{o2021matrix} demonstrating the effectiveness of deterministic optimism
(e.g., \ucb) in achieving sublinear regret. However, the potential of
randomised optimism in matrix games remains theoretically unexplored.
  We propose Competitive Co-evolutionary Bandit Learning (\coebl), a novel
algorithm that integrates evolutionary algorithms (EAs) into the bandit
framework to implement randomised optimism through EA variation operators. We
prove that \coebl achieves sublinear regret, matching the performance of
deterministic optimism-based methods. To the best of our knowledge, this is the
first theoretical regret analysis of an evolutionary bandit learning algorithm
in matrix games.
  Empirical evaluations on diverse matrix game benchmarks demonstrate that
\coebl not only achieves sublinear regret but also consistently outperforms
classical bandit algorithms, including \exptr~\citep{auer2002nonstochastic},
the variant \exptrni~\citep{cai2024uncoupled}, and \ucb~\citep{o2021matrix}.
These results highlight the potential of evolutionary bandit learning,
particularly the efficacy of randomised optimism via evolutionary algorithms in
game-theoretic settings.

</details>


### [359] [Scalable Bayesian Monte Carlo: fast uncertainty estimation beyond deep ensembles](https://arxiv.org/abs/2505.13585)
*Xinzhu Liang,Joseph M. Lukens,Sanjaya Lohani,Brian T. Kirby,Thomas A. Searles,Xin Qiu,Kody J. H. Law*

Main category: stat.ML

TL;DR: 本文提出了一种名为可扩展贝叶斯蒙特卡洛（SBMC）的新方法，通过并行实现SMC或MCMC算法，在保持性能的同时提升不确定性量化能力，并在多个数据集上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有贝叶斯深度学习方法在计算成本高且难以并行化，SBMC旨在通过结合点估计与后验分布，提供一种高效且可扩展的解决方案。

Method: SBMC方法在点估计和后验分布之间进行插值，采用并行化的SMC或MCMC算法实现，同时通过理论分析验证其合理性。

Result: 实验表明，并行化的SMC和MCMC在性能上与串行实现相当，且在MNIST、CIFAR和IMDb等数据集上达到了或超越了当前最优方法的精度，同时显著提升了不确定性量化能力。

Conclusion: 尽管并行实现仍存在较高的时间成本，但通过结合点估计，SBMC在保持高精度的同时提供了有价值的不确定性量化，整体性能优于现有方法。

Abstract: This work introduces a new method called scalable Bayesian Monte Carlo
(SBMC). The model interpolates between a point estimator and the posterior, and
the algorithm is a parallel implementation of a consistent (asymptotically
unbiased) Bayesian deep learning algorithm: sequential Monte Carlo (SMC) or
Markov chain Monte Carlo (MCMC). The method is motivated theoretically, and its
utility is demonstrated on practical examples: MNIST, CIFAR, IMDb. A systematic
numerical study reveals that parallel implementations of SMC and MCMC are
comparable to serial implementations in terms of performance and total cost,
and they achieve accuracy at or beyond the state-of-the-art (SOTA) methods like
deep ensembles at convergence, along with substantially improved uncertainty
quantification (UQ)--in particular, epistemic UQ. But even parallel
implementations are expensive, with an irreducible time barrier much larger
than the cost of the MAP estimator. Compressing time further leads to rapid
degradation of accuracy, whereas UQ remains valuable. By anchoring to a point
estimator we can recover accuracy, while retaining valuable UQ, ultimately
delivering strong performance across metrics for a cost comparable to the SOTA.

</details>


### [360] [Backward Conformal Prediction](https://arxiv.org/abs/2505.13732)
*Etienne Gauthier,Francis Bach,Michael I. Jordan*

Main category: stat.ML

TL;DR: 本文提出了一种名为“反向共形预测”的新方法，通过灵活控制预测集大小来保证共形覆盖，适用于需要小预测集的应用场景。


<details>
  <summary>Details</summary>
Motivation: 传统共形预测方法固定覆盖水平，导致预测集大小不可控，这在某些实际应用（如医学诊断）中不实用。本文旨在解决这一问题。

Method: 结合了基于e值的后验有效性和新颖的留一法估计器，通过数据依赖的错覆盖率调整覆盖水平，从而控制预测集大小。

Result: 理论分析和实证结果表明，该方法在保持可计算覆盖保证的同时，能够生成可解释且大小可控的预测集。

Conclusion: 反向共形预测方法在需要小预测集的应用中表现出色，兼具理论保证和实际可操作性。

Abstract: We introduce $\textit{Backward Conformal Prediction}$, a method that
guarantees conformal coverage while providing flexible control over the size of
prediction sets. Unlike standard conformal prediction, which fixes the coverage
level and allows the conformal set size to vary, our approach defines a rule
that constrains how prediction set sizes behave based on the observed data, and
adapts the coverage level accordingly. Our method builds on two key
foundations: (i) recent results by Gauthier et al. [2025] on post-hoc validity
using e-values, which ensure marginal coverage of the form $\mathbb{P}(Y_{\rm
test} \in \hat C_n^{\tilde{\alpha}}(X_{\rm test})) \ge 1 -
\mathbb{E}[\tilde{\alpha}]$ up to a first-order Taylor approximation for any
data-dependent miscoverage $\tilde{\alpha}$, and (ii) a novel leave-one-out
estimator $\hat{\alpha}^{\rm LOO}$ of the marginal miscoverage
$\mathbb{E}[\tilde{\alpha}]$ based on the calibration set, ensuring that the
theoretical guarantees remain computable in practice. This approach is
particularly useful in applications where large prediction sets are impractical
such as medical diagnosis. We provide theoretical results and empirical
evidence supporting the validity of our method, demonstrating that it maintains
computable coverage guarantees while ensuring interpretable, well-controlled
prediction set sizes.

</details>


### [361] [Graphon Mixtures](https://arxiv.org/abs/2505.13864)
*Sevvandi Kandanaarachchi,Cheng Soon Ong*

Main category: stat.ML

TL;DR: 提出了一种生成模型，结合了社交网络中的枢纽和密集社区结构，通过图混合和图论方法识别枢纽并估计其归一化度。


<details>
  <summary>Details</summary>
Motivation: 社交网络中通常存在少量大型枢纽和大量小型密集社区，现有模型难以同时捕捉这两种结构。

Method: 基于线图图论结果，提出图混合生成模型，结合稀疏和稠密图，并引入最大度条件识别枢纽。

Result: 理论证明可估计枢纽的归一化度和稀疏图成分的图函数，实验验证了在合成数据、引用图和社交网络中的有效性。

Conclusion: 显式建模稀疏图能有效捕捉社交网络的枢纽和密集社区结构，提升模型表现。

Abstract: Social networks have a small number of large hubs, and a large number of
small dense communities. We propose a generative model that captures both hub
and dense structures. Based on recent results about graphons on line graphs,
our model is a graphon mixture, enabling us to generate sequences of graphs
where each graph is a combination of sparse and dense graphs. We propose a new
condition on sparse graphs (the max-degree), which enables us to identify hubs.
We show theoretically that we can estimate the normalized degree of the hubs,
as well as estimate the graphon corresponding to sparse components of graph
mixtures. We illustrate our approach on synthetic data, citation graphs, and
social networks, showing the benefits of explicitly modeling sparse graphs.

</details>


### [362] [An Asymptotic Equation Linking WAIC and WBIC in Singular Models](https://arxiv.org/abs/2505.13902)
*Naoki Hayashi,Takuro Kutsuna,Sawa Takamuku*

Main category: stat.ML

TL;DR: 论文探讨了统计学习中正则与奇异模型的区别，针对奇异模型传统信息准则失效的问题，提出了WAIC与WBIC的渐近关系式，提升了计算效率。


<details>
  <summary>Details</summary>
Motivation: 传统信息准则（如AIC和BIC）在奇异模型（如含隐变量或层次结构的模型）中因正态近似失效而无法适用。WAIC和WBIC虽被提出，但需独立后验采样计算。本文旨在建立两者间的理论联系以减少计算成本。

Method: 通过理论推导，建立WAIC与WBIC之间的渐近方程，利用WBIC的后验分布无偏估计WAIC，避免重复采样。

Result: 得到了WAIC与WBIC的渐近关系式，揭示了它们在奇异学习理论框架下的结构关联，并阐明了其渐近行为。

Conclusion: 该理论成果为奇异模型选择的计算效率提升奠定了基础，深化了对WAIC和WBIC的理解。

Abstract: In statistical learning, models are classified as regular or singular
depending on whether the mapping from parameters to probability distributions
is injective. Most models with hierarchical structures or latent variables are
singular, for which conventional criteria such as the Akaike Information
Criterion and the Bayesian Information Criterion are inapplicable due to the
breakdown of normal approximations for the likelihood and posterior. To address
this, the Widely Applicable Information Criterion (WAIC) and the Widely
Applicable Bayesian Information Criterion (WBIC) have been proposed. Since WAIC
and WBIC are computed using posterior distributions at different temperature
settings, separate posterior sampling is generally required. In this paper, we
theoretically derive an asymptotic equation that links WAIC and WBIC, despite
their dependence on different posteriors. This equation yields an
asymptotically unbiased expression of WAIC in terms of the posterior
distribution used for WBIC. The result clarifies the structural relationship
between these criteria within the framework of singular learning theory, and
deepens understanding of their asymptotic behavior. This theoretical
contribution provides a foundation for future developments in the computational
efficiency of model selection in singular models.

</details>


### [363] [A Probabilistic Perspective on Model Collapse](https://arxiv.org/abs/2505.13947)
*Shirong Xu,Hengzhi He,Guang Cheng*

Main category: stat.ML

TL;DR: 该论文从概率角度研究了递归参数模型训练中的模型崩溃问题，提出了防止崩溃的样本量增长条件，并探讨了合成数据训练的潜在优势。


<details>
  <summary>Details</summary>
Motivation: 近年来，模型崩溃成为语言模型训练中的关键问题，理解其驱动机制及缓解方法至关重要。

Method: 将递归训练过程建模为模型估计的随机游走，分析样本量对步长的影响以及估计过程对方向与偏差的作用。

Result: 理论证明逐步增加样本量可防止模型崩溃，无偏估计需超线性增长，存在偏差时需更快增速；合成数据训练可能优于纯真实数据。

Conclusion: 通过概率框架和实验验证，论文为递归训练中的模型崩溃提供了理论解决方案，并扩展到一般参数模型家族。

Abstract: In recent years, model collapse has become a critical issue in language model
training, making it essential to understand the underlying mechanisms driving
this phenomenon. In this paper, we investigate recursive parametric model
training from a probabilistic perspective, aiming to characterize the
conditions under which model collapse occurs and, crucially, how it can be
mitigated. We conceptualize the recursive training process as a random walk of
the model estimate, highlighting how the sample size influences the step size
and how the estimation procedure determines the direction and potential bias of
the random walk. Under mild conditions, we rigorously show that progressively
increasing the sample size at each training step is necessary to prevent model
collapse. In particular, when the estimation is unbiased, the required growth
rate follows a superlinear pattern. This rate needs to be accelerated even
further in the presence of substantial estimation bias. Building on this
probabilistic framework, we also investigate the probability that recursive
training on synthetic data yields models that outperform those trained solely
on real data. Moreover, we extend these results to general parametric model
family in an asymptotic regime. Finally, we validate our theoretical results
through extensive simulations and a real-world dataset.

</details>


### [364] [Computational Efficiency under Covariate Shift in Kernel Ridge Regression](https://arxiv.org/abs/2505.14083)
*Andrea Della Vecchia,Arnaud Mavakala Watusadisi,Ernesto De Vito,Lorenzo Rosasco*

Main category: stat.ML

TL;DR: 该论文研究了在协变量偏移情况下，利用随机投影在再生核希尔伯特空间(RKHS)中进行非参数回归的计算效率与统计精度之间的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 协变量偏移问题在监督学习中普遍存在，即训练数据和测试数据的输入分布不同，增加了学习难度。尽管核方法具有最优统计特性，但其高计算和内存需求限制了在大规模数据集上的应用。

Method: 采用随机投影方法，将假设空间限定在给定RKHS的随机子空间内，以降低计算复杂度。

Result: 研究表明，即使在协变量偏移情况下，该方法能显著节省计算资源，同时不损害学习性能。

Conclusion: 通过随机投影技术，可以在协变量偏移环境下有效平衡计算效率与统计精度，为大规模数据集上的核方法应用提供了可行方案。

Abstract: This paper addresses the covariate shift problem in the context of
nonparametric regression within reproducing kernel Hilbert spaces (RKHSs).
Covariate shift arises in supervised learning when the input distributions of
the training and test data differ, presenting additional challenges for
learning. Although kernel methods have optimal statistical properties, their
high computational demands in terms of time and, particularly, memory, limit
their scalability to large datasets. To address this limitation, the main focus
of this paper is to explore the trade-off between computational efficiency and
statistical accuracy under covariate shift. We investigate the use of random
projections where the hypothesis space consists of a random subspace within a
given RKHS. Our results show that, even in the presence of covariate shift,
significant computational savings can be achieved without compromising learning
performance.

</details>


### [365] [High-dimensional Nonparametric Contextual Bandit Problem](https://arxiv.org/abs/2505.14102)
*Shogo Iwazaki,Junpei Komiyama,Masaaki Imaizumi*

Main category: stat.ML

TL;DR: 论文研究了高维特征空间下的核化上下文赌博机问题，提出了在特征维度与样本量同步增长时仍可实现无遗憾学习的方法，并分析了宽松遗憾的收敛速率。


<details>
  <summary>Details</summary>
Motivation: 现有方法在高斯核和特征维度Ω(log T)时只能得到O(T)的平凡遗憾界。为突破这一限制，需要建立更高效的学习框架以适应个性化广告推荐等决策场景。

Method: 引入上下文分布的随机性假设，扩展线性上下文赌博机至核化模型，通过宽松遗憾(Δ>0)的量化分析改进学习效率。

Result: 证明了即使特征维度增长至样本量级仍可实现无遗憾学习，并推导出宽松遗憾率与Δ的定量关系。

Conclusion: 核化建模显著提升了上下文赌博机的灵活性，随机性假设和宽松遗憾分析为高维决策问题提供了新的理论保证。

Abstract: We consider the kernelized contextual bandit problem with a large feature
space. This problem involves $K$ arms, and the goal of the forecaster is to
maximize the cumulative rewards through learning the relationship between the
contexts and the rewards. It serves as a general framework for various
decision-making scenarios, such as personalized online advertising and
recommendation systems. Kernelized contextual bandits generalize the linear
contextual bandit problem and offers a greater modeling flexibility. Existing
methods, when applied to Gaussian kernels, yield a trivial bound of $O(T)$ when
we consider $\Omega(\log T)$ feature dimensions. To address this, we introduce
stochastic assumptions on the context distribution and show that no-regret
learning is achievable even when the number of dimensions grows up to the
number of samples. Furthermore, we analyze lenient regret, which allows a
per-round regret of at most $\Delta > 0$. We derive the rate of lenient regret
in terms of $\Delta$.

</details>


### [366] [Hybrid Bernstein Normalizing Flows for Flexible Multivariate Density Regression with Interpretable Marginals](https://arxiv.org/abs/2505.14164)
*Marcel Arpogaus,Thomas Kneib,Thomas Nagler,David Rügamer*

Main category: stat.ML

TL;DR: 该论文结合多元条件转换模型（MCTM）与自回归归一化流（NF），以提升密度回归模型的解释性与灵活性。


<details>
  <summary>Details</summary>
Motivation: 现有方法如归一化流（NF）在多维数据中表现良好但缺乏可解释性，而统计方法如MCTM灵活性不足。论文旨在结合两者优势。

Method: 通过MCTM建模边际分布的可解释特征效应，并利用基于神经网络的NF技术捕捉联合数据分布中的复杂非线性关系。

Result: 在模拟和真实数据实验中，该方法展示了优于MCTM和其他NF模型的性能。

Conclusion: 结合MCTM与NF的方法在保持可解释性的同时，显著提升了模型对复杂多元概率分布的建模能力。

Abstract: Density regression models allow a comprehensive understanding of data by
modeling the complete conditional probability distribution. While flexible
estimation approaches such as normalizing flows (NF) work particularly well in
multiple dimensions, interpreting the input-output relationship of such models
is often difficult, due to the black-box character of deep learning models. In
contrast, existing statistical methods for multivariate outcomes such as
multivariate conditional transformation models (MCTM) are restricted in
flexibility and are often not expressive enough to represent complex
multivariate probability distributions. In this paper, we combine MCTM with
state-of-the-art and autoregressive NF to leverage the transparency of MCTM for
modeling interpretable feature effects on the marginal distributions in the
first step and the flexibility of neural-network-based NF techniques to account
for complex and non-linear relationships in the joint data distribution. We
demonstrate our method's versatility in various numerical experiments and
compare it with MCTM and other NF models on both simulated and real-world data.

</details>


### [367] [From stability of Langevin diffusion to convergence of proximal MCMC for non-log-concave sampling](https://arxiv.org/abs/2505.14177)
*Marien Renaud,Valentin De Bortoli,Arthur Leclaire,Nicolas Papadakis*

Main category: stat.ML

TL;DR: 本文研究了非凸势能下的采样问题，证明了离散时间ULA在势能强凸条件下的稳定性，并首次给出了PSGLA在非凸势能下的收敛性证明。实验验证了PSGLA在成像逆问题中的高效性。


<details>
  <summary>Details</summary>
Motivation: 在成像逆问题等场景中，势能通常是非凸且非光滑的，传统算法难以处理。本文旨在解决非凸势能下的采样问题，并提升算法的收敛速度和恢复性能。

Method: 结合前向-后向优化算法与ULA步骤，提出PSGLA方法，并利用Moreau包络的性质分析其稳定性。

Result: 理论证明了PSGLA在非凸势能下的收敛性，实验显示PSGLA比SGLD收敛更快且保持恢复性能。

Conclusion: PSGLA在非凸势能下具有理论保证和实际优势，适用于成像逆问题等复杂场景。

Abstract: We consider the problem of sampling distributions stemming from non-convex
potentials with Unadjusted Langevin Algorithm (ULA). We prove the stability of
the discrete-time ULA to drift approximations under the assumption that the
potential is strongly convex at infinity. In many context, e.g. imaging inverse
problems, potentials are non-convex and non-smooth. Proximal Stochastic
Gradient Langevin Algorithm (PSGLA) is a popular algorithm to handle such
potentials. It combines the forward-backward optimization algorithm with a ULA
step. Our main stability result combined with properties of the Moreau envelope
allows us to derive the first proof of convergence of the PSGLA for non-convex
potentials. We empirically validate our methodology on synthetic data and in
the context of imaging inverse problems. In particular, we observe that PSGLA
exhibits faster convergence rates than Stochastic Gradient Langevin Algorithm
for posterior sampling while preserving its restoration properties.

</details>


### [368] [A system identification approach to clustering vector autoregressive time series](https://arxiv.org/abs/2505.14421)
*Zuogong Yue,Xinyi Wang,Victor Solo*

Main category: stat.ML

TL;DR: 提出了一种基于自回归动态的向量时间序列聚类方法k-LMVAR，解决了计算复杂度问题，并在模拟实验中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列聚类方法大多仅处理标量时间序列，忽略自相关特征，或依赖领域知识构建高质量特征。本文旨在通过系统辨识方法，显式考虑自回归动态，实现向量时间序列的高效聚类。

Method: 首先基于混合自回归模型推导聚类算法，但因计算问题转而提出计算可行的‘小噪声’极限版本k-LMVAR，并开发了选择聚类数和模型阶数的BIC准则。

Result: k-LMVAR算法在模拟实验中表现优异，且计算扩展性良好。

Conclusion: k-LMVAR是一种高效、可扩展的向量时间序列聚类方法，适用于复杂系统建模。

Abstract: Clustering of time series based on their underlying dynamics is keeping
attracting researchers due to its impacts on assisting complex system
modelling. Most current time series clustering methods handle only scalar time
series, treat them as white noise, or rely on domain knowledge for high-quality
feature construction, where the autocorrelation pattern/feature is mostly
ignored. Instead of relying on heuristic feature/metric construction, the
system identification approach allows treating vector time series clustering by
explicitly considering their underlying autoregressive dynamics. We first
derive a clustering algorithm based on a mixture autoregressive model.
Unfortunately it turns out to have significant computational problems. We then
derive a `small-noise' limiting version of the algorithm, which we call k-LMVAR
(Limiting Mixture Vector AutoRegression), that is computationally manageable.
We develop an associated BIC criterion for choosing the number of clusters and
model order. The algorithm performs very well in comparative simulations and
also scales well computationally.

</details>


### [369] [A simple estimator of the correlation kernel matrix of a determinantal point process](https://arxiv.org/abs/2505.14529)
*Christian Gouriéroux,Yang Lu*

Main category: stat.ML

TL;DR: 本文提出了一种用于估计行列式点过程（DPP）核矩阵的闭式估计器，证明了其一致性、渐近正态性及大偏差性质。


<details>
  <summary>Details</summary>
Motivation: 行列式点过程（DPP）是一种多元二元变量的参数化模型，其核矩阵的估计通常复杂且计算量大。本文旨在提出一种简单易实现的闭式估计器，并验证其统计性质。

Method: 提出了一种闭式核矩阵估计器，可作为最大似然估计算法的初始值，简化了DPP模型的参数估计过程。

Result: 证明了该估计器的一致性、渐近正态性及大偏差性质，表明其在统计理论上的可靠性。

Conclusion: 本文提出的闭式估计器不仅易于实现，且具有优良的统计性质，为DPP模型的参数估计提供了高效的工具。

Abstract: The Determinantal Point Process (DPP) is a parameterized model for
multivariate binary variables, characterized by a correlation kernel matrix.
This paper proposes a closed form estimator of this kernel, which is
particularly easy to implement and can also be used as a starting value of
learning algorithms for maximum likelihood estimation. We prove the consistency
and asymptotic normality of our estimator, as well as its large deviation
properties.

</details>


### [370] [High-Dimensional Analysis of Bootstrap Ensemble Classifiers](https://arxiv.org/abs/2505.14587)
*Hamza Cherkaoui,Malik Tiomoko,Mohamed El Amine Seddik,Cosme Louart,Ekkehard Schnoor,Balazs Kegl*

Main category: stat.ML

TL;DR: 本文通过随机矩阵理论分析了Bootstrap方法在高维数据下对LSSVM集成学习性能的影响，并提出了优化子集数量和正则化参数的策略。


<details>
  <summary>Details</summary>
Motivation: 研究Bootstrap方法在大样本和高维特征下对最小二乘支持向量机(LSSVM)集成学习性能的理论影响，以提升其在高维数据中的分类效果。

Method: 利用随机矩阵理论分析Bootstrap技术应用于LSSVM集成的性能，并通过合成和真实数据集进行实验验证。

Result: 理论分析揭示了Bootstrap在高维环境下的作用，实验验证了提出的子集数量和正则化参数选择策略的有效性。

Conclusion: 研究为高维数据下Bootstrap方法在LSSVM集成中的应用提供了理论支持和实践指导，优化了分类器性能。

Abstract: Bootstrap methods have long been a cornerstone of ensemble learning in
machine learning. This paper presents a theoretical analysis of bootstrap
techniques applied to the Least Square Support Vector Machine (LSSVM) ensemble
in the context of large and growing sample sizes and feature dimensionalities.
Leveraging tools from Random Matrix Theory, we investigate the performance of
this classifier that aggregates decision functions from multiple weak
classifiers, each trained on different subsets of the data. We provide insights
into the use of bootstrap methods in high-dimensional settings, enhancing our
understanding of their impact. Based on these findings, we propose strategies
to select the number of subsets and the regularization parameter that maximize
the performance of the LSSVM. Empirical experiments on synthetic and real-world
datasets validate our theoretical results.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [371] [LLM Context Conditioning and PWP Prompting for Multimodal Validation of Chemical Formulas](https://arxiv.org/abs/2505.12257)
*Evgeny Markhasin*

Main category: cs.CY

TL;DR: 该研究探索了通过结构化提示策略（基于PWP原则）来改善通用大语言模型（如Gemini 2.5 Pro和ChatGPT Plus o3）在科学文档中识别技术错误的可靠性，初步结果显示该方法能提升文本和图像公式错误的检测能力。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）在复杂科学文档中识别技术错误时存在固有纠错倾向，可能掩盖不准确性，因此需要一种无需API或模型修改的方法来提升其验证任务的可靠性。

Method: 采用基于Persistent Workflow Prompting（PWP）原则的结构化提示策略，通过标准聊天界面测试不同提示方法在单一复杂测试论文中验证化学公式的效果。

Result: 基础提示不可靠，但基于PWP的提示策略改善了两种模型的文本错误识别能力，并成功引导Gemini 2.5 Pro发现人工审查遗漏的图像公式错误，而ChatGPT Plus o3未能做到。

Conclusion: PWP启发的上下文调节是一种有前景且易用的技术，可增强LLM在科学文档中细致错误检测的能力，但需进一步验证其广泛适用性。

Abstract: Identifying subtle technical errors within complex scientific and technical
documents, especially those requiring multimodal interpretation (e.g., formulas
in images), presents a significant hurdle for Large Language Models (LLMs)
whose inherent error-correction tendencies can mask inaccuracies. This
exploratory proof-of-concept (PoC) study investigates structured LLM context
conditioning, informed by Persistent Workflow Prompting (PWP) principles, as a
methodological strategy to modulate this LLM behavior at inference time. The
approach is designed to enhance the reliability of readily available,
general-purpose LLMs (specifically Gemini 2.5 Pro and ChatGPT Plus o3) for
precise validation tasks, crucially relying only on their standard chat
interfaces without API access or model modifications. To explore this
methodology, we focused on validating chemical formulas within a single,
complex test paper with known textual and image-based errors. Several prompting
strategies were evaluated: while basic prompts proved unreliable, an approach
adapting PWP structures to rigorously condition the LLM's analytical mindset
appeared to improve textual error identification with both models. Notably,
this method also guided Gemini 2.5 Pro to repeatedly identify a subtle
image-based formula error previously overlooked during manual review, a task
where ChatGPT Plus o3 failed in our tests. These preliminary findings highlight
specific LLM operational modes that impede detail-oriented validation and
suggest that PWP-informed context conditioning offers a promising and highly
accessible technique for developing more robust LLM-driven analytical
workflows, particularly for tasks requiring meticulous error detection in
scientific and technical documents. Extensive validation beyond this limited
PoC is necessary to ascertain broader applicability.

</details>


### [372] [Algorithmic Tradeoffs in Fair Lending: Profitability, Compliance, and Long-Term Impact](https://arxiv.org/abs/2505.13469)
*Aayam Bansal,Harsh Vardhan Narsaria*

Main category: cs.CY

TL;DR: 论文探讨了在自动化贷款决策中平衡算法公平性与盈利性的方法，发现忽略保护属性的方法在公平性和盈利性上表现最佳。


<details>
  <summary>Details</summary>
Motivation: 随着金融机构越来越多地依赖机器学习模型进行贷款决策，算法公平性问题日益突出。本文旨在研究如何在实施公平性约束的同时，最大化贷款机构的盈利能力。

Method: 通过模拟反映现实贷款模式的合成数据，量化不同公平性干预措施对利润率和违约率的影响。

Result: 结果显示，平等机会约束通常比人口统计平等约束带来更低的利润损失，而忽略保护属性的方法在公平性和盈利性上均优于显式公平性干预。

Conclusion: 研究为设计既考虑伦理又兼顾商业目标的贷款算法提供了实用指导，并识别了公平贷款变得有利可图的具体经济条件。

Abstract: As financial institutions increasingly rely on machine learning models to
automate lending decisions, concerns about algorithmic fairness have risen.
This paper explores the tradeoff between enforcing fairness constraints (such
as demographic parity or equal opportunity) and maximizing lender
profitability. Through simulations on synthetic data that reflects real-world
lending patterns, we quantify how different fairness interventions impact
profit margins and default rates. Our results demonstrate that equal
opportunity constraints typically impose lower profit costs than demographic
parity, but surprisingly, removing protected attributes from the model
(fairness through unawareness) outperforms explicit fairness interventions in
both fairness and profitability metrics. We further identify the specific
economic conditions under which fair lending becomes profitable and analyze the
feature-specific drivers of unfairness. These findings offer practical guidance
for designing lending algorithms that balance ethical considerations with
business objectives.

</details>


### [373] [AdAEM: An Adaptively and Automated Extensible Measurement of LLMs' Value Difference](https://arxiv.org/abs/2505.13531)
*Shitong Duan,Xiaoyuan Yi,Peng Zhang,Dongkuan Xu,Jing Yao,Tun Lu,Ning Gu,Xing Xie*

Main category: cs.CY

TL;DR: 论文提出AdAEM框架，通过自适应生成测试问题来评估大语言模型的价值差异，解决了现有数据集信息不足的问题。


<details>
  <summary>Details</summary>
Motivation: 当前评估大语言模型价值差异的数据集存在信息不足的问题，测试问题过时、污染或通用，导致结果饱和且无信息量。

Method: 引入AdAEM框架，通过上下文优化方式自动生成和扩展测试问题，最大化信息理论目标，提取最新或有文化争议的话题。

Result: 生成12,310个基于Schwartz价值理论的问题，对16个大语言模型进行了广泛分析，验证了方法的有效性和有效性。

Conclusion: AdAEM能够与大语言模型的发展共同进化，持续跟踪其价值动态，为更好的价值研究奠定了基础。

Abstract: Assessing Large Language Models (LLMs)' underlying value differences enables
comprehensive comparison of their misalignment, cultural adaptability, and
biases. Nevertheless, current value measurement datasets face the
informativeness challenge: with often outdated, contaminated, or generic test
questions, they can only capture the shared value orientations among different
LLMs, leading to saturated and thus uninformative results. To address this
problem, we introduce AdAEM, a novel, self-extensible assessment framework for
revealing LLMs' inclinations. Distinct from previous static benchmarks, AdAEM
can automatically and adaptively generate and extend its test questions. This
is achieved by probing the internal value boundaries of a diverse set of LLMs
developed across cultures and time periods in an in-context optimization
manner. The optimization process theoretically maximizes an
information-theoretic objective to extract the latest or culturally
controversial topics, providing more distinguishable and informative insights
about models' value differences. In this way, AdAEM is able to co-evolve with
the development of LLMs, consistently tracking their value dynamics. Using
AdAEM, we generate 12,310 questions grounded in Schwartz Value Theory, conduct
an extensive analysis to manifest our method's validity and effectiveness, and
benchmark the values of 16 LLMs, laying the groundwork for better value
research.

</details>


### [374] [Aligning Trustworthy AI with Democracy: A Dual Taxonomy of Opportunities and Risks](https://arxiv.org/abs/2505.13565)
*Oier Mentxaka,Natalia Díaz-Rodríguez,Mark Coeckelbergh,Marcos López de Prado,Emilia Gómez,David Fernández Llorca,Enrique Herrera-Viedma,Francisco Herrera*

Main category: cs.CY

TL;DR: 本文提出双重分类法评估AI对民主的影响：AIRD分类法识别AI如何威胁民主原则，AIPD分类法强调AI促进民主的潜力，并基于欧盟伦理框架提出缓解策略。


<details>
  <summary>Details</summary>
Motivation: 探讨AI对民主治理的双重影响（风险与机遇），为研究、政策制定和技术设计提供规范性框架，弥合伦理目标与实际操作的差距。

Method: 基于欧盟可信AI要求，构建AIRD（风险）和AIPD（贡献）分类法，结合民主理论与治理工具进行横向分析。

Result: 提出透明度和社会福利是贯穿所有风险类别的核心要素，并提供将AI系统与民主价值对齐的结构化视角。

Conclusion: 该框架为学者、政策制定者和技术人员提供行动指南，推动算法时代更具包容性、问责性和韧性的民主体系。

Abstract: Artificial Intelligence (AI) poses both significant risks and valuable
opportunities for democratic governance. This paper introduces a dual taxonomy
to evaluate AI's complex relationship with democracy: the AI Risks to Democracy
(AIRD) taxonomy, which identifies how AI can undermine core democratic
principles such as autonomy, fairness, and trust; and the AI's Positive
Contributions to Democracy (AIPD) taxonomy, which highlights AI's potential to
enhance transparency, participation, efficiency, and evidence-based
policymaking.
  Grounded in the European Union's approach to ethical AI governance, and
particularly the seven Trustworthy AI requirements proposed by the European
Commission's High-Level Expert Group on AI, each identified risk is aligned
with mitigation strategies based on EU regulatory and normative frameworks. Our
analysis underscores the transversal importance of transparency and societal
well-being across all risk categories and offers a structured lens for aligning
AI systems with democratic values.
  By integrating democratic theory with practical governance tools, this paper
offers a normative and actionable framework to guide research, regulation, and
institutional design to support trustworthy, democratic AI. It provides
scholars with a conceptual foundation to evaluate the democratic implications
of AI, equips policymakers with structured criteria for ethical oversight, and
helps technologists align system design with democratic principles. In doing
so, it bridges the gap between ethical aspirations and operational realities,
laying the groundwork for more inclusive, accountable, and resilient democratic
systems in the algorithmic age.

</details>


### [375] [Fuck the Algorithm: Conceptual Issues in Algorithmic Bias](https://arxiv.org/abs/2505.13509)
*Catherine Stinson*

Main category: cs.CY

TL;DR: 该论文探讨算法偏见争议，澄清算法本身是否可能具有偏见，并分析偏见的不同含义及其道德影响。


<details>
  <summary>Details</summary>
Motivation: 近年来，算法偏见引发广泛争议。为了澄清争议并推动解决，需要更好地理解相关概念，尤其是算法本身是否可能具有偏见。

Method: 论文通过分析“算法本身”的定义，区分“偏见”的多种含义，并探讨统计偏见如何导致道德问题，结合政治工具和压迫性事物的概念进行研究。

Result: 研究发现算法确实可能成为偏见的源头，例如推荐系统、学术搜索引擎和2020年英国A-level成绩算法。识别算法偏见对划分责任和防止歧视至关重要。

Conclusion: 算法是一种可能具有偏见的实体，认识到这一点对于明确责任和防止算法中介的歧视具有重要意义。

Abstract: Algorithmic bias has been the subject of much recent controversy. To clarify
what is at stake and to make progress resolving the controversy, a better
understanding of the concepts involved would be helpful. The discussion here
focuses on the disputed claim that algorithms themselves cannot be biased. To
clarify this claim we need to know what kind of thing 'algorithms themselves'
are, and to disambiguate the several meanings of 'bias' at play. This further
involves showing how bias of moral import can result from statistical biases,
and drawing connections to previous conceptual work about political artifacts
and oppressive things. Data bias has been identified in domains like hiring,
policing and medicine. Examples where algorithms themselves have been
pinpointed as the locus of bias include recommender systems that influence
media consumption, academic search engines that influence citation patterns,
and the 2020 UK algorithmically-moderated A-level grades. Recognition that
algorithms are a kind of thing that can be biased is key to making decisions
about responsibility for harm, and preventing algorithmically mediated
discrimination.

</details>


### [376] [Upgrading Democracies with Fairer Voting Methods](https://arxiv.org/abs/2505.14349)
*Evangelos Pournaras,Srijoni Majumdar,Thomas Wellings,Joshua C. Yang,Fatemeh B. Heravan,Regula Hänggli Fricker,Dirk Helbing*

Main category: cs.CY

TL;DR: 论文探讨了现代民主社会中投票方法的革新，通过瑞士Aarau市的参与式预算案例，展示了比例代表制投票方法如何提升决策公平性与公民代表性。


<details>
  <summary>Details</summary>
Motivation: 现有民主国家仍使用过时的投票方法，无法适应多元社会需求，缺乏社会创新。研究旨在探索如何通过新型投票方法（如累积投票、公平份额法）升级民主决策体系。

Method: 采用实证研究法，分析瑞士Aarau市参与式预算中比例代表制投票（如累积投票、公平份额法）的应用效果，并进行因果论证。

Result: 新型投票方法实现了：相同预算下更多项目当选、选民地理与偏好覆盖更广（尤其惠及弱势群体）、促进创新提案，且公民普遍偏好此类具合法性的方法。

Conclusion: 比例代表制投票能有效提升民主质量，其体现的利他主义与妥协精神符合现代民主价值观，为全球民主升级提供了实践蓝图。

Abstract: Voting methods are instrumental design element of democracies. Citizens use
them to express and aggregate their preferences to reach a collective decision.
However, voting outcomes can be as sensitive to voting rules as they are to
people's voting choices. Despite the significance and inter-disciplinary
scientific progress on voting methods, several democracies keep relying on
outdated voting methods that do not fit modern, pluralistic societies well,
while lacking social innovation. Here, we demonstrate how one can upgrade
real-world democracies, namely by using alternative preferential voting methods
such as cumulative voting and the method of equal shares designed for a
proportional representation of voters' preferences. By rigorously assessing a
new participatory budgeting approach applied in the city of Aarau, Switzerland,
we unravel the striking voting outcomes of fair voting methods: more winning
projects with the same budget and broader geographic and preference
representation of citizens by the elected projects, in particular for voters
who used to be under-represented, while promoting novel project ideas. We
provide profound causal evidence showing that citizens prefer proportional
voting methods, which possess strong legitimacy without the need of very
technical specialized explanations. We also reveal strong underlying democratic
values exhibited by citizens who support fair voting methods such as altruism
and compromise. These findings come with a global momentum to unleash a new and
long-awaited participation blueprint of how to upgrade democracies.

</details>


### [377] [Choosing a Model, Shaping a Future: Comparing LLM Perspectives on Sustainability and its Relationship with AI](https://arxiv.org/abs/2505.14435)
*Annika Bush,Meltem Aksoy,Markus Pauly,Greta Ontrup*

Main category: cs.CY

TL;DR: 研究发现不同大语言模型对可持续性和AI关系的理解存在显著差异，模型选择可能影响组织的可持续战略。


<details>
  <summary>Details</summary>
Motivation: 随着组织越来越多地依赖AI系统在可持续性决策中的支持，理解大语言模型中固有的偏见和观点变得至关重要。

Method: 研究对五种先进的大语言模型（Claude、DeepSeek、GPT、LLaMA和Mistral）进行了系统调查，每种模型各进行了100次经过验证的心理测量问卷调查。

Result: 不同模型在可持续性和AI关系的理解上表现出显著差异，例如GPT对AI与可持续性的兼容性持怀疑态度，而LLaMA则表现出极端的技术乐观主义。

Conclusion: 研究结果表明，模型选择可能对组织的可持续性战略产生重大影响，强调了在部署大语言模型进行可持续性相关决策时需要意识到模型特定的偏见。

Abstract: As organizations increasingly rely on AI systems for decision support in
sustainability contexts, it becomes critical to understand the inherent biases
and perspectives embedded in Large Language Models (LLMs). This study
systematically investigates how five state-of-the-art LLMs -- Claude, DeepSeek,
GPT, LLaMA, and Mistral - conceptualize sustainability and its relationship
with AI. We administered validated, psychometric sustainability-related
questionnaires - each 100 times per model -- to capture response patterns and
variability. Our findings revealed significant inter-model differences: For
example, GPT exhibited skepticism about the compatibility of AI and
sustainability, whereas LLaMA demonstrated extreme techno-optimism with perfect
scores for several Sustainable Development Goals (SDGs). Models also diverged
in attributing institutional responsibility for AI and sustainability
integration, a results that holds implications for technology governance
approaches. Our results demonstrate that model selection could substantially
influence organizational sustainability strategies, highlighting the need for
awareness of model-specific biases when deploying LLMs for
sustainability-related decision-making.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [378] [Sobolev Gradient Ascent for Optimal Transport: Barycenter Optimization and Convergence Analysis](https://arxiv.org/abs/2505.13660)
*Kaheon Kim,Bohan Zhou,Changbo Zhu,Xiaohui Chen*

Main category: math.OC

TL;DR: 本文提出了一种无约束的Wasserstein重心凹对偶公式，并设计了Sobolev梯度上升算法（SGA），在计算效率和理论简化上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统计算Wasserstein重心的对偶方法需要复杂的投影操作，计算成本高。本文旨在简化这一过程，提出更高效的无约束对偶方法。

Method: 基于Sobolev几何设计SGA算法，避免了传统方法中计算昂贵的c-凹投影操作，简化了计算流程。

Result: SGA算法在全局收敛性分析中达到与欧氏空间中非光滑凸函数经典次梯度下降法相同的收敛速率，且实验表现优于现有最优传输重心求解器。

Conclusion: SGA算法在理论和计算上均显著简化了Wasserstein重心的求解过程，具有优越的实证性能。

Abstract: This paper introduces a new constraint-free concave dual formulation for the
Wasserstein barycenter. Tailoring the vanilla dual gradient ascent algorithm to
the Sobolev geometry, we derive a scalable Sobolev gradient ascent (SGA)
algorithm to compute the barycenter for input distributions supported on a
regular grid. Despite the algorithmic simplicity, we provide a global
convergence analysis that achieves the same rate as the classical subgradient
descent methods for minimizing nonsmooth convex functions in the Euclidean
space. A central feature of our SGA algorithm is that the computationally
expensive $c$-concavity projection operator enforced on the Kantorovich dual
potentials is unnecessary to guarantee convergence, leading to significant
algorithmic and theoretical simplifications over all existing primal and dual
methods for computing the exact barycenter. Our numerical experiments
demonstrate the superior empirical performance of SGA over the existing optimal
transport barycenter solvers.

</details>


### [379] [Sequential QCQP for Bilevel Optimization with Line Search](https://arxiv.org/abs/2505.14647)
*Sina Sharifi,Erfan Yazdandoost Hamedani,Mahyar Fazlyab*

Main category: math.OC

TL;DR: 提出一种单循环、无需调参的双层优化算法，保证随时可行性和上层目标下降，具有O(1/k)的遍历收敛速率。


<details>
  <summary>Details</summary>
Motivation: 双层优化问题存在层次间复杂依赖关系，传统方法需要调参且难以保证可行性。

Method: 通过凸二次约束二次规划（QCQP）获取搜索方向，结合控制屏障函数启发的回溯线搜索确保步长安全。

Result: 算法在温和的局部正则性假设下收敛，并在典型双层任务中验证了有效性。

Conclusion: 该方法可扩展性强，无需超参数调优，为双层优化提供了实用解决方案。

Abstract: Bilevel optimization involves a hierarchical structure where one problem is
nested within another, leading to complex interdependencies between levels. We
propose a single-loop, tuning-free algorithm that guarantees anytime
feasibility, i.e., approximate satisfaction of the lower-level optimality
condition, while ensuring descent of the upper-level objective. At each
iteration, a convex quadratically-constrained quadratic program (QCQP) with a
closed-form solution yields the search direction, followed by a backtracking
line search inspired by control barrier functions to ensure safe, uniformly
positive step sizes. The resulting method is scalable, requires no
hyperparameter tuning, and converges under mild local regularity assumptions.
We establish an O(1/k) ergodic convergence rate and demonstrate the algorithm's
effectiveness on representative bilevel tasks.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [380] [LODGE: Joint Hierarchical Task Planning and Learning of Domain Models with Grounded Execution](https://arxiv.org/abs/2505.13497)
*Claudius Kienle,Benjamin Alt,Oleg Arenz,Jan Peters*

Main category: cs.RO

TL;DR: 该论文提出了一种分层学习规划领域的方法，结合仿真验证和错误推理器，提高了大语言模型在长程规划任务中的成功率。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型（LLMs）在自然语言指令规划中存在缺陷，且现有方法依赖大量人工反馈。论文旨在通过分层学习减少人工干预并提升规划质量。

Method: 采用分层领域学习方法，将低级谓词和动作组合为高级对应物，利用仿真验证前提条件和效果，并引入错误推理器确保规划一致性。

Result: 在国际规划竞赛（IPC）领域和机器人长程操作任务中，该方法比现有领域合成和LLM规划方法具有更高的规划成功率和领域建模质量。

Conclusion: 分层学习结合仿真验证和错误推理器显著提升了长程规划任务的性能，减少了人工干预需求。

Abstract: Large Language Models (LLMs) enable planning from natural language
instructions using implicit world knowledge, but often produce flawed plans
that require refinement. Instead of directly predicting plans, recent methods
aim to learn a problem domain that can be solved for different goal states
using classical planners. However, these approaches require significant human
feedback to obtain useful models. We address this shortcoming by learning
hierarchical domains, where low-level predicates and actions are composed into
higher-level counterparts, and by leveraging simulation to validate their
preconditions and effects. This hierarchical approach is particularly powerful
for long-horizon planning, where LLM-based planning approaches typically
struggle. Furthermore, we introduce a central error reasoner to ensure
consistency among the different planning levels. Evaluation on two challenging
International Planning Competition (IPC) domains and a long-horizon robot
manipulation task demonstrates higher planning success rates than
state-of-the-art domain synthesis and LLM-modulo planning methods, while
constructing high-quality models of the domain. Resources, videos and detailed
experiment results are available at https://claudius-kienle.github.io/lodge/.

</details>


### [381] [Distributional Soft Actor-Critic with Harmonic Gradient for Safe and Efficient Autonomous Driving in Multi-lane Scenarios](https://arxiv.org/abs/2505.13532)
*Feihong Zhang,Guojian Zhan,Bin Shuai,Tianyi Zhang,Jingliang Duan,Shengbo Eben Li*

Main category: cs.RO

TL;DR: 提出了一种名为谐波策略迭代（HPI）的安全导向强化学习技术，结合DSAC算法开发了DSAC-H，在多车道场景中实现了高效驾驶且几乎不违反安全约束。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习算法在处理约束条件（尤其是现实应用中的安全约束）方面存在挑战，需要一种更平衡和稳定的训练方法。

Method: 采用谐波策略迭代（HPI）技术，分别计算效率驾驶和安全约束的策略梯度，生成谐波梯度以更新策略，并集成DSAC算法形成DSAC-H。

Result: DSAC-H在多车道模拟场景中表现出色，实现了高效驾驶且安全约束违反率接近零。

Conclusion: HPI和DSAC-H为安全约束下的强化学习提供了有效解决方案，平衡了效率与安全性。

Abstract: Reinforcement learning (RL), known for its self-evolution capability, offers
a promising approach to training high-level autonomous driving systems.
However, handling constraints remains a significant challenge for existing RL
algorithms, particularly in real-world applications. In this paper, we propose
a new safety-oriented training technique called harmonic policy iteration
(HPI). At each RL iteration, it first calculates two policy gradients
associated with efficient driving and safety constraints, respectively. Then, a
harmonic gradient is derived for policy updating, minimizing conflicts between
the two gradients and consequently enabling a more balanced and stable training
process. Furthermore, we adopt the state-of-the-art DSAC algorithm as the
backbone and integrate it with our HPI to develop a new safe RL algorithm,
DSAC-H. Extensive simulations in multi-lane scenarios demonstrate that DSAC-H
achieves efficient driving performance with near-zero safety constraint
violations.

</details>


### [382] [SayCoNav: Utilizing Large Language Models for Adaptive Collaboration in Decentralized Multi-Robot Navigation](https://arxiv.org/abs/2505.13729)
*Abhinav Rajvanshi,Pritish Sahu,Tixiao Shan,Karan Sikka,Han-Pang Chiu*

Main category: cs.RO

TL;DR: SayCoNav利用大语言模型为机器人团队生成自适应协作策略，提升多目标导航任务效率。


<details>
  <summary>Details</summary>
Motivation: 在未知大规模环境中，机器人团队需根据各自技能和状态动态调整协作策略以完成复杂导航任务。

Method: 基于LLM的SayCoNav框架：1) 集中生成团队协作策略 2) 各机器人分散式生成行动规划 3) 通过信息共享实时更新计划。

Result: 在MultiON任务中，异构机器人团队搜索效率最高提升44.28%，并能动态适应执行过程中的环境变化。

Conclusion: SayCoNav证明了LLM在机器人协作规划中的有效性，能显著提升异构团队的导航任务性能。

Abstract: Adaptive collaboration is critical to a team of autonomous robots to perform
complicated navigation tasks in large-scale unknown environments. An effective
collaboration strategy should be determined and adapted according to each
robot's skills and current status to successfully achieve the shared goal. We
present SayCoNav, a new approach that leverages large language models (LLMs)
for automatically generating this collaboration strategy among a team of
robots. Building on the collaboration strategy, each robot uses the LLM to
generate its plans and actions in a decentralized way. By sharing information
to each other during navigation, each robot also continuously updates its
step-by-step plans accordingly. We evaluate SayCoNav on Multi-Object Navigation
(MultiON) tasks, that require the team of the robots to utilize their
complementary strengths to efficiently search multiple different objects in
unknown environments. By validating SayCoNav with varied team compositions and
conditions against baseline methods, our experimental results show that
SayCoNav can improve search efficiency by at most 44.28% through effective
collaboration among heterogeneous robots. It can also dynamically adapt to the
changing conditions during task execution.

</details>


### [383] [Toward Real-World Cooperative and Competitive Soccer with Quadrupedal Robot Teams](https://arxiv.org/abs/2505.13834)
*Zhi Su,Yuman Gao,Emily Lukas,Yunfei Li,Jiaze Cai,Faris Tulbah,Fei Gao,Chao Yu,Zhongyu Li,Yi Wu,Koushil Sreenath*

Main category: cs.RO

TL;DR: 本文提出了一种分层多智能体强化学习框架，用于实现四足机器人在足球比赛中的自主协作与对抗，包括底层运动技能和高层策略规划，并在真实机器人上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 实现腿式机器人的协调团队合作需要精细的运动控制和长远的战略决策。机器人足球作为一个动态、竞争性和多智能体交互的平台，为这一挑战提供了理想的测试环境。

Method: 采用分层多智能体强化学习框架：首先训练底层动态技能（如行走、带球和踢球），然后通过多智能体近端策略优化（MAPPO）和虚构自博弈（FSP）训练高层策略规划。

Result: 该方法在合作与竞争的多智能体足球游戏中表现出显著优势，能够在真实四足机器人上实现自主的机器人与机器人、机器人与人类的足球比赛。

Conclusion: 提出的学习框架成功实现了四足机器人在足球比赛中的自主协作与对抗，展示了其在复杂多智能体环境中的潜力。

Abstract: Achieving coordinated teamwork among legged robots requires both fine-grained
locomotion control and long-horizon strategic decision-making. Robot soccer
offers a compelling testbed for this challenge, combining dynamic, competitive,
and multi-agent interactions. In this work, we present a hierarchical
multi-agent reinforcement learning (MARL) framework that enables fully
autonomous and decentralized quadruped robot soccer. First, a set of highly
dynamic low-level skills is trained for legged locomotion and ball
manipulation, such as walking, dribbling, and kicking. On top of these, a
high-level strategic planning policy is trained with Multi-Agent Proximal
Policy Optimization (MAPPO) via Fictitious Self-Play (FSP). This learning
framework allows agents to adapt to diverse opponent strategies and gives rise
to sophisticated team behaviors, including coordinated passing, interception,
and dynamic role allocation. With an extensive ablation study, the proposed
learning method shows significant advantages in the cooperative and competitive
multi-agent soccer game. We deploy the learned policies to real quadruped
robots relying solely on onboard proprioception and decentralized localization,
with the resulting system supporting autonomous robot-robot and robot-human
soccer matches on indoor and outdoor soccer courts.

</details>


### [384] [Enhancing Robot Navigation Policies with Task-Specific Uncertainty Managements](https://arxiv.org/abs/2505.13837)
*Gokul Puthumanaillam,Paulo Padrao,Jose Fuentes,Leonardo Bobadilla,Melkior Ornik*

Main category: cs.RO

TL;DR: GUIDE框架通过任务特定不确定性地图(TSUMs)和强化学习，让机器人能根据任务需求动态调整不确定性管理策略，在复杂环境中实现更高效的导航。


<details>
  <summary>Details</summary>
Motivation: 机器人在复杂环境中导航时面临传感器噪声、环境变化和信息不完整等不确定性，不同任务在不同区域对精度的要求各异。现有方法缺乏对任务特定不确定性的动态管理能力。

Method: 提出GUIDE框架，通过Task-Specific Uncertainty Maps (TSUMs)为不同位置分配可接受的不确定性水平，并结合强化学习自动学习平衡任务完成和不确定性管理的策略。

Result: 实际测试表明，相比不考虑任务特定不确定性的方法，GUIDE框架显著提升了导航性能。

Conclusion: GUIDE框架通过显式建模任务相关的不确定性需求，实现了更灵活高效的机器人导航策略，且无需复杂的奖励函数设计。

Abstract: Robots navigating complex environments must manage uncertainty from sensor
noise, environmental changes, and incomplete information, with different tasks
requiring varying levels of precision in different areas. For example, precise
localization may be crucial near obstacles but less critical in open spaces. We
present GUIDE (Generalized Uncertainty Integration for Decision-Making and
Execution), a framework that integrates these task-specific requirements into
navigation policies via Task-Specific Uncertainty Maps (TSUMs). By assigning
acceptable uncertainty levels to different locations, TSUMs enable robots to
adapt uncertainty management based on context. When combined with reinforcement
learning, GUIDE learns policies that balance task completion and uncertainty
management without extensive reward engineering. Real-world tests show
significant performance gains over methods lacking task-specific uncertainty
awareness.

</details>


### [385] [Safety2Drive: Safety-Critical Scenario Benchmark for the Evaluation of Autonomous Driving](https://arxiv.org/abs/2505.13872)
*Jingzheng Li,Tiancheng Wang,Xingyu Peng,Jiacheng Chen,Zhijun Chen,Bing Li,Xianglong Liu*

Main category: cs.RO

TL;DR: 论文提出Safety2Drive，一个用于评估自动驾驶系统安全性的关键场景库，弥补现有数据集在闭环测试和真实事故场景覆盖上的不足。


<details>
  <summary>Details</summary>
Motivation: 当前自动驾驶数据集缺乏符合法规要求的闭环测试场景库，且真实事故场景不足，导致对自动驾驶系统安全性的评估不充分。

Method: 提出Safety2Drive场景库，覆盖70项标准测试项目，支持安全威胁注入（如自然环境影响和传感器对抗攻击）和多维度评估（包括感知任务）。

Result: Safety2Drive提供了从场景构建到验证的标准化测试框架，支持自动驾驶系统的全面安全评估。

Conclusion: Safety2Drive为自动驾驶的安全部署建立了标准化测试框架，填补了现有数据集的不足。

Abstract: Autonomous Driving (AD) systems demand the high levels of safety assurance.
Despite significant advancements in AD demonstrated on open-source benchmarks
like Longest6 and Bench2Drive, existing datasets still lack
regulatory-compliant scenario libraries for closed-loop testing to
comprehensively evaluate the functional safety of AD. Meanwhile, real-world AD
accidents are underrepresented in current driving datasets. This scarcity leads
to inadequate evaluation of AD performance, posing risks to safety validation
and practical deployment. To address these challenges, we propose Safety2Drive,
a safety-critical scenario library designed to evaluate AD systems.
Safety2Drive offers three key contributions. (1) Safety2Drive comprehensively
covers the test items required by standard regulations and contains 70 AD
function test items. (2) Safety2Drive supports the safety-critical scenario
generalization. It has the ability to inject safety threats such as natural
environment corruptions and adversarial attacks cross camera and LiDAR sensors.
(3) Safety2Drive supports multi-dimensional evaluation. In addition to the
evaluation of AD systems, it also supports the evaluation of various perception
tasks, such as object detection and lane detection. Safety2Drive provides a
paradigm from scenario construction to validation, establishing a standardized
test framework for the safe deployment of AD.

</details>


### [386] [APEX: Empowering LLMs with Physics-Based Task Planning for Real-time Insight](https://arxiv.org/abs/2505.13921)
*Wanjing Huang,Weixiang Yan,Zhen Zhang,Ambuj Singh*

Main category: cs.RO

TL;DR: APEX框架通过物理驱动的预见性增强LLMs的实时任务规划能力，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法在物理交互建模方面存在局限，无法捕捉动态对象交互或需要任务特定训练，限制了实际应用。

Method: APEX构建结构化图来建模环境中的动态交互，提供物理状态更新和低延迟前向模拟，以优化决策。

Result: APEX在物理推理、长时规划任务和动态避障三个基准测试中显著优于标准LLMs和VLM模型。

Conclusion: 显式物理推理对于弥合语言智能与现实任务执行之间的差距至关重要。

Abstract: Large Language Models (LLMs) demonstrate strong reasoning and task planning
capabilities but remain fundamentally limited in physical interaction modeling.
Existing approaches integrate perception via Vision-Language Models (VLMs) or
adaptive decision-making through Reinforcement Learning (RL), but they fail to
capture dynamic object interactions or require task-specific training, limiting
their real-world applicability. We introduce APEX (Anticipatory
Physics-Enhanced Execution), a framework that equips LLMs with physics-driven
foresight for real-time task planning. APEX constructs structured graphs to
identify and model the most relevant dynamic interactions in the environment,
providing LLMs with explicit physical state updates. Simultaneously, APEX
provides low-latency forward simulations of physically feasible actions,
allowing LLMs to select optimal strategies based on predictive outcomes rather
than static observations. We evaluate APEX on three benchmarks designed to
assess perception, prediction, and decision-making: (1) Physics Reasoning
Benchmark, testing causal inference and object motion prediction; (2) Tetris,
evaluating whether physics-informed prediction enhances decision-making
performance in long-horizon planning tasks; (3) Dynamic Obstacle Avoidance,
assessing the immediate integration of perception and action feasibility
analysis. APEX significantly outperforms standard LLMs and VLM-based models,
demonstrating the necessity of explicit physics reasoning for bridging the gap
between language-based intelligence and real-world task execution. The source
code and experiment setup are publicly available at
https://github.com/hwj20/APEX_EXP .

</details>


### [387] [Hypothesis on the Functional Advantages of the Selection-Broadcast Cycle Structure: Global Workspace Theory and Dealing with a Real-Time World](https://arxiv.org/abs/2505.13969)
*Junya Nakanishi,Jun Baba,Yuichiro Yoshikawa,Hiroko Kamide,Hiroshi Ishiguro*

Main category: cs.RO

TL;DR: 本文探讨了基于人类意识的全局工作空间理论（GWT）提出的选择-广播循环结构在动态实时场景中对人工智能和机器人技术的功能优势。


<details>
  <summary>Details</summary>
Motivation: 以往研究通常独立考察选择和广播过程，而本研究强调它们的循环结构及其对实时认知系统的综合效益。

Method: 聚焦于选择-广播循环结构的动态适应性，分析其在无监督动态环境中的认知架构潜力。

Result: 识别了三大优势：动态思维适应、基于经验的适应和即时实时适应，展示了GWT在复杂决策中的潜力。

Conclusion: GWT为开发能够处理复杂现实任务的通用AI和机器人系统提供了新的发展方向。

Abstract: This paper discusses the functional advantages of the Selection-Broadcast
Cycle structure proposed by Global Workspace Theory (GWT), inspired by human
consciousness, particularly focusing on its applicability to artificial
intelligence and robotics in dynamic, real-time scenarios. While previous
studies often examined the Selection and Broadcast processes independently,
this research emphasizes their combined cyclic structure and the resulting
benefits for real-time cognitive systems. Specifically, the paper identifies
three primary benefits: Dynamic Thinking Adaptation, Experience-Based
Adaptation, and Immediate Real-Time Adaptation. This work highlights GWT's
potential as a cognitive architecture suitable for sophisticated
decision-making and adaptive performance in unsupervised, dynamic environments.
It suggests new directions for the development and implementation of robust,
general-purpose AI and robotics systems capable of managing complex, real-world
tasks.

</details>


### [388] [Learning Collision Risk from Naturalistic Driving with Generalised Surrogate Safety Measures](https://arxiv.org/abs/2505.13556)
*Yiru Jiao,Simeon C. Calvert,Sander van Cranenburgh,Hans van Lint*

Main category: cs.RO

TL;DR: 该论文提出了一种无需事故标签的通用化替代安全度量（GSSM），通过神经网络学习正常驾驶模式，评估交通交互偏离正常值的风险，显著提升了碰撞预警的准确性和及时性。


<details>
  <summary>Details</summary>
Motivation: 现有驾驶安全预警方法依赖人工标注的稀疏风险数据，难以适应多变的交互场景，且泛化能力有限。亟需一种无需事故标签、能自适应多样化场景的通用风险评估方法。

Method: 提出GSSM框架：1) 通过神经网络建模多方向车距的上下文条件分布；2) 基于极值理论计算上下文自适应风险评分；3) 整合运动学、天气等多元上下文因素。

Result: 在SHRP2 NDS数据集测试中，基础版GSSM达到0.9的AUPRC，中位预警时间2.6秒。加入更多上下文因素后性能进一步提升，在追尾、变道等多种场景中均优于基线方法。

Conclusion: GSSM建立了可扩展、上下文感知的通用化风险评估框架，为主动预防交通事故提供了新范式。

Abstract: Accurate and timely alerts for drivers or automated systems to unfolding
collisions remains a challenge in road safety, particularly in highly
interactive urban traffic. Existing approaches require labour-intensive
annotation of sparse risk, struggle to consider varying interaction context, or
are useful only in the scenarios they are designed for. To address these
limits, this study introduces the generalised surrogate safety measure (GSSM),
a new approach that learns exclusively from naturalistic driving without crash
or risk labels. GSSM captures the patterns of normal driving and estimates the
extent to which a traffic interaction deviates from the norm towards unsafe
extreme. Utilising neural networks, normal interactions are characterised by
context-conditioned distributions of multi-directional spacing between road
users. In the same interaction context, a spacing closer than normal entails
higher risk of potential collision. Then a context-adaptive risk score and its
associated probability can be calculated based on the theory of extreme values.
Any measurable factors, such as motion kinematics, weather, lighting, can serve
as part of the context, allowing for diverse coverage of safety-critical
interactions. Multiple public driving datasets are used to train GSSMs, which
are tested with 4,875 real-world crashes and near-crashes reconstructed from
the SHRP2 NDS. A vanilla GSSM using only instantaneous states achieves AUPRC of
0.9 and secures a median time advance of 2.6 seconds to prevent potential
collisions. Additional data and contextual factors provide further performance
gains. Across various interaction types such as rear-end, merging, and
crossing, the accuracy and timeliness of GSSM consistently outperforms existing
baselines. GSSM therefore establishes a scalable, context-aware, and
generalisable foundation to proactively quantify collision risk in traffic
interactions.

</details>


### [389] [Certifiably Safe Manipulation of Deformable Linear Objects via Joint Shape and Tension Prediction](https://arxiv.org/abs/2505.13889)
*Yiting Zhang,Shichen Li*

Main category: cs.RO

TL;DR: 提出了一种可证明安全的运动规划与控制框架，用于处理可变形线性物体的操作，确保在接触密集环境中的安全性。


<details>
  <summary>Details</summary>
Motivation: 现有模型大多仅关注形状预测，忽略了接触和张力的约束，可能导致物体或机器人损坏。因此，需要一种能够同时预测形状和张力并确保安全的方法。

Method: 核心是一个预测模型，联合估计物体的未来形状和张力，并结合基于多项式zonotopes的实时轨迹优化器，强制执行安全约束。

Result: 在模拟线束装配任务中，相比现有方法，该方法实现了更高的任务成功率且避免了所有安全违规。

Conclusion: 该方法能够在接触密集环境中实现鲁棒且安全的可变形线性物体操作。

Abstract: Manipulating deformable linear objects (DLOs) is challenging due to their
complex dynamics and the need for safe interaction in contact-rich
environments. Most existing models focus on shape prediction alone and fail to
account for contact and tension constraints, which can lead to damage to both
the DLO and the robot. In this work, we propose a certifiably safe motion
planning and control framework for DLO manipulation. At the core of our method
is a predictive model that jointly estimates the DLO's future shape and
tension. These predictions are integrated into a real-time trajectory optimizer
based on polynomial zonotopes, allowing us to enforce safety constraints
throughout the execution. We evaluate our framework on a simulated wire harness
assembly task using a 7-DOF robotic arm. Compared to state-of-the-art methods,
our approach achieves a higher task success rate while avoiding all safety
violations. The results demonstrate that our method enables robust and safe DLO
manipulation in contact-rich environments.

</details>


### [390] [Time Reversal Symmetry for Efficient Robotic Manipulations in Deep Reinforcement Learning](https://arxiv.org/abs/2505.13925)
*Yunpeng Jiang,Jianshu Hu,Paul Weng,Yutong Ban*

Main category: cs.RO

TL;DR: 论文提出TR-DRL框架，利用时间反转对称性提升深度强化学习的样本效率和性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注空间对称性，而忽略了时间对称性。本文旨在填补这一空白，探索时间反转对称性在机器人任务中的应用。

Method: 提出TR-DRL框架，结合轨迹反转增强和时间反转引导的奖励塑造，通过动态一致性过滤器识别可逆过渡，并生成反转过渡以增强训练数据。

Result: 在Robosuite和MetaWorld基准测试中，TR-DRL在单任务和多任务设置中均表现出更高的样本效率和更强的最终性能。

Conclusion: TR-DRL有效利用时间反转对称性，显著提升了深度强化学习在时间对称任务中的表现。

Abstract: Symmetry is pervasive in robotics and has been widely exploited to improve
sample efficiency in deep reinforcement learning (DRL). However, existing
approaches primarily focus on spatial symmetries, such as reflection, rotation,
and translation, while largely neglecting temporal symmetries. To address this
gap, we explore time reversal symmetry, a form of temporal symmetry commonly
found in robotics tasks such as door opening and closing. We propose Time
Reversal symmetry enhanced Deep Reinforcement Learning (TR-DRL), a framework
that combines trajectory reversal augmentation and time reversal guided reward
shaping to efficiently solve temporally symmetric tasks. Our method generates
reversed transitions from fully reversible transitions, identified by a
proposed dynamics-consistent filter, to augment the training data. For
partially reversible transitions, we apply reward shaping to guide learning,
according to successful trajectories from the reversed task. Extensive
experiments on the Robosuite and MetaWorld benchmarks demonstrate that TR-DRL
is effective in both single-task and multi-task settings, achieving higher
sample efficiency and stronger final performance compared to baseline methods.

</details>


### [391] [NavBench: A Unified Robotics Benchmark for Reinforcement Learning-Based Autonomous Navigation](https://arxiv.org/abs/2505.14526)
*Matteo El-Hariry,Antoine Richard,Ricard M. Castan,Luis F. W. Batista,Matthieu Geist,Cedric Pradalier,Miguel Olivares-Mendez*

Main category: cs.RO

TL;DR: NavBench是一个多领域基准测试框架，用于训练和评估不同机器人平台的RL导航策略，解决了现有基准测试局限于特定平台的问题。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习基准测试通常局限于特定机器人平台，难以实现跨平台泛化和公平比较。本文旨在解决这一问题，提出一个统一的跨领域基准测试框架。

Method: 基于IsaacLab构建NavBench框架，标准化任务定义，支持不同机器人在多样化环境中执行导航任务，无需重新设计任务或定制评估指标。

Result: NavBench成功实现了跨介质统一基准测试、模块化设计以及仿真到现实的策略迁移，验证了其在卫星模拟器、无人水面舰艇和轮式地面车辆等多种机器人上的有效性。

Conclusion: NavBench通过标准化和模块化设计，简化了RL导航策略的开发，并支持广泛的应用场景，为跨平台导航研究提供了实用工具。

Abstract: Autonomous robots must navigate and operate in diverse environments, from
terrestrial and aquatic settings to aerial and space domains. While
Reinforcement Learning (RL) has shown promise in training policies for specific
autonomous robots, existing benchmarks are often constrained to unique
platforms, limiting generalization and fair comparisons across different
mobility systems. In this paper, we present NavBench, a multi-domain benchmark
for training and evaluating RL-based navigation policies across diverse robotic
platforms and operational environments. Built on IsaacLab, our framework
standardizes task definitions, enabling different robots to tackle various
navigation challenges without the need for ad-hoc task redesigns or custom
evaluation metrics. Our benchmark addresses three key challenges: (1) Unified
cross-medium benchmarking, enabling direct evaluation of diverse actuation
methods (thrusters, wheels, water-based propulsion) in realistic environments;
(2) Scalable and modular design, facilitating seamless robot-task
interchangeability and reproducible training pipelines; and (3) Robust
sim-to-real validation, demonstrated through successful policy transfer to
multiple real-world robots, including a satellite robotic simulator, an
unmanned surface vessel, and a wheeled ground vehicle. By ensuring consistency
between simulation and real-world deployment, NavBench simplifies the
development of adaptable RL-based navigation strategies. Its modular design
allows researchers to easily integrate custom robots and tasks by following the
framework's predefined templates, making it accessible for a wide range of
applications. Our code is publicly available at NavBench.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [392] [Optimizing Binary and Ternary Neural Network Inference on RRAM Crossbars using CIM-Explorer](https://arxiv.org/abs/2505.14303)
*Rebecca Pelke,José Cubero-Cascante,Nils Bosbach,Niklas Degener,Florian Idrizi,Lennart M. Reimann,Jan Moritz Joseph,Rainer Leupers*

Main category: cs.ET

TL;DR: 论文提出了CIM-Explorer工具包，用于优化基于RRAM交叉阵列的BNN和TNN推理，解决了现有工具在编译、仿真和设计空间探索方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有RRAM-CIM软件项目通常仅关注单一环节（如编译或仿真），且多采用8位量化，难以充分利用RRAM的二进制特性。需要一种端到端工具来支持从早期精度评估到最终芯片设计的全流程。

Method: 开发模块化工具包CIM-Explorer，包含端到端编译器栈、多模式映射方案和仿真器，支持通过设计空间探索流程评估不同交叉阵列参数下的推理精度。

Result: 案例研究表明该工具能有效分析不同映射方案和硬件参数对精度的影响，其GitHub已开源。

Conclusion: CIM-Explorer填补了RRAM-CIM全流程设计工具的空白，为二进制/三值神经网络在存内计算架构中的实现提供了系统级解决方案。

Abstract: Using Resistive Random Access Memory (RRAM) crossbars in Computing-in-Memory
(CIM) architectures offers a promising solution to overcome the von Neumann
bottleneck. Due to non-idealities like cell variability, RRAM crossbars are
often operated in binary mode, utilizing only two states: Low Resistive State
(LRS) and High Resistive State (HRS). Binary Neural Networks (BNNs) and Ternary
Neural Networks (TNNs) are well-suited for this hardware due to their efficient
mapping. Existing software projects for RRAM-based CIM typically focus on only
one aspect: compilation, simulation, or Design Space Exploration (DSE).
Moreover, they often rely on classical 8 bit quantization. To address these
limitations, we introduce CIM-Explorer, a modular toolkit for optimizing BNN
and TNN inference on RRAM crossbars. CIM-Explorer includes an end-to-end
compiler stack, multiple mapping options, and simulators, enabling a DSE flow
for accuracy estimation across different crossbar parameters and mappings.
CIM-Explorer can accompany the entire design process, from early accuracy
estimation for specific crossbar parameters, to selecting an appropriate
mapping, and compiling BNNs and TNNs for a finalized crossbar chip. In DSE case
studies, we demonstrate the expected accuracy for various mappings and crossbar
parameters. CIM-Explorer can be found on GitHub.

</details>


<div id='q-bio.GN'></div>

# q-bio.GN [[Back]](#toc)

### [393] [OmniGenBench: A Modular Platform for Reproducible Genomic Foundation Models Benchmarking](https://arxiv.org/abs/2505.14402)
*Heng Yang,Jack Cole,Yuan Li,Renzhi Chen,Geyong Min,Ke Li*

Main category: q-bio.GN

TL;DR: OmniGenBench是一个模块化基准测试平台，旨在统一基因组基础模型的数据、模型、基准测试和可解释性层，以解决可重复性挑战。


<details>
  <summary>Details</summary>
Motivation: 随着基因组基础模型（GFMs）的兴起，基因组学领域亟需一个严格且可重复的评估方法，以应对数据透明度、模型互操作性、基准测试碎片化和黑盒可解释性等挑战。

Method: 通过模块化设计，OmniGenBench整合了31个开源模型，提供标准化的一键评估功能，覆盖五个基准测试套件，并支持自动化流程和社区扩展功能。

Result: OmniGenBench成功解决了基因组AI研究中的可重复性问题，为基因组规模建模时代的可信发现和协作创新提供了基础架构。

Conclusion: OmniGenBench作为可重复基因组AI研究的基础设施，有望加速基因组规模建模时代的可信发现和协作创新。

Abstract: The code of nature, embedded in DNA and RNA genomes since the origin of life,
holds immense potential to impact both humans and ecosystems through genome
modeling. Genomic Foundation Models (GFMs) have emerged as a transformative
approach to decoding the genome. As GFMs scale up and reshape the landscape of
AI-driven genomics, the field faces an urgent need for rigorous and
reproducible evaluation. We present OmniGenBench, a modular benchmarking
platform designed to unify the data, model, benchmarking, and interpretability
layers across GFMs. OmniGenBench enables standardized, one-command evaluation
of any GFM across five benchmark suites, with seamless integration of over 31
open-source models. Through automated pipelines and community-extensible
features, the platform addresses critical reproducibility challenges, including
data transparency, model interoperability, benchmark fragmentation, and
black-box interpretability. OmniGenBench aims to serve as foundational
infrastructure for reproducible genomic AI research, accelerating trustworthy
discovery and collaborative innovation in the era of genome-scale modeling.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [394] [RAG/LLM Augmented Switching Driven Polymorphic Metaheuristic Framework](https://arxiv.org/abs/2505.13808)
*Faramarz Safi Esfahani,Ghassan Beydoun,Morteza Saberi,Brad McCusker,Biswajeet Pradhan*

Main category: cs.NE

TL;DR: 论文提出了一种自适应的多态元启发式框架（PMF），通过实时性能反馈和动态算法选择机制，显著提升了优化效率，适用于高维、动态和多模态环境。


<details>
  <summary>Details</summary>
Motivation: 传统元启发式算法因结构固定和需要大量调参而效果受限，PMF旨在解决这一问题，通过动态选择和切换算法来提升性能。

Method: PMF利用多态元启发式代理（PMA）和选择代理（PMSA），基于关键性能指标动态选择和切换算法，实现持续自适应优化。

Result: 实验结果表明，PMF在收敛速度、适应性和解质量上优于传统方法，有效平衡探索与开发策略，显著提升优化效率。

Conclusion: PMF通过AI驱动决策和自校正机制，为工程、物流等领域的复杂优化问题提供了可扩展的智能自治框架。

Abstract: Metaheuristic algorithms are widely used for solving complex optimization
problems, yet their effectiveness is often constrained by fixed structures and
the need for extensive tuning. The Polymorphic Metaheuristic Framework (PMF)
addresses this limitation by introducing a self-adaptive metaheuristic
switching mechanism driven by real-time performance feedback and dynamic
algorithmic selection. PMF leverages the Polymorphic Metaheuristic Agent (PMA)
and the Polymorphic Metaheuristic Selection Agent (PMSA) to dynamically select
and transition between metaheuristic algorithms based on key performance
indicators, ensuring continuous adaptation. This approach enhances convergence
speed, adaptability, and solution quality, outperforming traditional
metaheuristics in high-dimensional, dynamic, and multimodal environments.
Experimental results on benchmark functions demonstrate that PMF significantly
improves optimization efficiency by mitigating stagnation and balancing
exploration-exploitation strategies across various problem landscapes. By
integrating AI-driven decision-making and self-correcting mechanisms, PMF paves
the way for scalable, intelligent, and autonomous optimization frameworks, with
promising applications in engineering, logistics, and complex decision-making
systems.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [395] [InterFeat: An Automated Pipeline for Finding Interesting Hypotheses in Structured Biomedical Data](https://arxiv.org/abs/2505.13534)
*Dan Ofer,Michal Linial,Dafna Shahaf*

Main category: q-bio.QM

TL;DR: 该论文提出了一种自动化发现生物医学数据中有趣假设的流程，结合机器学习、知识图谱和大型语言模型，成功预测了疾病风险因素。


<details>
  <summary>Details</summary>
Motivation: 科学发现的核心是寻找有趣现象，但这一过程通常是手动且定义模糊的。论文旨在通过自动化流程解决这一问题。

Method: 结合机器学习、知识图谱、文献搜索和大型语言模型，将“有趣性”定义为新颖性、实用性和合理性的组合。

Result: 在英国生物银行的8种主要疾病数据中，该流程成功预测了风险因素，40-53%的候选假设被验证为有趣，远超基线方法。

Conclusion: 该流程可扩展地实现了“有趣性”的操作化，适用于任何目标，并已公开数据和代码供进一步研究。

Abstract: Finding interesting phenomena is the core of scientific discovery, but it is
a manual, ill-defined concept. We present an integrative pipeline for
automating the discovery of interesting simple hypotheses (feature-target
relations with effect direction and a potential underlying mechanism) in
structured biomedical data. The pipeline combines machine learning, knowledge
graphs, literature search and Large Language Models. We formalize
"interestingness" as a combination of novelty, utility and plausibility. On 8
major diseases from the UK Biobank, our pipeline consistently recovers risk
factors years before their appearance in the literature. 40--53% of our top
candidates were validated as interesting, compared to 0--7% for a SHAP-based
baseline. Overall, 28% of 109 candidates were interesting to medical experts.
The pipeline addresses the challenge of operationalizing "interestingness"
scalably and for any target. We release data and code:
https://github.com/LinialLab/InterFeat

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [396] [Uncertainty Quantification for Prior-Data Fitted Networks using Martingale Posteriors](https://arxiv.org/abs/2505.11325)
*Thomas Nagler,David Rügamer*

Main category: stat.ME

TL;DR: 该论文提出了一种基于Martingale后验的采样方法，为PFNs提供了贝叶斯后验的不确定性量化。


<details>
  <summary>Details</summary>
Motivation: PFNs虽然在表格数据预测上表现优异，但缺乏对预测均值、分位数等的不确定性量化。

Method: 提出了一种基于Martingale后验的采样方法，构建贝叶斯后验，并证明了其收敛性。

Result: 在模拟和真实数据实验中，该方法有效展示了不确定性量化的能力。

Conclusion: 该方法为PFNs提供了可靠的不确定性量化，适用于推理应用。

Abstract: Prior-data fitted networks (PFNs) have emerged as promising foundation models
for prediction from tabular data sets, achieving state-of-the-art performance
on small to moderate data sizes without tuning. While PFNs are motivated by
Bayesian ideas, they do not provide any uncertainty quantification for
predictive means, quantiles, or similar quantities. We propose a principled and
efficient sampling procedure to construct Bayesian posteriors for such
estimates based on Martingale posteriors, and prove its convergence. Several
simulated and real-world data examples showcase the uncertainty quantification
of our method in inference applications.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [397] [Learning to Program Quantum Measurements for Machine Learning](https://arxiv.org/abs/2505.13525)
*Samual Yen-Chi Chen,Huan-Hsin Tseng,Hsin-Yi Lin,Shinjae Yoo*

Main category: quant-ph

TL;DR: 该论文提出了一种创新的量子机器学习框架，通过动态编程可观测量的方法，优化了量子电路的性能，显著提升了分类准确率等指标。


<details>
  <summary>Details</summary>
Motivation: 量子机器学习（QML）的发展面临数据编码策略和参数化量子电路设计的挑战，且现有模型的测量方案往往与问题需求不匹配。论文旨在解决这些问题，推动QML的广泛应用。

Method: 论文提出了一种端到端的可微分学习框架，通过神经网络动态编程量子可观测量的参数，同时优化量子电路参数，实现实时自适应调整。

Result: 数值模拟表明，该方法在变分量子电路中有效动态编程可观测量，性能优于现有方法，尤其在分类准确率等指标上表现突出。

Conclusion: 该框架显著提升了QML模型的整体效能，为量子机器学习的进一步发展提供了有力工具。

Abstract: The rapid advancements in quantum computing (QC) and machine learning (ML)
have sparked significant interest, driving extensive exploration of quantum
machine learning (QML) algorithms to address a wide range of complex
challenges. The development of high-performance QML models requires
expert-level expertise, presenting a key challenge to the widespread adoption
of QML. Critical obstacles include the design of effective data encoding
strategies and parameterized quantum circuits, both of which are vital for the
performance of QML models. Furthermore, the measurement process is often
neglected-most existing QML models employ predefined measurement schemes that
may not align with the specific requirements of the targeted problem. We
propose an innovative framework that renders the observable of a quantum
system-specifically, the Hermitian matrix-trainable. This approach employs an
end-to-end differentiable learning framework, enabling simultaneous
optimization of the neural network used to program the parameterized
observables and the standard quantum circuit parameters. Notably, the quantum
observable parameters are dynamically programmed by the neural network,
allowing the observables to adapt in real time based on the input data stream.
Through numerical simulations, we demonstrate that the proposed method
effectively programs observables dynamically within variational quantum
circuits, achieving superior results compared to existing approaches. Notably,
it delivers enhanced performance metrics, such as higher classification
accuracy, thereby significantly improving the overall effectiveness of QML
models.

</details>


### [398] [Benchmarking data encoding methods in Quantum Machine Learning](https://arxiv.org/abs/2505.14295)
*Orlane Zang,Grégoire Barrué,Tony Quertier*

Main category: quant-ph

TL;DR: 该论文研究了量子机器学习中数据编码的重要性，比较了不同编码方法在不同数据集上的表现。


<details>
  <summary>Details</summary>
Motivation: 量子机器学习中的数据编码步骤对模型性能有显著影响，但目前缺乏选择合适编码方法的通用规则。

Method: 研究并比较了多种常用的量子数据编码方法，使用不同数据集进行基准测试。

Result: 不同编码方法在不同数据集上表现各异，没有一种通用的最佳编码方法。

Conclusion: 需要根据具体数据集特点选择合适的量子数据编码方法，这仍是量子机器学习中的一个开放性问题。

Abstract: Data encoding plays a fundamental and distinctive role in Quantum Machine
Learning (QML). While classical approaches process data directly as vectors,
QML may require transforming classical data into quantum states through
encoding circuits, known as quantum feature maps or quantum embeddings. This
step leverages the inherently high-dimensional and non-linear nature of Hilbert
space, enabling more efficient data separation in complex feature spaces that
may be inaccessible to classical methods. This encoding part significantly
affects the performance of the QML model, so it is important to choose the
right encoding method for the dataset to be encoded. However, this choice is
generally arbitrary, since there is no "universal" rule for knowing which
encoding to choose based on a specific set of data. There are currently a
variety of encoding methods using different quantum logic gates. We studied the
most commonly used types of encoding methods and benchmarked them using
different datasets.

</details>


### [399] [QSVM-QNN: Quantum Support Vector Machine Based Quantum Neural Network Learning Algorithm for Brain-Computer Interfacing Systems](https://arxiv.org/abs/2505.14192)
*Bikash K. Behera,Saif Al-Kuwari,Ahmed Farouk*

Main category: quant-ph

TL;DR: 该研究提出了一种名为QSVM-QNN的新型混合量子学习模型，结合量子支持向量机（QSVM）和量子神经网络（QNN），以提高基于EEG的脑机接口任务的分类准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 脑机接口（BCI）系统在辅助技术和高级人机交互方面具有巨大潜力，但仍面临信号变异性、分类效率低和实时适应个体用户等挑战。

Method: 研究提出了一种混合量子学习模型QSVM-QNN，结合了QSVM的决策边界能力和QNN的表达学习能力，并在两个基准EEG数据集上进行了评估。

Result: QSVM-QNN在两个数据集上分别达到了0.990和0.950的高准确率，优于经典和独立的量子模型，并在六种量子噪声模型下表现出稳定的性能。

Conclusion: QSVM-QNN不仅适用于BCI，还可推广到其他生物医学和时间序列分类任务，为下一代神经技术系统提供了可扩展且抗噪声的解决方案。

Abstract: A brain-computer interface (BCI) system enables direct communication between
the brain and external devices, offering significant potential for assistive
technologies and advanced human-computer interaction. Despite progress, BCI
systems face persistent challenges, including signal variability,
classification inefficiency, and difficulty adapting to individual users in
real time. In this study, we propose a novel hybrid quantum learning model,
termed QSVM-QNN, which integrates a Quantum Support Vector Machine (QSVM) with
a Quantum Neural Network (QNN), to improve classification accuracy and
robustness in EEG-based BCI tasks. Unlike existing models, QSVM-QNN combines
the decision boundary capabilities of QSVM with the expressive learning power
of QNN, leading to superior generalization performance. The proposed model is
evaluated on two benchmark EEG datasets, achieving high accuracies of 0.990 and
0.950, outperforming both classical and standalone quantum models. To
demonstrate real-world viability, we further validated the robustness of QNN,
QSVM, and QSVM-QNN against six realistic quantum noise models, including bit
flip and phase damping. These experiments reveal that QSVM-QNN maintains stable
performance under noisy conditions, establishing its applicability for
deployment in practical, noisy quantum environments. Beyond BCI, the proposed
hybrid quantum architecture is generalizable to other biomedical and
time-series classification tasks, offering a scalable and noise-resilient
solution for next-generation neurotechnological systems.

</details>


### [400] [Quantum Optimization via Gradient-Based Hamiltonian Descent](https://arxiv.org/abs/2505.14670)
*Jiaqi Leng,Bin Shi*

Main category: quant-ph

TL;DR: 量子哈密顿下降（QHD）通过结合梯度信息改进，显著提升了收敛速度和全局解发现能力。


<details>
  <summary>Details</summary>
Motivation: QHD算法在复杂优化问题中面临收敛速度慢和鲁棒性不足的问题，需要改进以提升性能。

Method: 提出基于梯度的QHD算法，通过引入梯度信息优化量子哈密顿动力学。

Result: 数值模拟显示，基于梯度的QHD在挑战性问题中表现优于现有量子和经典方法至少一个数量级。

Conclusion: 基于梯度的QHD显著提升了优化性能，为复杂优化问题提供了更高效的解决方案。

Abstract: With rapid advancements in machine learning, first-order algorithms have
emerged as the backbone of modern optimization techniques, owing to their
computational efficiency and low memory requirements. Recently, the connection
between accelerated gradient methods and damped heavy-ball motion, particularly
within the framework of Hamiltonian dynamics, has inspired the development of
innovative quantum algorithms for continuous optimization. One such algorithm,
Quantum Hamiltonian Descent (QHD), leverages quantum tunneling to escape saddle
points and local minima, facilitating the discovery of global solutions in
complex optimization landscapes. However, QHD faces several challenges,
including slower convergence rates compared to classical gradient methods and
limited robustness in highly non-convex problems due to the non-local nature of
quantum states. Furthermore, the original QHD formulation primarily relies on
function value information, which limits its effectiveness. Inspired by
insights from high-resolution differential equations that have elucidated the
acceleration mechanisms in classical methods, we propose an enhancement to QHD
by incorporating gradient information, leading to what we call gradient-based
QHD. Gradient-based QHD achieves faster convergence and significantly increases
the likelihood of identifying global solutions. Numerical simulations on
challenging problem instances demonstrate that gradient-based QHD outperforms
existing quantum and classical methods by at least an order of magnitude.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [401] [Autonomous nanoparticle synthesis by design](https://arxiv.org/abs/2505.13571)
*Andy S. Anker,Jonas H. Jensen,Miguel Gonzalez-Duque,Rodrigo Moreno,Aleksandra Smolska,Mikkel Juelsholt,Vincent Hardion,Mads R. V. Jorgensen,Andres Faina,Jonathan Quinson,Kasper Stoy,Tejs Vegge*

Main category: cond-mat.mtrl-sci

TL;DR: 该论文提出了一种自主设计纳米颗粒合成方法的新方法，通过实时匹配实验数据与模拟目标模式，无需先验知识即可合成特定原子结构的材料。


<details>
  <summary>Details</summary>
Motivation: 传统材料合成依赖试错法，难以精确控制纳米颗粒的原子结构。本文旨在开发一种自主方法，直接针对目标原子结构进行合成，以克服这一挑战。

Method: 利用实时实验总散射（TS）和对分布函数（PDF）数据，与模拟目标模式匹配，自主设计合成方案。

Result: 在同步辐射实验中成功合成了两种结构不同的金纳米颗粒：5纳米十面体和10纳米面心立方结构。

Conclusion: 该方法为自主合成特定原子结构的材料提供了通用蓝图，可能彻底改变材料设计方式。

Abstract: Controlled synthesis of materials with specified atomic structures underpins
technological advances yet remains reliant on iterative, trial-and-error
approaches. Nanoparticles (NPs), whose atomic arrangement dictates their
emergent properties, are particularly challenging to synthesise due to numerous
tunable parameters. Here, we introduce an autonomous approach explicitly
targeting synthesis of atomic-scale structures. Our method autonomously designs
synthesis protocols by matching real time experimental total scattering (TS)
and pair distribution function (PDF) data to simulated target patterns, without
requiring prior synthesis knowledge. We demonstrate this capability at a
synchrotron, successfully synthesising two structurally distinct gold NPs: 5 nm
decahedral and 10 nm face-centred cubic structures. Ultimately, specifying a
simulated target scattering pattern, thus representing a bespoke atomic
structure, and obtaining both the synthesised material and its reproducible
synthesis protocol on demand may revolutionise materials design. Thus,
ScatterLab provides a generalisable blueprint for autonomous, atomic
structure-targeted synthesis across diverse systems and applications.

</details>


### [402] [Path-integral molecular dynamics with actively-trained and universal machine learning force fields](https://arxiv.org/abs/2505.14245)
*A. A. Solovykh,N. E. Rybin,I. S. Novikov,A. V. Shapeev*

Main category: cond-mat.mtrl-sci

TL;DR: 该论文开发了一个接口，将MLIP-2软件包中的矩张量势（MTP）与i-PI软件包结合，用于路径积分分子动力学（PIMD）计算，以研究核量子效应对材料性质的影响，并展示了其高精度和有效性。


<details>
  <summary>Details</summary>
Motivation: 核量子效应（NQE）在有限温度下会显著改变材料性质。传统方法中，经验势计算速度快但精度不足，而量子力学计算精度高但计算成本大。机器学习原子间势能提供了一种兼顾精度和效率的解决方案。

Method: 开发了一个接口，将MLIP-2中的矩张量势（MTP）集成到i-PI软件包中，用于PIMD计算。通过主动学习势能，研究了NQE对氢化锂（LiH）和硅（Si）的晶格参数、热膨胀系数和径向分布函数的影响。

Result: MTP-PIMD方法的结果与实验数据、准谐近似计算以及MatterSim力场的预测进行了比较，证明了该方法的高精度和有效性。

Conclusion: MTP-PIMD方法在计算核量子效应时表现出高精度和高效性，为材料性质研究提供了可靠的工具。

Abstract: Accounting for nuclear quantum effects (NQEs) can significantly alter
material properties at finite temperatures. Atomic modeling using the
path-integral molecular dynamics (PIMD) method can fully account for such
effects, but requires computationally efficient and accurate models of
interatomic interactions. Empirical potentials are fast but may lack sufficient
accuracy, whereas quantum-mechanical calculations are highly accurate but
computationally expensive. Machine-learned interatomic potentials offer a
solution to this challenge, providing near-quantum-mechanical accuracy while
maintaining high computational efficiency compared to density functional theory
(DFT) calculations. In this context, an interface was developed to integrate
moment tensor potentials (MTPs) from the MLIP-2 software package into PIMD
calculations using the i-PI software package. This interface was then applied
to active learning of potentials and to investigate the influence of NQEs on
material properties, namely the temperature dependence of lattice parameters
and thermal expansion coefficients, as well as radial distribution functions,
for lithium hydride (LiH) and silicon (Si) systems. The results were compared
with experimental data, quasi-harmonic approximation calculations, and
predictions from the universal machine learning force field MatterSim. These
comparisons demonstrated the high accuracy and effectiveness of the MTP-PIMD
approach.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [403] [Exploring Emotional Synchrony in Dyadic Interactions: The Role of Speech Conditions in Facial and Vocal Affective Alignment](https://arxiv.org/abs/2505.13455)
*Von Ralph Dane Marquez Herbuela,Yukie Nagai*

Main category: eess.AS

TL;DR: 研究发现，非重叠对话比重叠对话更能促进情绪同步，且面部表情和语音在对话结构中有不同的主导模式。


<details>
  <summary>Details</summary>
Motivation: 研究人类如何通过多种沟通渠道（特别是面部表情和语音）表达和同步情绪，对情感识别系统和人机交互有重要意义。

Method: 使用IEMOCAP数据集中的双人互动数据，通过EmoNet（面部视频）和Wav2Vec2模型（语音音频）提取连续情绪估计，并根据语音重叠分类片段，采用皮尔逊相关、滞后调整分析和动态时间规整（DTW）评估情绪对齐。

Result: 非重叠对话表现出更稳定和可预测的情绪同步，而重叠对话则显示出更高的变异性。面部表情在轮流发言时更常先于语音，而在同时发声时语音则主导。

Conclusion: 对话结构在调节情绪沟通中起重要作用，研究为现实互动中多模态情感对齐的时空动态提供了新见解。

Abstract: Understanding how humans express and synchronize emotions across multiple
communication channels particularly facial expressions and speech has
significant implications for emotion recognition systems and human computer
interaction. Motivated by the notion that non-overlapping speech promotes
clearer emotional coordination, while overlapping speech disrupts synchrony,
this study examines how these conversational dynamics shape the spatial and
temporal alignment of arousal and valence across facial and vocal modalities.
Using dyadic interactions from the IEMOCAP dataset, we extracted continuous
emotion estimates via EmoNet (facial video) and a Wav2Vec2-based model (speech
audio). Segments were categorized based on speech overlap, and emotional
alignment was assessed using Pearson correlation, lag adjusted analysis, and
Dynamic Time Warping (DTW). Across analyses, non overlapping speech was
associated with more stable and predictable emotional synchrony than
overlapping speech. While zero-lag correlations were low and not statistically
different, non overlapping speech showed reduced variability, especially for
arousal. Lag adjusted correlations and best-lag distributions revealed clearer,
more consistent temporal alignment in these segments. In contrast, overlapping
speech exhibited higher variability and flatter lag profiles, though DTW
indicated unexpectedly tighter alignment suggesting distinct coordination
strategies. Notably, directionality patterns showed that facial expressions
more often preceded speech during turn-taking, while speech led during
simultaneous vocalizations. These findings underscore the importance of
conversational structure in regulating emotional communication and provide new
insight into the spatial and temporal dynamics of multimodal affective
alignment in real world interaction.

</details>


### [404] [Direction-Aware Neural Acoustic Fields for Few-Shot Interpolation of Ambisonic Impulse Responses](https://arxiv.org/abs/2505.13617)
*Christopher Ick,Gordon Wichern,Yoshiki Masuyama,François Germain,Jonathan Le Roux*

Main category: eess.AS

TL;DR: 提出方向感知神经场（DANF），通过Ambisonic格式的RIR更精确捕捉声场方向特性，并探索其在新房间的适应能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于神经场的方法仅关注单耳全向或双耳听者，无法精确捕捉单点真实声场的方向特性。

Method: 结合Ambisonic格式RIR的方向信息，提出方向感知神经场（DANF）及方向感知损失函数，并研究低秩适应等新房间适应方法。

Result: DANF能更显式地建模声源与听者间的空间关系，并展示出对新房间的适应潜力。

Conclusion: DANF通过方向感知建模提升了声场表征精度，为声学场景重建提供了新思路。

Abstract: The characteristics of a sound field are intrinsically linked to the
geometric and spatial properties of the environment surrounding a sound source
and a listener. The physics of sound propagation is captured in a time-domain
signal known as a room impulse response (RIR). Prior work using neural fields
(NFs) has allowed learning spatially-continuous representations of RIRs from
finite RIR measurements. However, previous NF-based methods have focused on
monaural omnidirectional or at most binaural listeners, which does not
precisely capture the directional characteristics of a real sound field at a
single point. We propose a direction-aware neural field (DANF) that more
explicitly incorporates the directional information by Ambisonic-format RIRs.
While DANF inherently captures spatial relations between sources and listeners,
we further propose a direction-aware loss. In addition, we investigate the
ability of DANF to adapt to new rooms in various ways including low-rank
adaptation.

</details>


### [405] [Articulatory Feature Prediction from Surface EMG during Speech Production](https://arxiv.org/abs/2505.13814)
*Jihwan Lee,Kevin Huang,Kleanthis Avramidis,Simon Pistrosch,Monica Gonzalez-Machorro,Yoonjeong Lee,Björn Schuller,Louis Goldstein,Shrikanth Narayanan*

Main category: eess.AS

TL;DR: 提出一种从表面肌电信号预测发音特征并合成语音的新方法，准确率约0.9，首次实现通过发音特征从肌电信号解码语音。


<details>
  <summary>Details</summary>
Motivation: 探索通过表面肌电信号(EMG)预测发音特征并合成语音的新途径，优化电极配置以提高预测性能。

Method: 结合卷积层和Transformer模块构建模型，分别预测发音特征，再解码为语音波形。

Result: 多数发音特征预测相关性达0.9，成功合成可懂语音，并分析了电极位置对预测的影响。

Conclusion: 该方法为基于EMG的语音合成提供了新思路，公开的代码和样本促进后续研究。

Abstract: We present a model for predicting articulatory features from surface
electromyography (EMG) signals during speech production. The proposed model
integrates convolutional layers and a Transformer block, followed by separate
predictors for articulatory features. Our approach achieves a high prediction
correlation of approximately 0.9 for most articulatory features. Furthermore,
we demonstrate that these predicted articulatory features can be decoded into
intelligible speech waveforms. To our knowledge, this is the first method to
decode speech waveforms from surface EMG via articulatory features, offering a
novel approach to EMG-based speech synthesis. Additionally, we analyze the
relationship between EMG electrode placement and articulatory feature
predictability, providing knowledge-driven insights for optimizing EMG
electrode configurations. The source code and decoded speech samples are
publicly available.

</details>


### [406] [SPIRIT: Patching Speech Language Models against Jailbreak Attacks](https://arxiv.org/abs/2505.13541)
*Amirbek Djanibekov,Nurdaulet Mukhituly,Kentaro Inui,Hanan Aldarmaki,Nils Lukas*

Main category: eess.AS

TL;DR: 语音语言模型（SLMs）因语音信号更丰富而面临更高的安全风险，研究发现其对越狱攻击极为脆弱。作者提出了一种无需重新训练的后处理防御方法，显著提升了安全性。


<details>
  <summary>Details</summary>
Motivation: 语音语言模型通过语音指令实现自然交互，能更准确地捕捉用户意图。然而，相比文本模型，语音信号更易受到攻击，如通过注入难以察觉的噪声绕过安全机制。

Method: 提出了一种后处理修补防御方法，通过干预推理过程中模型的激活值来提升安全性，无需重新训练模型。

Result: 该方法将SLMs的鲁棒性提升至99%，对模型实用性影响极小，并在大规模基准测试中验证了其有效性。

Conclusion: 后处理防御方法能有效提升语音语言模型的安全性，同时保持实用性，为SLMs的安全部署提供了可行方案。

Abstract: Speech Language Models (SLMs) enable natural interactions via spoken
instructions, which more effectively capture user intent by detecting nuances
in speech. The richer speech signal introduces new security risks compared to
text-based models, as adversaries can better bypass safety mechanisms by
injecting imperceptible noise to speech. We analyze adversarial attacks and
find that SLMs are substantially more vulnerable to jailbreak attacks, which
can achieve a perfect 100% attack success rate in some instances. To improve
security, we propose post-hoc patching defenses used to intervene during
inference by modifying the SLM's activations that improve robustness up to 99%
with (i) negligible impact on utility and (ii) without any re-training. We
conduct ablation studies to maximize the efficacy of our defenses and improve
the utility/security trade-off, validated with large-scale benchmarks unique to
SLMs.

</details>


### [407] [Pairwise Evaluation of Accent Similarity in Speech Synthesis](https://arxiv.org/abs/2505.14410)
*Jinzuomu Zhong,Suyuan Liu,Dan Wells,Korin Richmond*

Main category: eess.AS

TL;DR: 本文提出改进语音合成中口音相似性评估的主客观方法，主观上优化XAB听力测试，客观上采用发音相关指标，并指出常用指标在评估少数口音时的局限性。


<details>
  <summary>Details</summary>
Motivation: 当前高保真口音生成的研究日益增多，但口音相似性评估方法尚未充分探索，本文旨在提升主客观评估效果。

Method: 主观上改进XAB测试，增加文本转录和差异标注；客观上采用元音共振峰距离和音素后验图等发音相关指标。

Result: 实验表明，新提出的指标能有效评估口音相似性，同时揭示了词错误率等常用指标在少数口音评估中的不足。

Conclusion: 结合优化的主观测试和发音相关客观指标，可更准确评估口音相似性，为语音合成研究提供新方向。

Abstract: Despite growing interest in generating high-fidelity accents, evaluating
accent similarity in speech synthesis has been underexplored. We aim to enhance
both subjective and objective evaluation methods for accent similarity.
Subjectively, we refine the XAB listening test by adding components that
achieve higher statistical significance with fewer listeners and lower costs.
Our method involves providing listeners with transcriptions, having them
highlight perceived accent differences, and implementing meticulous screening
for reliability. Objectively, we utilise pronunciation-related metrics, based
on distances between vowel formants and phonetic posteriorgrams, to evaluate
accent generation. Comparative experiments reveal that these metrics, alongside
accent similarity, speaker similarity, and Mel Cepstral Distortion, can be
used. Moreover, our findings underscore significant limitations of common
metrics like Word Error Rate in assessing underrepresented accents.

</details>


### [408] [Mitigating Subgroup Disparities in Multi-Label Speech Emotion Recognition: A Pseudo-Labeling and Unsupervised Learning Approach](https://arxiv.org/abs/2505.14449)
*Yi-Cheng Lin,Huang-Cheng Chou,Hung-yi Lee*

Main category: eess.AS

TL;DR: 该论文提出了一种隐式人口统计推断（IDI）模块，通过伪标签和无监督学习减少语音情感识别中的偏见，显著提高了公平性指标。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常依赖显式人口统计标签，但由于隐私问题难以获取。因此，需要一种不依赖显式标签的公平性提升方法。

Method: 使用预训练模型的伪标签和无监督k-means聚类来推断隐式人口统计信息，从而减少偏见。

Result: 伪标签IDI将公平性指标提高33%以上，SER准确率下降不到3%；无监督IDI提高公平性指标26%以上，SER性能下降不到4%。

Conclusion: IDI模块在缺乏显式人口统计信息时，能有效减少种族和年龄差异，具有实际应用潜力。

Abstract: While subgroup disparities and performance bias are increasingly studied in
computational research, fairness in categorical Speech Emotion Recognition
(SER) remains underexplored. Existing methods often rely on explicit
demographic labels, which are difficult to obtain due to privacy concerns. To
address this limitation, we introduce an Implicit Demography Inference (IDI)
module that leverages pseudo-labeling from a pre-trained model and unsupervised
learning using k-means clustering to mitigate bias in SER. Our experiments show
that pseudo-labeling IDI reduces subgroup disparities, improving fairness
metrics by over 33% with less than a 3% decrease in SER accuracy. Also, the
unsupervised IDI yields more than a 26% improvement in fairness metrics with a
drop of less than 4% in SER performance. Further analyses reveal that the
unsupervised IDI consistently mitigates race and age disparities, demonstrating
its potential in scenarios where explicit demographic information is
unavailable.

</details>


### [409] [Teaching Audio-Aware Large Language Models What Does Not Hear: Mitigating Hallucinations through Synthesized Negative Samples](https://arxiv.org/abs/2505.14518)
*Chun-Yi Kuan,Hung-yi Lee*

Main category: eess.AS

TL;DR: 论文提出LISTEN方法，通过对比训练增强音频感知大语言模型区分真实与虚构声音的能力，无需修改模型参数且高效。


<details>
  <summary>Details</summary>
Motivation: 现有音频感知大语言模型常产生虚构声音事件，影响实际应用可靠性，需解决此问题。

Method: 采用LISTEN对比训练方法，利用轻量适配器集成音频表示，通过合成数据区分声音存在与否。

Result: LISTEN有效减少虚构声音，保持现有音频问答基准性能，数据与计算效率更高。

Conclusion: LISTEN方法在不改动模型参数下，显著提升音频感知大语言模型的可靠性及效率。

Abstract: Recent advancements in audio-aware large language models (ALLMs) enable them
to process and understand audio inputs. However, these models often hallucinate
non-existent sound events, reducing their reliability in real-world
applications. To address this, we propose LISTEN (Learning to Identify Sounds
Through Extended Negative Samples), a contrastive-like training method that
enhances ALLMs' ability to distinguish between present and absent sounds using
synthesized data from the backbone LLM. Unlike prior approaches, our method
requires no modification to LLM parameters and efficiently integrates audio
representations via a lightweight adapter. Experiments show that LISTEN
effectively mitigates hallucinations while maintaining impressive performance
on existing audio question and reasoning benchmarks. At the same time, it is
more efficient in both data and computation.

</details>


### [410] [FlowTSE: Target Speaker Extraction with Flow Matching](https://arxiv.org/abs/2505.14465)
*Aviv Navon,Aviv Shamsian,Yael Segal-Feldman,Neta Glazer,Gil Hetz,Joseph Keshet*

Main category: eess.AS

TL;DR: FlowTSE是一种基于条件流匹配的简单有效目标说话人提取方法，通过改进相位估计，在标准基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有的生成式目标说话人提取方法大多依赖复杂流程和预训练组件，计算开销大，且研究不足。

Method: 提出FlowTSE，基于条件流匹配，接收注册音频样本和混合语音信号（mel谱图），并设计新型声码器改进相位估计。

Result: 实验表明，FlowTSE在标准TSE基准测试中达到或超越现有强基线性能。

Conclusion: FlowTSE为生成式目标说话人提取提供了简单高效的解决方案，尤其在相位重建任务中表现突出。

Abstract: Target speaker extraction (TSE) aims to isolate a specific speaker's speech
from a mixture using speaker enrollment as a reference. While most existing
approaches are discriminative, recent generative methods for TSE achieve strong
results. However, generative methods for TSE remain underexplored, with most
existing approaches relying on complex pipelines and pretrained components,
leading to computational overhead. In this work, we present FlowTSE, a simple
yet effective TSE approach based on conditional flow matching. Our model
receives an enrollment audio sample and a mixed speech signal, both represented
as mel-spectrograms, with the objective of extracting the target speaker's
clean speech. Furthermore, for tasks where phase reconstruction is crucial, we
propose a novel vocoder conditioned on the complex STFT of the mixed signal,
enabling improved phase estimation. Experimental results on standard TSE
benchmarks show that FlowTSE matches or outperforms strong baselines.

</details>


### [411] [Steering Deep Non-Linear Spatially Selective Filters for Weakly Guided Extraction of Moving Speakers in Dynamic Scenarios](https://arxiv.org/abs/2505.14517)
*Jakob Kienegger,Timo Gerkmann*

Main category: eess.AS

TL;DR: 论文提出了一种弱引导的说话人提取方法，仅依赖目标的初始位置来处理空间动态场景，通过深度跟踪算法和联合训练策略，解决了空间模糊性问题。


<details>
  <summary>Details</summary>
Motivation: 现有的深度非线性空间滤波方法在目标方向已知且静止时表现优异，但在空间动态场景中（如说话人移动交叉）由于时变空间特征和模糊性而面临挑战。手动跟踪移动说话人不切实际，因此需要一种不依赖精确时变方向线索的方法。

Method: 提出了一种弱引导的说话人提取方法，仅依赖目标的初始位置，结合深度跟踪算法和合成数据集的联合训练策略。

Result: 该方法在解决空间模糊性方面表现出色，甚至优于不匹配但强引导的提取方法。

Conclusion: 弱引导的说话人提取方法在空间动态场景中具有潜力，能够有效处理移动说话人带来的挑战。

Abstract: Recent speaker extraction methods using deep non-linear spatial filtering
perform exceptionally well when the target direction is known and stationary.
However, spatially dynamic scenarios are considerably more challenging due to
time-varying spatial features and arising ambiguities, e.g. when moving
speakers cross. While in a static scenario it may be easy for a user to point
to the target's direction, manually tracking a moving speaker is impractical.
Instead of relying on accurate time-dependent directional cues, which we refer
to as strong guidance, in this paper we propose a weakly guided extraction
method solely depending on the target's initial position to cope with spatial
dynamic scenarios. By incorporating our own deep tracking algorithm and
developing a joint training strategy on a synthetic dataset, we demonstrate the
proficiency of our approach in resolving spatial ambiguities and even
outperform a mismatched, but strongly guided extraction method.

</details>


### [412] [SSPS: Self-Supervised Positive Sampling for Robust Self-Supervised Speaker Verification](https://arxiv.org/abs/2505.14561)
*Theo Lepage,Reda Dehak*

Main category: eess.AS

TL;DR: 该论文提出了一种名为SSPS的自监督正采样技术，用于改进说话人验证任务，通过聚类和记忆队列寻找相同说话人但不同录音条件的正样本，显著降低了类内方差并提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有的自监督学习框架在说话人验证中主要依赖同语句正采样和数据增强，导致模型主要编码了录音条件等通道信息，而忽略了说话人本身的特征。这限制了模型性能的提升。

Method: 提出了自监督正采样技术(SSPS)，通过聚类分配和正样本嵌入的记忆队列，在潜在空间中为给定锚点寻找相同说话人但不同录音条件的正样本。

Result: SSPS显著提升了SimCLR和DINO在VoxCeleb1-O上的性能，分别达到2.57%和2.53%的EER。特别是SimCLR-SSPS实现了58%的EER降低，性能与DINO-SSPS相当。

Conclusion: SSPS通过降低类内方差有效改进了说话人验证性能，为自监督学习在该领域的应用提供了新的技术路径。

Abstract: Self-Supervised Learning (SSL) has led to considerable progress in Speaker
Verification (SV). The standard framework uses same-utterance positive sampling
and data-augmentation to generate anchor-positive pairs of the same speaker.
This is a major limitation, as this strategy primarily encodes channel
information from the recording condition, shared by the anchor and positive. We
propose a new positive sampling technique to address this bottleneck:
Self-Supervised Positive Sampling (SSPS). For a given anchor, SSPS aims to find
an appropriate positive, i.e., of the same speaker identity but a different
recording condition, in the latent space using clustering assignments and a
memory queue of positive embeddings. SSPS improves SV performance for both
SimCLR and DINO, reaching 2.57% and 2.53% EER, outperforming SOTA SSL methods
on VoxCeleb1-O. In particular, SimCLR-SSPS achieves a 58% EER reduction by
lowering intra-speaker variance, providing comparable performance to DINO-SSPS.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [413] [An agentic system with reinforcement-learned subsystem improvements for parsing form-like documents](https://arxiv.org/abs/2505.13504)
*Ayesha Amjad,Saurav Sthapit,Tahir Qasim Syed*

Main category: cs.IR

TL;DR: 提出了一种基于LLM代理和强化学习的自改进信息提取框架，用于处理多样化的表单文档。


<details>
  <summary>Details</summary>
Motivation: 传统OCR和单一学习算法在表单数据提取中存在局限性，缺乏系统性改进潜力。

Method: 采用模块化多代理框架，结合任务特定提示和强化学习策略，通过元提示代理优化执行代理。

Result: 在SOIRE和CORD基准数据集上表现出色，实现了无需人工干预的自动化信息提取。

Conclusion: 该自适应系统能处理多样化文档和布局，为自动化信息提取提供了有效解决方案。

Abstract: Extracting alphanumeric data from form-like documents such as invoices,
purchase orders, bills, and financial documents is often performed via vision
(OCR) and learning algorithms or monolithic pipelines with limited potential
for systemic improvements. We propose an agentic AI system that leverages Large
Language Model (LLM) agents and a reinforcement learning (RL) driver agent to
automate consistent, self-improving extraction under LLM inference uncertainty.
Our work highlights the limitations of monolithic LLM-based extraction and
introduces a modular, multi-agent framework with task-specific prompts and an
RL policy of rewards and penalties to guide a meta-prompting agent to learn
from past errors and improve prompt-based actor agents. This self-corrective
adaptive system handles diverse documents, file formats, layouts, and LLMs,
aiming to automate accurate information extraction without the need for human
intervention. Results as reported on two benchmark datasets of SOIRE, and CORD,
are promising for the agentic AI framework.

</details>


### [414] [Beyond Retrieval: Joint Supervision and Multimodal Document Ranking for Textbook Question Answering](https://arxiv.org/abs/2505.13520)
*Hessa Alawwad,Usman Naseem,Areej Alhothali,Ali Alkhathlan,Amani Jamal*

Main category: cs.IR

TL;DR: 提出JETRTQA模型，通过多目标联合训练增强语义表示，提升教科书问答任务中检索文档的相关性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在教育场景中难以实现准确的语义对齐和任务特定文档检索，需改进复杂多模态上下文下的问答性能。

Method: 基于检索-生成架构的多模态框架，结合排序监督和答案隐式监督联合训练，优化语义表示。

Result: 在CK12-QA数据集上验证集准确率提升2.4%，测试集提升11.1%，显著区分复杂多模态文档。

Conclusion: JETRTQA通过联合训练机制有效提升教育场景下的问答性能，成为当前最佳方法。

Abstract: Textbook question answering (TQA) is a complex task, requiring the
interpretation of complex multimodal context. Although recent advances have
improved overall performance, they often encounter difficulties in educational
settings where accurate semantic alignment and task-specific document retrieval
are essential. In this paper, we propose a novel approach to multimodal
textbook question answering by introducing a mechanism for enhancing semantic
representations through multi-objective joint training. Our model, Joint
Embedding Training With Ranking Supervision for Textbook Question Answering
(JETRTQA), is a multimodal learning framework built on a retriever--generator
architecture that uses a retrieval-augmented generation setup, in which a
multimodal large language model generates answers. JETRTQA is designed to
improve the relevance of retrieved documents in complex educational contexts.
Unlike traditional direct scoring approaches, JETRTQA learns to refine the
semantic representations of questions and documents through a supervised signal
that combines pairwise ranking and implicit supervision derived from answers.
We evaluate our method on the CK12-QA dataset and demonstrate that it
significantly improves the discrimination between informative and irrelevant
documents, even when they are long, complex, and multimodal. JETRTQA
outperforms the previous state of the art, achieving a 2.4\% gain in accuracy
on the validation set and 11.1\% on the test set.

</details>


### [415] [Geography-Aware Large Language Models for Next POI Recommendation](https://arxiv.org/abs/2505.13526)
*Zhao Liu,Wei Liu,Huajie Zhu,Jianxing Yu,Jian Yin,Wang-Chien Lee,Shun Wang*

Main category: cs.IR

TL;DR: GA-LLM框架通过地理坐标注入和POI对齐模块增强LLM，提升POI推荐性能。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型在空间任务（如下一个POI推荐）中面临地理坐标建模困难和POI关系捕捉不足的挑战。

Method: 提出GA-LLM框架，包含地理坐标注入模块（GCIM）和POI对齐模块（PAM），分别处理空间表示和POI关系。

Result: 在三个真实数据集上验证了GA-LLM的先进性能。

Conclusion: GA-LLM有效结合地理信息和POI关系，显著提升下一个POI推荐的准确性。

Abstract: The next Point-of-Interest (POI) recommendation task aims to predict users'
next destinations based on their historical movement data and plays a key role
in location-based services and personalized applications. Accurate next POI
recommendation depends on effectively modeling geographic information and POI
transition relations, which are crucial for capturing spatial dependencies and
user movement patterns. While Large Language Models (LLMs) exhibit strong
capabilities in semantic understanding and contextual reasoning, applying them
to spatial tasks like next POI recommendation remains challenging. First, the
infrequent nature of specific GPS coordinates makes it difficult for LLMs to
model precise spatial contexts. Second, the lack of knowledge about POI
transitions limits their ability to capture potential POI-POI relationships. To
address these issues, we propose GA-LLM (Geography-Aware Large Language Model),
a novel framework that enhances LLMs with two specialized components. The
Geographic Coordinate Injection Module (GCIM) transforms GPS coordinates into
spatial representations using hierarchical and Fourier-based positional
encoding, enabling the model to understand geographic features from multiple
perspectives. The POI Alignment Module (PAM) incorporates POI transition
relations into the LLM's semantic space, allowing it to infer global POI
relationships and generalize to unseen POIs. Experiments on three real-world
datasets demonstrate the state-of-the-art performance of GA-LLM.

</details>


### [416] [LLM-Based User Simulation for Low-Knowledge Shilling Attacks on Recommender Systems](https://arxiv.org/abs/2505.13528)
*Shengkang Gu,Jiahao Liu,Dongsheng Li,Guangping Zhang,Mingzhe Han,Hansu Gu,Peng Zhang,Ning Gu,Li Shang,Tun Lu*

Main category: cs.IR

TL;DR: 论文提出Agent4SR框架，利用基于大语言模型的智能体进行低知识、高影响力的推荐系统攻击，通过生成评分和评论实现高效且隐蔽的操纵。


<details>
  <summary>Details</summary>
Motivation: 传统推荐系统攻击方法依赖简单启发式规则，需访问内部数据且忽视文本评论的操纵潜力。本文旨在探索LLM智能体在低知识条件下实现高效攻击的可能性。

Method: 设计Agent4SR框架，包含目标画像构建、混合记忆检索和评论攻击策略，通过模拟真实用户行为生成评分和特征传播式评论。

Result: 在多数据集和推荐架构上的实验表明，Agent4SR在攻击效果和隐蔽性上均优于现有低知识基线方法。

Conclusion: LLM驱动的智能体构成新型威胁，凸显现代推荐系统亟需加强防御机制。

Abstract: Recommender systems (RS) are increasingly vulnerable to shilling attacks,
where adversaries inject fake user profiles to manipulate system outputs.
Traditional attack strategies often rely on simplistic heuristics, require
access to internal RS data, and overlook the manipulation potential of textual
reviews. In this work, we introduce Agent4SR, a novel framework that leverages
Large Language Model (LLM)-based agents to perform low-knowledge, high-impact
shilling attacks through both rating and review generation. Agent4SR simulates
realistic user behavior by orchestrating adversarial interactions, selecting
items, assigning ratings, and crafting reviews, while maintaining behavioral
plausibility. Our design includes targeted profile construction, hybrid memory
retrieval, and a review attack strategy that propagates target item features
across unrelated reviews to amplify manipulation. Extensive experiments on
multiple datasets and RS architectures demonstrate that Agent4SR outperforms
existing low-knowledge baselines in both effectiveness and stealth. Our
findings reveal a new class of emergent threats posed by LLM-driven agents,
underscoring the urgent need for enhanced defenses in modern recommender
systems.

</details>


### [417] [Information Extraction from Visually Rich Documents using LLM-based Organization of Documents into Independent Textual Segments](https://arxiv.org/abs/2505.13535)
*Aniket Bhattacharyya,Anurag Tripathi,Ujjal Das,Archan Karmakar,Amit Pathak,Maneesh Gupta*

Main category: cs.IR

TL;DR: BLOCKIE是一种基于LLM的新方法，通过将视觉丰富文档组织成可重用的语义块，独立处理每个块，提高了信息提取的性能和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有的非LLM NLP方法缺乏推理能力，无法推断文档中未明确提及的值，且对新格式的泛化能力差；而现有的基于LLM的方法在理解文档布局方面表现不佳，尤其在未见过的文档格式中。

Method: 提出BLOCKIE方法，将视觉丰富文档划分为局部化、可重用的语义文本段（称为语义块），并独立处理这些块，以实现更集中和泛化的推理。

Result: BLOCKIE在公共VRD基准测试中F1分数比现有最优方法提高了1-3%，对未见过的文档格式具有鲁棒性，并能正确提取文档中未明确提及的信息。

Conclusion: BLOCKIE通过语义块的组织和独立处理，显著提升了信息提取的性能和泛化能力，尤其在处理复杂和未见过的文档格式时表现优异。

Abstract: Information extraction (IE) from Visually Rich Documents (VRDs) containing
layout features along with text is a critical and well-studied task.
Specialized non-LLM NLP-based solutions typically involve training models using
both textual and geometric information to label sequences/tokens as named
entities or answers to specific questions. However, these approaches lack
reasoning, are not able to infer values not explicitly present in documents,
and do not generalize well to new formats. Generative LLM-based approaches
proposed recently are capable of reasoning, but struggle to comprehend clues
from document layout especially in previously unseen document formats, and do
not show competitive performance in heterogeneous VRD benchmark datasets. In
this paper, we propose BLOCKIE, a novel LLM-based approach that organizes VRDs
into localized, reusable semantic textual segments called $\textit{semantic
blocks}$, which are processed independently. Through focused and more
generalizable reasoning,our approach outperforms the state-of-the-art on public
VRD benchmarks by 1-3% in F1 scores, is resilient to document formats
previously not encountered and shows abilities to correctly extract information
not explicitly present in documents.

</details>


### [418] [RAGXplain: From Explainable Evaluation to Actionable Guidance of RAG Pipelines](https://arxiv.org/abs/2505.13538)
*Dvir Cohen,Lin Burg,Gilad Barkan*

Main category: cs.IR

TL;DR: RAGXplain是一个评估框架，通过量化RAG系统性能并提供可操作的改进建议，帮助用户理解和优化AI系统。


<details>
  <summary>Details</summary>
Motivation: 传统RAG评估方法主要提供定量分数，但缺乏对复杂多阶段流程的具体指导和改进建议，难以帮助用户优化系统。

Method: RAGXplain利用LLM推理将原始分数转化为连贯的叙述，识别性能差距并提出针对性改进建议。

Result: 实验表明，RAGXplain的评估与人类判断高度一致，其建议显著提升了系统性能。

Conclusion: RAGXplain架起了定量评估与实际优化之间的桥梁，增强了用户对AI系统的理解、信任和优化能力。

Abstract: Retrieval-Augmented Generation (RAG) systems show promise by coupling large
language models with external knowledge, yet traditional RAG evaluation methods
primarily report quantitative scores while offering limited actionable guidance
for refining these complex pipelines. In this paper, we introduce RAGXplain, an
evaluation framework that quantifies RAG performance and translates these
assessments into clear insights that clarify the workings of its complex,
multi-stage pipeline and offer actionable recommendations. Using LLM reasoning,
RAGXplain converts raw scores into coherent narratives identifying performance
gaps and suggesting targeted improvements. By providing transparent
explanations for AI decision-making, our framework fosters user trust-a key
challenge in AI adoption. Our LLM-based metric assessments show strong
alignment with human judgments, and experiments on public question-answering
datasets confirm that applying RAGXplain's actionable recommendations
measurably improves system performance. RAGXplain thus bridges quantitative
evaluation and practical optimization, empowering users to understand, trust,
and enhance their AI systems.

</details>


### [419] [Know Or Not: a library for evaluating out-of-knowledge base robustness](https://arxiv.org/abs/2505.13545)
*Jessica Foo,Pradyumna Shyama Prasad,Shaun Khoo*

Main category: cs.IR

TL;DR: 论文提出了一种评估LLMs在RAG设置中OOKB鲁棒性的新方法，并开发了开源工具knowornot。


<details>
  <summary>Details</summary>
Motivation: 由于LLMs在关键应用中存在幻觉风险，特别是在RAG设置中面对知识库外问题时仍可能产生幻觉，因此需要一种无需人工标注的评估方法。

Method: 提出了一种系统评估LLMs在RAG设置中OOKB鲁棒性的方法，并开发了开源库knowornot，提供统一API、模块化架构、数据建模和定制工具。

Result: 通过PolicyBench基准测试展示了knowornot的实用性，分析了四个政府政策QA聊天机器人的OOKB鲁棒性。

Conclusion: knowornot为评估LLMs的OOKB鲁棒性提供了有效工具，支持用户自定义评估数据与流程。

Abstract: While the capabilities of large language models (LLMs) have progressed
significantly, their use in high-stakes applications have been limited due to
risks of hallucination. One key approach in reducing hallucination is
retrieval-augmented generation (RAG), but even in such setups, LLMs may still
hallucinate when presented with questions outside of the knowledge base. Such
behavior is unacceptable in high-stake applications where LLMs are expected to
abstain from answering queries it does not have sufficient context on. In this
work, we present a novel methodology for systematically evaluating
out-of-knowledge base (OOKB) robustness of LLMs (whether LLMs know or do not
know) in the RAG setting, without the need for manual annotation of gold
standard answers. We implement our methodology in knowornot, an open-source
library that enables users to develop their own customized evaluation data and
pipelines for OOKB robustness. knowornot comprises four main features. Firstly,
it provides a unified, high-level API that streamlines the process of setting
up and running robustness benchmarks. Secondly, its modular architecture
emphasizes extensibility and flexibility, allowing users to easily integrate
their own LLM clients and RAG settings. Thirdly, its rigorous data modeling
design ensures experiment reproducibility, reliability and traceability.
Lastly, it implements a comprehensive suite of tools for users to customize
their pipelines. We demonstrate the utility of knowornot by developing a
challenging benchmark, PolicyBench, which spans four Question-Answer (QA)
chatbots on government policies, and analyze its OOKB robustness. The source
code of knowornot is available
https://github.com/govtech-responsibleai/KnowOrNot.

</details>


### [420] [JIR-Arena: The First Benchmark Dataset for Just-in-time Information Recommendation](https://arxiv.org/abs/2505.13550)
*Ke Yang,Kevin Ros,Shankar Kumar Senthil Kumar,ChengXiang Zhai*

Main category: cs.IR

TL;DR: 该论文首次定义了即时信息推荐（JIR）任务并建立了评估框架，提出了JIR-Arena基准数据集和基线系统，发现基于基础模型的JIR系统在模拟用户需求时存在召回率和内容检索效率的挑战。


<details>
  <summary>Details</summary>
Motivation: 尽管即时信息推荐（JIR）服务在提升决策效率和日常生活便利性方面具有潜力，但目前缺乏对JIR任务的系统化定义和评估框架。论文旨在填补这一空白。

Method: 论文提出了JIR任务的数学定义和评估指标，并引入了多模态基准数据集JIR-Arena。该数据集通过结合人类和AI模型的输入来近似信息需求分布，并采用多轮多实体验证框架提高客观性。

Result: 基于JIR-Arena的评估显示，虽然基于基础模型的JIR系统能够以合理精度模拟用户需求，但在召回率和有效内容检索方面仍面临挑战。

Conclusion: 论文为JIR研究提供了首个系统化定义和评估框架，并开源了代码和数据，为未来研究奠定了基础。

Abstract: Just-in-time Information Recommendation (JIR) is a service designed to
deliver the most relevant information precisely when users need it, ,
addressing their knowledge gaps with minimal effort and boosting
decision-making and efficiency in daily life. Advances in device-efficient
deployment of foundation models and the growing use of intelligent wearable
devices have made always-on JIR assistants feasible. However, there has been no
systematic effort to formally define JIR tasks or establish evaluation
frameworks. To bridge this gap, we present the first mathematical definition of
JIR tasks and associated evaluation metrics. Additionally, we introduce
JIR-Arena, a multimodal benchmark dataset featuring diverse,
information-request-intensive scenarios to evaluate JIR systems across critical
dimensions: i) accurately inferring user information needs, ii) delivering
timely and relevant recommendations, and iii) avoiding irrelevant content that
may distract users.
  Developing a JIR benchmark dataset poses challenges due to subjectivity in
estimating user information needs and uncontrollable system variables affecting
reproducibility. To address these, JIR-Arena: i) combines input from multiple
humans and large AI models to approximate information need distributions; ii)
assesses JIR quality through information retrieval outcomes using static
knowledge base snapshots; and iii) employs a multi-turn, multi-entity
validation framework to improve objectivity and generality. Furthermore, we
implement a baseline JIR system capable of processing real-time information
streams aligned with user inputs. Our evaluation of this baseline system on
JIR-Arena indicates that while foundation model-based JIR systems simulate user
needs with reasonable precision, they face challenges in recall and effective
content retrieval. To support future research in this new area, we fully
release our code and data.

</details>


### [421] [AMAQA: A Metadata-based QA Dataset for RAG Systems](https://arxiv.org/abs/2505.13557)
*Davide Bruni,Marco Avvenuti,Nicola Tonellotto,Maurizio Tesconi*

Main category: cs.IR

TL;DR: AMAQA是一个新的开放访问QA数据集，用于评估结合文本和元数据的任务，特别是在需要快速分析大量数据的领域，如网络安全和情报。


<details>
  <summary>Details</summary>
Motivation: 当前基准测试缺乏元数据集成，阻碍了在需要文本数据和外部信息的场景中的评估。

Method: AMAQA包括约110万条来自26个公共Telegram群的英文消息，并丰富了元数据，如时间戳、主题、情感语气和毒性指标，以及450个高质量的QA对。

Result: 利用元数据将准确率从0.12提高到0.61，通过迭代上下文和丰富噪声文档，进一步提高了3分，比简单元数据过滤提高了14分。

Conclusion: AMAQA是第一个集成元数据和标签的单跳QA基准，为未来研究设立了新标准。

Abstract: Retrieval-augmented generation (RAG) systems are widely used in
question-answering (QA) tasks, but current benchmarks lack metadata
integration, hindering evaluation in scenarios requiring both textual data and
external information. To address this, we present AMAQA, a new open-access QA
dataset designed to evaluate tasks combining text and metadata. The integration
of metadata is especially important in fields that require rapid analysis of
large volumes of data, such as cybersecurity and intelligence, where timely
access to relevant information is critical. AMAQA includes about 1.1 million
English messages collected from 26 public Telegram groups, enriched with
metadata such as timestamps, topics, emotional tones, and toxicity indicators,
which enable precise and contextualized queries by filtering documents based on
specific criteria. It also includes 450 high-quality QA pairs, making it a
valuable resource for advancing research on metadata-driven QA and RAG systems.
To the best of our knowledge, AMAQA is the first single-hop QA benchmark to
incorporate metadata and labels such as topics covered in the messages. We
conduct extensive tests on the benchmark, establishing a new standard for
future research. We show that leveraging metadata boosts accuracy from 0.12 to
0.61, highlighting the value of structured context. Building on this, we
explore several strategies to refine the LLM input by iterating over provided
context and enriching it with noisy documents, achieving a further 3-point gain
over the best baseline and a 14-point improvement over simple metadata
filtering. The dataset is available at
https://anonymous.4open.science/r/AMAQA-5D0D/

</details>


### [422] [MedEIR: A Specialized Medical Embedding Model for Enhanced Information Retrieval](https://arxiv.org/abs/2505.13482)
*Anand Selvadurai,Jasheen Shaik,Girish Chandrasekar,ShriRadhaKrishnan Balamurugan,Eswara Reddy*

Main category: cs.IR

TL;DR: 提出MedEIR模型，联合优化医学和通用NLP任务，在多个基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有嵌入模型在医学文档语义理解、长文本处理等方面存在局限，需更通用的解决方案。

Method: 联合优化嵌入模型和分词器，采用ALiBi技术支持长文本（8192词），预训练60亿词后微调300万句对。

Result: 在MTEB基准测试中全面超越Jina V2和MiniLM，医学和通用任务均取得最高分。

Conclusion: MedEIR展示了跨领域的高效嵌入能力，为当前模型局限提供了有效解决方案。

Abstract: Embedding models have become essential for retrieval-augmented generation
(RAG) tasks, semantic clustering, and text re-ranking. But despite their
growing use, many of these come with notable limitations. For example, Jina
fails to capture the semantic content of medical documents, while models such
as MiniLM often perform poorly on long-form documents. Domain-adapted models,
while specialized, often underperform in general-purpose tasks, reducing their
overall applicability. General-domain tokenizers often misinterpret medical
vocabulary. The limitations of current embedding models, whether in
tokenization accuracy, domain comprehension, or handling long sequences,
highlight the need for more versatile solutions. In this work, we present
MedEIR, a novel embedding model and tokenizer jointly optimized for both
medical and general NLP tasks, incorporating ALiBi-based long-context
processing to support sequences of up to 8,192 tokens. MedEIR was pre-trained
on only 6 billion tokens, significantly fewer than Jina's, followed by
fine-tuning on 3 million sentence pairs. MedEIR consistently outperforms Jina
V2 and MiniLM across MTEB benchmarks, achieving top scores on ArguAna (55.24),
NFCorpus (38.44), MedicalQARetrieval (74.25), SciFact (72.04), and TRECCOVID
(79.56). These results highlight the potential of MedEIR as a highly effective
embedding model, demonstrating strong performance across both general-purpose
and domain-specific tasks and outperforming existing models on multiple
benchmarks.

</details>


### [423] [RAR: Setting Knowledge Tripwires for Retrieval Augmented Rejection](https://arxiv.org/abs/2505.13581)
*Tommaso Mario Buonocore,Enea Parimbelli*

Main category: cs.IR

TL;DR: 论文提出了一种名为RAR的新方法，通过检索增强生成架构动态拒绝不安全用户查询，无需重新训练模型。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的内容审核面临挑战，需要灵活、适应性强的解决方案以快速应对新威胁。

Method: 利用RAG架构，通过在向量数据库中插入并标记恶意文档，系统能在检索到这些文档时识别并拒绝有害请求。

Result: 初步结果显示，RAR性能与Claude 3.5 Sonnet等LLM的内置审核相当，同时提供更优的灵活性和实时定制能力。

Conclusion: RAR方法无需改变现有RAG系统架构，仅需添加特制文档和基于检索结果的简单拒绝机制，即可有效应对关键漏洞。

Abstract: Content moderation for large language models (LLMs) remains a significant
challenge, requiring flexible and adaptable solutions that can quickly respond
to emerging threats. This paper introduces Retrieval Augmented Rejection (RAR),
a novel approach that leverages a retrieval-augmented generation (RAG)
architecture to dynamically reject unsafe user queries without model
retraining. By strategically inserting and marking malicious documents into the
vector database, the system can identify and reject harmful requests when these
documents are retrieved. Our preliminary results show that RAR achieves
comparable performance to embedded moderation in LLMs like Claude 3.5 Sonnet,
while offering superior flexibility and real-time customization capabilities, a
fundamental feature to timely address critical vulnerabilities. This approach
introduces no architectural changes to existing RAG systems, requiring only the
addition of specially crafted documents and a simple rejection mechanism based
on retrieval results.

</details>


### [424] [Boosting LLM-based Relevance Modeling with Distribution-Aware Robust Learning](https://arxiv.org/abs/2412.12504)
*Hong Liu,Saisai Gong,Yixin Ji,Kaixin Wu,Jia Xu,Jinjie Gu*

Main category: cs.IR

TL;DR: 论文提出了一种名为DaRL的新框架，通过设计有效的损失函数和样本增强模块，提升基于LLM的相关性建模在细粒度区分和数据分布变化下的性能，并已应用于支付宝保险产品搜索。


<details>
  <summary>Details</summary>
Motivation: 当前基于大语言模型（LLM）的相关性建模存在两个主要问题：一是难以区分搜索引擎中的细粒度相关性（如强相关、弱相关、无关），二是面对现实场景中的数据分布变化时性能显著下降。

Method: 论文提出了Distribution-Aware Robust Learning（DaRL）框架，包括设计增强细粒度区分能力的损失函数，以及Distribution-Aware Sample Augmentation（DASA）模块，通过OOD检测技术选择训练集中未充分覆盖的样本进行微调，并采用多阶段微调策略同时提升ID和OOD性能。

Result: DaRL框架在支付宝保险产品搜索中成功部署，提升了相关性建模的判别性和泛化能力。

Conclusion: DaRL框架有效解决了LLM在相关性建模中的细粒度区分和数据分布变化问题，为实际应用提供了可靠解决方案。

Abstract: With the rapid advancement of pre-trained large language models (LLMs),
recent endeavors have leveraged the capabilities of LLMs in relevance modeling,
resulting in enhanced performance. This is usually done through the process of
fine-tuning LLMs on specifically annotated datasets to determine the relevance
between queries and items. However, there are two limitations when LLMs are
naively employed for relevance modeling through fine-tuning and inference.
First, it is not inherently efficient for performing nuanced tasks beyond
simple yes or no answers, such as assessing search relevance. It may therefore
tend to be overconfident and struggle to distinguish fine-grained degrees of
relevance (e.g., strong relevance, weak relevance, irrelevance) used in search
engines. Second, it exhibits significant performance degradation when
confronted with data distribution shift in real-world scenarios. In this paper,
we propose a novel Distribution-Aware Robust Learning framework (DaRL) for
relevance modeling in Alipay Search. Specifically, we design an effective loss
function to enhance the discriminability of LLM-based relevance modeling across
various fine-grained degrees of query-item relevance. To improve the
generalizability of LLM-based relevance modeling, we first propose the
Distribution-Aware Sample Augmentation (DASA) module. This module utilizes
out-of-distribution (OOD) detection techniques to actively select appropriate
samples that are not well covered by the original training set for model
fine-tuning. Furthermore, we adopt a multi-stage fine-tuning strategy to
simultaneously improve in-distribution (ID) and OOD performance, bridging the
performance gap between them. DaRL has been deployed online to serve the
Alipay's insurance product search...

</details>


### [425] [LLM-Based Compact Reranking with Document Features for Scientific Retrieval](https://arxiv.org/abs/2505.13757)
*Runchu Tian,Xueqiang Xu,Bowen Jin,SeongKu Kang,Jiawei Han*

Main category: cs.IR

TL;DR: 论文提出CoRank框架，通过结合紧凑文档表示和全文重排序，提升科学文献检索效果。


<details>
  <summary>Details</summary>
Motivation: 科学文献检索中，传统基于大语言模型的重排序方法面临候选文档数量受限和相关文档排名低的问题。

Method: CoRank框架分三阶段：离线提取文档特征、基于紧凑表示的粗排序、对候选文档全文的细粒度重排序。

Result: 在LitSearch和CSFCube数据集上，nDCG@10指标从32.0提升至39.7，显著优于基线方法。

Conclusion: 实验证明，通过信息提取和混合排序策略可有效提升科学检索性能，且框架兼容不同大语言模型。

Abstract: Scientific retrieval is essential for advancing academic discovery. Within
this process, document reranking plays a critical role by refining first-stage
retrieval results. However, large language model (LLM) listwise reranking faces
unique challenges in the scientific domain. First-stage retrieval is often
suboptimal in the scientific domain, so relevant documents are ranked lower.
Moreover, conventional listwise reranking uses the full text of candidate
documents in the context window, limiting the number of candidates that can be
considered. As a result, many relevant documents are excluded before reranking,
which constrains overall retrieval performance. To address these challenges, we
explore compact document representations based on semantic features such as
categories, sections, and keywords, and propose a training-free, model-agnostic
reranking framework for scientific retrieval called CoRank. The framework
involves three stages: (i) offline extraction of document-level features, (ii)
coarse reranking using these compact representations, and (iii) fine-grained
reranking on full texts of the top candidates from stage (ii). This hybrid
design provides a high-level abstraction of document semantics, expands
candidate coverage, and retains critical details required for precise ranking.
Experiments on LitSearch and CSFCube show that CoRank significantly improves
reranking performance across different LLM backbones, increasing nDCG@10 from
32.0 to 39.7. Overall, these results highlight the value of information
extraction for reranking in scientific retrieval.

</details>


### [426] [Field Matters: A lightweight LLM-enhanced Method for CTR Prediction](https://arxiv.org/abs/2505.14057)
*Yu Cui,Feng Liu,Jiawei Chen,Xingyu Lou,Changwang Zhang,Jun Wang,Yuegang Sun,Xiaohu Yang,Can Wang*

Main category: cs.IR

TL;DR: LLaCTR是一种轻量级的LLM增强CTR预测方法，通过字段级语义知识提升效果和效率。


<details>
  <summary>Details</summary>
Motivation: 现有LLM增强CTR方法需要处理大规模文本描述，导致计算开销大，需更轻量级解决方案。

Method: LLaCTR利用LLM从小规模特征字段中提取轻量级语义知识，增强特征表示和交互。

Result: 在六个CTR模型和四个数据集上验证，LLaCTR在效果和效率上均优于现有方法。

Conclusion: LLaCTR通过字段级增强范式，显著降低了计算开销，同时提升了CTR预测性能。

Abstract: Click-through rate (CTR) prediction is a fundamental task in modern
recommender systems. In recent years, the integration of large language models
(LLMs) has been shown to effectively enhance the performance of traditional CTR
methods. However, existing LLM-enhanced methods often require extensive
processing of detailed textual descriptions for large-scale instances or
user/item entities, leading to substantial computational overhead. To address
this challenge, this work introduces LLaCTR, a novel and lightweight
LLM-enhanced CTR method that employs a field-level enhancement paradigm.
Specifically, LLaCTR first utilizes LLMs to distill crucial and lightweight
semantic knowledge from small-scale feature fields through self-supervised
field-feature fine-tuning. Subsequently, it leverages this field-level semantic
knowledge to enhance both feature representation and feature interactions. In
our experiments, we integrate LLaCTR with six representative CTR models across
four datasets, demonstrating its superior performance in terms of both
effectiveness and efficiency compared to existing LLM-enhanced methods. Our
code is available at https://anonymous.4open.science/r/LLaCTR-EC46.

</details>


### [427] [Rank-K: Test-Time Reasoning for Listwise Reranking](https://arxiv.org/abs/2505.14432)
*Eugene Yang,Andrew Yates,Kathryn Ricci,Orion Weller,Vivek Chari,Benjamin Van Durme,Dawn Lawrie*

Main category: cs.IR

TL;DR: Rank-K是一种新型的列表式段落重排模型，利用大型语言模型的推理能力提升检索效率，在多语言检索中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有的神经重排模型虽然效果显著，但计算资源消耗大，即使经过优化也难以满足实际需求。因此，需要一种更高效的重排方法。

Method: Rank-K是一种列表式段落重排模型，利用大型语言模型的推理能力，在查询时提供可扩展性，适用于复杂查询。

Result: Rank-K在BM25初始排名列表上的检索效果比当前最先进的列表式重排模型RankZephyr提高了23%，在SPLADE-v3的强检索结果上提高了19%。

Conclusion: Rank-K不仅显著提升了检索效果，还在多语言检索中表现出色，证明了其广泛适用性。

Abstract: Retrieve-and-rerank is a popular retrieval pipeline because of its ability to
make slow but effective rerankers efficient enough at query time by reducing
the number of comparisons. Recent works in neural rerankers take advantage of
large language models for their capability in reasoning between queries and
passages and have achieved state-of-the-art retrieval effectiveness. However,
such rerankers are resource-intensive, even after heavy optimization. In this
work, we introduce Rank-K, a listwise passage reranking model that leverages
the reasoning capability of the reasoning language model at query time that
provides test time scalability to serve hard queries. We show that Rank-K
improves retrieval effectiveness by 23\% over the RankZephyr, the
state-of-the-art listwise reranker, when reranking a BM25 initial ranked list
and 19\% when reranking strong retrieval results by SPLADE-v3. Since Rank-K is
inherently a multilingual model, we found that it ranks passages based on
queries in different languages as effectively as it does in monolingual
retrieval.

</details>


### [428] [TranSUN: A Preemptive Paradigm to Eradicate Retransformation Bias Intrinsically from Regression Models in Recommender Systems](https://arxiv.org/abs/2505.13881)
*Jiahao Yu,Haozhuang Liu,Yeqiu Yang,Lu Chen,Wu Jian,Yuning Jiang,Bo Zheng*

Main category: cs.IR

TL;DR: 该论文提出了一种名为TranSUN的新方法，通过模型内部微调解决推荐系统中长期被忽视的回归模型重转换偏差问题，并进一步推广为通用回归模型家族GTS，在实际工业推荐场景中验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 推荐系统中的回归模型存在重转换偏差问题，现有方法多为事后修正且难以实际应用，因此需要一种能从模型内部根除偏差的解决方案。

Method: 提出TranSUN方法，采用联合偏差学习方式实现理论保证的无偏性；进一步扩展为通用回归模型家族GTS，提供灵活开发无偏模型的框架。

Result: 跨领域数据实验证明方法优越性，已成功应用于淘宝App首页'猜你喜欢'业务场景的商品和短视频推荐，服务主要线上流量。

Conclusion: 通过模型内部革新而非外部修正的预发式范式，从根本上解决了推荐系统的回归偏差问题，GTS框架为未来无偏模型开发提供了通用理论基础。

Abstract: Regression models are crucial in recommender systems. However,
retransformation bias problem has been conspicuously neglected within the
community. While many works in other fields have devised effective bias
correction methods, all of them are post-hoc cures externally to the model,
facing practical challenges when applied to real-world recommender systems.
Hence, we propose a preemptive paradigm to eradicate the bias intrinsically
from the models via minor model refinement. Specifically, a novel TranSUN
method is proposed with a joint bias learning manner to offer theoretically
guaranteed unbiasedness under empirical superior convergence. It is further
generalized into a novel generic regression model family, termed Generalized
TranSUN (GTS), which not only offers more theoretical insights but also serves
as a generic framework for flexibly developing various bias-free models.
Comprehensive experimental results demonstrate the superiority of our methods
across data from various domains, which have been successfully deployed in two
real-world industrial recommendation scenarios, i.e. product and short video
recommendation scenarios in Guess What You Like business domain in the homepage
of Taobao App (a leading e-commerce platform), to serve the major online
traffic. Codes will be released after this paper is published.

</details>


### [429] [NExT-Search: Rebuilding User Feedback Ecosystem for Generative AI Search](https://arxiv.org/abs/2505.14680)
*Sunhao Dai,Wenjie Wang,Liang Pang,Jun Xu,See-Kiong Ng,Ji-Rong Wen,Tat-Seng Chua*

Main category: cs.IR

TL;DR: 生成式AI搜索通过端到端回答复杂查询改变信息检索方式，但削弱了传统搜索基于用户反馈的改进循环。论文提出NExT-Search框架，通过两种模式重新引入细粒度反馈，实现搜索系统的持续优化。


<details>
  <summary>Details</summary>
Motivation: 传统网络搜索依赖文档级细粒度用户反馈（如点击、停留时间）持续优化排序模型，而生成式AI搜索仅能获得最终答案的粗粒度反馈，导致无法有效改进中间环节。这种反馈循环的断裂阻碍了系统演进。

Method: 提出NExT-Search框架：1）用户调试模式（允许干预关键阶段）；2）影子用户模式（AI代理模拟用户偏好）。通过在线实时调整和离线模型更新，将反馈信号作用于查询分解、检索和生成环节。

Result: 该框架通过双模式反馈机制，在保持生成式AI搜索便利性的同时，重建了细粒度反馈循环，使系统各环节可针对性优化。

Conclusion: NExT-Search通过重新赋予用户对关键环节的控制权，为构建可持续进化的AI搜索系统提供了可行路径，实现了人类反馈与机器学习的协同演进。

Abstract: Generative AI search is reshaping information retrieval by offering
end-to-end answers to complex queries, reducing users' reliance on manually
browsing and summarizing multiple web pages. However, while this paradigm
enhances convenience, it disrupts the feedback-driven improvement loop that has
historically powered the evolution of traditional Web search. Web search can
continuously improve their ranking models by collecting large-scale,
fine-grained user feedback (e.g., clicks, dwell time) at the document level. In
contrast, generative AI search operates through a much longer search pipeline,
spanning query decomposition, document retrieval, and answer generation, yet
typically receives only coarse-grained feedback on the final answer. This
introduces a feedback loop disconnect, where user feedback for the final output
cannot be effectively mapped back to specific system components, making it
difficult to improve each intermediate stage and sustain the feedback loop. In
this paper, we envision NExT-Search, a next-generation paradigm designed to
reintroduce fine-grained, process-level feedback into generative AI search.
NExT-Search integrates two complementary modes: User Debug Mode, which allows
engaged users to intervene at key stages; and Shadow User Mode, where a
personalized user agent simulates user preferences and provides AI-assisted
feedback for less interactive users. Furthermore, we envision how these
feedback signals can be leveraged through online adaptation, which refines
current search outputs in real-time, and offline update, which aggregates
interaction logs to periodically fine-tune query decomposition, retrieval, and
generation models. By restoring human control over key stages of the generative
AI search pipeline, we believe NExT-Search offers a promising direction for
building feedback-rich AI search systems that can evolve continuously alongside
human feedback.

</details>


### [430] [Taming Recommendation Bias with Causal Intervention on Evolving Personal Popularity](https://arxiv.org/abs/2505.14310)
*Shiyin Tan,Dongyuan Li,Renhe Jiang,Zhen Wang,Xingtong Yu,Manabu Okumura*

Main category: cs.IR

TL;DR: 论文提出CausalEPP方法，通过量化用户对热门项目的偏好并考虑其时间演化，有效减少推荐系统中的流行度偏差。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常对所有用户统一处理流行度偏差，且未充分考虑用户或项目的时间演化。用户对热门项目的偏好不同且随时间变化，需针对性解决。

Method: 引入'演化个人流行度'指标量化用户偏好，设计因果图整合该指标，并应用去混淆训练减少偏差。推理时考虑用户与项目的演化一致性。

Result: 实验表明，CausalEPP在降低流行度偏差的同时，提高了推荐准确性，优于基线方法。

Conclusion: CausalEPP通过动态建模用户偏好演化，显著改善了推荐系统的公平性和准确性。

Abstract: Popularity bias occurs when popular items are recommended far more frequently
than they should be, negatively impacting both user experience and
recommendation accuracy. Existing debiasing methods mitigate popularity bias
often uniformly across all users and only partially consider the time evolution
of users or items. However, users have different levels of preference for item
popularity, and this preference is evolving over time. To address these issues,
we propose a novel method called CausalEPP (Causal Intervention on Evolving
Personal Popularity) for taming recommendation bias, which accounts for the
evolving personal popularity of users. Specifically, we first introduce a
metric called {Evolving Personal Popularity} to quantify each user's preference
for popular items. Then, we design a causal graph that integrates evolving
personal popularity into the conformity effect, and apply deconfounded training
to mitigate the popularity bias of the causal graph. During inference, we
consider the evolution consistency between users and items to achieve a better
recommendation. Empirical studies demonstrate that CausalEPP outperforms
baseline methods in reducing popularity bias while improving recommendation
accuracy.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [431] [Advancing Software Quality: A Standards-Focused Review of LLM-Based Assurance Techniques](https://arxiv.org/abs/2505.13766)
*Avinash Patil*

Main category: cs.SE

TL;DR: 本文探讨了如何利用大语言模型（LLMs）增强软件质量保证（SQA）流程，同时确保符合ISO/IEC等国际标准，并分析了实际应用中的挑战与未来方向。


<details>
  <summary>Details</summary>
Motivation: 软件质量保证（SQA）对交付可靠、安全、高效的软件产品至关重要。随着大语言模型（LLMs）的发展，为自动化SQA任务（如需求分析、代码审查等）提供了新机遇，但需确保其符合现有质量标准。

Method: 论文首先回顾了软件质量标准和LLMs的技术基础，然后探讨了LLM在SQA中的应用（如需求验证、缺陷检测等），并将其映射到关键质量框架中，结合案例和开源项目验证可行性。

Result: 研究表明，LLMs能够有效支持传统SQA流程，但需解决数据隐私、模型偏差等挑战。实证案例展示了LLMs在实际应用中的潜力。

Conclusion: 未来应关注自适应学习、隐私保护部署和多模态分析，同时推动AI驱动的软件质量标准演进，以实现更高效的SQA流程。

Abstract: Software Quality Assurance (SQA) is critical for delivering reliable, secure,
and efficient software products. The Software Quality Assurance Process aims to
provide assurance that work products and processes comply with predefined
provisions and plans. Recent advancements in Large Language Models (LLMs)
present new opportunities to enhance existing SQA processes by automating tasks
like requirement analysis, code review, test generation, and compliance checks.
Simultaneously, established standards such as ISO/IEC 12207, ISO/IEC 25010,
ISO/IEC 5055, ISO 9001/ISO/IEC 90003, CMMI, and TMM provide structured
frameworks for ensuring robust quality practices. This paper surveys the
intersection of LLM-based SQA methods and these recognized standards,
highlighting how AI-driven solutions can augment traditional approaches while
maintaining compliance and process maturity. We first review the foundational
software quality standards and the technical fundamentals of LLMs in software
engineering. Next, we explore various LLM-based SQA applications, including
requirement validation, defect detection, test generation, and documentation
maintenance. We then map these applications to key software quality frameworks,
illustrating how LLMs can address specific requirements and metrics within each
standard. Empirical case studies and open-source initiatives demonstrate the
practical viability of these methods. At the same time, discussions on
challenges (e.g., data privacy, model bias, explainability) underscore the need
for deliberate governance and auditing. Finally, we propose future directions
encompassing adaptive learning, privacy-focused deployments, multimodal
analysis, and evolving standards for AI-driven software quality.

</details>


### [432] [Guided Search Strategies in Non-Serializable Environments with Applications to Software Engineering Agents](https://arxiv.org/abs/2505.13652)
*Karina Zainullina,Alexander Golubev,Maria Trofimova,Sergei Polezhaev,Ibragim Badertdinov,Daria Litvintseva,Simon Karasik,Filipp Fisin,Sergei Skvortsov,Maksim Nekrashevich,Anton Shevtsov,Boris Yangel*

Main category: cs.SE

TL;DR: 论文提出两种搜索策略提升大语言模型在不可序列化环境中的表现，在SWE-bench基准上实现开源模型新SOTA。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在多步任务中表现不稳定，传统搜索方法不适用于不可序列化环境（如Docker容器），需要开发新的搜索策略。

Method: 采用1-step前瞻和轨迹选择两种搜索策略，通过学习的动作价值函数估计器进行引导。

Result: 在SWE-bench基准上，Qwen-72B模型成功率翻倍至40.8%；方法同样适用于GPT-4o等闭源模型。

Conclusion: 提出的搜索策略有效提升了LLMs在不可序列化环境中的性能，且具有跨模型迁移能力。

Abstract: Large language models (LLMs) have recently achieved remarkable results in
complex multi-step tasks, such as mathematical reasoning and agentic software
engineering. However, they often struggle to maintain consistent performance
across multiple solution attempts. One effective approach to narrow the gap
between average-case and best-case performance is guided test-time search,
which explores multiple solution paths to identify the most promising one.
Unfortunately, effective search techniques (e.g. MCTS) are often unsuitable for
non-serializable RL environments, such as Docker containers, where intermediate
environment states cannot be easily saved and restored. We investigate two
complementary search strategies applicable to such environments: 1-step
lookahead and trajectory selection, both guided by a learned action-value
function estimator. On the SWE-bench Verified benchmark, a key testbed for
agentic software engineering, we find these methods to double the average
success rate of a fine-tuned Qwen-72B model, achieving 40.8%, the new
state-of-the-art for open-weights models. Additionally, we show that these
techniques are transferable to more advanced closed models, yielding similar
improvements with GPT-4o.

</details>


### [433] [Selective Code Generation for Functional Guarantees](https://arxiv.org/abs/2505.13553)
*Jaewoo Jeong,Taesoo Kim,Sangdon Park*

Main category: cs.SE

TL;DR: 该论文提出了一种通过动态代码分析工具自动生成单元测试的方法，以解决代码生成模型的幻觉问题，并引入选择性代码生成器来控制幻觉率。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）及其代码生成模型在复杂任务中表现出色，但幻觉问题阻碍了其在需要高安全标准的系统中的应用。代码的复杂性使得识别生成代码的功能正确性变得困难，而传统单元测试方法扩展成本高昂。

Method: 论文提出使用动态代码分析工具自动生成单元测试，并设计选择性代码生成器，通过控制非弃权答案中的幻觉率来保证代码生成的可信度。此外，还提出了FuzzEval评估范式，将生成的单元测试用于学习和评估。

Result: 实验表明，选择性代码生成器在开放和封闭代码生成器中均表现出色，能够有效控制代码幻觉率，并保持合理的选择效率。

Conclusion: 通过自动生成单元测试和选择性代码生成器，论文提供了一种可信赖的代码生成方法，解决了代码幻觉问题，并展示了其在评估和学习中的有效性。

Abstract: Large language models (LLMs) show human-level performance and their
specialized descendants, code generation models, play core roles in solving
complex tasks, including mathematical reasoning and software development. On
the downside, the hallucination of LLMs mainly hinders their applicability to
systems requiring higher safety standards, thus drawing the attention of the AI
community. However, the hallucination of code generation models is rarely
considered. One critical bottleneck in considering code hallucination is the
intricate property of code to identify whether generated code has the intended
functionality due to its un-natural form, different to natural languages.
Handful of unit tests have been considered to address this issue, but
scaling-up its size is extremely expensive. We address this core bottleneck by
automatically generating unit tests using dynamic code analysis tools, which
leverages the \emph{executable nature} of code. Given generated unit tests from
true code for measuring functional correctness of generated code, we propose to
learn a \emph{selective code generator}, which abstains from answering for
unsure generation, to control the rate of code hallucination among
non-abstaining answers in terms of a false discovery rate. This learning
algorithm provides a controllability guarantee, providing trustworthiness of
code generation. Finally, we propose to use generated unit tests in evaluation
as well as in learning for precise code evaluation, calling this evaluation
paradigm \emph{FuzzEval}. We demonstrate the efficacy of our selective code
generator over open and closed code generators, showing clear benefit of
leveraging generated unit tests along with the controllability of code
hallucination and reasonable selection efficiency via our selective code
generator.

</details>


### [434] [HarmonE: A Self-Adaptive Approach to Architecting Sustainable MLOps](https://arxiv.org/abs/2505.13693)
*Hiya Bhatt,Shaunak Biswas,Srinivasan Rakhunathan,Karthik Vaidhyanathan*

Main category: cs.SE

TL;DR: 论文提出HarmonE架构，通过MAPE-K循环增强MLOps管道的自适应能力，以应对机器学习系统在动态环境中的可持续性挑战，并在智能交通系统案例中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 机器学习系统在动态环境中运行时面临数据漂移和模型退化等不确定性，影响技术、经济、环境和社会多维度可持续性。现有MLOps方法仅关注技术维度，且传统做法（如频繁重训练）带来高能耗问题。

Method: 提出HarmonE架构，允许设计时定义可持续性目标和适应阈值，运行时监测关键指标（如预测精度、能耗、数据分布变化），并触发自适应策略。采用数字孪生技术验证，以交通流量预测为用例。

Result: 实验表明，HarmonE能在动态条件下有效适应，保持预测精度并满足可持续性目标。

Conclusion: HarmonE通过自适应机制解决了ML系统多维度可持续性问题，为实际应用提供了兼顾性能与可持续性的解决方案。

Abstract: Machine Learning Enabled Systems (MLS) are becoming integral to real-world
applications, but ensuring their sustainable performance over time remains a
significant challenge. These systems operate in dynamic environments and face
runtime uncertainties like data drift and model degradation, which affect the
sustainability of MLS across multiple dimensions: technical, economical,
environmental, and social. While Machine Learning Operations (MLOps) addresses
the technical dimension by streamlining the ML model lifecycle, it overlooks
other dimensions. Furthermore, some traditional practices, such as frequent
retraining, incur substantial energy and computational overhead, thus
amplifying sustainability concerns. To address them, we introduce HarmonE, an
architectural approach that enables self-adaptive capabilities in MLOps
pipelines using the MAPE-K loop. HarmonE allows system architects to define
explicit sustainability goals and adaptation thresholds at design time, and
performs runtime monitoring of key metrics, such as prediction accuracy, energy
consumption, and data distribution shifts, to trigger appropriate adaptation
strategies. We validate our approach using a Digital Twin (DT) of an
Intelligent Transportation System (ITS), focusing on traffic flow prediction as
our primary use case. The DT employs time series ML models to simulate
real-time traffic and assess various flow scenarios. Our results show that
HarmonE adapts effectively to evolving conditions while maintaining accuracy
and meeting sustainability goals.

</details>


<div id='cond-mat.soft'></div>

# cond-mat.soft [[Back]](#toc)

### [435] [Polymer Data Challenges in the AI Era: Bridging Gaps for Next-Generation Energy Materials](https://arxiv.org/abs/2505.13494)
*Ying Zhao,Guanhua Chen,Jie Liu*

Main category: cond-mat.soft

TL;DR: 聚合物科学因数据碎片化阻碍能源技术发展，需通过技术创新和协作解决。


<details>
  <summary>Details</summary>
Motivation: 聚合物科学在能源技术（如光伏、固态电池、氢存储）中的应用受限于数据碎片化、缺乏互操作性数据库及不可复现的测试协议，阻碍了材料发现和机器学习应用。

Method: 采用自然语言处理（NLP）工具从文献中提取结构化数据，利用高通量机器人平台生成自洽数据集，并遵循FAIR原则确保数据可重用性。

Result: 通过技术创新和协作治理，聚合物科学有望克服数据碎片化问题，加速关键材料的发现。

Conclusion: 通过技术革新、开放科学文化和协作治理，聚合物科学可以突破数据瓶颈，推动能源技术发展。

Abstract: The pursuit of advanced polymers for energy technologies, spanning
photovoltaics, solid-state batteries, and hydrogen storage, is hindered by
fragmented data ecosystems that fail to capture the hierarchical complexity of
these materials. Polymer science lacks interoperable databases, forcing
reliance on disconnected literature and legacy records riddled with
unstructured formats and irreproducible testing protocols. This fragmentation
stifles machine learning (ML) applications and delays the discovery of
materials critical for global decarbonization. Three systemic barriers compound
the challenge. First, academic-industrial data silos restrict access to
proprietary industrial datasets, while academic publications often omit
critical synthesis details. Second, inconsistent testing methods undermine
cross-study comparability. Third, incomplete metadata in existing databases
limits their utility for training reliable ML models. Emerging solutions
address these gaps through technological and collaborative innovation. Natural
language processing (NLP) tools extract structured polymer data from decades of
literature, while high-throughput robotic platforms generate self-consistent
datasets via autonomous experimentation. Central to these advances is the
adoption of FAIR (Findable, Accessible, Interoperable, Reusable) principles,
adapted to polymer-specific ontologies, ensuring machine-readability and
reproducibility. Future breakthroughs hinge on cultural shifts toward open
science, accelerated by decentralized data markets and autonomous laboratories
that merge robotic experimentation with real-time ML validation. By addressing
data fragmentation through technological innovation, collaborative governance,
and ethical stewardship, the polymer community can transform bottlenecks into
accelerants.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [436] [PandaGuard: Systematic Evaluation of LLM Safety in the Era of Jailbreaking Attacks](https://arxiv.org/abs/2505.13862)
*Guobin Shen,Dongcheng Zhao,Linghao Feng,Xiang He,Jihang Wang,Sicheng Shen,Haibo Tong,Yiting Dong,Jindong Li,Xiang Zheng,Yi Zeng*

Main category: cs.CR

TL;DR: 论文提出了PandaGuard框架和PandaBench基准，用于系统评估大语言模型的安全性和对抗性攻击防御方法。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型（LLM）安全性评估往往零散，缺乏系统性和可复现性，难以全面评估模型的脆弱性和防御效果。

Method: 通过多智能体系统（攻击者、防御者和评判者）构建PandaGuard框架，集成19种攻击方法、12种防御机制和多种评判策略，支持灵活配置和可复现实验。

Result: 评估了49种LLM，发现没有单一防御方法在所有维度上最优，且评判者的不一致性会显著影响安全性评估结果。

Conclusion: LLM安全性需多维度评估，框架和基准的发布有助于推动透明、可复现的安全研究。

Abstract: Large language models (LLMs) have achieved remarkable capabilities but remain
vulnerable to adversarial prompts known as jailbreaks, which can bypass safety
alignment and elicit harmful outputs. Despite growing efforts in LLM safety
research, existing evaluations are often fragmented, focused on isolated attack
or defense techniques, and lack systematic, reproducible analysis. In this
work, we introduce PandaGuard, a unified and modular framework that models LLM
jailbreak safety as a multi-agent system comprising attackers, defenders, and
judges. Our framework implements 19 attack methods and 12 defense mechanisms,
along with multiple judgment strategies, all within a flexible plugin
architecture supporting diverse LLM interfaces, multiple interaction modes, and
configuration-driven experimentation that enhances reproducibility and
practical deployment. Built on this framework, we develop PandaBench, a
comprehensive benchmark that evaluates the interactions between these
attack/defense methods across 49 LLMs and various judgment approaches,
requiring over 3 billion tokens to execute. Our extensive evaluation reveals
key insights into model vulnerabilities, defense cost-performance trade-offs,
and judge consistency. We find that no single defense is optimal across all
dimensions and that judge disagreement introduces nontrivial variance in safety
assessments. We release the code, configurations, and evaluation results to
support transparent and reproducible research in LLM safety.

</details>


### [437] [Beyond Text: Unveiling Privacy Vulnerabilities in Multi-modal Retrieval-Augmented Generation](https://arxiv.org/abs/2505.13957)
*Jiankun Zhang,Shenglai Zeng,Jie Ren,Tianqi Zheng,Hui Liu,Xianfeng Tang,Hui Liu,Yi Chang*

Main category: cs.CR

TL;DR: 该论文首次系统分析了多模态检索增强生成（MRAG）系统的隐私漏洞，通过黑盒攻击展示了如何提取私有信息，并指出当前技术迫切需要加强隐私保护。


<details>
  <summary>Details</summary>
Motivation: 尽管文本检索增强生成（RAG）的隐私风险已有研究，但多模态数据（如视觉-语言和语音-语言）带来的独特隐私挑战尚未被探索。论文旨在填补这一空白。

Method: 采用新颖的组合式结构化提示攻击方法，在无需了解系统内部细节的黑盒环境下，通过操纵查询来测试MRAG系统的隐私泄露风险。

Result: 实验表明，多模态大模型（LMMs）既能直接生成与检索内容相似的输出，也能通过间接描述暴露敏感信息，证实了MRAG存在严重隐私漏洞。

Conclusion: 研究揭示了当前MRAG系统在隐私保护上的缺陷，强调了开发鲁棒性隐私保护技术的紧迫性。

Abstract: Multimodal Retrieval-Augmented Generation (MRAG) systems enhance LMMs by
integrating external multimodal databases, but introduce unexplored privacy
vulnerabilities. While text-based RAG privacy risks have been studied,
multimodal data presents unique challenges. We provide the first systematic
analysis of MRAG privacy vulnerabilities across vision-language and
speech-language modalities. Using a novel compositional structured prompt
attack in a black-box setting, we demonstrate how attackers can extract private
information by manipulating queries. Our experiments reveal that LMMs can both
directly generate outputs resembling retrieved content and produce descriptions
that indirectly expose sensitive information, highlighting the urgent need for
robust privacy-preserving MRAG techniques.

</details>


### [438] [Is Your Prompt Safe? Investigating Prompt Injection Attacks Against Open-Source LLMs](https://arxiv.org/abs/2505.14368)
*Jiawen Wang,Pritha Gupta,Ivan Habernal,Eyke Hüllermeier*

Main category: cs.CR

TL;DR: 研究发现主流开源大语言模型易受提示注入攻击，提出新评估指标ASP并开发催眠攻击方法，成功突破14种模型的安全防护。


<details>
  <summary>Details</summary>
Motivation: 现有研究对开源和闭源大语言模型的提示攻击脆弱性探索不足，且传统评估指标无法反映攻击可行性中的不确定性。

Method: 在5个基准测试上对14种流行开源LLMs进行提示注入攻击测试，提出ASP指标量化攻击成功率，开发催眠攻击和忽略前缀攻击技术。

Result: 催眠攻击使Stablelm2等对齐模型产生90%的不良行为，忽略前缀攻击对全部14种模型达到60%以上成功率，发现中等知名度模型更脆弱。

Conclusion: 需提升公众对提示注入攻击的认知，并优先开发有效的防御策略，尤其是针对中等知名度语言模型的安全防护。

Abstract: Recent studies demonstrate that Large Language Models (LLMs) are vulnerable
to different prompt-based attacks, generating harmful content or sensitive
information. Both closed-source and open-source LLMs are underinvestigated for
these attacks. This paper studies effective prompt injection attacks against
the $\mathbf{14}$ most popular open-source LLMs on five attack benchmarks.
Current metrics only consider successful attacks, whereas our proposed Attack
Success Probability (ASP) also captures uncertainty in the model's response,
reflecting ambiguity in attack feasibility. By comprehensively analyzing the
effectiveness of prompt injection attacks, we propose a simple and effective
hypnotism attack; results show that this attack causes aligned language models,
including Stablelm2, Mistral, Openchat, and Vicuna, to generate objectionable
behaviors, achieving around $90$% ASP. They also indicate that our ignore
prefix attacks can break all $\mathbf{14}$ open-source LLMs, achieving over
$60$% ASP on a multi-categorical dataset. We find that moderately well-known
LLMs exhibit higher vulnerability to prompt injection attacks, highlighting the
need to raise public awareness and prioritize efficient mitigation strategies.

</details>


### [439] [CSAGC-IDS: A Dual-Module Deep Learning Network Intrusion Detection Model for Complex and Imbalanced Data](https://arxiv.org/abs/2505.14027)
*Yifan Zeng*

Main category: cs.CR

TL;DR: 本文提出了一种基于深度学习的网络入侵检测模型CSAGC-IDS，通过生成高质量数据和改进特征提取方法，有效解决了高维复杂流量数据和不平衡类别问题，并在NSL-KDD数据集上取得了较高的准确率和F1分数。


<details>
  <summary>Details</summary>
Motivation: 随着计算机网络的普及，网络入侵的严重性日益增加，网络入侵检测系统对保障安全至关重要。然而，深度学习模型在处理高维复杂流量模式和不平衡数据类别时面临挑战。

Method: CSAGC-IDS模型结合了SC-CGAN（一种自注意力增强的卷积条件生成对抗网络）来生成高质量数据以缓解类别不平衡问题，并采用CSCA-CNN（一种通过成本敏感学习和通道注意力机制增强的卷积神经网络）来提取复杂流量数据的特征以实现精确检测。

Result: 在NSL-KDD数据集上的实验表明，CSAGC-IDS在五分类任务中达到了84.55%的准确率和84.52%的F1分数，在二分类任务中达到了91.09%的准确率和92.04%的F1分数。此外，本文还通过SHAP和LIME对模型的决策机制进行了解释性分析。

Conclusion: CSAGC-IDS模型通过结合生成对抗网络和改进的卷积神经网络，有效提升了网络入侵检测的性能，并在实验中展示了较高的准确率和F1分数，同时提供了模型的解释性分析。

Abstract: As computer networks proliferate, the gravity of network intrusions has
escalated, emphasizing the criticality of network intrusion detection systems
for safeguarding security. While deep learning models have exhibited promising
results in intrusion detection, they face challenges in managing
high-dimensional, complex traffic patterns and imbalanced data categories. This
paper presents CSAGC-IDS, a network intrusion detection model based on deep
learning techniques. CSAGC-IDS integrates SC-CGAN, a self-attention-enhanced
convolutional conditional generative adversarial network that generates
high-quality data to mitigate class imbalance. Furthermore, CSAGC-IDS
integrates CSCA-CNN, a convolutional neural network enhanced through cost
sensitive learning and channel attention mechanism, to extract features from
complex traffic data for precise detection. Experiments conducted on the
NSL-KDD dataset. CSAGC-IDS achieves an accuracy of 84.55% and an F1-score of
84.52% in five-class classification task, and an accuracy of 91.09% and an F1
score of 92.04% in binary classification task.Furthermore, this paper provides
an interpretability analysis of the proposed model, using SHAP and LIME to
explain the decision-making mechanisms of the model.

</details>


### [440] [Traceable Black-box Watermarks for Federated Learning](https://arxiv.org/abs/2505.13651)
*Jiahao Xu,Rui Hu,Olivera Kotevska,Zikai Zhang*

Main category: cs.CR

TL;DR: 该论文提出了一种名为TraMark的服务器端水印方法，用于在联邦学习系统中注入可追踪的黑盒水印，以保护模型知识产权。


<details>
  <summary>Details</summary>
Motivation: 现有的水印方法要么不可追踪，要么只能在白盒设置下追踪，缺乏对可追踪黑盒水印的正式定义和问题描述。

Method: TraMark将模型参数空间划分为主任务区域和水印区域，为每个客户端构建个性化全局模型，并在水印区域学习独特水印。

Result: 实验结果表明，TraMark在保持主任务性能的同时，确保了所有水印模型的可追踪性。

Conclusion: TraMark有效解决了联邦学习系统中模型泄露的可追踪性问题，为知识产权保护提供了新方法。

Abstract: Due to the distributed nature of Federated Learning (FL) systems, each local
client has access to the global model, posing a critical risk of model leakage.
Existing works have explored injecting watermarks into local models to enable
intellectual property protection. However, these methods either focus on
non-traceable watermarks or traceable but white-box watermarks. We identify a
gap in the literature regarding the formal definition of traceable black-box
watermarking and the formulation of the problem of injecting such watermarks
into FL systems. In this work, we first formalize the problem of injecting
traceable black-box watermarks into FL. Based on the problem, we propose a
novel server-side watermarking method, $\mathbf{TraMark}$, which creates a
traceable watermarked model for each client, enabling verification of model
leakage in black-box settings. To achieve this, $\mathbf{TraMark}$ partitions
the model parameter space into two distinct regions: the main task region and
the watermarking region. Subsequently, a personalized global model is
constructed for each client by aggregating only the main task region while
preserving the watermarking region. Each model then learns a unique watermark
exclusively within the watermarking region using a distinct watermark dataset
before being sent back to the local client. Extensive results across various FL
systems demonstrate that $\mathbf{TraMark}$ ensures the traceability of all
watermarked models while preserving their main task performance.

</details>


### [441] [Optimal Client Sampling in Federated Learning with Client-Level Heterogeneous Differential Privacy](https://arxiv.org/abs/2505.13655)
*Jiahao Xu,Rui Hu,Olivera Kotevska*

Main category: cs.CR

TL;DR: 论文提出GDPFed和GDPFed$^+$方法，通过分组客户隐私预算和模型稀疏化，在异构隐私需求下提升联邦学习性能。


<details>
  <summary>Details</summary>
Motivation: 传统DP-FedAvg方法在客户隐私需求异构时，因统一采用最严格隐私级别导致噪声过大、模型性能下降。现有方法多假设服务器可信且缺乏理论支持，难以在实际攻击模型下优化性能。

Method: 提出GDPFed，按隐私预算分组客户并在组内实现客户级DP以减少隐私预算浪费；进一步提出GDPFed$^+$，结合模型稀疏化消除不必要噪声，并优化组内客户采样比例以最小化收敛误差。

Result: 在多个基准数据集上的实验表明，GDPFed$^+$相比现有方法显著提升模型性能，验证了其有效性。

Conclusion: GDPFed$^+$通过分组隐私预算和噪声优化，在诚实但好奇的攻防模型下实现了隐私保护与模型效用的平衡，为异构隐私需求的联邦学习提供了理论支撑和实用方案。

Abstract: Federated Learning with client-level differential privacy (DP) provides a
promising framework for collaboratively training models while rigorously
protecting clients' privacy. However, classic approaches like DP-FedAvg
struggle when clients have heterogeneous privacy requirements, as they must
uniformly enforce the strictest privacy level across clients, leading to
excessive DP noise and significant model utility degradation. Existing methods
to improve the model utility in such heterogeneous privacy settings often
assume a trusted server and are largely heuristic, resulting in suboptimal
performance and lacking strong theoretical underpinnings. In this work, we
address these challenges under a practical attack model where both clients and
the server are honest-but-curious. We propose GDPFed, which partitions clients
into groups based on their privacy budgets and achieves client-level DP within
each group to reduce the privacy budget waste and hence improve the model
utility. Based on the privacy and convergence analysis of GDPFed, we find that
the magnitude of DP noise depends on both model dimensionality and the
per-group client sampling ratios. To further improve the performance of GDPFed,
we introduce GDPFed$^+$, which integrates model sparsification to eliminate
unnecessary noise and optimizes per-group client sampling ratios to minimize
convergence error. Extensive empirical evaluations on multiple benchmark
datasets demonstrate the effectiveness of GDPFed$^+$, showing substantial
performance gains compared with state-of-the-art methods.

</details>


### [442] [AudioJailbreak: Jailbreak Attacks against End-to-End Large Audio-Language Models](https://arxiv.org/abs/2505.14103)
*Guangke Chen,Fu Song,Zhe Zhao,Xiaojun Jia,Yang Liu,Yanchen Qiao,Weizhe Zhang*

Main category: cs.CR

TL;DR: 本文提出了一种新型音频越狱攻击AudioJailbreak，具有异步性、通用性、隐蔽性和空中传播鲁棒性，显著提升了攻击效果和适用场景。


<details>
  <summary>Details</summary>
Motivation: 现有的大型音频语言模型（LALMs）越狱攻击效果不佳，且假设攻击者能完全操控用户提示。本文旨在解决这些问题，提出更有效的攻击方法。

Method: 提出AudioJailbreak攻击方法，通过异步设计、多提示整合、意图隐藏策略和房间脉冲响应模拟，实现高效、隐蔽且鲁棒的音频越狱攻击。

Result: 实验表明，AudioJailbreak在多种LALMs上表现出高效性，且适用于攻击者无法完全操控用户提示的场景。

Conclusion: 本文揭示了音频越狱攻击对LALMs的安全威胁，为提升模型安全性提供了现实依据。

Abstract: Jailbreak attacks to Large audio-language models (LALMs) are studied
recently, but they achieve suboptimal effectiveness, applicability, and
practicability, particularly, assuming that the adversary can fully manipulate
user prompts. In this work, we first conduct an extensive experiment showing
that advanced text jailbreak attacks cannot be easily ported to end-to-end
LALMs via text-to speech (TTS) techniques. We then propose AudioJailbreak, a
novel audio jailbreak attack, featuring (1) asynchrony: the jailbreak audio
does not need to align with user prompts in the time axis by crafting suffixal
jailbreak audios; (2) universality: a single jailbreak perturbation is
effective for different prompts by incorporating multiple prompts into
perturbation generation; (3) stealthiness: the malicious intent of jailbreak
audios will not raise the awareness of victims by proposing various intent
concealment strategies; and (4) over-the-air robustness: the jailbreak audios
remain effective when being played over the air by incorporating the
reverberation distortion effect with room impulse response into the generation
of the perturbations. In contrast, all prior audio jailbreak attacks cannot
offer asynchrony, universality, stealthiness, or over-the-air robustness.
Moreover, AudioJailbreak is also applicable to the adversary who cannot fully
manipulate user prompts, thus has a much broader attack scenario. Extensive
experiments with thus far the most LALMs demonstrate the high effectiveness of
AudioJailbreak. We highlight that our work peeks into the security implications
of audio jailbreak attacks against LALMs, and realistically fosters improving
their security robustness. The implementation and audio samples are available
at our website https://audiojailbreak.github.io/AudioJailbreak.

</details>


### [443] [Exploring Jailbreak Attacks on LLMs through Intent Concealment and Diversion](https://arxiv.org/abs/2505.14316)
*Tiehan Cui,Yanxu Mao,Peipei Liu,Congying Liu,Datao You*

Main category: cs.CR

TL;DR: 该论文针对大语言模型(LLM)的越狱攻击问题，提出了一种高效的黑盒攻击方法ICE和一个全面评估数据集BiSceneEval，以提升模型安全性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型取得了显著进展，但其安全性仍是一个紧迫问题。越狱攻击通过对抗性提示绕过模型保护机制生成有害内容。现有方法存在迭代查询过多和跨模型泛化能力差的问题，且评估数据集主要关注问答场景，忽视了文本生成任务。

Method: 论文提出两种贡献：(1) ICE方法，通过意图隐藏和转移策略高效绕过安全限制；(2) BiSceneEval数据集，用于全面评估LLM在问答和文本生成任务中的鲁棒性。

Result: 实验表明，ICE方法在单次查询下实现了高攻击成功率，显著提升了效率和跨模型迁移能力，同时揭示了现有防御机制的关键漏洞。

Conclusion: 研究结果表明，需要将预定义安全机制与实时语义分解相结合的混合安全策略，以增强LLM的安全性。

Abstract: Although large language models (LLMs) have achieved remarkable advancements,
their security remains a pressing concern. One major threat is jailbreak
attacks, where adversarial prompts bypass model safeguards to generate harmful
or objectionable content. Researchers study jailbreak attacks to understand
security and robustness of LLMs. However, existing jailbreak attack methods
face two main challenges: (1) an excessive number of iterative queries, and (2)
poor generalization across models. In addition, recent jailbreak evaluation
datasets focus primarily on question-answering scenarios, lacking attention to
text generation tasks that require accurate regeneration of toxic content. To
tackle these challenges, we propose two contributions: (1) ICE, a novel
black-box jailbreak method that employs Intent Concealment and divErsion to
effectively circumvent security constraints. ICE achieves high attack success
rates (ASR) with a single query, significantly improving efficiency and
transferability across different models. (2) BiSceneEval, a comprehensive
dataset designed for assessing LLM robustness in question-answering and
text-generation tasks. Experimental results demonstrate that ICE outperforms
existing jailbreak techniques, revealing critical vulnerabilities in current
defense mechanisms. Our findings underscore the necessity of a hybrid security
strategy that integrates predefined security mechanisms with real-time semantic
decomposition to enhance the security of LLMs.

</details>


### [444] [Vulnerability of Transfer-Learned Neural Networks to Data Reconstruction Attacks in Small-Data Regime](https://arxiv.org/abs/2505.14323)
*Tomasz Maciążek,Robert Allison*

Main category: cs.CR

TL;DR: 该论文研究了一种针对迁移学习神经网络的训练数据重构攻击，表明在小数据场景下差分隐私难以有效防御且会严重损害模型精度。


<details>
  <summary>Details</summary>
Motivation: 现有研究证明，在攻击者掌握模型权重及几乎所有训练数据时，差分隐私可有效防御数据重构攻击。但现实中攻击者可能仅知道小规模训练数据的分布，论文旨在探索这种更现实威胁场景下的攻击可行性。

Method: 提出基于重构神经网络的攻击方法，通过逆映射从迁移学习分类器的权重中恢复训练数据。采用Neyman-Pearson准则构建ROC曲线改进评估指标。

Result: 在MNIST、CIFAR-10和CelebA数据集上的实验表明，针对VGG/ResNet/EfficientNet等模型的攻击有效，且DP-SGD防御会显著降低小数据场景下的分类准确率。

Conclusion: 当训练数据保护至关重要时，使用迁移学习分类器存在重大安全隐患，现有差分隐私防御在小数据场景面临效用与隐私的严峻权衡。

Abstract: Training data reconstruction attacks enable adversaries to recover portions
of a released model's training data. We consider the attacks where a
reconstructor neural network learns to invert the (random) mapping between
training data and model weights. Prior work has shown that an informed
adversary with access to released model's weights and all but one training data
point can achieve high-quality reconstructions in this way. However,
differential privacy can defend against such an attack with little to no loss
in model's utility when the amount of training data is sufficiently large. In
this work we consider a more realistic adversary who only knows the
distribution from which a small training dataset has been sampled and who
attacks a transfer-learned neural network classifier that has been trained on
this dataset. We exhibit an attack that works in this realistic threat model
and demonstrate that in the small-data regime it cannot be defended against by
DP-SGD without severely damaging the classifier accuracy. This raises
significant concerns about the use of such transfer-learned classifiers when
protection of training-data is paramount. We demonstrate the effectiveness and
robustness of our attack on VGG, EfficientNet and ResNet image classifiers
transfer-learned on MNIST, CIFAR-10 and CelebA respectively. Additionally, we
point out that the commonly used (true-positive) reconstruction success rate
metric fails to reliably quantify the actual reconstruction effectiveness.
Instead, we make use of the Neyman-Pearson lemma to construct the receiver
operating characteristic curve and consider the associated true-positive
reconstruction rate at a fixed level of the false-positive reconstruction rate.

</details>


### [445] [Can Large Language Models Really Recognize Your Name?](https://arxiv.org/abs/2505.14549)
*Dzung Pham,Peter Kairouz,Niloofar Mireshghallah,Eugene Bagdasarian,Chau Minh Pham,Amir Houmansadr*

Main category: cs.CR

TL;DR: 研究发现大语言模型在识别模糊人名时存在系统性缺陷，导致隐私保护效果下降，提出AMBENCH基准并揭示LLMs在隐私任务中的潜在风险。


<details>
  <summary>Details</summary>
Motivation: 当前基于大语言模型（LLMs）的隐私解决方案假设模型能可靠检测个人身份信息（PII），尤其是命名实体。本文质疑这一假设，揭示LLMs在模糊上下文中的系统性失败。

Method: 提出AMBENCH基准数据集，包含看似模糊的人名，利用名称规律性偏见现象，嵌入简短文本片段和良性提示注入，测试现代LLMs和专用工具的PII检测能力。

Result: 实验显示，现代LLMs对模糊人名的召回率比易识别名称低20-40%；存在良性提示注入时，模糊人名在隐私保护摘要中被忽略的概率增加四倍。

Conclusion: 研究强调仅依赖LLMs保护用户隐私存在未被充分探索的风险，需系统研究其隐私失败模式。

Abstract: Large language models (LLMs) are increasingly being used to protect sensitive
user data. However, current LLM-based privacy solutions assume that these
models can reliably detect personally identifiable information (PII),
particularly named entities. In this paper, we challenge that assumption by
revealing systematic failures in LLM-based privacy tasks. Specifically, we show
that modern LLMs regularly overlook human names even in short text snippets due
to ambiguous contexts, which cause the names to be misinterpreted or
mishandled. We propose AMBENCH, a benchmark dataset of seemingly ambiguous
human names, leveraging the name regularity bias phenomenon, embedded within
concise text snippets along with benign prompt injections. Our experiments on
modern LLMs tasked to detect PII as well as specialized tools show that recall
of ambiguous names drops by 20--40% compared to more recognizable names.
Furthermore, ambiguous human names are four times more likely to be ignored in
supposedly privacy-preserving summaries generated by LLMs when benign prompt
injections are present. These findings highlight the underexplored risks of
relying solely on LLMs to safeguard user privacy and underscore the need for a
more systematic investigation into their privacy failure modes.

</details>


### [446] [Lessons from Defending Gemini Against Indirect Prompt Injections](https://arxiv.org/abs/2505.14534)
*Chongyang Shi,Sharon Lin,Shuang Song,Jamie Hayes,Ilia Shumailov,Itay Yona,Juliette Pluto,Aneesh Pappu,Christopher A. Choquette-Choo,Milad Nasr,Chawin Sitawarin,Gena Gibson,Andreas Terzis,John "Four" Flynn*

Main category: cs.CR

TL;DR: Google DeepMind评估了Gemini模型对抗恶意指令的鲁棒性，通过持续对抗测试提升模型安全性。


<details>
  <summary>Details</summary>
Motivation: Gemini模型在访问用户数据和不可信数据时可能受到恶意指令攻击，导致数据或权限处理异常，需评估其对抗鲁棒性。

Method: 采用对抗性评估框架，部署自适应攻击技术对Gemini的过去、当前和未来版本进行持续测试。

Result: 持续评估帮助Gemini模型增强了对操纵的抵御能力，提升了模型的安全性。

Conclusion: 通过对抗性评估，Gemini模型在面对恶意指令时展现出更强的鲁棒性，未来将持续改进以应对潜在威胁。

Abstract: Gemini is increasingly used to perform tasks on behalf of users, where
function-calling and tool-use capabilities enable the model to access user
data. Some tools, however, require access to untrusted data introducing risk.
Adversaries can embed malicious instructions in untrusted data which cause the
model to deviate from the user's expectations and mishandle their data or
permissions. In this report, we set out Google DeepMind's approach to
evaluating the adversarial robustness of Gemini models and describe the main
lessons learned from the process. We test how Gemini performs against a
sophisticated adversary through an adversarial evaluation framework, which
deploys a suite of adaptive attack techniques to run continuously against past,
current, and future versions of Gemini. We describe how these ongoing
evaluations directly help make Gemini more resilient against manipulation.

</details>


<div id='econ.EM'></div>

# econ.EM [[Back]](#toc)

### [447] [CATS: Clustering-Aggregated and Time Series for Business Customer Purchase Intention Prediction](https://arxiv.org/abs/2505.13558)
*Yingjie Kuang,Tianchen Zhang,Zhen-Wei Huang,Zhongjie Zeng,Zhe-Yuan Li,Ling Huang,Yuefang Gao*

Main category: econ.EM

TL;DR: 该论文提出了一种结合聚类和注意力机制的GRU模型（CAGRU），用于解决客户购买意图预测中的不平衡分布问题，并通过多模态数据提升预测准确性。


<details>
  <summary>Details</summary>
Motivation: 当前研究主要关注客户未来可能购买的产品类型，而忽略了客户是否会重复购买这一关键因素。由于客户群体存在不平衡分布（大量偶尔购买者和少量忠实客户），传统时间序列预测方法在此类问题上表现有限。

Method: 论文提出CAGRU模型，首先通过客户特征进行聚类分析，然后使用GRU神经网络提取时间序列特征，并引入注意力机制捕捉序列位置的重要性。针对不同客户群体分别训练模型以缓解不平衡分布问题。

Result: 通过构建四个数据集并进行大量实验，证明了CAGRU方法在客户购买意图预测上的优越性。

Conclusion: CAGRU模型能够有效捕捉不同客户群体的行为差异，提升预测准确性，为商业策略的成功提供了有力支持。

Abstract: Accurately predicting customers' purchase intentions is critical to the
success of a business strategy. Current researches mainly focus on analyzing
the specific types of products that customers are likely to purchase in the
future, little attention has been paid to the critical factor of whether
customers will engage in repurchase behavior. Predicting whether a customer
will make the next purchase is a classic time series forecasting task. However,
in real-world purchasing behavior, customer groups typically exhibit imbalance
- i.e., there are a large number of occasional buyers and a small number of
loyal customers. This head-to-tail distribution makes traditional time series
forecasting methods face certain limitations when dealing with such problems.
To address the above challenges, this paper proposes a unified Clustering and
Attention mechanism GRU model (CAGRU) that leverages multi-modal data for
customer purchase intention prediction. The framework first performs customer
profiling with respect to the customer characteristics and clusters the
customers to delineate the different customer clusters that contain similar
features. Then, the time series features of different customer clusters are
extracted by GRU neural network and an attention mechanism is introduced to
capture the significance of sequence locations. Furthermore, to mitigate the
head-to-tail distribution of customer segments, we train the model separately
for each customer segment, to adapt and capture more accurately the differences
in behavioral characteristics between different customer segments, as well as
the similar characteristics of the customers within the same customer segment.
We constructed four datasets and conducted extensive experiments to demonstrate
the superiority of the proposed CAGRU approach.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [448] [HALO: Hierarchical Autonomous Logic-Oriented Orchestration for Multi-Agent LLM Systems](https://arxiv.org/abs/2505.13516)
*Zhipeng Hou,Junyi Tang,Yipeng Wang*

Main category: cs.MA

TL;DR: HALO是一个基于分层推理架构的多智能体协作框架，通过任务分解、角色设计和推理执行提升复杂任务处理能力，在多个基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的多智能体系统依赖预定义的角色设计和静态通信结构，限制了其在复杂交互环境中的适应性和灵活性，导致在高度专业化和专家级任务上表现不佳。

Method: HALO采用分层推理架构，包括高层规划智能体（任务分解）、中层角色设计智能体（子任务实例化）和低层推理智能体（子任务执行），并利用蒙特卡洛树搜索优化推理轨迹。

Result: 在代码生成（HumanEval）、通用推理（MMLU）和算术推理（MATH）基准测试中，HALO平均性能提升14.4%，在特定子任务上最高提升19.6%。

Conclusion: HALO通过动态分层协作和自适应提示优化，显著提升了多智能体系统在复杂任务中的性能，尤其在专家级任务上表现突出。

Abstract: Recent advancements in Multi-Agent Systems (MAS) powered by Large Language
Models (LLMs) have demonstrated tremendous potential in diverse task scenarios.
Nonetheless, existing agentic systems typically rely on predefined agent-role
design spaces and static communication structures, limiting their adaptability
as well as flexibility in complex interaction environments and leading to
subpar performance on highly specialized and expert-level tasks. To address
these issues, we introduce HALO, a multi-agent collaboration framework based on
a hierarchical reasoning architecture. Specifically, we incorporate a
high-level planning agent for task decomposition, mid-level role-design agents
for subtask-specific agent instantiation, and low-level inference agents for
subtask execution. Particularly, subtask execution is reformulated as a
structured workflow search problem, where Monte Carlo Tree Search (MCTS)
systematically explores the agentic action space to construct optimal reasoning
trajectories. Additionally, as the majority of users lack expertise in prompt
engineering, we leverage an Adaptive Prompt Refinement module to transform raw
queries into task-specific prompts. Empirical evaluations on Code Generation
(HumanEval), General Reasoning (MMLU), and Arithmetic Reasoning (MATH)
benchmark datasets highlight the effectiveness of HALO, yielding a 14.4%
average improvement over state-of-the-art baselines. Notably, HALO achieves up
to 13.3% performance gain on the Moral Scenarios subject in the MMLU benchmark
and up to 19.6% performance gain on the Algebra subarea in the MATH benchmark,
indicating its advanced proficiency in tackling highly specialized and
expert-level tasks. The code repository is available at
https://github.com/23japhone/HALO.

</details>


### [449] [ACPs: Agent Collaboration Protocols for the Internet of Agents](https://arxiv.org/abs/2505.13523)
*Jun Liu,Ke Yu,Keliang Chen,Ke Li,Yuxinyue Qian,Xiaolian Guo,Haozhe Song,Yinming Li*

Main category: cs.MA

TL;DR: 本文提出了一种名为Agent Collaboration Protocols (ACPs)的综合协议套件，旨在解决异构智能体间的互操作性、可扩展性和协调性问题，为构建安全、开放、可扩展的智能体互联网基础设施奠定基础。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能的快速发展，自主智能体的激增带来了互操作性、可扩展性和协调性方面的挑战。现有的智能体通信协议如MCP、A2A和ANP仍然分散且局限于特定场景，无法满足异构智能体间的无缝协作需求。

Method: 提出了Agent Collaboration Protocols (ACPs)，一个包含注册、发现、交互和工具协议的全面协议套件，支持可信访问、能力编排和工作流构建。

Result: ACPs在协作餐厅预订场景中展示了其有效性，能够实现智能体间的无缝协作和智能任务执行。

Conclusion: ACPs为构建安全、开放、可扩展的智能体互联网基础设施奠定了基础，解决了现有协议的分散性和场景局限性问题。

Abstract: With the rapid advancement of artificial intelligence, the proliferation of
autonomous agents has introduced new challenges in interoperability,
scalability, and coordination. The Internet of Agents (IoA) aims to
interconnect heterogeneous agents through standardized communication protocols,
enabling seamless collaboration and intelligent task execution. However,
existing agent communication protocols such as MCP, A2A, and ANP remain
fragmented and scenario-specific. To address this gap, we propose Agent
Collaboration Protocols (ACPs), a comprehensive protocol suite for the IoA.
ACPs include registration, discovery, interaction, and tooling protocols to
support trustable access, capability orchestration, and workflow construction.
We present the architecture, key technologies, and application workflows of
ACPs, and demonstrate its effectiveness in a collaborative restaurant booking
scenario. ACPs lay the foundation for building a secure, open, and scalable
agent internet infrastructure.

</details>


### [450] [MLZero: A Multi-Agent System for End-to-end Machine Learning Automation](https://arxiv.org/abs/2505.13941)
*Haoyang Fang,Boran Han,Nick Erickson,Xiyuan Zhang,Su Zhou,Anirudh Dagar,Jiani Zhang,Ali Caner Turkmen,Cuixiong Hu,Huzefa Rangwala,Ying Nian Wu,Bernie Wang,George Karypis*

Main category: cs.MA

TL;DR: MLZero是一个基于大型语言模型的多智能体框架，实现了跨模态数据的端到端机器学习自动化，显著减少人工干预，并在多个基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有的AutoML系统在处理多模态数据时仍需大量人工配置和专家输入，MLZero旨在通过LLM驱动的多智能体框架实现更高程度的自动化。

Method: 采用认知感知模块将原始多模态输入转化为感知上下文，并通过语义和情景记忆增强迭代代码生成过程，以解决LLM的幻觉代码和过时API知识问题。

Result: 在MLE-Bench Lite上表现最佳，获得六枚金牌；在多模态AutoML代理基准测试中，成功率提升263.6%，平均排名2.28，即使使用8B LLM也优于现有全尺寸系统。

Conclusion: MLZero通过创新的多智能体框架和记忆增强机制，显著提升了多模态数据下的机器学习自动化水平，且计算效率优异。

Abstract: Existing AutoML systems have advanced the automation of machine learning
(ML); however, they still require substantial manual configuration and expert
input, particularly when handling multimodal data. We introduce MLZero, a novel
multi-agent framework powered by Large Language Models (LLMs) that enables
end-to-end ML automation across diverse data modalities with minimal human
intervention. A cognitive perception module is first employed, transforming raw
multimodal inputs into perceptual context that effectively guides the
subsequent workflow. To address key limitations of LLMs, such as hallucinated
code generation and outdated API knowledge, we enhance the iterative code
generation process with semantic and episodic memory. MLZero demonstrates
superior performance on MLE-Bench Lite, outperforming all competitors in both
success rate and solution quality, securing six gold medals. Additionally, when
evaluated on our Multimodal AutoML Agent Benchmark, which includes 25 more
challenging tasks spanning diverse data modalities, MLZero outperforms the
competing methods by a large margin with a success rate of 0.92 (+263.6\%) and
an average rank of 2.28. Our approach maintains its robust effectiveness even
with a compact 8B LLM, outperforming full-size systems from existing solutions.

</details>


### [451] [Personalized and Resilient Distributed Learning Through Opinion Dynamics](https://arxiv.org/abs/2505.14081)
*Luca Ballotta,Nicola Bastianello,Riccardo M. G. Ferrari,Karl H. Johansson*

Main category: cs.MA

TL;DR: 该论文提出了一种结合分布式梯度下降和意见动态模型的分布式学习算法，旨在解决多智能体网络系统中的个性化和鲁棒性问题。


<details>
  <summary>Details</summary>
Motivation: 多智能体网络系统中，智能体需要学习适合自身数据和任务的本地模型（个性化），同时学习过程需对网络攻击或异常数据具有鲁棒性。这两种需求在概念上具有相似性，因此作者试图设计一种算法同时满足它们。

Method: 作者结合了分布式梯度下降和Friedkin-Johnsen意见动态模型，设计了一种分布式学习算法，并通过调整参数来控制个性化和鲁棒性。

Result: 算法在合成和真实世界的分布式学习任务中表现出色，相比标准策略，在个性化模型和存在恶意智能体的情况下均实现了较高的全局准确性。

Conclusion: 该算法有效平衡了个性化和鲁棒性需求，通过参数调整可灵活控制模型行为，适用于多智能体网络系统的分布式学习场景。

Abstract: In this paper, we address two practical challenges of distributed learning in
multi-agent network systems, namely personalization and resilience.
Personalization is the need of heterogeneous agents to learn local models
tailored to their own data and tasks, while still generalizing well; on the
other hand, the learning process must be resilient to cyberattacks or anomalous
training data to avoid disruption. Motivated by a conceptual affinity between
these two requirements, we devise a distributed learning algorithm that
combines distributed gradient descent and the Friedkin-Johnsen model of opinion
dynamics to fulfill both of them. We quantify its convergence speed and the
neighborhood that contains the final learned models, which can be easily
controlled by tuning the algorithm parameters to enforce a more
personalized/resilient behavior. We numerically showcase the effectiveness of
our algorithm on synthetic and real-world distributed learning tasks, where it
achieves high global accuracy both for personalized models and with malicious
agents compared to standard strategies.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [452] [VocalAgent: Large Language Models for Vocal Health Diagnostics with Safety-Aware Evaluation](https://arxiv.org/abs/2505.13577)
*Yubin Kim,Taehan Kim,Wonjune Kang,Eugene Park,Joonsik Yoon,Dongjae Lee,Xin Liu,Daniel McDuff,Hyeonhoon Lee,Cynthia Breazeal,Hae Won Park*

Main category: cs.SD

TL;DR: VocalAgent是一个基于音频大语言模型的工具，用于语音健康诊断，通过医院患者数据集微调，并在多方面评估中表现出色。


<details>
  <summary>Details</summary>
Motivation: 全球范围内语音障碍普遍存在，但许多人缺乏便捷的诊断和治疗途径，因此需要一种可扩展的解决方案。

Method: 利用Qwen-Audio-Chat模型，基于医院患者的三个数据集进行微调，并进行安全性、跨语言性能和模态消融研究等多方面评估。

Result: VocalAgent在语音障碍分类上表现出优于现有基准的准确性，同时强调了伦理和技术验证的重要性。

Conclusion: VocalAgent为语音健康诊断提供了可扩展的解决方案，并展示了其在准确性和多语言支持方面的潜力。

Abstract: Vocal health plays a crucial role in peoples' lives, significantly impacting
their communicative abilities and interactions. However, despite the global
prevalence of voice disorders, many lack access to convenient diagnosis and
treatment. This paper introduces VocalAgent, an audio large language model
(LLM) to address these challenges through vocal health diagnosis. We leverage
Qwen-Audio-Chat fine-tuned on three datasets collected in-situ from hospital
patients, and present a multifaceted evaluation framework encompassing a safety
assessment to mitigate diagnostic biases, cross-lingual performance analysis,
and modality ablation studies. VocalAgent demonstrates superior accuracy on
voice disorder classification compared to state-of-the-art baselines. Its
LLM-based method offers a scalable solution for broader adoption of health
diagnostics, while underscoring the importance of ethical and technical
validation.

</details>


### [453] [ClapFM-EVC: High-Fidelity and Flexible Emotional Voice Conversion with Dual Control from Natural Language and Speech](https://arxiv.org/abs/2505.13805)
*Yu Pan,Yanni Hu,Yuguang Yang,Jixun Yao,Jianhao Ye,Hongbin Zhou,Lei Ma,Jianjun Zhao*

Main category: cs.SD

TL;DR: ClapFM-EVC提出了一种新型情感语音转换框架，通过自然语言提示或参考语音实现高质量情感语音转换，并支持情感强度调节。


<details>
  <summary>Details</summary>
Motivation: 尽管情感语音转换（EVC）取得了很大进展，但实现高保真、灵活且可解释的控制仍具挑战性。

Method: 提出EVC-CLAP模型进行情感对比语言-音频预训练，结合FuEncoder和自适应强度门融合情感特征，并采用流匹配模型重建梅尔频谱。

Result: 主观和客观评估验证了ClapFM-EVC在情感表达和语音自然度方面的有效性。

Conclusion: ClapFM-EVC框架能够生成高质量的情感语音，为情感语音转换提供了灵活且可解释的解决方案。

Abstract: Despite great advances, achieving high-fidelity emotional voice conversion
(EVC) with flexible and interpretable control remains challenging. This paper
introduces ClapFM-EVC, a novel EVC framework capable of generating high-quality
converted speech driven by natural language prompts or reference speech with
adjustable emotion intensity. We first propose EVC-CLAP, an emotional
contrastive language-audio pre-training model, guided by natural language
prompts and categorical labels, to extract and align fine-grained emotional
elements across speech and text modalities. Then, a FuEncoder with an adaptive
intensity gate is presented to seamless fuse emotional features with Phonetic
PosteriorGrams from a pre-trained ASR model. To further improve emotion
expressiveness and speech naturalness, we propose a flow matching model
conditioned on these captured features to reconstruct Mel-spectrogram of source
speech. Subjective and objective evaluations validate the effectiveness of
ClapFM-EVC.

</details>


### [454] [Forensic deepfake audio detection using segmental speech features](https://arxiv.org/abs/2505.13847)
*Tianle Yang,Chengzhe Sun,Siwei Lyu,Phil Rose*

Main category: cs.SD

TL;DR: 研究探讨利用语音片段声学特征检测深度伪造音频的潜力，发现部分片段特征有效，而全局特征价值有限。


<details>
  <summary>Details</summary>
Motivation: 深度伪造音频检测需要新方法，片段声学特征因其与人类发音过程的紧密关联且难以被伪造模型复制，具有高解释性和检测潜力。

Method: 通过分析语音片段的声学特征（与法医语音比对常用特征一致），评估其在识别深度伪造音频中的有效性。

Result: 某些片段特征能有效检测深度伪造音频，而部分全局特征作用不明显。

Conclusion: 音频深度伪造检测需区别于法医语音比对方法，片段特征为此提供了新视角。

Abstract: This study explores the potential of using acoustic features of segmental
speech sounds to detect deepfake audio. These features are highly interpretable
because of their close relationship with human articulatory processes and are
expected to be more difficult for deepfake models to replicate. The results
demonstrate that certain segmental features commonly used in forensic voice
comparison are effective in identifying deep-fakes, whereas some global
features provide little value. These findings underscore the need to approach
audio deepfake detection differently for forensic voice comparison and offer a
new perspective on leveraging segmental features for this purpose.

</details>


### [455] [The Multimodal Information Based Speech Processing (MISP) 2025 Challenge: Audio-Visual Diarization and Recognition](https://arxiv.org/abs/2505.13971)
*Ming Gao,Shilong Wu,Hang Chen,Jun Du,Chin-Hui Lee,Shinji Watanabe,Jingdong Chen,Siniscalchi Sabato Marco,Odette Scharenborg*

Main category: cs.SD

TL;DR: MISP 2025挑战赛聚焦多模态会议转录，结合视频提升音频处理性能，在说话人日志、语音识别等任务上取得显著进展。


<details>
  <summary>Details</summary>
Motivation: 会议场景因复杂声学条件对语音应用构成挑战，需结合视频模态提升多设备会议转录的准确性。

Method: 通过设立AVSD、AVSR、AVDR三项任务，利用多模态数据集和基线系统，评估参与者提出的解决方案。

Result: 最佳系统性能显著提升：AVSD错误率降低7.43%，AVSR字符错误率降低10.62%，AVDR综合错误率降低72.49%。

Conclusion: 多模态方法有效改善会议转录任务，视频模态的引入带来突破性性能提升。

Abstract: Meetings are a valuable yet challenging scenario for speech applications due
to complex acoustic conditions. This paper summarizes the outcomes of the MISP
2025 Challenge, hosted at Interspeech 2025, which focuses on multi-modal,
multi-device meeting transcription by incorporating video modality alongside
audio. The tasks include Audio-Visual Speaker Diarization (AVSD), Audio-Visual
Speech Recognition (AVSR), and Audio-Visual Diarization and Recognition (AVDR).
We present the challenge's objectives, tasks, dataset, baseline systems, and
solutions proposed by participants. The best-performing systems achieved
significant improvements over the baseline: the top AVSD model achieved a
Diarization Error Rate (DER) of 8.09%, improving by 7.43%; the top AVSR system
achieved a Character Error Rate (CER) of 9.48%, improving by 10.62%; and the
best AVDR system achieved a concatenated minimum-permutation Character Error
Rate (cpCER) of 11.56%, improving by 72.49%.

</details>


### [456] [FMSD-TTS: Few-shot Multi-Speaker Multi-Dialect Text-to-Speech Synthesis for Ü-Tsang, Amdo and Kham Speech Dataset Generation](https://arxiv.org/abs/2505.14351)
*Yutong Liu,Ziyue Zhang,Ban Ma-bao,Yuqing Cai,Yongbin Yu,Renzeng Duojie,Xiangxiang Wang,Fan Gao,Cheng Huang,Nyima Tashi*

Main category: cs.SD

TL;DR: 提出FMSD-TTS框架，解决藏语多方言语音合成资源匮乏问题，通过少量参考音频和方言标签生成并行方言语音，显著提升方言表现力和说话人相似度。


<details>
  <summary>Details</summary>
Motivation: 藏语作为低资源语言，三大方言（卫藏、安多、康巴）的平行语音语料稀缺，限制了语音建模的进展。

Method: 采用说话人-方言融合模块和方言专用动态路由网络（DSDR-Net），捕捉跨方言的细粒度声学与语言变异，同时保持说话人身份。

Result: 主客观评估表明，FMSD-TTS在方言表现力和说话人相似度上显著优于基线，并通过语音到语音方言转换任务验证了合成语音的质量和实用性。

Conclusion: 贡献包括：针对藏语多方言语音合成的少样本TTS系统、公开大规模合成藏语语音语料库、开源评估工具包，用于标准化评估说话人相似度、方言一致性和音频质量。

Abstract: Tibetan is a low-resource language with minimal parallel speech corpora
spanning its three major dialects-\"U-Tsang, Amdo, and Kham-limiting progress
in speech modeling. To address this issue, we propose FMSD-TTS, a few-shot,
multi-speaker, multi-dialect text-to-speech framework that synthesizes parallel
dialectal speech from limited reference audio and explicit dialect labels. Our
method features a novel speaker-dialect fusion module and a Dialect-Specialized
Dynamic Routing Network (DSDR-Net) to capture fine-grained acoustic and
linguistic variations across dialects while preserving speaker identity.
Extensive objective and subjective evaluations demonstrate that FMSD-TTS
significantly outperforms baselines in both dialectal expressiveness and
speaker similarity. We further validate the quality and utility of the
synthesized speech through a challenging speech-to-speech dialect conversion
task. Our contributions include: (1) a novel few-shot TTS system tailored for
Tibetan multi-dialect speech synthesis, (2) the public release of a large-scale
synthetic Tibetan speech corpus generated by FMSD-TTS, and (3) an open-source
evaluation toolkit for standardized assessment of speaker similarity, dialect
consistency, and audio quality.

</details>


### [457] [PersonaTAB: Predicting Personality Traits using Textual, Acoustic, and Behavioral Cues in Fully-Duplex Speech Dialogs](https://arxiv.org/abs/2505.14356)
*Sho Inoue,Shai Wang,Haizhou Li*

Main category: cs.SD

TL;DR: 提出了一种为语音对话数据集添加人格标注的流程，并设计了一个基于大语言模型的系统来预测对话人格，效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前神经语音对话系统在人格感知方面研究不足，主要原因是语音数据缺乏人格标注。

Method: 通过ASR系统提取文本和时间戳，生成对话级标注，并利用大语言模型预测对话人格。

Result: 系统预测结果与人类评估的一致性优于现有方法。

Conclusion: 该流程和系统能有效解决语音数据人格标注缺失问题，提升对话系统的人格感知能力。

Abstract: Despite significant progress in neural spoken dialog systems,
personality-aware conversation agents -- capable of adapting behavior based on
personalities -- remain underexplored due to the absence of personality
annotations in speech datasets. We propose a pipeline that preprocesses raw
audio recordings to create a dialogue dataset annotated with timestamps,
response types, and emotion/sentiment labels. We employ an automatic speech
recognition (ASR) system to extract transcripts and timestamps, then generate
conversation-level annotations. Leveraging these annotations, we design a
system that employs large language models to predict conversational
personality. Human evaluators were engaged to identify conversational
characteristics and assign personality labels. Our analysis demonstrates that
the proposed system achieves stronger alignment with human judgments compared
to existing approaches.

</details>


### [458] [S2SBench: A Benchmark for Quantifying Intelligence Degradation in Speech-to-Speech Large Language Models](https://arxiv.org/abs/2505.14438)
*Yuanbo Fang,Haoze Sun,Jun Liu,Tao Zhang,Zenan Zhou,Weipeng Chen,Xiaofen Xing,Xiangmin Xu*

Main category: cs.SD

TL;DR: 该论文提出了S2SBench基准，用于量化语音大语言模型在音频输入下的性能退化问题，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 语音大语言模型在处理音频输入时，相比文本输入会出现推理和生成性能的下降，即智能退化现象。为了系统评估这一差距，作者提出了S2SBench基准。

Method: S2SBench包含针对句子延续和常识推理的诊断数据集，并引入基于困惑度差异的成对评估协议，以测量相对于文本输入的性能退化。

Result: 通过将S2SBench应用于Baichuan-Audio的训练过程分析，验证了该基准的有效性。

Conclusion: S2SBench能够有效量化语音大语言模型在音频输入下的性能退化，为相关研究提供了实用的评估工具。

Abstract: End-to-end speech large language models ((LLMs)) extend the capabilities of
text-based models to directly process and generate audio tokens. However, this
often leads to a decline in reasoning and generation performance compared to
text input, a phenomenon referred to as intelligence degradation. To
systematically evaluate this gap, we propose S2SBench, a benchmark designed to
quantify performance degradation in Speech LLMs. It includes diagnostic
datasets targeting sentence continuation and commonsense reasoning under audio
input. We further introduce a pairwise evaluation protocol based on perplexity
differences between plausible and implausible samples to measure degradation
relative to text input. We apply S2SBench to analyze the training process of
Baichuan-Audio, which further demonstrates the benchmark's effectiveness. All
datasets and evaluation code are available at
https://github.com/undobug/S2SBench.

</details>


### [459] [PAST: Phonetic-Acoustic Speech Tokenizer](https://arxiv.org/abs/2505.14470)
*Nadav Har-Tuv,Or Tal,Yossi Adi*

Main category: cs.SD

TL;DR: PAST是一个新颖的端到端框架，联合建模语音信息和信号重建，无需依赖外部预训练模型，并在实时语音应用中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常依赖预训练的自监督模型，缺乏对领域知识的直接整合。PAST旨在通过监督语音数据直接集成领域知识，提升语音表示和重建的效果。

Method: PAST通过辅助任务在标记化过程中直接整合语音数据，并引入了可流式处理的因果变体，支持实时语音应用。

Result: PAST在语音表示和语音重建等常见评估指标上超越了现有基线标记器，并在作为语音语言模型的表示时表现出色。

Conclusion: PAST作为一种无需外部预训练模型的端到端框架，在语音表示和生成任务中表现出卓越性能，为语音研究提供了新的基础工具。

Abstract: We present PAST, a novel end-to-end framework that jointly models phonetic
information alongside signal reconstruction, eliminating the need for external
pretrained models. Unlike previous approaches that rely on pretrained
self-supervised models, PAST employs supervised phonetic data, directly
integrating domain knowledge into the tokenization process via auxiliary tasks.
Additionally, we introduce a streamable, causal variant of PAST, enabling
real-time speech applications. Results demonstrate that PAST surpasses existing
evaluated baseline tokenizers across common evaluation metrics, including
phonetic representation and speech reconstruction. Notably, PAST also achieves
superior performance when serving as a speech representation for speech
language models, further highlighting its effectiveness as a foundation for
spoken language generation. To foster further research, we release the full
implementation. For code, model checkpoints, and samples see:
https://pages.cs.huji.ac.il/adiyoss-lab/PAST

</details>


### [460] [Score-Based Training for Energy-Based TTS Models](https://arxiv.org/abs/2505.13771)
*Wanli Sun,Anton Ragni*

Main category: cs.SD

TL;DR: 论文提出了一种新准则，用于训练更适合一阶优化方案的分数，对比了噪声对比估计(NCE)和切片分数匹配(SSM)在训练能量基模型(EBM)中的表现。


<details>
  <summary>Details</summary>
Motivation: NCE和SSM方法在训练能量基模型时忽略了对数似然函数的形式，而EBM和扩散模型(DM)在推理时使用一阶优化，这可能导致问题。因此，需要一种新方法来学习更适合一阶优化方案的分数。

Method: 论文提出了一种新准则，通过学习更适合一阶优化方案的分数来改进EBM的训练。该方法对比了NCE和SSM在训练EBM中的表现。

Result: 实验结果表明，新提出的准则在学习分数方面优于NCE和SSM，更适合一阶优化方案。

Conclusion: 论文提出的新准则在训练EBM时表现更好，尤其是在一阶优化方案中，为EBM和DM的训练提供了更有效的方法。

Abstract: Noise contrastive estimation (NCE) is a popular method for training
energy-based models (EBM) with intractable normalisation terms. The key idea of
NCE is to learn by comparing unnormalised log-likelihoods of the reference and
noisy samples, thus avoiding explicitly computing normalisation terms. However,
NCE critically relies on the quality of noisy samples. Recently, sliced score
matching (SSM) has been popularised by closely related diffusion models (DM).
Unlike NCE, SSM learns a gradient of log-likelihood, or score, by learning
distribution of its projections on randomly chosen directions. However, both
NCE and SSM disregard the form of log-likelihood function, which is problematic
given that EBMs and DMs make use of first-order optimisation during inference.
This paper proposes a new criterion that learns scores more suitable for
first-order schemes. Experiments contrasts these approaches for training EBMs.

</details>


### [461] [AquaSignal: An Integrated Framework for Robust Underwater Acoustic Analysis](https://arxiv.org/abs/2505.14285)
*Eirini Panteli,Paulo E. Santos,Nabil Humphrey*

Main category: cs.SD

TL;DR: AquaSignal是一个模块化、可扩展的水下声学信号处理系统，结合深度学习技术提升信号分析的准确性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 针对水下声学信号在嘈杂和动态海洋环境中的分析挑战，开发一个高效的系统以提升信号处理的性能。

Method: 采用U-Net进行降噪，ResNet18分类已知声学事件，AutoEncoder检测异常信号，并在Deepship和ONC数据集上评估。

Result: 系统在分类任务中达到71%准确率，异常检测准确率为91%，信号清晰度和任务性能显著提升。

Conclusion: AquaSignal展示了在科学、环境和海事领域实时水下声学监测的强大潜力。

Abstract: This paper presents AquaSignal, a modular and scalable pipeline for
preprocessing, denoising, classification, and novelty detection of underwater
acoustic signals. Designed to operate effectively in noisy and dynamic marine
environments, AquaSignal integrates state-of-the-art deep learning
architectures to enhance the reliability and accuracy of acoustic signal
analysis. The system is evaluated on a combined dataset from the Deepship and
Ocean Networks Canada (ONC) benchmarks, providing a diverse set of real-world
underwater scenarios. AquaSignal employs a U-Net architecture for denoising, a
ResNet18 convolutional neural network for classifying known acoustic events,
and an AutoEncoder-based model for unsupervised detection of novel or anomalous
signals. To our knowledge, this is the first comprehensive study to apply and
evaluate this combination of techniques on maritime vessel acoustic data.
Experimental results show that AquaSignal improves signal clarity and task
performance, achieving 71% classification accuracy and 91% accuracy in novelty
detection. Despite slightly lower classification performance compared to some
state-of-the-art models, differences in data partitioning strategies limit
direct comparisons. Overall, AquaSignal demonstrates strong potential for
real-time underwater acoustic monitoring in scientific, environmental, and
maritime domains.

</details>


<div id='cs.CG'></div>

# cs.CG [[Back]](#toc)

### [462] [EuLearn: A 3D database for learning Euler characteristics](https://arxiv.org/abs/2505.13539)
*Rodrigo Fritz,Pablo Suárez-Serrato,Victor Mijangos,Anayanzi D. Martinez-Hernandez,Eduardo Ivan Velazquez Richards*

Main category: cs.CG

TL;DR: EuLearn是首个公平表示多种拓扑类型的表面数据集，通过随机节点生成不同属的嵌入表面，并改进3D神经网络架构以提升拓扑特征识别性能。


<details>
  <summary>Details</summary>
Motivation: 旨在为机器学习系统提供能识别拓扑特征的多样化数据集，解决现有方法在属分类上表现不佳的问题。

Method: 基于随机节点生成均匀变化的属表面，开发非欧几里得统计采样方法，并改进PointNet和Transformer架构。

Result: 实验表明，将拓扑信息融入深度学习流程显著提升了在EuLearn数据集上的分类性能。

Conclusion: EuLearn数据集及改进的架构有效提升了拓扑特征的识别能力，为相关研究提供了新工具。

Abstract: We present EuLearn, the first surface datasets equitably representing a
diversity of topological types. We designed our embedded surfaces of uniformly
varying genera relying on random knots, thus allowing our surfaces to knot with
themselves. EuLearn contributes new topological datasets of meshes, point
clouds, and scalar fields in 3D. We aim to facilitate the training of machine
learning systems that can discern topological features. We experimented with
specific emblematic 3D neural network architectures, finding that their vanilla
implementations perform poorly on genus classification. To enhance performance,
we developed a novel, non-Euclidean, statistical sampling method adapted to
graph and manifold data. We also introduce adjacency-informed adaptations of
PointNet and Transformer architectures that rely on our non-Euclidean sampling
strategy. Our results demonstrate that incorporating topological information
into deep learning workflows significantly improves performance on these
otherwise challenging EuLearn datasets.

</details>


### [463] [Towards Non-Euclidean Foundation Models: Advancing AI Beyond Euclidean Frameworks](https://arxiv.org/abs/2505.14417)
*Menglin Yang,Yifei Zhang,Jialin Chen,Melanie Weber,Rex Ying*

Main category: cs.CG

TL;DR: 该论文探讨了在基础模型和大型语言模型时代，非欧几里得学习如何提升网络相关应用的性能，特别是在处理复杂关系和数据结构方面。


<details>
  <summary>Details</summary>
Motivation: 当前机器学习架构主要基于欧几里得空间，但这一选择存在根本性限制。非欧几里得空间（如双曲、球面和混合曲率空间）能更高效地表示具有内在几何特性的数据，如社交网络拓扑、查询-文档关系和用户-物品交互。

Method: 通过将基础模型与非欧几里得几何相结合，探索其在捕捉和建模底层结构方面的潜力，以提升搜索、推荐和内容理解的性能。

Result: 非欧几里得学习在网络相关应用中展现出更高的效率和效果，特别是在处理复杂数据结构和关系时表现优异。

Conclusion: 非欧几里得基础模型与几何学习的结合具有巨大潜力，能够推动网络相关技术的进步，但仍需解决一些挑战并探索未来方向。

Abstract: In the era of foundation models and Large Language Models (LLMs), Euclidean
space is the de facto geometric setting of our machine learning architectures.
However, recent literature has demonstrated that this choice comes with
fundamental limitations. To that end, non-Euclidean learning is quickly gaining
traction, particularly in web-related applications where complex relationships
and structures are prevalent. Non-Euclidean spaces, such as hyperbolic,
spherical, and mixed-curvature spaces, have been shown to provide more
efficient and effective representations for data with intrinsic geometric
properties, including web-related data like social network topology,
query-document relationships, and user-item interactions. Integrating
foundation models with non-Euclidean geometries has great potential to enhance
their ability to capture and model the underlying structures, leading to better
performance in search, recommendations, and content understanding. This
workshop focuses on the intersection of Non-Euclidean Foundation Models and
Geometric Learning (NEGEL), exploring its potential benefits, including the
potential benefits for advancing web-related technologies, challenges, and
future directions. Workshop page:
[https://hyperboliclearning.github.io/events/www2025workshop](https://hyperboliclearning.github.io/events/www2025workshop)

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [464] [FreeMesh: Boosting Mesh Generation with Coordinates Merging](https://arxiv.org/abs/2505.13573)
*Jian Liu,Haohan Weng,Biwen Lei,Xianghui Yang,Zibo Zhao,Zhuo Chen,Song Guo,Tao Han,Chunchao Guo*

Main category: cs.GR

TL;DR: 本文提出了一种新的度量标准PTME来评估网格标记化方法，并引入坐标合并技术提升现有标记化器的压缩率。


<details>
  <summary>Details</summary>
Motivation: 当前自回归网格生成方法缺乏对网格序列化标记化器的有效评估标准。

Method: 提出PTME度量标准，并设计坐标合并技术优化标记化器的压缩效率。

Result: 实验验证PTME的有效性，坐标合并技术显著提升了MeshXL等标记化器的性能。

Conclusion: PTME和坐标合并技术可提升现有网格标记化器性能，指导原生网格生成的未来发展。

Abstract: The next-coordinate prediction paradigm has emerged as the de facto standard
in current auto-regressive mesh generation methods. Despite their
effectiveness, there is no efficient measurement for the various tokenizers
that serialize meshes into sequences. In this paper, we introduce a new metric
Per-Token-Mesh-Entropy (PTME) to evaluate the existing mesh tokenizers
theoretically without any training. Building upon PTME, we propose a
plug-and-play tokenization technique called coordinate merging. It further
improves the compression ratios of existing tokenizers by rearranging and
merging the most frequent patterns of coordinates. Through experiments on
various tokenization methods like MeshXL, MeshAnything V2, and Edgerunner, we
further validate the performance of our method. We hope that the proposed PTME
and coordinate merging can enhance the existing mesh tokenizers and guide the
further development of native mesh generation.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [465] [Performance Optimization of Energy-Harvesting Underlay Cognitive Radio Networks Using Reinforcement Learning](https://arxiv.org/abs/2505.14581)
*Deemah H. Tashman,Soumaya Cherkaoui,Walaa Hamouda*

Main category: eess.SP

TL;DR: 该论文利用强化学习优化认知无线电网络性能，通过能量收集和深度Q网络策略提升次级用户的数据传输率。


<details>
  <summary>Details</summary>
Motivation: 在存在主用户的情况下，次级用户需要高效利用有限能量资源进行通信，同时避免干扰主用户。

Method: 采用时间切换方法从主用户或环境射频源收集能量，并基于深度Q网络决定能量收集或数据传输的时机与功率。

Result: 所提方法在平均数据率上优于基线策略，并显示出良好的收敛性。

Conclusion: 结合能量收集和强化学习的策略能有效提升认知无线电网络的性能。

Abstract: In this paper, a reinforcement learning technique is employed to maximize the
performance of a cognitive radio network (CRN). In the presence of primary
users (PUs), it is presumed that two secondary users (SUs) access the licensed
band within underlay mode. In addition, the SU transmitter is assumed to be an
energy-constrained device that requires harvesting energy in order to transmit
signals to their intended destination. Therefore, we propose that there are two
main sources of energy; the interference of PUs' transmissions and ambient
radio frequency (RF) sources. The SU will select whether to gather energy from
PUs or only from ambient sources based on a predetermined threshold. The
process of energy harvesting from the PUs' messages is accomplished via the
time switching approach. In addition, based on a deep Q-network (DQN) approach,
the SU transmitter determines whether to collect energy or transmit messages
during each time slot as well as selects the suitable transmission power in
order to maximize its average data rate. Our approach outperforms a baseline
strategy and converges, as shown by our findings.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [466] [Pel, A Programming Language for Orchestrating AI Agents](https://arxiv.org/abs/2505.13453)
*Behnam Mohammadi*

Main category: cs.PL

TL;DR: 本文提出了一种名为Pel的新型编程语言，旨在解决大型语言模型（LLMs）在复杂操作、控制流和跨代理通信方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 当前的方法（如函数/工具调用和直接代码生成）在表达能力、可扩展性、成本、安全性和细粒度控制方面存在不足，Pel旨在填补这一空白。

Method: Pel是一种受Lisp、Elixir、Gleam和Haskell启发的编程语言，具有语法简单、同像性和语义丰富的特点，支持线性组合、闭包、自然语言条件和自动并行化等关键功能。

Result: Pel提供了一个更强大、安全和表达力强的范式，用于LLM的编排，为更复杂和可靠的AI代理框架铺平了道路。

Conclusion: Pel通过其设计和功能，为LLM的复杂操作和控制提供了更高效的解决方案，推动了AI代理系统的发展。

Abstract: The proliferation of Large Language Models (LLMs) has opened new frontiers in
computing, yet controlling and orchestrating their capabilities beyond simple
text generation remains a challenge. Current methods, such as function/tool
calling and direct code generation, suffer from limitations in expressiveness,
scalability, cost, security, and the ability to enforce fine-grained control.
This paper introduces Pel, a novel programming language specifically designed
to bridge this gap. Inspired by the strengths of Lisp, Elixir, Gleam, and
Haskell, Pel provides a syntactically simple, homoiconic, and semantically rich
platform for LLMs to express complex actions, control flow, and inter-agent
communication safely and efficiently. Pel's design emphasizes a minimal, easily
modifiable grammar suitable for constrained LLM generation, eliminating the
need for complex sandboxing by enabling capability control at the syntax level.
Key features include a powerful piping mechanism for linear composition,
first-class closures enabling easy partial application and functional patterns,
built-in support for natural language conditions evaluated by LLMs, and an
advanced Read-Eval-Print-Loop (REPeL) with Common Lisp-style restarts and
LLM-powered helper agents for automated error correction. Furthermore, Pel
incorporates automatic parallelization of independent operations via static
dependency analysis, crucial for performant agentic systems. We argue that Pel
offers a more robust, secure, and expressive paradigm for LLM orchestration,
paving the way for more sophisticated and reliable AI agentic frameworks.

</details>


### [467] [RTL++: Graph-enhanced LLM for RTL Code Generation](https://arxiv.org/abs/2505.13479)
*Mohammad Akyash,Kimia Azar,Hadi Kamali*

Main category: cs.PL

TL;DR: 论文提出RTL++方法，利用图结构增强LLM生成RTL代码的质量，解决了传统方法的局限性和开源模型的质量问题。


<details>
  <summary>Details</summary>
Motivation: 随着硬件设计复杂性增加，传统RTL设计方法耗时且易错，商业LLM存在安全和隐私问题，开源模型则因数据不足导致质量不佳。

Method: RTL++将RTL代码编码为文本化的控制流图（CFG）和数据流图（DFG），通过图结构增强LLM对代码的理解和生成能力。

Result: 实验表明，RTL++在VerilogEval基准测试中优于现有模型，证明了图增强上下文在提升RTL代码生成能力方面的有效性。

Conclusion: RTL++通过图表示方法显著提升了LLM辅助RTL代码生成的质量和多样性，为EDA自动化提供了新思路。

Abstract: As hardware design complexity escalates, there is an urgent need for advanced
automation in electronic design automation (EDA). Traditional register transfer
level (RTL) design methods are manual, time-consuming, and prone to errors.
While commercial (instruction-tuned) large language models (LLMs) shows
promising performance for automation, they pose security and privacy concerns.
Open-source models offer alternatives; however, they frequently fall short in
quality/correctness, largely due to limited, high-quality RTL code data
essential for effective training and generalization. This paper proposes RTL++,
a first-of-its-kind LLM-assisted method for RTL code generation that utilizes
graph representations of code structures to enhance the quality of generated
code. By encoding RTL code into a textualized control flowgraphs (CFG) and data
flow graphs (DFG), RTL++ captures the inherent hierarchy, dependencies, and
relationships within the code. This structured graph-based approach enhances
the context available to LLMs, enabling them to better understand and generate
instructions. By focusing on data generation through graph representations,
RTL++ addresses the limitations of previous approaches that rely solely on code
and suffer from lack of diversity. Experimental results demonstrate that RTL++
outperforms state-of-the-art models fine-tuned for RTL generation, as evaluated
using the VerilogEval benchmark's Pass@1/5/10 metric, as well as the RTLLM1.1
model, which highlight the effectiveness of graph-enhanced context in advancing
the capabilities of LLM-assisted RTL code generation.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [468] [Low-Cost FlashAttention with Fused Exponential and Multiplication Hardware Operators](https://arxiv.org/abs/2505.14314)
*Kosmas Alexandridis,Vasileios Titopoulos,Giorgos Dimitrakopoulos*

Main category: cs.AR

TL;DR: 该论文提出了一种优化基于浮点数的FlashAttention内核的新硬件操作符ExpMul，显著降低了硬件加速器的面积和功耗。


<details>
  <summary>Details</summary>
Motivation: 随着序列长度的增加，计算注意力机制的需求也在增长，需要更高效的硬件加速器来支持。

Method: 通过融合指数计算和向量乘法的新硬件操作符ExpMul来优化FlashAttention内核。

Result: 在28nm ASIC技术中实现时，面积平均减少28.8%，功耗平均降低17.6%。

Conclusion: ExpMul硬件操作符显著提升了FlashAttention硬件加速器的效率和性能。

Abstract: Attention mechanisms, particularly within Transformer architectures and large
language models (LLMs), have revolutionized sequence modeling in machine
learning and artificial intelligence applications. To compute attention for
increasingly long sequences, specialized accelerators have been proposed to
execute key attention steps directly in hardware. Among the various recently
proposed architectures, those based on variants of the FlashAttention
algorithm, originally designed for GPUs, stand out due to their optimized
computation, tiling capabilities, and reduced memory traffic. In this work, we
focus on optimizing the kernel of floating-point-based FlashAttention using new
hardware operators that fuse the computation of exponentials and vector
multiplications, e.g., e^x, V. The proposed ExpMul hardware operators
significantly reduce the area and power costs of FlashAttention-based hardware
accelerators. When implemented in a 28nm ASIC technology, they achieve
improvements of 28.8% in area and 17.6% in power, on average, compared to
state-of-the-art hardware architectures with separate exponentials and vector
multiplications hardware operators.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [469] [Sat2Sound: A Unified Framework for Zero-Shot Soundscape Mapping](https://arxiv.org/abs/2505.13777)
*Subash Khanal,Srikumar Sastry,Aayush Dhakal,Adeel Ahmad,Nathan Jacobs*

Main category: cs.CV

TL;DR: Sat2Sound是一个多模态学习框架，用于预测地球上任意位置的声音分布，通过结合卫星图像和音频数据，并利用视觉语言模型生成丰富的声景描述，实现了跨模态检索和声景合成。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖卫星图像和地理标记音频样本，但难以捕捉特定位置的声源多样性。Sat2Sound旨在通过增强数据集和跨模态学习解决这一问题。

Method: 利用视觉语言模型生成卫星图像的声景描述，通过对比学习结合音频、音频字幕、卫星图像及其字幕，学习共享的声景概念代码本。

Result: Sat2Sound在GeoSound和SoundingEarth数据集上实现了卫星图像与音频跨模态检索的最先进性能，并支持基于位置的声景合成。

Conclusion: Sat2Sound通过多模态学习和共享概念代码本，显著提升了声景映射的准确性和多样性，为沉浸式声景体验提供了新应用。

Abstract: We present Sat2Sound, a multimodal representation learning framework for
soundscape mapping, designed to predict the distribution of sounds at any
location on Earth. Existing methods for this task rely on satellite image and
paired geotagged audio samples, which often fail to capture the diversity of
sound sources at a given location. To address this limitation, we enhance
existing datasets by leveraging a Vision-Language Model (VLM) to generate
semantically rich soundscape descriptions for locations depicted in satellite
images. Our approach incorporates contrastive learning across audio, audio
captions, satellite images, and satellite image captions. We hypothesize that
there is a fixed set of soundscape concepts shared across modalities. To this
end, we learn a shared codebook of soundscape concepts and represent each
sample as a weighted average of these concepts. Sat2Sound achieves
state-of-the-art performance in cross-modal retrieval between satellite image
and audio on two datasets: GeoSound and SoundingEarth. Additionally, building
on Sat2Sound's ability to retrieve detailed soundscape captions, we introduce a
novel application: location-based soundscape synthesis, which enables immersive
acoustic experiences. Our code and models will be publicly available.

</details>


### [470] [An Edge AI Solution for Space Object Detection](https://arxiv.org/abs/2505.13468)
*Wenxuan Zhang,Peng Hu*

Main category: cs.CV

TL;DR: 该论文提出了一种基于深度学习的边缘AI解决方案，用于空间物体检测任务，结合SE层、Vision Transformers和YOLOv9框架，实现了高精度和低延迟的检测。


<details>
  <summary>Details</summary>
Motivation: 随着近地轨道空间资产的增加，实时碰撞评估和避障变得至关重要。低地球轨道卫星需要高精度、低延迟的空间物体检测能力。

Method: 论文探索了一种基于深度学习的视觉感知方案，结合Squeeze-and-Excitation层、Vision Transformers和YOLOv9框架，构建了一个深度学习模型。

Result: 模型在多种真实场景下的空间物体检测任务中表现优异，能够高精度、低延迟地检测多个卫星。

Conclusion: 提出的边缘AI解决方案在空间物体检测任务中表现出色，为实时碰撞评估和避障提供了有效支持。

Abstract: Effective Edge AI for space object detection (SOD) tasks that can facilitate
real-time collision assessment and avoidance is essential with the increasing
space assets in near-Earth orbits. In SOD, low Earth orbit (LEO) satellites
must detect other objects with high precision and minimal delay. We explore an
Edge AI solution based on deep-learning-based vision sensing for SOD tasks and
propose a deep learning model based on Squeeze-and-Excitation (SE) layers,
Vision Transformers (ViT), and YOLOv9 framework. We evaluate the performance of
these models across various realistic SOD scenarios, demonstrating their
ability to detect multiple satellites with high accuracy and very low latency.

</details>


### [471] [Domain Adaptation of VLM for Soccer Video Understanding](https://arxiv.org/abs/2505.13860)
*Tiancheng Jiang,Henry Wang,Md Sirajus Salekin,Parmida Atighehchian,Shinan Zhang*

Main category: cs.CV

TL;DR: 该研究探索了开源视觉语言模型（VLM）在特定领域（如足球）的适应性，通过课程学习和指令微调显著提升了模型在足球相关任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 当前大多数视频理解VLM研究是领域无关的，缺乏对模型在特定领域迁移学习能力的深入探索。本研究旨在填补这一空白，以足球为例探索VLM的领域适应性。

Method: 使用大规模足球数据集和LLM生成指令跟随数据，采用课程学习方式（先教授关键足球概念，再进行问答任务）迭代微调通用VLM。最终模型基于2万条精选视频片段训练。

Result: 微调后的模型在足球特定任务上表现显著提升：视觉问答任务相对改进37.5%，足球动作分类任务准确率从11.8%提升至63.5%。

Conclusion: 研究表明通过领域适配方法，通用VLM可以有效地迁移到特定领域（如足球），并在相关任务上取得显著性能提升。

Abstract: Vision Language Models (VLMs) have demonstrated strong performance in
multi-modal tasks by effectively aligning visual and textual representations.
However, most video understanding VLM research has been domain-agnostic,
leaving the understanding of their transfer learning capability to specialized
domains under-explored. In this work, we address this by exploring the
adaptability of open-source VLMs to specific domains, and focusing on soccer as
an initial case study. Our approach uses large-scale soccer datasets and LLM to
create instruction-following data, and use them to iteratively fine-tune the
general-domain VLM in a curriculum learning fashion (first teaching the model
key soccer concepts to then question answering tasks). The final adapted model,
trained using a curated dataset of 20k video clips, exhibits significant
improvement in soccer-specific tasks compared to the base model, with a 37.5%
relative improvement for the visual question-answering task and an accuracy
improvement from 11.8% to 63.5% for the downstream soccer action classification
task.

</details>


### [472] [RADAR: Enhancing Radiology Report Generation with Supplementary Knowledge Injection](https://arxiv.org/abs/2505.14318)
*Wenjun Hou,Yi Cheng,Kaishuai Xu,Heng Li,Yan Hu,Wenjie Li,Jiang Liu*

Main category: cs.CV

TL;DR: 提出RADAR框架，通过结合LLM内部知识与外部检索信息，提升放射学报告生成的准确性和信息量。


<details>
  <summary>Details</summary>
Motivation: 现有方法多依赖外部知识检索，忽略了LLM内部已嵌入的知识，导致信息冗余和低效利用。

Method: RADAR首先提取LLM内部与专家分类一致的知识，再检索补充信息，最后整合两者生成报告。

Result: 在MIMIC-CXR等数据集上，RADAR在语言质量和临床准确性上超越现有LLM方法。

Conclusion: RADAR有效结合内外知识，为放射学报告生成提供了更优解决方案。

Abstract: Large language models (LLMs) have demonstrated remarkable capabilities in
various domains, including radiology report generation. Previous approaches
have attempted to utilize multimodal LLMs for this task, enhancing their
performance through the integration of domain-specific knowledge retrieval.
However, these approaches often overlook the knowledge already embedded within
the LLMs, leading to redundant information integration and inefficient
utilization of learned representations. To address this limitation, we propose
RADAR, a framework for enhancing radiology report generation with supplementary
knowledge injection. RADAR improves report generation by systematically
leveraging both the internal knowledge of an LLM and externally retrieved
information. Specifically, it first extracts the model's acquired knowledge
that aligns with expert image-based classification outputs. It then retrieves
relevant supplementary knowledge to further enrich this information. Finally,
by aggregating both sources, RADAR generates more accurate and informative
radiology reports. Extensive experiments on MIMIC-CXR, CheXpert-Plus, and IU
X-ray demonstrate that our model outperforms state-of-the-art LLMs in both
language quality and clinical accuracy

</details>


### [473] [AppleGrowthVision: A large-scale stereo dataset for phenological analysis, fruit detection, and 3D reconstruction in apple orchards](https://arxiv.org/abs/2505.14029)
*Laura-Sophia von Hirschhausen,Jannes S. Magnusson,Mykyta Kovalenko,Fredrik Boye,Tanay Rawat,Peter Eisert,Anna Hilsmann,Sebastian Pretzsch,Sebastian Bosse*

Main category: cs.CV

TL;DR: 论文提出了AppleGrowthVision数据集，解决了苹果园监测中数据不足的问题，提升了果实检测和生长建模的精度。


<details>
  <summary>Details</summary>
Motivation: 现有数据集缺乏多样性和真实性，且忽略了不同生长阶段和立体图像，限制了苹果园监测的精度和应用。

Method: 构建了包含9,317张高分辨率立体图像和1,125张密集标注图像的大规模数据集，覆盖六个农业验证的生长阶段。

Result: 使用该数据集显著提升了YOLOv8和Faster R-CNN的性能，生长阶段预测准确率超过95%。

Conclusion: AppleGrowthVision填补了农业科学与计算机视觉之间的空白，为精准农业提供了可靠的数据支持。

Abstract: Deep learning has transformed computer vision for precision agriculture, yet
apple orchard monitoring remains limited by dataset constraints. The lack of
diverse, realistic datasets and the difficulty of annotating dense,
heterogeneous scenes. Existing datasets overlook different growth stages and
stereo imagery, both essential for realistic 3D modeling of orchards and tasks
like fruit localization, yield estimation, and structural analysis. To address
these gaps, we present AppleGrowthVision, a large-scale dataset comprising two
subsets. The first includes 9,317 high resolution stereo images collected from
a farm in Brandenburg (Germany), covering six agriculturally validated growth
stages over a full growth cycle. The second subset consists of 1,125 densely
annotated images from the same farm in Brandenburg and one in Pillnitz
(Germany), containing a total of 31,084 apple labels. AppleGrowthVision
provides stereo-image data with agriculturally validated growth stages,
enabling precise phenological analysis and 3D reconstructions. Extending
MinneApple with our data improves YOLOv8 performance by 7.69 % in terms of
F1-score, while adding it to MinneApple and MAD boosts Faster R-CNN F1-score by
31.06 %. Additionally, six BBCH stages were predicted with over 95 % accuracy
using VGG16, ResNet152, DenseNet201, and MobileNetv2. AppleGrowthVision bridges
the gap between agricultural science and computer vision, by enabling the
development of robust models for fruit detection, growth modeling, and 3D
analysis in precision agriculture. Future work includes improving annotation,
enhancing 3D reconstruction, and extending multimodal analysis across all
growth stages.

</details>


### [474] [RAVENEA: A Benchmark for Multimodal Retrieval-Augmented Visual Culture Understanding](https://arxiv.org/abs/2505.14462)
*Jiaang Li,Yifei Yuan,Wenyan Li,Mohammad Aliannejadi,Daniel Hershcovich,Anders Søgaard,Ivan Vulić,Wenxuan Zhang,Paul Pu Liang,Yang Deng,Serge Belongie*

Main category: cs.CV

TL;DR: 论文提出RAVENEA基准，通过检索增强方法提升视觉语言模型对文化细微差别的理解能力，在两项任务上显著提升模型表现。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型在理解文化细微差别方面表现不足，而检索增强生成方法在文本领域已证明有效，但在多模态场景中的应用尚未充分探索。

Method: 构建RAVENEA基准，整合超过10,000篇人工标注的维基百科文档，训练并评估7种多模态检索器，测量检索增强输入对14种先进视觉语言模型的影响。

Result: 轻量级视觉语言模型在文化感知检索增强后表现显著提升（cVQA任务提升3.2%，cIC任务提升6.2%）。

Conclusion: 检索增强方法和文化包容性基准对提升多模态理解具有重要价值。

Abstract: As vision-language models (VLMs) become increasingly integrated into daily
life, the need for accurate visual culture understanding is becoming critical.
Yet, these models frequently fall short in interpreting cultural nuances
effectively. Prior work has demonstrated the effectiveness of
retrieval-augmented generation (RAG) in enhancing cultural understanding in
text-only settings, while its application in multimodal scenarios remains
underexplored. To bridge this gap, we introduce RAVENEA (Retrieval-Augmented
Visual culturE uNdErstAnding), a new benchmark designed to advance visual
culture understanding through retrieval, focusing on two tasks: culture-focused
visual question answering (cVQA) and culture-informed image captioning (cIC).
RAVENEA extends existing datasets by integrating over 10,000 Wikipedia
documents curated and ranked by human annotators. With RAVENEA, we train and
evaluate seven multimodal retrievers for each image query, and measure the
downstream impact of retrieval-augmented inputs across fourteen
state-of-the-art VLMs. Our results show that lightweight VLMs, when augmented
with culture-aware retrieval, outperform their non-augmented counterparts (by
at least 3.2% absolute on cVQA and 6.2% absolute on cIC). This highlights the
value of retrieval-augmented methods and culturally inclusive benchmarks for
multimodal understanding.

</details>


### [475] [Unify Graph Learning with Text: Unleashing LLM Potentials for Session Search](https://arxiv.org/abs/2505.14156)
*Songhao Wu,Quan Tu,Hong Liu,Jia Xu,Zhongyi Liu,Guannan Zhang,Ran Wang,Xiuying Chen,Rui Yan*

Main category: cs.CV

TL;DR: 提出符号图排序器(SGR)，结合大语言模型(LLM)的文本理解与图结构建模能力，通过符号语法规则将会话图转为文本，并设计自监督任务增强LLM对图结构的捕捉能力。


<details>
  <summary>Details</summary>
Motivation: 现有会话搜索方法或过度依赖序列建模忽略图结构，或仅用通用文档表示忽视词级语义。需融合文本与图结构优势。

Method: 1) 定义符号语法规则将图结构转为文本；2) 设计链接预测、节点生成等自监督任务，分层次增强LLM的图结构理解能力。

Result: 在AOL和Tiangong-ST数据集上验证了SGR的优越性，实验表明其有效桥接了传统搜索策略与现代LLM。

Conclusion: SGR为结合图结构与LLM提供了创新范式，通过符号化学习任务成功将拓扑信息编码到文本表示中。

Abstract: Session search involves a series of interactive queries and actions to
fulfill user's complex information need. Current strategies typically
prioritize sequential modeling for deep semantic understanding, overlooking the
graph structure in interactions. While some approaches focus on capturing
structural information, they use a generalized representation for documents,
neglecting the word-level semantic modeling. In this paper, we propose Symbolic
Graph Ranker (SGR), which aims to take advantage of both text-based and
graph-based approaches by leveraging the power of recent Large Language Models
(LLMs). Concretely, we first introduce a set of symbolic grammar rules to
convert session graph into text. This allows integrating session history,
interaction process, and task instruction seamlessly as inputs for the LLM.
Moreover, given the natural discrepancy between LLMs pre-trained on textual
corpora, and the symbolic language we produce using our graph-to-text grammar,
our objective is to enhance LLMs' ability to capture graph structures within a
textual format. To achieve this, we introduce a set of self-supervised symbolic
learning tasks including link prediction, node content generation, and
generative contrastive learning, to enable LLMs to capture the topological
information from coarse-grained to fine-grained. Experiment results and
comprehensive analysis on two benchmark datasets, AOL and Tiangong-ST, confirm
the superiority of our approach. Our paradigm also offers a novel and effective
methodology that bridges the gap between traditional search strategies and
modern LLMs.

</details>


### [476] [Dual Precision Quantization for Efficient and Accurate Deep Neural Networks Inference](https://arxiv.org/abs/2505.14638)
*Tomer Gafni,Asaf Karnieli,Yair Hanani*

Main category: cs.CV

TL;DR: 提出了一种硬件高效的W4A8量化推理方案，通过4位整数权重和8位浮点计算提升性能，同时保持精度。


<details>
  <summary>Details</summary>
Motivation: 随着深度学习模型规模增大，延迟和内存效率成为挑战，需要高效的量化解决方案。

Method: 采用W4A8量化方案，结合双精度量化算法(DPQ)减少精度损失。

Result: 实验显示在保持可接受精度损失的同时，显著提升了吞吐量和内存利用率。

Conclusion: 该方案在多种现代加速器上实现了高效推理，平衡了性能与精度。

Abstract: Deep neural networks have achieved state-of-the-art results in a wide range
of applications, from natural language processing and computer vision to speech
recognition. However, as tasks become increasingly complex, model sizes
continue to grow, posing challenges in latency and memory efficiency. To meet
these constraints, post-training quantization has emerged as a promising
solution. In this paper, we propose a novel hardware-efficient quantization and
inference scheme that exploits hardware advantages with minimal accuracy
degradation. Specifically, we introduce a W4A8 scheme, where weights are
quantized and stored using 4-bit integer precision, and inference computations
are performed using 8-bit floating-point arithmetic, demonstrating significant
speedups and improved memory utilization compared to 16-bit operations,
applicable on various modern accelerators. To mitigate accuracy loss, we
develop a novel quantization algorithm, dubbed Dual Precision Quantization
(DPQ), that leverages the unique structure of our scheme without introducing
additional inference overhead. Experimental results demonstrate improved
performance (i.e., increased throughput) while maintaining tolerable accuracy
degradation relative to the full-precision model.

</details>


### [477] [Beyond Words: Multimodal LLM Knows When to Speak](https://arxiv.org/abs/2505.14654)
*Zikai Liao,Yi Ouyang,Yi-Lun Lee,Chen-Ping Yu,Yi-Hsuan Tsai,Zhaozheng Yin*

Main category: cs.CV

TL;DR: 论文提出多模态模型MM-When2Speak，通过整合视觉、听觉和文本信息，显著提升了对话AI的响应时机预测能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于大型语言模型的聊天机器人在实时对话中难以准确判断何时回应，主要因为缺乏现实对话中的多模态上下文线索。

Method: 构建包含视觉、听觉和文本对齐的多模态数据集，并基于此提出MM-When2Speak模型，自适应整合多模态信息预测响应时机和类型。

Result: MM-When2Speak在响应时机准确率上比现有单模态和LLM基线提升高达4倍。

Conclusion: 多模态输入对实现自然、及时的对话AI至关重要，MM-When2Speak验证了多模态整合的有效性。

Abstract: While large language model (LLM)-based chatbots have demonstrated strong
capabilities in generating coherent and contextually relevant responses, they
often struggle with understanding when to speak, particularly in delivering
brief, timely reactions during ongoing conversations. This limitation arises
largely from their reliance on text input, lacking the rich contextual cues in
real-world human dialogue. In this work, we focus on real-time prediction of
response types, with an emphasis on short, reactive utterances that depend on
subtle, multimodal signals across vision, audio, and text. To support this, we
introduce a new multimodal dataset constructed from real-world conversational
videos, containing temporally aligned visual, auditory, and textual streams.
This dataset enables fine-grained modeling of response timing in dyadic
interactions. Building on this dataset, we propose MM-When2Speak, a multimodal
LLM-based model that adaptively integrates visual, auditory, and textual
context to predict when a response should occur, and what type of response is
appropriate. Experiments show that MM-When2Speak significantly outperforms
state-of-the-art unimodal and LLM-based baselines, achieving up to a 4x
improvement in response timing accuracy over leading commercial LLMs. These
results underscore the importance of multimodal inputs for producing timely,
natural, and engaging conversational AI.

</details>


### [478] [VoQA: Visual-only Question Answering](https://arxiv.org/abs/2505.14227)
*Luyang Jiang,Jianing An,Jie Luo,Wenjun Wu,Lei Huang*

Main category: cs.CV

TL;DR: 论文提出VoQA任务，要求模型仅通过视觉输入回答嵌入图像中的问题，并开发GRT-SFT微调策略提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有大型视觉语言模型在处理纯视觉嵌入的文本问题时表现不佳，需提升模型在复杂多模态场景中的视觉理解能力。

Method: 采用GRT-SFT结构化微调策略，指导模型基于纯视觉输入进行逐步推理。

Result: GRT-SFT显著提高了模型在VoQA任务上的表现。

Conclusion: 该研究增强了模型在纯视觉信息场景下类人化推理的能力，推动了多模态理解技术的发展。

Abstract: We propose Visual-only Question Answering (VoQA), a novel multimodal task in
which questions are visually embedded within images, without any accompanying
textual input. This requires models to locate, recognize, and reason over
visually embedded textual questions, posing challenges for existing large
vision-language models (LVLMs), which show notable performance drops even with
carefully designed prompts. To bridge this gap, we introduce Guided Response
Triggering Supervised Fine-tuning (GRT-SFT), a structured fine-tuning strategy
that guides the model to perform step-by-step reasoning purely based on visual
input, significantly improving model performance. Our work enhances models'
capacity for human-like visual understanding in complex multimodal scenarios,
where information, including language, is perceived visually.

</details>


### [479] [Visual Agentic Reinforcement Fine-Tuning](https://arxiv.org/abs/2505.14246)
*Ziyu Liu,Yuhang Zang,Yushan Zou,Zijian Liang,Xiaoyi Dong,Yuhang Cao,Haodong Duan,Dahua Lin,Jiaqi Wang*

Main category: cs.CV

TL;DR: 该论文提出了Visual-ARFT方法，通过强化微调赋予大型视觉语言模型多模态代理能力，使其能实时搜索信息并编写代码处理图像，在多个基准测试中超越基线模型及GPT-4o。


<details>
  <summary>Details</summary>
Motivation: 当前开源社区在纯语言代理能力（如函数调用）上进展显著，但涉及图像思维的多模态代理能力及其基准测试仍待探索。本文旨在填补这一空白。

Method: 采用Visual-ARFT（视觉代理强化微调）方法，使模型具备网页浏览实时更新信息的能力，并能通过编写代码实现裁剪、旋转等图像处理操作。同时提出MAT多模态代理工具基准（含搜索和编码两个子任务）进行评估。

Result: Visual-ARFT在MAT-Coding任务上F1/EM分数分别提升18.6%/13.0%，在MAT-Search任务上提升10.3%/8.7%，超越GPT-4o。在2Wiki和HotpotQA等多跳QA基准上也获得29.3%/25.9%的显著提升。

Conclusion: Visual-ARFT为构建鲁棒、可泛化的多模态代理提供了有效路径，其强化微调框架能显著提升模型在视觉-语言联合任务中的推理适应性。

Abstract: A key trend in Large Reasoning Models (e.g., OpenAI's o3) is the native
agentic ability to use external tools such as web browsers for searching and
writing/executing code for image manipulation to think with images. In the
open-source research community, while significant progress has been made in
language-only agentic abilities such as function calling and tool integration,
the development of multi-modal agentic capabilities that involve truly thinking
with images, and their corresponding benchmarks, are still less explored. This
work highlights the effectiveness of Visual Agentic Reinforcement Fine-Tuning
(Visual-ARFT) for enabling flexible and adaptive reasoning abilities for Large
Vision-Language Models (LVLMs). With Visual-ARFT, open-source LVLMs gain the
ability to browse websites for real-time information updates and write code to
manipulate and analyze input images through cropping, rotation, and other image
processing techniques. We also present a Multi-modal Agentic Tool Bench (MAT)
with two settings (MAT-Search and MAT-Coding) designed to evaluate LVLMs'
agentic search and coding abilities. Our experimental results demonstrate that
Visual-ARFT outperforms its baseline by +18.6% F1 / +13.0% EM on MAT-Coding and
+10.3% F1 / +8.7% EM on MAT-Search, ultimately surpassing GPT-4o. Visual-ARFT
also achieves +29.3 F1% / +25.9% EM gains on existing multi-hop QA benchmarks
such as 2Wiki and HotpotQA, demonstrating strong generalization capabilities.
Our findings suggest that Visual-ARFT offers a promising path toward building
robust and generalizable multimodal agents.

</details>


### [480] [CONSIGN: Conformal Segmentation Informed by Spatial Groupings via Decomposition](https://arxiv.org/abs/2505.14113)
*Bruno Viti,Elias Karabelas,Martin Holler*

Main category: cs.CV

TL;DR: 论文提出CONSIGN方法，通过结合空间相关性改进图像分割中的不确定性量化，相比传统像素级方法显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于机器学习的图像分割模型生成的置信分数缺乏严格的统计有效性，且忽略像素间空间相关性，导致不确定性估计保守且难以解释。

Method: 提出CONSIGN方法，基于共形预测框架，通过空间分组分解整合像素间空间相关性，兼容任何能生成多样本输出的预训练分割模型。

Result: 在三个医学影像数据集和两个COCO子集上测试表明，考虑空间结构显著提升多指标性能，并改善不确定性估计质量。

Conclusion: CONSIGN通过空间相关性建模，为图像分割提供了统计有效且可解释的不确定性量化方法，适用于高风险领域如医学影像。

Abstract: Most machine learning-based image segmentation models produce pixel-wise
confidence scores - typically derived from softmax outputs - that represent the
model's predicted probability for each class label at every pixel. While this
information can be particularly valuable in high-stakes domains such as medical
imaging, these (uncalibrated) scores are heuristic in nature and do not
constitute rigorous quantitative uncertainty estimates. Conformal prediction
(CP) provides a principled framework for transforming heuristic confidence
scores into statistically valid uncertainty estimates. However, applying CP
directly to image segmentation ignores the spatial correlations between pixels,
a fundamental characteristic of image data. This can result in overly
conservative and less interpretable uncertainty estimates. To address this, we
propose CONSIGN (Conformal Segmentation Informed by Spatial Groupings via
Decomposition), a CP-based method that incorporates spatial correlations to
improve uncertainty quantification in image segmentation. Our method generates
meaningful prediction sets that come with user-specified, high-probability
error guarantees. It is compatible with any pre-trained segmentation model
capable of generating multiple sample outputs - such as those using dropout,
Bayesian modeling, or ensembles. We evaluate CONSIGN against a standard
pixel-wise CP approach across three medical imaging datasets and two COCO
dataset subsets, using three different pre-trained segmentation models. Results
demonstrate that accounting for spatial structure significantly improves
performance across multiple metrics and enhances the quality of uncertainty
estimates.

</details>


### [481] [Speculative Decoding Reimagined for Multimodal Large Language Models](https://arxiv.org/abs/2505.14260)
*Luxi Lin,Zhihang Lin,Zhanpeng Zeng,Rongrong Ji*

Main category: cs.CV

TL;DR: 本文提出多模态推测解码（MSD）方法，通过分离处理文本和视觉标记以及两阶段训练策略，显著加速多模态大语言模型（MLLMs）的推理速度。


<details>
  <summary>Details</summary>
Motivation: 当前推测解码方法在多模态大语言模型（MLLMs）上的加速效果不如在纯文本大语言模型（LLMs）上显著，因此需要针对MLLMs的特性重新设计推测解码方法。

Method: MSD采用两种设计原则：1) 在草案模型中分离处理文本和视觉标记；2) 通过两阶段训练（先文本指令调优，再逐步引入多模态数据）提升草案模型的语言建模和视觉感知能力。

Result: 实验表明，MSD在LLaVA-1.5-7B和LLaVA-1.5-13B模型上分别实现了最高2.29倍和2.46倍的推理加速。

Conclusion: MSD有效解决了MLLMs推理加速问题，为多模态模型的高效部署提供了可行方案。

Abstract: This paper introduces Multimodal Speculative Decoding (MSD) to accelerate
Multimodal Large Language Models (MLLMs) inference. Speculative decoding has
been shown to accelerate Large Language Models (LLMs) without sacrificing
accuracy. However, current speculative decoding methods for MLLMs fail to
achieve the same speedup as they do for LLMs. To address this, we reimagine
speculative decoding specifically for MLLMs. Our analysis of MLLM
characteristics reveals two key design principles for MSD: (1) Text and visual
tokens have fundamentally different characteristics and need to be processed
separately during drafting. (2) Both language modeling ability and visual
perception capability are crucial for the draft model. For the first principle,
MSD decouples text and visual tokens in the draft model, allowing each to be
handled based on its own characteristics. For the second principle, MSD uses a
two-stage training strategy: In stage one, the draft model is trained on
text-only instruction-tuning datasets to improve its language modeling ability.
In stage two, MSD gradually introduces multimodal data to enhance the visual
perception capability of the draft model. Experiments show that MSD boosts
inference speed by up to $2.29\times$ for LLaVA-1.5-7B and up to $2.46\times$
for LLaVA-1.5-13B on multimodal benchmarks, demonstrating its effectiveness.
Our code is available at https://github.com/Lyn-Lucy/MSD.

</details>


### [482] [Handloom Design Generation Using Generative Networks](https://arxiv.org/abs/2505.14330)
*Rajat Kanti Bhattacharjee,Meghali Nandi,Amrit Jha,Gunajit Kalita,Ferdous Ahmed Barbhuiya*

Main category: cs.CV

TL;DR: 该论文提出使用深度学习技术生成手织布料设计，探讨了相关挑战及应用，并通过用户评分评估了多种生成模型和风格迁移算法的性能。


<details>
  <summary>Details</summary>
Motivation: 生成神经网络模型在理解艺术设计和合成方面的能力尚未得到充分探索，尤其是在手织布料设计领域。

Method: 结合当前最先进的生成模型和风格迁移算法，采用多种方法进行研究，并创建了新数据集NeuralLoom。

Result: 通过用户评分评估了生成设计的性能，展示了生成模型在手织布料设计中的潜力。

Conclusion: 该研究为手织布料设计生成提供了新方法，并证明了生成模型在这一领域的应用前景。

Abstract: This paper proposes deep learning techniques of generating designs for
clothing, focused on handloom fabric and discusses the associated challenges
along with its application. The capability of generative neural network models
in understanding artistic designs and synthesizing those is not yet explored
well. In this work, multiple methods are employed incorporating the current
state of the art generative models and style transfer algorithms to study and
observe their performance for the task. The results are then evaluated through
user score. This work also provides a new dataset NeuralLoom for the task of
the design generation.

</details>


### [483] [Replace in Translation: Boost Concept Alignment in Counterfactual Text-to-Image](https://arxiv.org/abs/2505.14341)
*Sifan Li,Ming Tao,Hao Zhao,Ling Shao,Hao Tang*

Main category: cs.CV

TL;DR: 该论文提出了一种通过潜在空间逐步替换对象的方法，结合显式逻辑叙事提示（ELNP），提升反事实文本到图像生成中的概念对齐能力。


<details>
  <summary>Details</summary>
Motivation: 当前文本到图像生成在常见任务上表现良好，但在反事实场景（如违反物理规律或现实中不可能发生的场景）中，生成图像的概念对齐和真实感仍有待提升。

Method: 利用可控文本到图像模型，在潜在空间中逐步替换合成图像中的对象，将其从常见场景转变为反事实场景，并通过语言模型生成显式逻辑叙事提示（ELNP）指导替换过程。

Result: 实验表明，该方法能有效提升反事实文本到图像生成中的概念对齐，并通过新设计的指标验证了其性能优势。

Conclusion: 通过潜在空间对象替换和ELNP策略，论文显著改善了反事实图像生成的概念对齐，为更灵活的AIGC体验提供了新思路。

Abstract: Text-to-Image (T2I) has been prevalent in recent years, with most common
condition tasks having been optimized nicely. Besides, counterfactual
Text-to-Image is obstructing us from a more versatile AIGC experience. For
those scenes that are impossible to happen in real world and anti-physics, we
should spare no efforts in increasing the factual feel, which means
synthesizing images that people think very likely to be happening, and concept
alignment, which means all the required objects should be in the same frame. In
this paper, we focus on concept alignment. As controllable T2I models have
achieved satisfactory performance for real applications, we utilize this
technology to replace the objects in a synthesized image in latent space
step-by-step to change the image from a common scene to a counterfactual scene
to meet the prompt. We propose a strategy to instruct this replacing process,
which is called as Explicit Logical Narrative Prompt (ELNP), by using the newly
SoTA language model DeepSeek to generate the instructions. Furthermore, to
evaluate models' performance in counterfactual T2I, we design a metric to
calculate how many required concepts in the prompt can be covered averagely in
the synthesized images. The extensive experiments and qualitative comparisons
demonstrate that our strategy can boost the concept alignment in counterfactual
T2I.

</details>


### [484] [Plane Geometry Problem Solving with Multi-modal Reasoning: A Survey](https://arxiv.org/abs/2505.14340)
*Seunghyuk Cho,Zhenyue Qin,Yang Liu,Youngbin Choi,Seungbeom Lee,Dongwoo Kim*

Main category: cs.CV

TL;DR: 该论文综述了平面几何问题求解（PGPS）领域的研究现状，总结了编码器-解码器框架下的方法分类，并指出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 平面几何问题求解（PGPS）作为评估多模态推理能力的基准受到关注，但缺乏系统性综述，本文旨在填补这一空白。

Method: 将PGPS方法归类为编码器-解码器框架，总结编码器和解码器的输出格式，并分析其架构设计。

Result: 提出了PGPS领域的分类框架，并揭示了编码阶段的幻觉问题和当前基准中的数据泄漏问题。

Conclusion: 论文总结了PGPS研究的现状，并讨论了未来研究的主要挑战和方向，特别是编码器-解码器架构中的幻觉问题和数据泄漏问题。

Abstract: Plane geometry problem solving (PGPS) has recently gained significant
attention as a benchmark to assess the multi-modal reasoning capabilities of
large vision-language models. Despite the growing interest in PGPS, the
research community still lacks a comprehensive overview that systematically
synthesizes recent work in PGPS. To fill this gap, we present a survey of
existing PGPS studies. We first categorize PGPS methods into an encoder-decoder
framework and summarize the corresponding output formats used by their encoders
and decoders. Subsequently, we classify and analyze these encoders and decoders
according to their architectural designs. Finally, we outline major challenges
and promising directions for future research. In particular, we discuss the
hallucination issues arising during the encoding phase within encoder-decoder
architectures, as well as the problem of data leakage in current PGPS
benchmarks.

</details>


### [485] [Vid2World: Crafting Video Diffusion Models to Interactive World Models](https://arxiv.org/abs/2505.14357)
*Siqiao Huang,Jialong Wu,Qixing Zhou,Shangchen Miao,Mingsheng Long*

Main category: cs.CV

TL;DR: Vid2World将预训练视频扩散模型转化为交互式世界模型，通过因果化架构和动作引导机制提升可控性，在机器人操作和游戏模拟中验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 现有世界模型需要大量领域特定训练且预测粗糙，而视频扩散模型能生成高质量视频但缺乏交互性。本文旨在结合两者优势，构建通用、高保真的交互世界模型。

Method: 提出Vid2World方法：1) 对预训练视频扩散模型进行因果化改造，支持自回归生成；2) 引入因果动作引导机制增强动作可控性。

Result: 在机器人操作和游戏模拟领域的实验表明，该方法能有效迁移视频扩散模型能力，构建可交互的高保真世界模型。

Conclusion: Vid2World为利用大规模预训练视频模型构建交互世界模型提供了可扩展方案，突破了传统方法的领域限制和保真度瓶颈。

Abstract: World models, which predict transitions based on history observation and
action sequences, have shown great promise in improving data efficiency for
sequential decision making. However, existing world models often require
extensive domain-specific training and still produce low-fidelity, coarse
predictions, limiting their applicability in complex environments. In contrast,
video diffusion models trained on large, internet-scale datasets have
demonstrated impressive capabilities in generating high-quality videos that
capture diverse real-world dynamics. In this work, we present Vid2World, a
general approach for leveraging and transferring pre-trained video diffusion
models into interactive world models. To bridge the gap, Vid2World performs
casualization of a pre-trained video diffusion model by crafting its
architecture and training objective to enable autoregressive generation.
Furthermore, it introduces a causal action guidance mechanism to enhance action
controllability in the resulting interactive world model. Extensive experiments
in robot manipulation and game simulation domains show that our method offers a
scalable and effective approach for repurposing highly capable video diffusion
models to interactive world models.

</details>


### [486] [Enhancing Interpretability of Sparse Latent Representations with Class Information](https://arxiv.org/abs/2505.14476)
*Farshad Sangari Abiz,Reshad Hosseini,Babak N. Araabi*

Main category: cs.CV

TL;DR: 该论文提出了一种改进变分自编码器（VAE）潜在空间可解释性的方法，通过确保同一类别样本在潜在空间中激活的维度一致，从而增强潜在结构的解释性。


<details>
  <summary>Details</summary>
Motivation: 标准VAE生成的潜在空间分散且无结构，限制了其可解释性。虽然变分稀疏编码（VSC）通过引入稀疏潜在表示提高了可解释性，但未能实现同一类别样本间潜在维度的结构化一致性。

Method: 提出了一种新的损失函数，鼓励同一类别样本共享相似的激活维度，从而在潜在空间中形成结构化模式，每个共享维度对应一个高级概念或“因子”。

Result: 该方法不仅捕获了跨类别的全局因子，还捕捉了类别特异性因子，显著提升了潜在表示的实用性和可解释性。

Conclusion: 通过结构化潜在空间的激活维度，该方法为生成模型提供了更清晰、更具解释性的潜在表示，同时支持全局和类别特异性因子的识别。

Abstract: Variational Autoencoders (VAEs) are powerful generative models for learning
latent representations. Standard VAEs generate dispersed and unstructured
latent spaces by utilizing all dimensions, which limits their interpretability,
especially in high-dimensional spaces. To address this challenge, Variational
Sparse Coding (VSC) introduces a spike-and-slab prior distribution, resulting
in sparse latent representations for each input. These sparse representations,
characterized by a limited number of active dimensions, are inherently more
interpretable. Despite this advantage, VSC falls short in providing structured
interpretations across samples within the same class. Intuitively, samples from
the same class are expected to share similar attributes while allowing for
variations in those attributes. This expectation should manifest as consistent
patterns of active dimensions in their latent representations, but VSC does not
enforce such consistency.
  In this paper, we propose a novel approach to enhance the latent space
interpretability by ensuring that the active dimensions in the latent space are
consistent across samples within the same class. To achieve this, we introduce
a new loss function that encourages samples from the same class to share
similar active dimensions. This alignment creates a more structured and
interpretable latent space, where each shared dimension corresponds to a
high-level concept, or "factor." Unlike existing disentanglement-based methods
that primarily focus on global factors shared across all classes, our method
captures both global and class-specific factors, thereby enhancing the utility
and interpretability of latent representations.

</details>


### [487] [Instance Segmentation for Point Sets](https://arxiv.org/abs/2505.14583)
*Abhimanyu Talwar,Julien Laasri*

Main category: cs.CV

TL;DR: 该论文提出两种基于采样的方法，用于解决SGPN在3D点集实例分割中内存消耗大的问题。


<details>
  <summary>Details</summary>
Motivation: SGPN在3D点集实例分割中使用了内存密集的相似度矩阵，导致内存占用随点数平方增长，亟需改进。

Method: 采用两种采样方法：先在子采样点集上计算实例分割，再通过最近邻方法将标签扩展到完整点集。

Result: 两种方法在大子采样集上表现相当，但随机采样策略在速度和内存使用上提升更显著。

Conclusion: 随机采样策略能有效降低内存占用并提升速度，为3D点集实例分割提供了实用解决方案。

Abstract: Recently proposed neural network architectures like PointNet [QSMG16] and
PointNet++ [QYSG17] have made it possible to apply Deep Learning to 3D point
sets. The feature representations of shapes learned by these two networks
enabled training classifiers for Semantic Segmentation, and more recently for
Instance Segmentation via the Similarity Group Proposal Network (SGPN)
[WYHN17]. One area of improvement which has been highlighted by SGPN's authors,
pertains to use of memory intensive similarity matrices which occupy memory
quadratic in the number of points. In this report, we attempt to tackle this
issue through use of two sampling based methods, which compute Instance
Segmentation on a sub-sampled Point Set, and then extrapolate labels to the
complete set using the nearest neigbhour approach. While both approaches
perform equally well on large sub-samples, the random-based strategy gives the
most improvements in terms of speed and memory usage.

</details>


### [488] [CAD-Coder: An Open-Source Vision-Language Model for Computer-Aided Design Code Generation](https://arxiv.org/abs/2505.14646)
*Anna C. Doris,Md Ferdous Alam,Amin Heyrani Nobari,Faez Ahmed*

Main category: cs.CV

TL;DR: CAD-Coder是一个开源视觉语言模型，通过微调直接从视觉输入生成可编辑的CAD代码，显著提升了CAD工作流的效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 当前手动创建CAD模型耗时且需要专业知识，现有AI模型在CAD操作表示、泛化能力和输出准确性方面存在局限。

Method: 利用新构建的GenCAD-Code数据集（包含16.3万对CAD模型图像和代码），微调视觉语言模型生成CadQuery Python代码。

Result: CAD-Coder在语法有效率和3D实体相似度上超越GPT-4.5等基线模型，并能处理未见过的CAD操作和真实世界图像。

Conclusion: 研究表明，针对代码微调的VLM有潜力简化工程师的CAD工作流程，CAD-Coder已开源。

Abstract: Efficient creation of accurate and editable 3D CAD models is critical in
engineering design, significantly impacting cost and time-to-market in product
innovation. Current manual workflows remain highly time-consuming and demand
extensive user expertise. While recent developments in AI-driven CAD generation
show promise, existing models are limited by incomplete representations of CAD
operations, inability to generalize to real-world images, and low output
accuracy. This paper introduces CAD-Coder, an open-source Vision-Language Model
(VLM) explicitly fine-tuned to generate editable CAD code (CadQuery Python)
directly from visual input. Leveraging a novel dataset that we
created--GenCAD-Code, consisting of over 163k CAD-model image and code
pairs--CAD-Coder outperforms state-of-the-art VLM baselines such as GPT-4.5 and
Qwen2.5-VL-72B, achieving a 100% valid syntax rate and the highest accuracy in
3D solid similarity. Notably, our VLM demonstrates some signs of
generalizability, successfully generating CAD code from real-world images and
executing CAD operations unseen during fine-tuning. The performance and
adaptability of CAD-Coder highlights the potential of VLMs fine-tuned on code
to streamline CAD workflows for engineers and designers. CAD-Coder is publicly
available at: https://github.com/anniedoris/CAD-Coder.

</details>


### [489] [3D Reconstruction from Sketches](https://arxiv.org/abs/2505.14621)
*Abhimanyu Talwar,Julien Laasri*

Main category: cs.CV

TL;DR: 提出了一种从多张草图重建3D场景的流程，包括草图拼接、CycleGAN转换和深度图估计，尽管拼接效果一般，但整体流程在单张草图上表现良好。


<details>
  <summary>Details</summary>
Motivation: 解决从手绘草图重建3D场景的问题，特别是在缺乏真实图像数据的情况下，通过合成数据训练模型来实现这一目标。

Method: 流程分为三步：(1)通过对应点拼接多张草图，(2)使用CycleGAN将拼接后的草图转换为真实图像，(3)利用预训练的MegaDepth网络估计深度图。

Result: 拼接过程对真实草图泛化能力有限，但整体流程在单张草图的3D重建上表现良好，适用于多种类型的草图。

Conclusion: 尽管草图拼接存在局限性，但提出的流程在单张草图的3D重建任务中表现出色，展示了合成数据训练模型的潜力。

Abstract: We consider the problem of reconstructing a 3D scene from multiple sketches.
We propose a pipeline which involves (1) stitching together multiple sketches
through use of correspondence points, (2) converting the stitched sketch into a
realistic image using a CycleGAN, and (3) estimating that image's depth-map
using a pre-trained convolutional neural network based architecture called
MegaDepth. Our contribution includes constructing a dataset of image-sketch
pairs, the images for which are from the Zurich Building Database, and sketches
have been generated by us. We use this dataset to train a CycleGAN for our
pipeline's second step. We end up with a stitching process that does not
generalize well to real drawings, but the rest of the pipeline that creates a
3D reconstruction from a single sketch performs quite well on a wide variety of
drawings.

</details>


### [490] [AKRMap: Adaptive Kernel Regression for Trustworthy Visualization of Cross-Modal Embeddings](https://arxiv.org/abs/2505.14664)
*Yilin Ye,Junchao Huang,Xingchen Zeng,Jiazhi Xia,Wei Zeng*

Main category: cs.CV

TL;DR: 本文提出AKRMap，一种新的降维技术，用于更准确地可视化跨模态嵌入度量。


<details>
  <summary>Details</summary>
Motivation: 传统降维方法如PCA和t-SNE主要关注单模态内的特征分布，无法有效捕捉跨模态度量（如CLIPScore）。

Method: AKRMap通过学习投影空间中度量景观的核回归，构建有监督的投影网络，并采用可联合优化的自适应广义核。

Result: 定量实验表明，AKRMap在生成更准确、可信的可视化效果上优于现有降维方法。

Conclusion: AKRMap能有效可视化并比较文本到图像模型的跨模态嵌入，支持交互式探索。

Abstract: Cross-modal embeddings form the foundation for multi-modal models. However,
visualization methods for interpreting cross-modal embeddings have been
primarily confined to traditional dimensionality reduction (DR) techniques like
PCA and t-SNE. These DR methods primarily focus on feature distributions within
a single modality, whilst failing to incorporate metrics (e.g., CLIPScore)
across multiple modalities.This paper introduces AKRMap, a new DR technique
designed to visualize cross-modal embeddings metric with enhanced accuracy by
learning kernel regression of the metric landscape in the projection space.
Specifically, AKRMap constructs a supervised projection network guided by a
post-projection kernel regression loss, and employs adaptive generalized
kernels that can be jointly optimized with the projection. This approach
enables AKRMap to efficiently generate visualizations that capture complex
metric distributions, while also supporting interactive features such as zoom
and overlay for deeper exploration. Quantitative experiments demonstrate that
AKRMap outperforms existing DR methods in generating more accurate and
trustworthy visualizations. We further showcase the effectiveness of AKRMap in
visualizing and comparing cross-modal embeddings for text-to-image models. Code
and demo are available at https://github.com/yilinye/AKRMap.

</details>


### [491] [Training-Free Watermarking for Autoregressive Image Generation](https://arxiv.org/abs/2505.14673)
*Yu Tong,Zihao Pan,Shuai Yang,Kaiyang Zhou*

Main category: cs.CV

TL;DR: 提出IndexMark，一种针对自回归图像生成模型的无训练水印框架，利用码本冗余特性嵌入水印且不影响图像质量，验证效果优异且抗干扰强。


<details>
  <summary>Details</summary>
Motivation: 现有生成水印方法主要针对扩散模型，而自回归图像生成模型的水印技术研究不足，需要开发有效解决方案。

Method: 基于码本冗余特性，通过匹配替换策略选择相似水印标记嵌入，结合索引编码器和辅助验证方案提升鲁棒性。

Result: 实验表明IndexMark在图像质量和验证准确率上达到SOTA，并对裁剪、噪声等多种干扰具有鲁棒性。

Conclusion: IndexMark为自回归模型提供高效水印方案，平衡不可见性与鲁棒性，拓展了生成水印的应用场景。

Abstract: Invisible image watermarking can protect image ownership and prevent
malicious misuse of visual generative models. However, existing generative
watermarking methods are mainly designed for diffusion models while
watermarking for autoregressive image generation models remains largely
underexplored. We propose IndexMark, a training-free watermarking framework for
autoregressive image generation models. IndexMark is inspired by the redundancy
property of the codebook: replacing autoregressively generated indices with
similar indices produces negligible visual differences. The core component in
IndexMark is a simple yet effective match-then-replace method, which carefully
selects watermark tokens from the codebook based on token similarity, and
promotes the use of watermark tokens through token replacement, thereby
embedding the watermark without affecting the image quality. Watermark
verification is achieved by calculating the proportion of watermark tokens in
generated images, with precision further improved by an Index Encoder.
Furthermore, we introduce an auxiliary validation scheme to enhance robustness
against cropping attacks. Experiments demonstrate that IndexMark achieves
state-of-the-art performance in terms of image quality and verification
accuracy, and exhibits robustness against various perturbations, including
cropping, noises, Gaussian blur, random erasing, color jittering, and JPEG
compression.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [492] [Learning Wavelet-Sparse FDK for 3D Cone-Beam CT Reconstruction](https://arxiv.org/abs/2505.13579)
*Yipeng Sun,Linda-Sophie Schneider,Chengze Ye,Mingxuan Gu,Siyuan Mei,Siming Bayer,Andreas Maier*

Main category: eess.IV

TL;DR: 提出了一种基于FDK算法的改进神经网络方法，通过选择性集成可训练元素和利用小波变换稀疏化参数，在保持算法可解释性的同时提升图像质量并减少计算复杂度。


<details>
  <summary>Details</summary>
Motivation: FDK算法在CBCT重建中效率高但易受噪声和伪影影响，现有深度学习方法虽提升图像质量但计算复杂且缺乏可解释性。

Method: 在FDK算法的余弦加权和滤波阶段选择性集成可训练元素，并利用小波变换创建稀疏表示以减少参数数量。

Result: 参数数量减少93.75%，保持推理计算成本与FDK算法相当，提升抗噪性且易于集成到现有CT重建流程。

Conclusion: 该方法在保持FDK算法可解释性和计算效率的同时，显著提升了图像质量和抗噪性，适用于计算资源有限的临床环境。

Abstract: Cone-Beam Computed Tomography (CBCT) is essential in medical imaging, and the
Feldkamp-Davis-Kress (FDK) algorithm is a popular choice for reconstruction due
to its efficiency. However, FDK is susceptible to noise and artifacts. While
recent deep learning methods offer improved image quality, they often increase
computational complexity and lack the interpretability of traditional methods.
In this paper, we introduce an enhanced FDK-based neural network that maintains
the classical algorithm's interpretability by selectively integrating trainable
elements into the cosine weighting and filtering stages. Recognizing the
challenge of a large parameter space inherent in 3D CBCT data, we leverage
wavelet transformations to create sparse representations of the cosine weights
and filters. This strategic sparsification reduces the parameter count by
$93.75\%$ without compromising performance, accelerates convergence, and
importantly, maintains the inference computational cost equivalent to the
classical FDK algorithm. Our method not only ensures volumetric consistency and
boosts robustness to noise, but is also designed for straightforward
integration into existing CT reconstruction pipelines. This presents a
pragmatic enhancement that can benefit clinical applications, particularly in
environments with computational limitations.

</details>


### [493] [XDementNET: An Explainable Attention Based Deep Convolutional Network to Detect Alzheimer Progression from MRI data](https://arxiv.org/abs/2505.13906)
*Soyabul Islam Lincoln,Mirza Mohd Shahriar Maswood*

Main category: eess.IV

TL;DR: 该论文提出了一种结合多残差块、空间注意力机制和分组查询注意力的新型深度学习架构，用于阿尔茨海默病（AD）的精准诊断，并在多个公开数据集上取得了极高的分类准确率。


<details>
  <summary>Details</summary>
Motivation: 随着医疗成本上升和人工智能在医学诊断中的应用扩展，阿尔茨海默病需要更精确的诊断和高效治疗方法。结合脑部MRI和深度神经网络的研究显示出良好前景。

Method: 论文提出了一种新型深度学习架构，整合了多残差块、专用空间注意力块、分组查询注意力和多头注意力机制，并在多个公开数据集上进行了性能评估。

Result: 在Kaggle数据集上，模型在4类、3类和二分类任务中分别达到99.66%、99.63%和100%的准确率；在OASIS数据集上分别为99.92%、99.90%和99.95%；在ADNI-1数据集的不同平面上也表现优异。

Conclusion: 该模型在AD阶段分类中表现出色，能够从MRI图像中提取重要信息，其性能优于当前最先进的方法，展示了在AD诊断中的巨大潜力。

Abstract: A common neurodegenerative disease, Alzheimer's disease requires a precise
diagnosis and efficient treatment, particularly in light of escalating
healthcare expenses and the expanding use of artificial intelligence in medical
diagnostics. Many recent studies shows that the combination of brain Magnetic
Resonance Imaging (MRI) and deep neural networks have achieved promising
results for diagnosing AD. Using deep convolutional neural networks, this paper
introduces a novel deep learning architecture that incorporates multiresidual
blocks, specialized spatial attention blocks, grouped query attention, and
multi-head attention. The study assessed the model's performance on four
publicly accessible datasets and concentrated on identifying binary and
multiclass issues across various categories. This paper also takes into account
of the explainability of AD's progression and compared with state-of-the-art
methods namely Gradient Class Activation Mapping (GradCAM), Score-CAM, Faster
Score-CAM, and XGRADCAM. Our methodology consistently outperforms current
approaches, achieving 99.66\% accuracy in 4-class classification, 99.63\% in
3-class classification, and 100\% in binary classification using Kaggle
datasets. For Open Access Series of Imaging Studies (OASIS) datasets the
accuracies are 99.92\%, 99.90\%, and 99.95\% respectively. The Alzheimer's
Disease Neuroimaging Initiative-1 (ADNI-1) dataset was used for experiments in
three planes (axial, sagittal, and coronal) and a combination of all planes.
The study achieved accuracies of 99.08\% for axis, 99.85\% for sagittal, 99.5\%
for coronal, and 99.17\% for all axis, and 97.79\% and 8.60\% respectively for
ADNI-2. The network's ability to retrieve important information from MRI images
is demonstrated by its excellent accuracy in categorizing AD stages.

</details>


### [494] [Bronchovascular Tree-Guided Weakly Supervised Learning Method for Pulmonary Segment Segmentation](https://arxiv.org/abs/2505.13911)
*Ruijie Zhao,Zuopeng Tan,Xiao Xue,Longfei Zhao,Bing Li,Zicheng Liao,Ying Ming,Jiaru Wang,Ran Xiao,Sirong Piao,Rui Zhao,Qiqi Xu,Wei Song*

Main category: eess.IV

TL;DR: 提出了一种基于解剖层次监督学习（AHSL）的弱监督方法，用于肺段分割，通过结合段级和叶级监督以及支气管血管先验信息，有效提升了分割精度和边界平滑度。


<details>
  <summary>Details</summary>
Motivation: 肺段分割对于癌症定位和手术规划至关重要，但由于医学图像中肺段边界难以区分，像素级标注耗时耗力。因此，需要一种弱监督学习方法以减少标注负担并提高分割效果。

Method: 采用解剖层次监督学习（AHSL），结合段级和叶级监督，设计了两阶段分割策略，引入支气管血管先验信息，并提出一致性损失以优化边界平滑度。

Result: 在私有数据集上的实验表明，该方法在视觉检查和评估指标上均表现出色，有效提升了肺段分割的准确性和边界平滑度。

Conclusion: AHSL方法通过弱监督学习和解剖层次监督，显著提高了肺段分割的效果，为临床应用提供了有力支持。

Abstract: Pulmonary segment segmentation is crucial for cancer localization and
surgical planning. However, the pixel-wise annotation of pulmonary segments is
laborious, as the boundaries between segments are indistinguishable in medical
images. To this end, we propose a weakly supervised learning (WSL) method,
termed Anatomy-Hierarchy Supervised Learning (AHSL), which consults the precise
clinical anatomical definition of pulmonary segments to perform pulmonary
segment segmentation. Since pulmonary segments reside within the lobes and are
determined by the bronchovascular tree, i.e., artery, airway and vein, the
design of the loss function is founded on two principles. First, segment-level
labels are utilized to directly supervise the output of the pulmonary segments,
ensuring that they accurately encompass the appropriate bronchovascular tree.
Second, lobe-level supervision indirectly oversees the pulmonary segment,
ensuring their inclusion within the corresponding lobe. Besides, we introduce a
two-stage segmentation strategy that incorporates bronchovascular priori
information. Furthermore, a consistency loss is proposed to enhance the
smoothness of segment boundaries, along with an evaluation metric designed to
measure the smoothness of pulmonary segment boundaries. Visual inspection and
evaluation metrics from experiments conducted on a private dataset demonstrate
the effectiveness of our method.

</details>


### [495] [NOVA: A Benchmark for Anomaly Localization and Clinical Reasoning in Brain MRI](https://arxiv.org/abs/2505.14064)
*Cosmin I. Bercea,Jun Li,Philipp Raffler,Evamaria O. Riedel,Lena Schmitzer,Angela Kurz,Felix Bitzer,Paula Roßmüller,Julian Canisius,Mirjam L. Beyrle,Che Liu,Wenjia Bai,Bernhard Kainz,Julia A. Schnabel,Benedikt Wiestler*

Main category: eess.IV

TL;DR: 论文提出了NOVA基准测试，用于评估模型在真实世界极端分布偏移下的泛化能力，特别是在医学影像中对罕见异常的检测和推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有模型在训练数据与真实世界输入存在分布差异时表现不佳，尤其在医学影像中，罕见或全新病症的检测和识别成为挑战。

Method: 构建NOVA基准测试，包含约900个脑部MRI扫描，涵盖281种罕见病理和异构采集协议，提供临床叙述和专家标注，用于评估异常定位、视觉描述和诊断推理。

Result: 主流视觉语言模型（如GPT-4o、Gemini 2.0 Flash和Qwen2.5-VL-72B）在NOVA上表现显著下降，显示现有模型在极端分布偏移下的局限性。

Conclusion: NOVA作为一个严格的测试平台，揭示了现有模型在真实世界医学影像应用中的不足，为未来模型改进提供了方向。

Abstract: In many real-world applications, deployed models encounter inputs that differ
from the data seen during training. Out-of-distribution detection identifies
whether an input stems from an unseen distribution, while open-world
recognition flags such inputs to ensure the system remains robust as
ever-emerging, previously $unknown$ categories appear and must be addressed
without retraining. Foundation and vision-language models are pre-trained on
large and diverse datasets with the expectation of broad generalization across
domains, including medical imaging. However, benchmarking these models on test
sets with only a few common outlier types silently collapses the evaluation
back to a closed-set problem, masking failures on rare or truly novel
conditions encountered in clinical use.
  We therefore present $NOVA$, a challenging, real-life $evaluation-only$
benchmark of $\sim$900 brain MRI scans that span 281 rare pathologies and
heterogeneous acquisition protocols. Each case includes rich clinical
narratives and double-blinded expert bounding-box annotations. Together, these
enable joint assessment of anomaly localisation, visual captioning, and
diagnostic reasoning. Because NOVA is never used for training, it serves as an
$extreme$ stress-test of out-of-distribution generalisation: models must bridge
a distribution gap both in sample appearance and in semantic space. Baseline
results with leading vision-language models (GPT-4o, Gemini 2.0 Flash, and
Qwen2.5-VL-72B) reveal substantial performance drops across all tasks,
establishing NOVA as a rigorous testbed for advancing models that can detect,
localize, and reason about truly unknown anomalies.

</details>


### [496] [Automated Fetal Biometry Assessment with Deep Ensembles using Sparse-Sampling of 2D Intrapartum Ultrasound Images](https://arxiv.org/abs/2505.14572)
*Jayroop Ramesh,Valentin Bacher,Mark C. Eid,Hoda Kalabizadeh,Christian Rupprecht,Ana IL Namburete,Pak-Hei Yeung,Madeleine K. Wyburd,Nicola K. Dinsdale*

Main category: eess.IV

TL;DR: 该论文提出了一种自动化胎儿生物测量流程，用于减少观察者间差异并提高测量可靠性，通过分类、分割和计算关键参数来预测分娩结果。


<details>
  <summary>Details</summary>
Motivation: 国际超声学会提倡使用产时超声监测分娩进程，但现有方法存在观察者间差异和测量可靠性问题。本研究旨在通过自动化流程解决这些问题。

Method: 研究提出三阶段流程：1) 从超声视频中分类标准平面；2) 分割胎儿头部和耻骨联合；3) 计算关键参数（AoP和HSD）。采用稀疏采样和集成深度学习提高泛化性。

Result: 在4名患者和224帧超声图像上，模型取得ACC: 0.9452，F1: 0.9225等优异指标，AoP和HSD误差分别为8.90和14.35。

Conclusion: 自动化流程可提高产程停滞原因分析的准确性，并为临床风险分层工具开发提供支持，从而实现高效产前护理。

Abstract: The International Society of Ultrasound advocates Intrapartum Ultrasound (US)
Imaging in Obstetrics and Gynecology (ISUOG) to monitor labour progression
through changes in fetal head position. Two reliable ultrasound-derived
parameters that are used to predict outcomes of instrumental vaginal delivery
are the angle of progression (AoP) and head-symphysis distance (HSD). In this
work, as part of the Intrapartum Ultrasounds Grand Challenge (IUGC) 2024, we
propose an automated fetal biometry measurement pipeline to reduce intra- and
inter-observer variability and improve measurement reliability. Our pipeline
consists of three key tasks: (i) classification of standard planes (SP) from US
videos, (ii) segmentation of fetal head and pubic symphysis from the detected
SPs, and (iii) computation of the AoP and HSD from the segmented regions. We
perform sparse sampling to mitigate class imbalances and reduce spurious
correlations in task (i), and utilize ensemble-based deep learning methods for
task (i) and (ii) to enhance generalizability under different US acquisition
settings. Finally, to promote robustness in task iii) with respect to the
structural fidelity of measurements, we retain the largest connected components
and apply ellipse fitting to the segmentations. Our solution achieved ACC:
0.9452, F1: 0.9225, AUC: 0.983, MCC: 0.8361, DSC: 0.918, HD: 19.73, ASD: 5.71,
$\Delta_{AoP}$: 8.90 and $\Delta_{HSD}$: 14.35 across an unseen hold-out set of
4 patients and 224 US frames. The results from the proposed automated pipeline
can improve the understanding of labour arrest causes and guide the development
of clinical risk stratification tools for efficient and effective prenatal
care.

</details>


<div id='astro-ph.EP'></div>

# astro-ph.EP [[Back]](#toc)

### [497] [ThermoONet -- a deep learning-based small body thermophysical network: applications to modelling water activity of comets](https://arxiv.org/abs/2505.14016)
*Shunjing Zhao,Xian Shi,Hanlun Lei*

Main category: astro-ph.EP

TL;DR: 论文提出了一种名为ThermoONet的机器学习模型，用于高效预测彗星的地下温度和水冰升华通量，显著降低了计算时间并保持了高精度。


<details>
  <summary>Details</summary>
Motivation: 传统的小天体热物理模型数值解法计算量大，难以满足高分辨率或重复建模的需求，因此需要一种更高效的方法来研究彗星活动。

Method: 采用机器学习方法开发了ThermoONet神经网络，用于预测彗星的地下温度和水冰升华通量，并结合全局优化算法反演目标天体的物理特性。

Result: ThermoONet在预测地下温度时平均误差约为2%，计算时间减少了近六个数量级，并成功拟合了67P和21P彗星的水生成率曲线。

Conclusion: ThermoONet在彗星热物理建模中表现出高效性和准确性，为彗星活动研究提供了有力工具。

Abstract: Cometary activity is a compelling subject of study, with thermophysical
models playing a pivotal role in its understanding. However, traditional
numerical solutions for small body thermophysical models are computationally
intensive, posing challenges for investigations requiring high-resolution or
repetitive modeling. To address this limitation, we employed a machine learning
approach to develop ThermoONet - a neural network designed to predict the
temperature and water ice sublimation flux of comets. Performance evaluations
indicate that ThermoONet achieves a low average error in subsurface temperature
of approximately 2% relative to the numerical simulation, while reducing
computational time by nearly six orders of magnitude. We applied ThermoONet to
model the water activity of comets 67P/Churyumov-Gerasimenko and
21P/Giacobini-Zinner. By successfully fitting the water production rate curves
of these comets, as obtained by the Rosetta mission and the SOHO telescope,
respectively, we demonstrate the network's effectiveness and efficiency.
Furthermore, when combined with a global optimization algorithm, ThermoONet
proves capable of retrieving the physical properties of target bodies.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [498] [Trustworthy Reputation Games and Applications to Proof-of-Reputation Blockchains](https://arxiv.org/abs/2505.14551)
*Petros Drineas,Rohit Nema,Rafail Ostrovsky,Vassilis Zikas*

Main category: cs.GT

TL;DR: 该论文提出了一种新的可信声誉系统模型，通过博弈论方法确保用户真实报告对服务器可信度的评估，并应用于PoR区块链中。


<details>
  <summary>Details</summary>
Motivation: 当前去中心化账本（如区块链）中的声誉系统易受操纵，缺乏经济稳健性的博弈论支持。

Method: 设计了一类称为'可信声誉博弈'的游戏，结合PageRank算法，激励用户按真实信念报告服务器可信度。

Result: 证明了在用户信念接近真实可信度时，系统能通过纳什均衡策略估算服务器的相对可信度。

Conclusion: 该模型为PoR区块链提供了理论基础，实现了抗操纵且经济稳健的可信声誉系统。

Abstract: Reputation systems play an essential role in the Internet era, as they enable
people to decide whom to trust, by collecting and aggregating data about users'
behavior. Recently, several works proposed the use of reputation for the design
and scalability improvement of decentralized (blockchain) ledgers; however,
such systems are prone to manipulation and to our knowledge no game-theoretic
treatment exists that can support their economic robustness.
  In this work we put forth a new model for the design of what we call, {\em
trustworthy reputation systems}. Concretely, we describe a class of games,
which we term {\em trustworthy reputation games}, that enable a set of users to
report a function of their beliefs about the trustworthiness of each server in
a set -- i.e., their estimate of the probability that this server will behave
according to its specified strategy -- in a way that satisfies the following
properties:
  1. It is $(\epsilon$-)best response for any rational user in the game to play
a prescribed (truthful) strategy according to their true belief.
  2. Assuming that the users' beliefs are not too far from the {\em true}
trustworthiness of the servers, playing the above ($\epsilon-$)Nash equilibrium
allows anyone who observes the users' strategies to estimate the relative
trustworthiness of any two servers.
  Our utilities and decoding function build on a connection between the well
known PageRank algorithm and the problem of trustworthiness discovery, which
can be of independent interest. Finally, we show how the above games are
motivated by and can be leveraged in proof-of-reputation (PoR) blockchains.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [499] [Robust learning of halfspaces under log-concave marginals](https://arxiv.org/abs/2505.13708)
*Jane Lange,Arsen Vasilyan*

Main category: cs.DS

TL;DR: 本文提出一种算法，能够高效学习具有小边界体积的线性阈值分类器，对抗扰动具有鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 研究如何在对抗性扰动下，高效学习具有小边界体积的分类器，特别是在输入服从亚高斯各向同性对数凹分布时。

Method: 算法在多项式回归基础上增加了三个步骤：带噪声敏感度约束的ℓ₁误差回归、结构化分区与舍入、以及局部校正平滑。

Result: 算法在扰动半径r下，学习到的分类器边界体积为O(r+ε)，时间和样本复杂度与多项式回归相当。

Conclusion: 通过增强多项式回归方法，实现了对抗鲁棒性分类器的高效学习，边界体积可控。

Abstract: We say that a classifier is \emph{adversarially robust} to perturbations of
norm $r$ if, with high probability over a point $x$ drawn from the input
distribution, there is no point within distance $\le r$ from $x$ that is
classified differently. The \emph{boundary volume} is the probability that a
point falls within distance $r$ of a point with a different label. This work
studies the task of computationally efficient learning of hypotheses with small
boundary volume, where the input is distributed as a subgaussian isotropic
log-concave distribution over $\mathbb{R}^d$.
  Linear threshold functions are adversarially robust; they have boundary
volume proportional to $r$. Such concept classes are efficiently learnable by
polynomial regression, which produces a polynomial threshold function (PTF),
but PTFs in general may have boundary volume $\Omega(1)$, even for $r \ll 1$.
  We give an algorithm that agnostically learns linear threshold functions and
returns a classifier with boundary volume $O(r+\varepsilon)$ at radius of
perturbation $r$. The time and sample complexity of
$d^{\tilde{O}(1/\varepsilon^2)}$ matches the complexity of polynomial
regression.
  Our algorithm augments the classic approach of polynomial regression with
three additional steps: a) performing the $\ell_1$-error regression under noise
sensitivity constraints, b) a structured partitioning and rounding step that
returns a Boolean classifier with error $\textsf{opt} + O(\varepsilon)$ and
noise sensitivity $O(r+\varepsilon)$ simultaneously, and c) a local corrector
that ``smooths'' a function with low noise sensitivity into a function that is
adversarially robust.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [500] [Q${}^2$Forge: Minting Competency Questions and SPARQL Queries for Question-Answering Over Knowledge Graphs](https://arxiv.org/abs/2505.13572)
*Yousouf Taghzouti,Franck Michel,Tao Jiang,Louis-Félix Nothias,Fabien Gandon*

Main category: cs.DB

TL;DR: Q²Forge是一个开源工具，通过迭代验证和LLM辅助，自动生成知识图谱的SPARQL查询和对应问题，解决非专家用户查询难题。


<details>
  <summary>Details</summary>
Motivation: SPARQL查询对非专家用户具有挑战性，现有知识图谱缺乏足够的示例查询。需要自动化工具生成高质量查询-问题对以支持LLM训练和应用开发。

Method: 提出模块化框架Q²Forge，包含问题生成、查询生成和迭代优化（结合人类反馈和LLM评估）三个可独立替换的组件。

Result: 实现从自然语言问题到SPARQL查询的完整生成流程，支持为任意知识图谱创建参考查询集，工具具有通用性和可扩展性。

Conclusion: Q²Forge通过自动化生成和验证流程，显著降低了知识图谱查询门槛，为LLM训练和用户查询提供了实用解决方案。

Abstract: The SPARQL query language is the standard method to access knowledge graphs
(KGs). However, formulating SPARQL queries is a significant challenge for
non-expert users, and remains time-consuming for the experienced ones. Best
practices recommend to document KGs with competency questions and example
queries to contextualise the knowledge they contain and illustrate their
potential applications. In practice, however, this is either not the case or
the examples are provided in limited numbers. Large Language Models (LLMs) are
being used in conversational agents and are proving to be an attractive
solution with a wide range of applications, from simple question-answering
about common knowledge to generating code in a targeted programming language.
However, training and testing these models to produce high quality SPARQL
queries from natural language questions requires substantial datasets of
question-query pairs. In this paper, we present Q${}^2$Forge that addresses the
challenge of generating new competency questions for a KG and corresponding
SPARQL queries. It iteratively validates those queries with human feedback and
LLM as a judge. Q${}^2$Forge is open source, generic, extensible and modular,
meaning that the different modules of the application (CQ generation, query
generation and query refinement) can be used separately, as an integrated
pipeline, or replaced by alternative services. The result is a complete
pipeline from competency question formulation to query evaluation, supporting
the creation of reference query sets for any target KG.

</details>


### [501] [Abacus: A Cost-Based Optimizer for Semantic Operator Systems](https://arxiv.org/abs/2505.14661)
*Matthew Russo,Sivaprasad Sudhir,Gerardo Vitagliano,Chunwei Liu,Tim Kraska,Samuel Madden,Michael Cafarella*

Main category: cs.DB

TL;DR: Abacus是一个可扩展的基于成本的优化器，用于优化由语义操作符组成的LLM数据处理应用，显著提升质量、降低成本与延迟。


<details>
  <summary>Details</summary>
Motivation: 现有语义操作符系统在基准测试中表现优异，但优化困难，缺乏能全局优化质量、成本或延迟的约束优化器。

Method: Abacus通过验证示例和先验性能评估，搜索语义操作符系统的最佳实现方案，支持多目标约束优化。

Result: 在生物医学、法律领域及多模态问答任务中，Abacus优化后的系统质量提升18.7%-39.2%，成本降低23.6倍，延迟减少4.2倍。

Conclusion: Abacus有效解决了语义操作符系统的优化难题，为LLM数据处理应用提供了高效的约束优化方案。

Abstract: LLMs enable an exciting new class of data processing applications over large
collections of unstructured documents. Several new programming frameworks have
enabled developers to build these applications by composing them out of
semantic operators: a declarative set of AI-powered data transformations with
natural language specifications. These include LLM-powered maps, filters,
joins, etc. used for document processing tasks such as information extraction,
summarization, and more. While systems of semantic operators have achieved
strong performance on benchmarks, they can be difficult to optimize. An
optimizer for this setting must determine how to physically implement each
semantic operator in a way that optimizes the system globally. Existing
optimizers are limited in the number of optimizations they can apply, and most
(if not all) cannot optimize system quality, cost, or latency subject to
constraint(s) on the other dimensions. In this paper we present Abacus, an
extensible, cost-based optimizer which searches for the best implementation of
a semantic operator system given a (possibly constrained) optimization
objective. Abacus estimates operator performance by leveraging a minimal set of
validation examples and, if available, prior beliefs about operator
performance. We evaluate Abacus on document processing workloads in the
biomedical and legal domains (BioDEX; CUAD) and multi-modal question answering
(MMQA). We demonstrate that systems optimized by Abacus achieve 18.7%-39.2%
better quality and up to 23.6x lower cost and 4.2x lower latency than the next
best system.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [502] [Federated prediction for scalable and privacy-preserved knowledge-based planning in radiotherapy](https://arxiv.org/abs/2505.14507)
*Jingyun Chen,David Horowitz,Yading Yuan*

Main category: cs.DC

TL;DR: 该论文提出了FedKBP+，一个用于放射治疗规划中预测任务的联邦学习平台，旨在解决数据稀缺和隐私问题。


<details>
  <summary>Details</summary>
Motivation: 深度学习在放射治疗规划中具有潜力，但数据稀缺和机构间异构性限制了模型的泛化能力。数据共享又因隐私和技术障碍难以实现。

Method: 开发了基于gRPC的统一通信栈，支持集中式和完全去中心化的联邦学习策略，使用SA-Net作为预测模型进行评估。

Result: FedKBP+在三个预测任务中表现出高效、有效和鲁棒性，显示出作为放射治疗联邦学习平台的巨大潜力。

Conclusion: FedKBP+是一个高效、稳健的联邦学习平台，有望推动放射治疗规划中的预测任务应用。

Abstract: Background: Deep learning has potential to improve the efficiency and
consistency of radiation therapy planning, but clinical adoption is hindered by
the limited model generalizability due to data scarcity and heterogeneity among
institutions. Although aggregating data from different institutions could
alleviate this problem, data sharing is a practical challenge due to concerns
about patient data privacy and other technical obstacles. Purpose: This work
aims to address this dilemma by developing FedKBP+, a comprehensive federated
learning (FL) platform for predictive tasks in real-world applications in
radiotherapy treatment planning. Methods: We implemented a unified
communication stack based on Google Remote Procedure Call (gRPC) to support
communication between participants whether located on the same workstation or
distributed across multiple workstations. In addition to supporting the
centralized FL strategies commonly available in existing open-source
frameworks, FedKBP+ also provides a fully decentralized FL model where
participants directly exchange model weights to each other through Peer-to-Peer
communication. We evaluated FedKBP+ on three predictive tasks using
scale-attention network (SA-Net) as the predictive model. Conclusions: Our
results demonstrate that FedKBP+ is highly effective, efficient and robust,
showing great potential as a federated learning platform for radiation therapy.

</details>
