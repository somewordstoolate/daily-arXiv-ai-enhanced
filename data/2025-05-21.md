<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 137]
- [cs.AI](#cs.AI) [Total: 73]
- [cs.LG](#cs.LG) [Total: 143]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 2]
- [cs.AR](#cs.AR) [Total: 1]
- [cs.ET](#cs.ET) [Total: 1]
- [eess.SP](#eess.SP) [Total: 1]
- [cs.DC](#cs.DC) [Total: 1]
- [cs.MA](#cs.MA) [Total: 4]
- [cs.CG](#cs.CG) [Total: 2]
- [cs.RO](#cs.RO) [Total: 12]
- [cs.CR](#cs.CR) [Total: 11]
- [cs.IR](#cs.IR) [Total: 18]
- [cs.NE](#cs.NE) [Total: 1]
- [cs.CV](#cs.CV) [Total: 23]
- [stat.ME](#stat.ME) [Total: 1]
- [econ.EM](#econ.EM) [Total: 1]
- [cs.GR](#cs.GR) [Total: 1]
- [eess.IV](#eess.IV) [Total: 5]
- [astro-ph.EP](#astro-ph.EP) [Total: 1]
- [cs.HC](#cs.HC) [Total: 2]
- [cs.PL](#cs.PL) [Total: 2]
- [eess.AS](#eess.AS) [Total: 10]
- [cs.DS](#cs.DS) [Total: 1]
- [q-bio.QM](#q-bio.QM) [Total: 1]
- [quant-ph](#quant-ph) [Total: 4]
- [cond-mat.soft](#cond-mat.soft) [Total: 1]
- [cs.CY](#cs.CY) [Total: 7]
- [cs.SE](#cs.SE) [Total: 4]
- [q-bio.GN](#q-bio.GN) [Total: 1]
- [cs.GT](#cs.GT) [Total: 1]
- [math.OC](#math.OC) [Total: 2]
- [cs.SD](#cs.SD) [Total: 10]
- [cs.DB](#cs.DB) [Total: 2]
- [stat.ML](#stat.ML) [Total: 15]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Evaluating Reasoning LLMs for Suicide Screening with the Columbia-Suicide Severity Rating Scale](https://arxiv.org/abs/2505.13480)
*Avinash Patil,Siru Tao,Amardeep Gedhu*

Main category: cs.CL

TL;DR: 研究评估了六种大型语言模型（LLM）在自杀风险评估中的表现，发现Claude和GPT与人类标注最接近，Mistral预测误差最低，但需谨慎部署并保持人类监督。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型的普及，人们可能向AI而非人类透露自杀倾向，研究旨在评估LLM在自杀风险评估中的能力。

Method: 使用哥伦比亚自杀严重程度评定量表（C-SSRS），对六种模型（Claude、GPT、Mistral、LLaMA等）进行零样本性能测试，分类7级严重程度帖子。

Result: Claude和GPT与人类标注高度一致，Mistral的序数预测误差最低，模型通常在相邻严重级别间出现误分类。

Conclusion: 尽管部分LLM表现良好，但需强调人类监督、透明度和谨慎部署的重要性，以确保伦理和安全。

Abstract: Suicide prevention remains a critical public health challenge. While online
platforms such as Reddit's r/SuicideWatch have historically provided spaces for
individuals to express suicidal thoughts and seek community support, the advent
of large language models (LLMs) introduces a new paradigm-where individuals may
begin disclosing ideation to AI systems instead of humans. This study evaluates
the capability of LLMs to perform automated suicide risk assessment using the
Columbia-Suicide Severity Rating Scale (C-SSRS). We assess the zero-shot
performance of six models-including Claude, GPT, Mistral, and LLaMA-in
classifying posts across a 7-point severity scale (Levels 0-6). Results
indicate that Claude and GPT closely align with human annotations, while
Mistral achieves the lowest ordinal prediction error. Most models exhibit
ordinal sensitivity, with misclassifications typically occurring between
adjacent severity levels. We further analyze confusion patterns,
misclassification sources, and ethical considerations, underscoring the
importance of human oversight, transparency, and cautious deployment. Full code
and supplementary materials are available at
https://github.com/av9ash/llm_cssrs_code.

</details>


### [2] [EmoMeta: A Multimodal Dataset for Fine-grained Emotion Classification in Chinese Metaphors](https://arxiv.org/abs/2505.13483)
*Xingyuan Lu,Yuxi Liu,Dongyu Zhang,Zhiyao Wu,Jing Ren,Feng Xia*

Main category: cs.CL

TL;DR: 该论文提出了一个中文多模态隐喻广告数据集，包含5000个文本-图像对，标注了隐喻、领域关系和细粒度情感分类，以促进多模态隐喻情感研究。


<details>
  <summary>Details</summary>
Motivation: 当前多模态隐喻情感研究缺乏高质量数据集，且现有研究主要集中在英语，忽略了不同语言间情感表达的差异。

Method: 构建了一个包含5000个中文文本-图像对的隐喻广告数据集，每个条目标注了隐喻、领域关系和10种细粒度情感分类。

Result: 数据集公开可用（https://github.com/DUTIR-YSQ/EmoMeta），为多模态隐喻情感研究提供了资源支持。

Conclusion: 该数据集填补了多模态隐喻情感研究的空白，尤其为中文语境下的研究提供了重要工具。

Abstract: Metaphors play a pivotal role in expressing emotions, making them crucial for
emotional intelligence. The advent of multimodal data and widespread
communication has led to a proliferation of multimodal metaphors, amplifying
the complexity of emotion classification compared to single-mode scenarios.
However, the scarcity of research on constructing multimodal metaphorical
fine-grained emotion datasets hampers progress in this domain. Moreover,
existing studies predominantly focus on English, overlooking potential
variations in emotional nuances across languages. To address these gaps, we
introduce a multimodal dataset in Chinese comprising 5,000 text-image pairs of
metaphorical advertisements. Each entry is meticulously annotated for metaphor
occurrence, domain relations and fine-grained emotion classification
encompassing joy, love, trust, fear, sadness, disgust, anger, surprise,
anticipation, and neutral. Our dataset is publicly accessible
(https://github.com/DUTIR-YSQ/EmoMeta), facilitating further advancements in
this burgeoning field.

</details>


### [3] [Detecting Prefix Bias in LLM-based Reward Models](https://arxiv.org/abs/2505.13487)
*Ashwin Kumar,Yuzi He,Aram H. Markosyan,Bobbie Chern,Imanol Arrieta-Ibarra*

Main category: cs.CL

TL;DR: 该研究揭示了基于人类反馈的强化学习（RLHF）中奖励模型存在的前缀偏见问题，并提出了一种数据增强方法来减轻这种偏见。


<details>
  <summary>Details</summary>
Motivation: 尽管已有许多公开的偏好数据集用于语言模型的微调，但这些数据可能隐含偏见，特别是在奖励模型中。本研究旨在检测和评估这些偏见，特别是在种族和性别维度上的影响。

Method: 研究者引入了新的方法来检测和评估前缀偏见，即查询前缀的微小变化引发的模型偏好系统性偏移。此外，提出了一种数据增强策略来减轻这些偏见。

Result: 研究发现，无论底层模型架构如何，奖励模型普遍存在前缀偏见。数据增强策略有效减少了偏见的影响。

Conclusion: 研究强调了在设计公平可靠的奖励模型时，需要关注偏见感知的数据集设计和评估，为AI公平性提供了重要见解。

Abstract: Reinforcement Learning with Human Feedback (RLHF) has emerged as a key
paradigm for task-specific fine-tuning of language models using human
preference data. While numerous publicly available preference datasets provide
pairwise comparisons of responses, the potential for biases in the resulting
reward models remains underexplored. In this work, we introduce novel methods
to detect and evaluate prefix bias -- a systematic shift in model preferences
triggered by minor variations in query prefixes -- in LLM-based reward models
trained on such datasets. We leverage these metrics to reveal significant
biases in preference models across racial and gender dimensions. Our
comprehensive evaluation spans diverse open-source preference datasets and
reward model architectures, demonstrating susceptibility to this kind of bias
regardless of the underlying model architecture. Furthermore, we propose a data
augmentation strategy to mitigate these biases, showing its effectiveness in
reducing the impact of prefix bias. Our findings highlight the critical need
for bias-aware dataset design and evaluation in developing fair and reliable
reward models, contributing to the broader discourse on fairness in AI.

</details>


### [4] [Source framing triggers systematic evaluation bias in Large Language Models](https://arxiv.org/abs/2505.13488)
*Federico Germani,Giovanni Spitale*

Main category: cs.CL

TL;DR: 研究发现，大型语言模型在评估文本时存在框架效应，尤其是当文本被标注为中国作者时，模型间一致性显著下降。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）越来越多地用于文本生成和评估，研究其判断是否一致、无偏见且对框架效应具有鲁棒性变得尤为重要。

Method: 研究系统地考察了四种先进LLMs（OpenAI o3-mini、Deepseek Reasoner、xAI Grok 2和Mistral）在评估4800条叙述性声明时的模型间和模型内一致性，并操纵了每条声明的来源标注（来自其他LLM或特定国籍的人类作者）。

Result: 在盲测条件下，不同LLMs在多个主题上表现出高度一致性；但当引入来源框架后，一致性显著下降，尤其是当声明被标注为中国作者时，所有模型的一致性评分均降低，其中Deepseek Reasoner受影响最大。

Conclusion: 框架效应会显著影响LLMs的文本评估，这对LLM介导的信息系统的完整性、中立性和公平性具有重要启示。

Abstract: Large Language Models (LLMs) are increasingly used not only to generate text
but also to evaluate it, raising urgent questions about whether their judgments
are consistent, unbiased, and robust to framing effects. In this study, we
systematically examine inter- and intra-model agreement across four
state-of-the-art LLMs (OpenAI o3-mini, Deepseek Reasoner, xAI Grok 2, and
Mistral) tasked with evaluating 4,800 narrative statements on 24 different
topics of social, political, and public health relevance, for a total of
192,000 assessments. We manipulate the disclosed source of each statement to
assess how attribution to either another LLM or a human author of specified
nationality affects evaluation outcomes. We find that, in the blind condition,
different LLMs display a remarkably high degree of inter- and intra-model
agreement across topics. However, this alignment breaks down when source
framing is introduced. Here we show that attributing statements to Chinese
individuals systematically lowers agreement scores across all models, and in
particular for Deepseek Reasoner. Our findings reveal that framing effects can
deeply affect text evaluation, with significant implications for the integrity,
neutrality, and fairness of LLM-mediated information systems.

</details>


### [5] [ProdRev: A DNN framework for empowering customers using generative pre-trained transformers](https://arxiv.org/abs/2505.13491)
*Aakash Gupta,Nataraj Das*

Main category: cs.CL

TL;DR: 该论文提出使用GPT-3微调模型进行电商评论的抽象摘要，帮助用户快速决策。


<details>
  <summary>Details</summary>
Motivation: 疫情后电商评论数量激增导致用户决策困难，现有评分工具无法解决信息过载问题。

Method: 采用130亿参数的GPT-3（Curie引擎）进行微调，实现评论的抽象摘要而非简单提取。

Result: 系统能生成具有常识性的评论优缺点总结，揭示评论间真实关联而非简单复制。

Conclusion: 该方法有效提升用户决策效率，通过生成式模型赋予评论摘要常识理解能力。

Abstract: Following the pandemic, customers, preference for using e-commerce has
accelerated. Since much information is available in multiple reviews (sometimes
running in thousands) for a single product, it can create decision paralysis
for the buyer. This scenario disempowers the consumer, who cannot be expected
to go over so many reviews since its time consuming and can confuse them.
Various commercial tools are available, that use a scoring mechanism to arrive
at an adjusted score. It can alert the user to potential review manipulations.
This paper proposes a framework that fine-tunes a generative pre-trained
transformer to understand these reviews better. Furthermore, using
"common-sense" to make better decisions. These models have more than 13 billion
parameters. To fine-tune the model for our requirement, we use the curie engine
from generative pre-trained transformer (GPT3). By using generative models, we
are introducing abstractive summarization. Instead of using a simple extractive
method of summarizing the reviews. This brings out the true relationship
between the reviews and not simply copy-paste. This introduces an element of
"common sense" for the user and helps them to quickly make the right decisions.
The user is provided the pros and cons of the processed reviews. Thus the
user/customer can take their own decisions.

</details>


### [6] [LLM4CD: Leveraging Large Language Models for Open-World Knowledge Augmented Cognitive Diagnosis](https://arxiv.org/abs/2505.13492)
*Weiming Zhang,Lingyue Fu,Qingyao Li,Kounianhua Du,Jianghao Lin,Jingwei Yu,Wei Xia,Weinan Zhang,Ruiming Tang,Yong Yu*

Main category: cs.CL

TL;DR: LLM4CD利用大语言模型的开放世界知识增强认知诊断，通过语义表示解决冷启动问题，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前认知诊断方法仅依赖ID关系建模，忽视教育数据中的语义信息，且难以处理新增学生和习题的冷启动问题。

Method: 提出LLM4CD框架：利用LLM构建认知文本表示，并设计双层编码器（宏观认知文本编码器+微观知识状态编码器）替代传统ID嵌入。

Result: 在多个真实数据集上超越现有认知诊断模型，验证了引入语义信息的有效性。

Conclusion: 通过LLM的开放世界知识注入语义信息，能显著提升认知诊断性能并解决冷启动问题。

Abstract: Cognitive diagnosis (CD) plays a crucial role in intelligent education,
evaluating students' comprehension of knowledge concepts based on their test
histories. However, current CD methods often model students, exercises, and
knowledge concepts solely on their ID relationships, neglecting the abundant
semantic relationships present within educational data space. Furthermore,
contemporary intelligent tutoring systems (ITS) frequently involve the addition
of new students and exercises, a situation that ID-based methods find
challenging to manage effectively. The advent of large language models (LLMs)
offers the potential for overcoming this challenge with open-world knowledge.
In this paper, we propose LLM4CD, which Leverages Large Language Models for
Open-World Knowledge Augmented Cognitive Diagnosis. Our method utilizes the
open-world knowledge of LLMs to construct cognitively expressive textual
representations, which are then encoded to introduce rich semantic information
into the CD task. Additionally, we propose an innovative bi-level encoder
framework that models students' test histories through two levels of encoders:
a macro-level cognitive text encoder and a micro-level knowledge state encoder.
This approach substitutes traditional ID embeddings with semantic
representations, enabling the model to accommodate new students and exercises
with open-world knowledge and address the cold-start problem. Extensive
experimental results demonstrate that our proposed method consistently
outperforms previous CD models on multiple real-world datasets, validating the
effectiveness of leveraging LLMs to introduce rich semantic information into
the CD task.

</details>


### [7] [IRLBench: A Multi-modal, Culturally Grounded, Parallel Irish-English Benchmark for Open-Ended LLM Reasoning Evaluation](https://arxiv.org/abs/2505.13498)
*Khanh-Tung Tran,Barry O'Sullivan,Hoang D. Nguyen*

Main category: cs.CL

TL;DR: 论文提出了IRLBench基准，用于评估大语言模型在低资源语言（爱尔兰语）和英语上的表现，发现模型在爱尔兰语上的表现显著落后。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型在多语言和低资源环境下的表现尚未充分探索，且现有基准存在文化偏见、仅限于文本或多选题形式，尤其缺乏对极低资源语言的评估。

Method: 作者开发了IRLBench基准，基于2024年爱尔兰毕业考试内容，包含12个科目，以长文本生成任务和官方评分标准进行多维度评估。

Result: 实验显示，模型在爱尔兰语上的表现显著低于英语，最佳模型的爱尔兰语回答正确率仅为55.8%，而英语为76.2%。

Conclusion: IRLBench为多语言AI开发提供了新基准，揭示了模型在低资源语言上的不足，呼吁进一步研究。

Abstract: Recent advances in Large Language Models (LLMs) have demonstrated promising
knowledge and reasoning abilities, yet their performance in multilingual and
low-resource settings remains underexplored. Existing benchmarks often exhibit
cultural bias, restrict evaluation to text-only, rely on multiple-choice
formats, and, more importantly, are limited for extremely low-resource
languages. To address these gaps, we introduce IRLBench, presented in parallel
English and Irish, which is considered definitely endangered by UNESCO. Our
benchmark consists of 12 representative subjects developed from the 2024 Irish
Leaving Certificate exams, enabling fine-grained analysis of model capabilities
across domains. By framing the task as long-form generation and leveraging the
official marking scheme, it does not only support a comprehensive evaluation of
correctness but also language fidelity. Our extensive experiments of leading
closed-source and open-source LLMs reveal a persistent performance gap between
English and Irish, in which models produce valid Irish responses less than 80\%
of the time, and answer correctly 55.8\% of the time compared to 76.2\% in
English for the best-performing model. We release IRLBench
(https://huggingface.co/datasets/ReliableAI/IRLBench) and an accompanying
evaluation codebase (https://github.com/ReML-AI/IRLBench) to enable future
research on robust, culturally aware multilingual AI development.

</details>


### [8] [Noise Injection Systemically Degrades Large Language Model Safety Guardrails](https://arxiv.org/abs/2505.13500)
*Prithviraj Singh Shahani,Matthias Scheutz*

Main category: cs.CL

TL;DR: 研究发现，当前大语言模型的安全防护措施在受到高斯噪声干扰时表现脆弱，即使深度微调也无法提供额外保护，而推理能力则相对稳定。


<details>
  <summary>Details</summary>
Motivation: 探讨大语言模型安全微调在扰动下的鲁棒性，以评估现有安全防护措施的实际效果。

Method: 通过系统性地向模型激活注入高斯噪声，测试多个开源模型的安全防护表现。

Result: 高斯噪声可使有害输出率提升27%（p<0.001），深度安全微调无额外保护作用，但推理链保持稳定。

Conclusion: 当前安全对齐技术存在重大漏洞，基于推理和强化学习的方法可能是构建更鲁棒AI安全系统的方向。

Abstract: Safety guardrails in large language models (LLMs) are a critical component in
preventing harmful outputs. Yet, their resilience under perturbation remains
poorly understood. In this paper, we investigate the robustness of safety
fine-tuning in LLMs by systematically injecting Gaussian noise into model
activations. We show across multiple open-weight models that (1) Gaussian noise
raises harmful-output rates (p < 0.001) by up to 27%, (2) that deeper safety
fine-tuning affords no extra protection, and (3) that chain-of-thought
reasoning remains largely intact. The findings reveal critical vulnerabilities
in current safety alignment techniques and highlight the potential of
reasoning-based and reinforcement learning approaches as promising direction
for developing more robust AI safety systems. These results have important
implications for real-world deployment of LLMs in safety-critical applications
as these results imply that widely-deployed safety tuning methods can fail even
without adversarial prompts.

</details>


### [9] [EcoSafeRAG: Efficient Security through Context Analysis in Retrieval-Augmented Generation](https://arxiv.org/abs/2505.13506)
*Ruobing Yao,Yifei Zhang,Shuang Song,Neng Gao,Chenyang Tu*

Main category: cs.CL

TL;DR: EcoSafeRAG提出了一种不依赖LLM内部知识的新型防御方法，通过句子级处理和诱饵引导的上下文多样性检测来识别恶意内容，在保持RAG性能的同时提升安全性。


<details>
  <summary>Details</summary>
Motivation: 现有RAG防御方法依赖大语言模型内部知识，与RAG的设计理念冲突。EcoSafeRAG旨在填补这一空白，提供不依赖模型内部知识的防御方案。

Method: 采用句子级处理和诱饵引导的上下文多样性检测技术，通过分析候选文档的上下文多样性来识别恶意内容。

Result: 实验表明EcoSafeRAG在提供最先进安全性的同时，保持了即插即用部署特性，并提升了干净场景下的RAG性能（延迟仅1.2倍，token减少48%-80%）。

Conclusion: EcoSafeRAG成功实现了不依赖LLM内部知识的安全防御，在安全性和实用性之间取得了良好平衡。

Abstract: Retrieval-Augmented Generation (RAG) compensates for the static knowledge
limitations of Large Language Models (LLMs) by integrating external knowledge,
producing responses with enhanced factual correctness and query-specific
contextualization. However, it also introduces new attack surfaces such as
corpus poisoning at the same time. Most of the existing defense methods rely on
the internal knowledge of the model, which conflicts with the design concept of
RAG. To bridge the gap, EcoSafeRAG uses sentence-level processing and
bait-guided context diversity detection to identify malicious content by
analyzing the context diversity of candidate documents without relying on LLM
internal knowledge. Experiments show EcoSafeRAG delivers state-of-the-art
security with plug-and-play deployment, simultaneously improving clean-scenario
RAG performance while maintaining practical operational costs (relatively
1.2$\times$ latency, 48\%-80\% token reduction versus Vanilla RAG).

</details>


### [10] [Time-R1: Towards Comprehensive Temporal Reasoning in LLMs](https://arxiv.org/abs/2505.13508)
*Zijia Liu,Peixuan Han,Haofei Yu,Haoru Li,Jiaxuan You*

Main category: cs.CL

TL;DR: 论文提出Time-R1框架，通过强化学习课程赋予中等规模LLM全面的时间能力，在预测和创意生成任务上超越超大模型。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型缺乏稳健的时间智能，难以整合过去推理与未来预测/生成，且现有方法泛化能力差。

Method: 采用三阶段强化学习课程：1)历史数据建立时间理解基础 2)知识截止后事件预测 3)零样本创意未来场景生成。

Result: Time-R1在预测和创意生成任务上超越参数量200倍以上的模型（如671B DeepSeek-R1），并发布Time-Bench数据集。

Conclusion: 证明通过精心设计的渐进式强化学习微调，小模型可实现卓越时间性能，为时间感知AI提供可行路径。

Abstract: Large Language Models (LLMs) demonstrate impressive capabilities but lack
robust temporal intelligence, struggling to integrate reasoning about the past
with predictions and plausible generations of the future. Meanwhile, existing
methods typically target isolated temporal skills, such as question answering
about past events or basic forecasting, and exhibit poor generalization,
particularly when dealing with events beyond their knowledge cutoff or
requiring creative foresight. To address these limitations, we introduce
\textit{Time-R1}, the first framework to endow a moderate-sized (3B-parameter)
LLM with comprehensive temporal abilities: understanding, prediction, and
creative generation. Our approach features a novel three-stage development
path; the first two constitute a \textit{reinforcement learning (RL)
curriculum} driven by a meticulously designed dynamic rule-based reward system.
This framework progressively builds (1) foundational temporal understanding and
logical event-time mappings from historical data, (2) future event prediction
skills for events beyond its knowledge cutoff, and finally (3) enables
remarkable generalization to creative future scenario generation without any
fine-tuning. Strikingly, experiments demonstrate that Time-R1 outperforms
models over 200 times larger, including the state-of-the-art 671B DeepSeek-R1,
on highly challenging future event prediction and creative scenario generation
benchmarks. This work provides strong evidence that thoughtfully engineered,
progressive RL fine-tuning allows smaller, efficient models to achieve superior
temporal performance, offering a practical and scalable path towards truly
time-aware AI. To foster further research, we also release \textit{Time-Bench},
a large-scale multi-task temporal reasoning dataset derived from 10 years of
news data, and our series of \textit{Time-R1} checkpoints.

</details>


### [11] [Induction Head Toxicity Mechanistically Explains Repetition Curse in Large Language Models](https://arxiv.org/abs/2505.13514)
*Shuxun Wang,Qingyu Yin,Chak Tou Leong,Qiang Zhang,Linyi Yang*

Main category: cs.CL

TL;DR: 研究发现大语言模型中的归纳头是导致重复诅咒的关键因素，并提出通过注意力头正则化技术来缓解这一问题。


<details>
  <summary>Details</summary>
Motivation: 尽管重复诅咒现象在大语言模型中被广泛观察到，但其内在机制尚不明确。本文旨在探究归纳头在驱动重复行为中的作用。

Method: 研究聚焦于归纳头的“毒性”，即其在重复生成过程中主导模型输出logits的趋势，并提出一种注意力头正则化技术来减少这种主导。

Result: 研究发现归纳头是重复诅咒的主要驱动因素，并验证了注意力头正则化技术可以有效降低归纳头的支配性，从而生成更多样和连贯的输出。

Conclusion: 通过识别归纳头的作用，本文为重复诅咒提供了机制性解释，并提出了缓解该问题的潜在方法，对大语言模型的设计和训练具有重要意义。

Abstract: Repetition curse is a phenomenon where Large Language Models (LLMs) generate
repetitive sequences of tokens or cyclic sequences. While the repetition curse
has been widely observed, its underlying mechanisms remain poorly understood.
In this work, we investigate the role of induction heads--a specific type of
attention head known for their ability to perform in-context learning--in
driving this repetitive behavior. Specifically, we focus on the "toxicity" of
induction heads, which we define as their tendency to dominate the model's
output logits during repetition, effectively excluding other attention heads
from contributing to the generation process. Our findings have important
implications for the design and training of LLMs. By identifying induction
heads as a key driver of the repetition curse, we provide a mechanistic
explanation for this phenomenon and suggest potential avenues for mitigation.
We also propose a technique with attention head regularization that could be
employed to reduce the dominance of induction heads during generation, thereby
promoting more diverse and coherent outputs.

</details>


### [12] [Logic Jailbreak: Efficiently Unlocking LLM Safety Restrictions Through Formal Logical Expression](https://arxiv.org/abs/2505.13527)
*Jingyu Peng,Maolin Wang,Nan Wang,Xiangyu Zhao,Jiatong Li,Kai Zhang,Qi Liu*

Main category: cs.CL

TL;DR: 提出LogiBreak方法，通过逻辑表达式转换绕过LLM安全机制，揭示对齐数据与逻辑输入间的分布差异。


<details>
  <summary>Details</summary>
Motivation: 现有LLM安全机制易受越狱攻击，作者认为这是由于对齐提示与恶意提示间的分布差异所致。

Method: 将有害自然语言提示转换为形式逻辑表达式，利用对齐数据与逻辑输入的分布差距规避安全约束。

Result: 在多语言越狱数据集上验证了方法的有效性，适用于不同评估设置和语言环境。

Conclusion: LogiBreak证实了逻辑表达转换在绕过LLM安全系统方面的潜力，突显了分布差异带来的安全隐患。

Abstract: Despite substantial advancements in aligning large language models (LLMs)
with human values, current safety mechanisms remain susceptible to jailbreak
attacks. We hypothesize that this vulnerability stems from distributional
discrepancies between alignment-oriented prompts and malicious prompts. To
investigate this, we introduce LogiBreak, a novel and universal black-box
jailbreak method that leverages logical expression translation to circumvent
LLM safety systems. By converting harmful natural language prompts into formal
logical expressions, LogiBreak exploits the distributional gap between
alignment data and logic-based inputs, preserving the underlying semantic
intent and readability while evading safety constraints. We evaluate LogiBreak
on a multilingual jailbreak dataset spanning three languages, demonstrating its
effectiveness across various evaluation settings and linguistic contexts.

</details>


### [13] [Combining the Best of Both Worlds: A Method for Hybrid NMT and LLM Translation](https://arxiv.org/abs/2505.13554)
*Zhanglin Wu,Daimeng Wei,Xiaoyu Chen,Hengchao Shang,Jiaxin Guo,Zongyao Li,Yuanchang Luo,Jinlong Yang,Zhiqiang Rao,Hao Yang*

Main category: cs.CL

TL;DR: 该论文提出了一种结合NMT和LLM的翻译调度策略，通过智能决策减少LLM使用，在保证翻译质量的同时降低计算成本和延迟。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型(LLM)在机器翻译等任务中表现优异，但存在计算成本高、延迟大的问题。研究发现LLM与神经机器翻译(NMT)系统各有优势，因此需要一种优化方案来平衡性能与效率。

Method: 提出基于源语句特征的决策机制，设计多种调度策略进行比较，选择性地使用LLM进行翻译。

Result: 在多语言测试集上的实验表明，该方法能以最少的LLM使用量获得最优的翻译性能。

Conclusion: 通过智能调度NMT和LLM的协同工作，可以在保证翻译质量的前提下显著降低计算资源消耗。

Abstract: Large language model (LLM) shows promising performances in a variety of
downstream tasks, such as machine translation (MT). However, using LLMs for
translation suffers from high computational costs and significant latency.
Based on our evaluation, in most cases, translations using LLMs are comparable
to that generated by neural machine translation (NMT) systems. Only in
particular scenarios, LLM and NMT models show respective advantages. As a
result, integrating NMT and LLM for translation and using LLM only when
necessary seems to be a sound solution. A scheduling policy that optimizes
translation result while ensuring fast speed and as little LLM usage as
possible is thereby required. We compare several scheduling policies and
propose a novel and straightforward decider that leverages source sentence
features. We conduct extensive experiments on multilingual test sets and the
result shows that we can achieve optimal translation performance with minimal
LLM usage, demonstrating effectiveness of our decider.

</details>


### [14] [CS-Sum: A Benchmark for Code-Switching Dialogue Summarization and the Limits of Large Language Models](https://arxiv.org/abs/2505.13559)
*Sathya Krishnan Suresh,Tanmay Surana,Lim Zhi Hao,Eng Siong Chng*

Main category: cs.CL

TL;DR: 论文提出CS-Sum基准测试，评估大语言模型对混合语言对话的理解能力，发现模型虽自动评分高但存在语义错误。


<details>
  <summary>Details</summary>
Motivation: 混合语言（Code-switching）对大语言模型构成挑战，但其理解能力尚未充分研究。

Method: 引入CS-Sum基准，涵盖三种语言对，通过少量样本、翻译后总结和微调方法评估十种大语言模型。

Result: 尽管自动评分高，模型在处理混合语言时仍会犯改变语义的细微错误，且错误率因语言对和模型而异。

Conclusion: 需针对混合语言数据进行专门训练以提升模型表现。

Abstract: Code-switching (CS) poses a significant challenge for Large Language Models
(LLMs), yet its comprehensibility remains underexplored in LLMs. We introduce
CS-Sum, to evaluate the comprehensibility of CS by the LLMs through CS dialogue
to English summarization. CS-Sum is the first benchmark for CS dialogue
summarization across Mandarin-English (EN-ZH), Tamil-English (EN-TA), and
Malay-English (EN-MS), with 900-1300 human-annotated dialogues per language
pair. Evaluating ten LLMs, including open and closed-source models, we analyze
performance across few-shot, translate-summarize, and fine-tuning (LoRA, QLoRA
on synthetic data) approaches. Our findings show that though the scores on
automated metrics are high, LLMs make subtle mistakes that alter the complete
meaning of the dialogue. To this end, we introduce 3 most common type of errors
that LLMs make when handling CS input. Error rates vary across CS pairs and
LLMs, with some LLMs showing more frequent errors on certain language pairs,
underscoring the need for specialized training on code-switched data.

</details>


### [15] [Cross-Lingual Representation Alignment Through Contrastive Image-Caption Tuning](https://arxiv.org/abs/2505.13628)
*Nathaniel Krasner,Nicholas Lanuzo,Antonios Anastasopoulos*

Main category: cs.CL

TL;DR: 利用图像信息实现多语言句子表征对齐，替代传统双语文本对齐方法，适用于低资源语言。


<details>
  <summary>Details</summary>
Motivation: 传统多语言句子表征对齐依赖双语文本，但获取成本高。本文探索视觉信息是否能替代双语文本，降低低资源语言的对齐难度。

Method: 通过多语言图像-标题对齐任务，隐式学习跨语言文本表征对齐，无需预训练时接触目标语言。

Result: 该方法能有效对齐未见过的语言，并支持跨语言自然语言理解和双语检索任务。

Conclusion: 视觉信息可作为跨语言对齐的高效桥梁，尤其适用于缺乏双语数据的低资源语言场景。

Abstract: Multilingual alignment of sentence representations has mostly required
bitexts to bridge the gap between languages. We investigate whether visual
information can bridge this gap instead. Image caption datasets are very easy
to create without requiring multilingual expertise, so this offers a more
efficient alternative for low-resource languages. We find that multilingual
image-caption alignment can implicitly align the text representations between
languages, languages unseen by the encoder in pretraining can be incorporated
into this alignment post-hoc, and these aligned representations are usable for
cross-lingual Natural Language Understanding (NLU) and bitext retrieval.

</details>


### [16] [Clarifying orthography: Orthographic transparency as compressibility](https://arxiv.org/abs/2505.13657)
*Charles J. Torres,Richard Futrell*

Main category: cs.CL

TL;DR: 该论文提出了一种基于算法信息理论的拼写透明度度量方法，通过计算拼写和语音字符串之间的相互压缩性来量化不同文字系统的透明度。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏一种统一的、与文字系统无关的拼写透明度度量标准。论文旨在填补这一空白，提供一种能够同时捕捉拼写不规则性和规则复杂性的量化方法。

Method: 使用算法信息理论中的相互压缩性概念，通过神经序列模型估计拼写和语音字符串之间的预编码长度，从而量化拼写透明度。

Result: 在22种不同文字系统（包括字母文字、辅音音素文字、元音附标文字、音节文字和表意文字）上的实验验证了该方法能够有效反映文字系统的相对透明度。

Conclusion: 相互压缩性提供了一种简单、原则性强且通用的拼写透明度度量标准，能够跨文字系统进行比较。

Abstract: Orthographic transparency -- how directly spelling is related to sound --
lacks a unified, script-agnostic metric. Using ideas from algorithmic
information theory, we quantify orthographic transparency in terms of the
mutual compressibility between orthographic and phonological strings. Our
measure provides a principled way to combine two factors that decrease
orthographic transparency, capturing both irregular spellings and rule
complexity in one quantity. We estimate our transparency measure using
prequential code-lengths derived from neural sequence models. Evaluating 22
languages across a broad range of script types (alphabetic, abjad, abugida,
syllabic, logographic) confirms common intuitions about relative transparency
of scripts. Mutual compressibility offers a simple, principled, and general
yardstick for orthographic transparency.

</details>


### [17] [Are Large Language Models Good at Detecting Propaganda?](https://arxiv.org/abs/2505.13706)
*Julia Jose,Rachel Greenstadt*

Main category: cs.CL

TL;DR: 研究比较了多种大语言模型在检测新闻文章中的宣传技巧方面的性能，发现GPT-4表现优于GPT-3.5和Claude 3 Opus，但不及RoBERTa-CRF基线。


<details>
  <summary>Details</summary>
Motivation: 宣传者常使用修辞手法和情感诉求来推进议程，识别这些技巧对做出明智决策至关重要。自然语言处理的进步使得开发检测操纵性内容的系统成为可能。

Method: 研究比较了几种大语言模型（LLMs）和基于Transformer的模型在检测新闻文章中宣传技巧的性能。

Result: GPT-4在F1分数上优于GPT-3.5和Claude 3 Opus（F1=0.16），但不及RoBERTa-CRF基线（F1=0.67）。所有三种LLMs在检测六种宣传技巧中的一种（辱骂）上优于MGN基线，GPT-3.5和GPT-4在检测恐惧诉求和旗帜挥舞上也优于MGN基线。

Conclusion: 虽然大语言模型在检测某些宣传技巧上表现良好，但仍需改进以达到更优性能，尤其是在与专用基线模型相比时。

Abstract: Propagandists use rhetorical devices that rely on logical fallacies and
emotional appeals to advance their agendas. Recognizing these techniques is key
to making informed decisions. Recent advances in Natural Language Processing
(NLP) have enabled the development of systems capable of detecting manipulative
content. In this study, we look at several Large Language Models and their
performance in detecting propaganda techniques in news articles. We compare the
performance of these LLMs with transformer-based models. We find that, while
GPT-4 demonstrates superior F1 scores (F1=0.16) compared to GPT-3.5 and Claude
3 Opus, it does not outperform a RoBERTa-CRF baseline (F1=0.67). Additionally,
we find that all three LLMs outperform a MultiGranularity Network (MGN)
baseline in detecting instances of one out of six propaganda techniques
(name-calling), with GPT-3.5 and GPT-4 also outperforming the MGN baseline in
detecting instances of appeal to fear and flag-waving.

</details>


### [18] [SQLForge: Synthesizing Reliable and Diverse Data to Enhance Text-to-SQL Reasoning in LLMs](https://arxiv.org/abs/2505.13725)
*Yu Guo,Dong Jin,Shenghao Ye,Shuangwu Chen,Jian Yang,Xiaobin Tan*

Main category: cs.CL

TL;DR: SQLForge通过合成可靠多样的数据提升开源LLMs在text-to-SQL任务中的性能，显著缩小与闭源模型的差距。


<details>
  <summary>Details</summary>
Motivation: 现有开源模型在text-to-SQL任务中与闭源模型存在显著性能差距，需提升数据质量和多样性以改善表现。

Method: 采用SQL语法约束和反向翻译确保数据可靠性，通过模板丰富化和迭代领域探索增强数据多样性，并微调不同架构的模型。

Result: SQLForge-LM在Spider和BIRD基准上达到开源模型最佳性能（Spider Dev 85.7%，BIRD Dev 59.8%）。

Conclusion: SQLForge有效提升了开源LLMs的text-to-SQL能力，为缩小与闭源模型的差距提供了可行方案。

Abstract: Large Language models (LLMs) have demonstrated significant potential in
text-to-SQL reasoning tasks, yet a substantial performance gap persists between
existing open-source models and their closed-source counterparts. In this
paper, we introduce SQLForge, a novel approach for synthesizing reliable and
diverse data to enhance text-to-SQL reasoning in LLMs. We improve data
reliability through SQL syntax constraints and SQL-to-question reverse
translation, ensuring data logic at both structural and semantic levels. We
also propose an SQL template enrichment and iterative data domain exploration
mechanism to boost data diversity. Building on the augmented data, we fine-tune
a variety of open-source models with different architectures and parameter
sizes, resulting in a family of models termed SQLForge-LM. SQLForge-LM achieves
the state-of-the-art performance on the widely recognized Spider and BIRD
benchmarks among the open-source models. Specifically, SQLForge-LM achieves EX
accuracy of 85.7% on Spider Dev and 59.8% on BIRD Dev, significantly narrowing
the performance gap with closed-source methods.

</details>


### [19] [Simulation Agent: A Framework for Integrating Simulation and Large Language Models for Enhanced Decision-Making](https://arxiv.org/abs/2505.13761)
*Jacob Kleiman,Kevin Frank,Sindy Campagna*

Main category: cs.CL

TL;DR: 论文提出了一种结合仿真模型与大型语言模型(LLM)优势的仿真智能体框架，使非技术用户能通过自然语言交互访问复杂仿真系统。


<details>
  <summary>Details</summary>
Motivation: 传统仿真系统对非技术用户过于复杂，而LLM缺乏对现实世界的结构化因果理解。需要一种融合两者优势的方案。

Method: 开发仿真智能体框架，利用LLM的对话能力与用户交互，同时用仿真系统为LLM提供准确的结构化现实世界表征。

Result: 该集成方法为实证验证提供了稳健且可推广的基础，在多个领域具有广泛适用性。

Conclusion: 结合仿真系统与LLM的框架，既保持了仿真的准确性，又通过自然语言交互降低了使用门槛。

Abstract: Simulations, although powerful in accurately replicating real-world systems,
often remain inaccessible to non-technical users due to their complexity.
Conversely, large language models (LLMs) provide intuitive, language-based
interactions but can lack the structured, causal understanding required to
reliably model complex real-world dynamics. We introduce our simulation agent
framework, a novel approach that integrates the strengths of both simulation
models and LLMs. This framework helps empower users by leveraging the
conversational capabilities of LLMs to interact seamlessly with sophisticated
simulation systems, while simultaneously utilizing the simulations to ground
the LLMs in accurate and structured representations of real-world phenomena.
This integrated approach helps provide a robust and generalizable foundation
for empirical validation and offers broad applicability across diverse domains.

</details>


### [20] [Krikri: Advancing Open Large Language Models for Greek](https://arxiv.org/abs/2505.13772)
*Dimitris Roussis,Leon Voukoutis,Georgios Paraskevopoulos,Sokratis Sofianopoulos,Prokopis Prokopidis,Vassilis Papavasileiou,Athanasios Katsamanis,Stelios Piperidis,Vassilis Katsouros*

Main category: cs.CL

TL;DR: Llama-Krikri-8B是基于Meta Llama 3.1-8B优化的希腊语大语言模型，支持现代希腊语、英语及古希腊语，性能优于同类模型。


<details>
  <summary>Details</summary>
Motivation: 开发专为希腊语优化的高性能大语言模型，填补希腊语及古希腊语处理的空白。

Method: 基于Llama 3.1-8B架构，使用高质量希腊语数据训练，并采用MAGPIE等技术进行多阶段后训练。

Result: 在自然语言理解、生成及代码生成任务上表现显著优于同类希腊语及多语言模型。

Conclusion: Llama-Krikri-8B为希腊语任务提供了先进且高效的解决方案，并通过新基准测试验证了其优越性。

Abstract: We introduce Llama-Krikri-8B, a cutting-edge Large Language Model tailored
for the Greek language, built on Meta's Llama 3.1-8B. Llama-Krikri-8B has been
extensively trained on high-quality Greek data to ensure superior adaptation to
linguistic nuances. With 8 billion parameters, it offers advanced capabilities
while maintaining efficient computational performance. Llama-Krikri-8B supports
both Modern Greek and English, and is also equipped to handle polytonic text
and Ancient Greek. The chat version of Llama-Krikri-8B features a multi-stage
post-training pipeline, utilizing both human and synthetic instruction and
preference data, by applying techniques such as MAGPIE. In addition, for
evaluation, we propose three novel public benchmarks for Greek. Our evaluation
on existing as well as the proposed benchmarks shows notable improvements over
comparable Greek and multilingual LLMs in both natural language understanding
and generation as well as code generation.

</details>


### [21] [Interpretable Traces, Unexpected Outcomes: Investigating the Disconnect in Trace-Based Knowledge Distillation](https://arxiv.org/abs/2505.13792)
*Siddhant Bhambri,Upasana Biswas,Subbarao Kambhampati*

Main category: cs.CL

TL;DR: 该论文探讨了在知识蒸馏（KD）中评估推理轨迹（reasoning traces）的忠实性及其与最终性能的相关性，发现正确轨迹不一定导致正确解答，挑战了现有假设。


<details>
  <summary>Details</summary>
Motivation: 当前小型语言模型（SLMs）在问答任务中表现不佳，知识蒸馏方法虽能提升性能，但依赖的推理轨迹常难以评估。论文旨在解决如何评估这些轨迹的忠实性及其对最终输出的影响。

Method: 采用基于规则的问题分解方法，将复杂查询拆解为结构化子问题（如分类+信息检索），生成可解释的轨迹，并在多个QA数据集上进行监督微调（SFT）实验。

Result: 实验表明：1）正确推理轨迹不一定对应正确解答；2）中间轨迹正确性与最终答案正确性相关性低，质疑了通过推理轨迹提升SLMs性能的隐含假设。

Conclusion: 论文揭示了当前基于推理轨迹的知识蒸馏方法的局限性，强调需重新审视轨迹质量与模型性能的关系，为未来改进KD方法提供了新方向。

Abstract: Question Answering (QA) poses a challenging and critical problem,
particularly in today's age of interactive dialogue systems such as ChatGPT,
Perplexity, Microsoft Copilot, etc. where users demand both accuracy and
transparency in the model's outputs. Since smaller language models (SLMs) are
computationally more efficient but often under-perform compared to larger
models, Knowledge Distillation (KD) methods allow for finetuning these smaller
models to improve their final performance. Lately, the intermediate tokens or
the so called `reasoning' traces produced by Chain-of-Thought (CoT) or by
reasoning models such as DeepSeek R1 are used as a training signal for KD.
However, these reasoning traces are often verbose and difficult to interpret or
evaluate. In this work, we aim to address the challenge of evaluating the
faithfulness of these reasoning traces and their correlation with the final
performance. To this end, we employ a KD method leveraging rule-based problem
decomposition. This approach allows us to break down complex queries into
structured sub-problems, generating interpretable traces whose correctness can
be readily evaluated, even at inference time. Specifically, we demonstrate this
approach on Open Book QA, decomposing the problem into a Classification step
and an Information Retrieval step, thereby simplifying trace evaluation. Our
SFT experiments with correct and incorrect traces on the CoTemp QA, Microsoft
Machine Reading Comprehension QA, and Facebook bAbI QA datasets reveal the
striking finding that correct traces do not necessarily imply that the model
outputs the correct final solution. Similarly, we find a low correlation
between correct final solutions and intermediate trace correctness. These
results challenge the implicit assumption behind utilizing reasoning traces for
improving SLMs' final performance via KD.

</details>


### [22] [EfficientLLM: Efficiency in Large Language Models](https://arxiv.org/abs/2505.13840)
*Zhengqing Yuan,Weixiang Sun,Yixin Liu,Huichi Zhou,Rong Zhou,Yiyang Li,Zheyuan Zhang,Wei Song,Yue Huang,Haolong Jia,Keerthiram Murugesan,Yu Wang,Lifang He,Jianfeng Gao,Lichao Sun,Yanfang Ye*

Main category: cs.CL

TL;DR: EfficientLLM是一个评估大语言模型效率技术的基准研究，探讨了预训练、微调和推理三个关键方面的效率优化方法，并提出了六项细粒度指标来衡量硬件利用率、延迟-吞吐量平衡和碳成本。研究发现效率优化存在量化权衡，且最优方法因任务和规模而异。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型参数数量和上下文窗口的增加，计算、能源和成本急剧上升，亟需研究如何优化模型效率以降低这些成本。

Method: 研究在48xGH200和8xH200 GPU的生产级集群上，系统评估了预训练架构（如高效注意力变体、稀疏专家混合）、微调方法（如LoRA、RSLoRA、DoRA）和推理技术（如int4、float16量化），并定义了六项细粒度指标。

Result: 研究发现：(i) 效率优化存在量化权衡，如MoE减少FLOPs但增加VRAM40%；(ii) 最优方法因任务和规模而异，如MQA适合受限设备，MLA适合质量关键任务；(iii) 这些技术可跨模态推广到视觉和视觉语言模型。

Conclusion: EfficientLLM为研究人员和工程师提供了下一代基础模型效率-性能权衡的重要指导，并通过开源数据集、评估流程和排行榜支持进一步研究。

Abstract: Large Language Models (LLMs) have driven significant progress, yet their
growing parameter counts and context windows incur prohibitive compute, energy,
and monetary costs. We introduce EfficientLLM, a novel benchmark and the first
comprehensive empirical study evaluating efficiency techniques for LLMs at
scale. Conducted on a production-class cluster (48xGH200, 8xH200 GPUs), our
study systematically explores three key axes: (1) architecture pretraining
(efficient attention variants: MQA, GQA, MLA, NSA; sparse Mixture-of-Experts
(MoE)), (2) fine-tuning (parameter-efficient methods: LoRA, RSLoRA, DoRA), and
(3) inference (quantization methods: int4, float16). We define six fine-grained
metrics (Memory Utilization, Compute Utilization, Latency, Throughput, Energy
Consumption, Compression Rate) to capture hardware saturation,
latency-throughput balance, and carbon cost. Evaluating over 100
model-technique pairs (0.5B-72B parameters), we derive three core insights: (i)
Efficiency involves quantifiable trade-offs: no single method is universally
optimal; e.g., MoE reduces FLOPs and improves accuracy but increases VRAM by
40%, while int4 quantization cuts memory/energy by up to 3.9x at a 3-5%
accuracy drop. (ii) Optima are task- and scale-dependent: MQA offers optimal
memory-latency trade-offs for constrained devices, MLA achieves lowest
perplexity for quality-critical tasks, and RSLoRA surpasses LoRA efficiency
only beyond 14B parameters. (iii) Techniques generalize across modalities: we
extend evaluations to Large Vision Models (Stable Diffusion 3.5, Wan 2.1) and
Vision-Language Models (Qwen2.5-VL), confirming effective transferability. By
open-sourcing datasets, evaluation pipelines, and leaderboards, EfficientLLM
provides essential guidance for researchers and engineers navigating the
efficiency-performance landscape of next-generation foundation models.

</details>


### [23] [Improve Language Model and Brain Alignment via Associative Memory](https://arxiv.org/abs/2505.13844)
*Congchi Yin,Yongpeng Zhang,Xuyun Wen,Piji Li*

Main category: cs.CL

TL;DR: 通过整合联想记忆，提升语言模型与人类大脑在处理语音信息时的对齐效果，并构建专门的数据集进行监督微调。


<details>
  <summary>Details</summary>
Motivation: 联想记忆在人类认知系统中整合相关信息以促进理解，本研究旨在通过整合联想记忆来提升语言模型与大脑在处理语音信息时的对齐效果。

Method: 首先验证语言模型与大脑活动的对齐性，然后将扩展了模拟联想记忆的原始文本作为语言模型的输入，并构建包含1000个故事样本的《Association》数据集进行监督微调。

Result: 发现语言模型与大脑在联想记忆处理相关区域的对其性有所提升，且经过特定监督微调的大型语言模型能更好地与大脑反应对齐。

Conclusion: 整合联想记忆和特定监督微调能有效提升语言模型与人类大脑在处理信息时的对齐性。

Abstract: Associative memory engages in the integration of relevant information for
comprehension in the human cognition system. In this work, we seek to improve
alignment between language models and human brain while processing speech
information by integrating associative memory. After verifying the alignment
between language model and brain by mapping language model activations to brain
activity, the original text stimuli expanded with simulated associative memory
are regarded as input to computational language models. We find the alignment
between language model and brain is improved in brain regions closely related
to associative memory processing. We also demonstrate large language models
after specific supervised fine-tuning better align with brain response, by
building the \textit{Association} dataset containing 1000 samples of stories,
with instructions encouraging associative memory as input and associated
content as output.

</details>


### [24] [Domain Gating Ensemble Networks for AI-Generated Text Detection](https://arxiv.org/abs/2505.13855)
*Arihant Tripathi,Liam Dugan,Charis Gao,Maggie Huan,Emma Jin,Peter Zhang,David Zhang,Julia Zhao,Chris Callison-Burch*

Main category: cs.CL

TL;DR: 论文提出DoGEN技术，通过集成多个领域专家检测模型并结合领域分类器权重，提升机器生成文本检测器在新领域的适应能力。


<details>
  <summary>Details</summary>
Motivation: 随着语言模型不断进步，检测机器生成文本的需求日益迫切，但现有检测器难以适应新领域和新生成模型。

Method: 采用DoGEN（Domain Gating Ensemble Networks）技术，集成多个领域专家检测模型，利用领域分类器的权重进行自适应调整。

Result: DoGEN在多个领先基准测试中表现出色，在领域内检测达到最优性能，在跨领域检测上优于两倍规模的模型。

Conclusion: DoGEN为领域自适应AI检测提供了有效解决方案，作者公开了代码和训练模型以促进未来研究。

Abstract: As state-of-the-art language models continue to improve, the need for robust
detection of machine-generated text becomes increasingly critical. However,
current state-of-the-art machine text detectors struggle to adapt to new unseen
domains and generative models. In this paper we present DoGEN (Domain Gating
Ensemble Networks), a technique that allows detectors to adapt to unseen
domains by ensembling a set of domain expert detector models using weights from
a domain classifier. We test DoGEN on a wide variety of domains from leading
benchmarks and find that it achieves state-of-the-art performance on in-domain
detection while outperforming models twice its size on out-of-domain detection.
We release our code and trained models to assist in future research in
domain-adaptive AI detection.

</details>


### [25] [Reasoning Path Compression: Compressing Generation Trajectories for Efficient LLM Reasoning](https://arxiv.org/abs/2505.13866)
*Jiwon Song,Dongwon Jo,Yulhwa Kim,Jae-Joon Kim*

Main category: cs.CL

TL;DR: 提出了一种无需训练的方法RPC，通过压缩推理路径中的KV缓存来加速推理，提高生成效率，同时保持较高准确率。


<details>
  <summary>Details</summary>
Motivation: 当前基于推理路径的语言模型虽然准确率高，但长推理路径导致内存占用高、生成速度慢，限制了实际部署。

Method: RPC通过定期压缩KV缓存，保留重要性高的部分，利用语义稀疏性加速推理。

Result: 实验显示RPC将QwQ-32B的生成吞吐量提升1.6倍，AIME 2024基准上准确率仅下降1.2%。

Conclusion: RPC有效利用推理路径的语义稀疏性进行压缩，为高效部署推理型大模型提供了实用方案。

Abstract: Recent reasoning-focused language models achieve high accuracy by generating
lengthy intermediate reasoning paths before producing final answers. While this
approach is effective in solving problems that require logical thinking, long
reasoning paths significantly increase memory usage and throughput of token
generation, limiting the practical deployment of such models. We propose
Reasoning Path Compression (RPC), a training-free method that accelerates
inference by leveraging the semantic sparsity of reasoning paths. RPC
periodically compresses the KV cache by retaining KV cache that receive high
importance score, which are computed using a selector window composed of
recently generated queries. Experiments show that RPC improves generation
throughput of QwQ-32B by up to 1.60$\times$ compared to the inference with full
KV cache, with an accuracy drop of 1.2% on the AIME 2024 benchmark. Our
findings demonstrate that semantic sparsity in reasoning traces can be
effectively exploited for compression, offering a practical path toward
efficient deployment of reasoning LLMs. Our code is available at
https://github.com/jiwonsong-dev/ReasoningPathCompression.

</details>


### [26] [Code2Logic: Game-Code-Driven Data Synthesis for Enhancing VLMs General Reasoning](https://arxiv.org/abs/2505.13886)
*Jingqi Tong,Jixin Tang,Hangcheng Li,Yurong Mou,Ming Zhang,Jun Zhao,Yanbo Wen,Fan Song,Jiahao Zhan,Yuyang Lu,Chaoran Tao,Zhiyuan Guo,Jizhou Yu,Tianhao Cheng,Changhao Jiang,Zhen Wang,Tao Liang,Zhihui Fei,Mingyang Wan,Guojun Ma,Weifeng Ge,Guanhua Chen,Tao Gui,Xipeng Qiu,Qi Zhang,Xuanjing Huang*

Main category: cs.CL

TL;DR: 论文提出Code2Logic方法，利用游戏代码自动生成视觉语言推理数据，构建GameQA数据集，提升视觉语言模型的推理能力。


<details>
  <summary>Details</summary>
Motivation: 视觉语言链式思维（CoT）数据资源稀缺，高质量标注成本高，游戏代码蕴含逻辑结构和状态转换过程，可作为替代资源。

Method: 通过大型语言模型（LLM）适配游戏代码，自动获取推理过程和结果，构建GameQA数据集。

Result: GameQA数据集包含30款游戏和158个任务，训练后的视觉语言模型Qwen2.5-VL-7B在7个跨领域基准上性能提升2.33%。

Conclusion: Code2Logic方法高效且可扩展，游戏数据能有效提升视觉语言模型的泛化能力。

Abstract: Visual-language Chain-of-Thought (CoT) data resources are relatively scarce
compared to text-only counterparts, limiting the improvement of reasoning
capabilities in Vision Language Models (VLMs). However, high-quality
vision-language reasoning data is expensive and labor-intensive to annotate. To
address this issue, we leverage a promising resource: game code, which
naturally contains logical structures and state transition processes.
Therefore, we propose Code2Logic, a novel game-code-driven approach for
multimodal reasoning data synthesis. Our approach leverages Large Language
Models (LLMs) to adapt game code, enabling automatic acquisition of reasoning
processes and results through code execution. Using the Code2Logic approach, we
developed the GameQA dataset to train and evaluate VLMs. GameQA is
cost-effective and scalable to produce, challenging for state-of-the-art
models, and diverse with 30 games and 158 tasks. Surprisingly, despite training
solely on game data, VLMs demonstrated out of domain generalization,
specifically Qwen2.5-VL-7B improving performance by 2.33\% across 7 diverse
vision-language benchmarks. Our code and dataset are available at
https://github.com/tongjingqi/Code2Logic.

</details>


### [27] [Mapping the Minds of LLMs: A Graph-Based Analysis of Reasoning LLM](https://arxiv.org/abs/2505.13890)
*Zhen Xiong,Yujun Cai,Zhecheng Li,Yiwei Wang*

Main category: cs.CL

TL;DR: 本文提出了一种基于图的统一分析框架，用于更好地建模大型语言模型的推理过程，揭示了推理结构与准确性之间的强相关性。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型在测试时扩展方面取得了进展，但其推理行为仍存在不稳定和反直觉的现象，如少样本提示下的性能下降，这挑战了当前对推理模型的理解。

Method: 通过将冗长的思维链输出聚类为语义连贯的推理步骤，并构建有向推理图来捕捉这些步骤之间的上下文和逻辑依赖关系。

Result: 研究发现，探索密度、分支和收敛比率等结构特性与推理准确性密切相关，提示策略显著重塑了模型的内部推理结构。

Conclusion: 该框架不仅能够定量评估推理质量，还为提示工程和大型语言模型的认知分析提供了实用见解。

Abstract: Recent advances in test-time scaling have enabled Large Language Models
(LLMs) to display sophisticated reasoning abilities via extended
Chain-of-Thought (CoT) generation. Despite their potential, these Reasoning
LLMs (RLMs) often demonstrate counterintuitive and unstable behaviors, such as
performance degradation under few-shot prompting, that challenge our current
understanding of RLMs. In this work, we introduce a unified graph-based
analytical framework for better modeling the reasoning processes of RLMs. Our
method first clusters long, verbose CoT outputs into semantically coherent
reasoning steps, then constructs directed reasoning graphs to capture
contextual and logical dependencies among these steps. Through comprehensive
analysis across models and prompting regimes, we reveal that structural
properties, such as exploration density, branching, and convergence ratios,
strongly correlate with reasoning accuracy. Our findings demonstrate how
prompting strategies substantially reshape the internal reasoning structure of
RLMs, directly affecting task outcomes. The proposed framework not only enables
quantitative evaluation of reasoning quality beyond conventional metrics but
also provides practical insights for prompt engineering and the cognitive
analysis of LLMs. Code and resources will be released to facilitate future
research in this direction.

</details>


### [28] [InfiGFusion: Graph-on-Logits Distillation via Efficient Gromov-Wasserstein for Model Fusion](https://arxiv.org/abs/2505.13893)
*Yuanyi Wang,Zhaoyi Yan,Yiming Zhang,Qi Zhou,Yanggan Gu,Fei Wu,Hongxia Yang*

Main category: cs.CL

TL;DR: 提出InfiGFusion框架，通过图蒸馏损失建模词汇维度间的语义依赖，显著提升多模型融合效果。


<details>
  <summary>Details</summary>
Motivation: 现有基于logit的融合方法忽视词汇维度间的语义依赖关系，难以有效整合异构模型的互补优势。

Method: 设计全局共激活图表示词汇通道关系，提出O(n log n)的排序近似算法降低计算复杂度。

Result: 在11个基准测试中超越SOTA，复杂推理任务提升显著（如多步算术+35.6分）。

Conclusion: InfiGFusion通过结构感知融合机制，为异构模型整合提供了可扩展的高效解决方案。

Abstract: Recent advances in large language models (LLMs) have intensified efforts to
fuse heterogeneous open-source models into a unified system that inherits their
complementary strengths. Existing logit-based fusion methods maintain inference
efficiency but treat vocabulary dimensions independently, overlooking semantic
dependencies encoded by cross-dimension interactions. These dependencies
reflect how token types interact under a model's internal reasoning and are
essential for aligning models with diverse generation behaviors. To explicitly
model these dependencies, we propose \textbf{InfiGFusion}, the first
structure-aware fusion framework with a novel \textit{Graph-on-Logits
Distillation} (GLD) loss. Specifically, we retain the top-$k$ logits per output
and aggregate their outer products across sequence positions to form a global
co-activation graph, where nodes represent vocabulary channels and edges
quantify their joint activations. To ensure scalability and efficiency, we
design a sorting-based closed-form approximation that reduces the original
$O(n^4)$ cost of Gromov-Wasserstein distance to $O(n \log n)$, with provable
approximation guarantees. Experiments across multiple fusion settings show that
GLD consistently improves fusion quality and stability. InfiGFusion outperforms
SOTA models and fusion baselines across 11 benchmarks spanning reasoning,
coding, and mathematics. It shows particular strength in complex reasoning
tasks, with +35.6 improvement on Multistep Arithmetic and +37.06 on Causal
Judgement over SFT, demonstrating superior multi-step and relational inference.

</details>


### [29] [Let's Verify Math Questions Step by Step](https://arxiv.org/abs/2505.13903)
*Chengyu Shen,Zhen Hao Wong,Runming He,Hao Liang,Meiyi Qiang,Zimo Meng,Zhengyang Zhao,Bohan Zeng,Zhengzhou Zhu,Bin Cui,Wentao Zhang*

Main category: cs.CL

TL;DR: 该论文提出了MathQ-Verify，一个五阶段流程，用于严格筛选数学问题中的不良或未明确定义的问题，以提高数学数据集的质量。


<details>
  <summary>Details</summary>
Motivation: 现有工作主要关注生成正确的推理路径和答案，但忽视了问题本身的有效性。MathQ-Verify旨在解决这一问题，通过验证问题的格式、逻辑一致性和信息完整性，确保数学问题的质量。

Method: MathQ-Verify采用五阶段流程：格式验证、问题形式化、条件分解、逻辑矛盾检测和目标导向的完整性检查，以过滤不良数学问题。

Result: 实验表明，MathQ-Verify在多个基准测试中达到最先进性能，F1分数提升高达25个百分点，并通过轻量级模型投票方案实现约90%的精确度和63%的召回率。

Conclusion: MathQ-Verify为构建可靠的数学数据集提供了可扩展且准确的解决方案，减少了标签噪声并避免了对无效问题的不必要计算。

Abstract: Large Language Models (LLMs) have recently achieved remarkable progress in
mathematical reasoning. To enable such capabilities, many existing works
distill strong reasoning models into long chains of thought or design
algorithms to construct high-quality math QA data for training. However, these
efforts primarily focus on generating correct reasoning paths and answers,
while largely overlooking the validity of the questions themselves. In this
work, we propose Math Question Verification (MathQ-Verify), a novel five-stage
pipeline designed to rigorously filter ill-posed or under-specified math
problems. MathQ-Verify first performs format-level validation to remove
redundant instructions and ensure that each question is syntactically
well-formed. It then formalizes each question, decomposes it into atomic
conditions, and verifies them against mathematical definitions. Next, it
detects logical contradictions among these conditions, followed by a
goal-oriented completeness check to ensure the question provides sufficient
information for solving. To evaluate this task, we use existing benchmarks
along with an additional dataset we construct, containing 2,147 math questions
with diverse error types, each manually double-validated. Experiments show that
MathQ-Verify achieves state-of-the-art performance across multiple benchmarks,
improving the F1 score by up to 25 percentage points over the direct
verification baseline. It further attains approximately 90% precision and 63%
recall through a lightweight model voting scheme. MathQ-Verify offers a
scalable and accurate solution for curating reliable mathematical datasets,
reducing label noise and avoiding unnecessary computation on invalid questions.
Our code and data are available at https://github.com/scuuy/MathQ-Verify.

</details>


### [30] [Cross-Linguistic Transfer in Multilingual NLP: The Role of Language Families and Morphology](https://arxiv.org/abs/2505.13908)
*Ajitesh Bankula,Praney Bankula*

Main category: cs.CL

TL;DR: 该论文研究了跨语言迁移在语言家族和形态学视角下的表现，探讨了语言家族邻近性和形态相似性对NLP任务性能的影响。


<details>
  <summary>Details</summary>
Motivation: 研究跨语言迁移对于多语言NLP至关重要，尤其是在资源丰富的语言模型应用于低资源语言时。

Method: 通过分析语言家族邻近性和形态相似性，比较多语言模型的性能，并探讨语言学距离指标与迁移结果的相关性。

Result: 研究发现语言家族邻近性和形态相似性对跨语言迁移性能有显著影响，并讨论了如何整合类型学和形态学信息以改进迁移效果。

Conclusion: 论文总结了多语言模型性能的比较结果，并提出了整合语言学信息以优化跨语言迁移的新方法。

Abstract: Cross-lingual transfer has become a crucial aspect of multilingual NLP, as it
allows for models trained on resource-rich languages to be applied to
low-resource languages more effectively. Recently massively multilingual
pre-trained language models (e.g., mBERT, XLM-R) demonstrate strong zero-shot
transfer capabilities[14] [13]. This paper investigates cross-linguistic
transfer through the lens of language families and morphology. Investigating
how language family proximity and morphological similarity affect performance
across NLP tasks. We further discuss our results and how it relates to findings
from recent literature. Overall, we compare multilingual model performance and
review how linguistic distance metrics correlate with transfer outcomes. We
also look into emerging approaches that integrate typological and morphological
information into model pre-training to improve transfer to diverse
languages[18] [19].

</details>


### [31] [Word length predicts word order: "Min-max"-ing drives language evolution](https://arxiv.org/abs/2505.13913)
*Hiram Ring*

Main category: cs.CL

TL;DR: 该论文通过分析1500多种语言的平行数据集，提出了一种基于处理和信息结构竞争压力的'Min-Max'语言演化理论，部分支持了现有对立理论，并解释了历史词序变化。


<details>
  <summary>Details</summary>
Motivation: 当前关于语言结构的理论存在分歧，一派主张天赋起源，另一派强调功能或使用基础。本文旨在通过大规模语言数据分析，探索词序变化的普遍机制，以调和这些对立观点。

Method: 研究使用了包含1500多种语言、133个语系和111个孤立语言的大规模标记平行数据集，进行跨语言相关性分析和回归建模。

Result: 研究发现词类长度与词序在跨语言层面显著相关，但关系复杂；该模型比谱系或语言区域更能解释变异，并成功预测了两个不同谱系线的历史词序变化。

Conclusion: 研究提出了整合处理压力和信息结构的'Min-Max'语言演化理论，与近期效率导向和信息论提案一致，为语言结构演化提供了新解释框架。

Abstract: Current theories of language propose an innate (Baker 2001; Chomsky 1981) or
a functional (Greenberg 1963; Dryer 2007; Hawkins 2014) origin for the surface
structures (i.e. word order) that we observe in languages of the world, while
evolutionary modeling (Dunn et al. 2011) suggests that descent is the primary
factor influencing such patterns. Although there are hypotheses for word order
change from both innate and usage-based perspectives for specific languages and
families, there are key disagreements between the two major proposals for
mechanisms that drive the evolution of language more broadly (Wasow 2002; Levy
2008). This paper proposes a universal underlying mechanism for word order
change based on a large tagged parallel dataset of over 1,500 languages
representing 133 language families and 111 isolates. Results indicate that word
class length is significantly correlated with word order crosslinguistically,
but not in a straightforward manner, partially supporting opposing theories of
processing, while at the same time predicting historical word order change in
two different phylogenetic lines and explaining more variance than descent or
language area in regression models. Such findings suggest an integrated
"Min-Max" theory of language evolution driven by competing pressures of
processing and information structure, aligning with recent efficiency-oriented
(Levshina 2023) and information-theoretic proposals (Zaslavsky 2020; Tucker et
al. 2025).

</details>


### [32] [EEG-to-Text Translation: A Model for Deciphering Human Brain Activity](https://arxiv.org/abs/2505.13936)
*Saydul Akbar Murad,Ashim Dahal,Nick Rahimi*

Main category: cs.CL

TL;DR: 论文提出了一种名为R1 Translator的新模型，通过结合双向LSTM编码器和预训练Transformer解码器，显著提升了EEG信号到文本的解码性能，在多项指标上超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型的快速发展，弥合人脑与语言处理之间的差距成为重要研究方向。现有EEG信号解码模型性能有限，亟需改进。

Method: R1 Translator模型采用双向LSTM编码器处理EEG嵌入以捕捉序列依赖，再通过预训练Transformer解码器生成高质量文本。

Result: R1在ROUGE-1（38.00%）、ROUGE-L（32.51%）、CER（0.5795）和WER（0.7280）等指标上全面超越T5和Brain Translator模型，最高提升达9%。

Conclusion: R1 Translator通过创新架构实现了EEG到文本解码的性能突破，为脑机接口语言解码提供了有效解决方案。

Abstract: With the rapid advancement of large language models like Gemini, GPT, and
others, bridging the gap between the human brain and language processing has
become an important area of focus. To address this challenge, researchers have
developed various models to decode EEG signals into text. However, these models
still face significant performance limitations. To overcome these shortcomings,
we propose a new model, R1 Translator, which aims to improve the performance of
EEG-to-text decoding. The R1 Translator model combines a bidirectional LSTM
encoder with a pretrained transformer-based decoder, utilizing EEG features to
produce high-quality text outputs. The model processes EEG embeddings through
the LSTM to capture sequential dependencies, which are then fed into the
transformer decoder for effective text generation. The R1 Translator excels in
ROUGE metrics, outperforming both T5 (previous research) and Brain Translator.
Specifically, R1 achieves a ROUGE-1 score of 38.00% (P), which is up to 9%
higher than T5 (34.89%) and 3% better than Brain (35.69%). It also leads in
ROUGE-L, with a F1 score of 32.51%, outperforming T5 by 3% (29.67%) and Brain
by 2% (30.38%). In terms of CER, R1 achieves a CER of 0.5795, which is 2% lower
than T5 (0.5917) and 4% lower than Brain (0.6001). Additionally, R1 performs
better in WER with a score of 0.7280, outperforming T5 by 4.3% (0.7610) and
Brain by 3.6% (0.7553). Code is available at
https://github.com/Mmurrad/EEG-To-text.

</details>


### [33] [Towards Rehearsal-Free Continual Relation Extraction: Capturing Within-Task Variance with Adaptive Prompting](https://arxiv.org/abs/2505.13944)
*Bao-Ngoc Dao,Quang Nguyen,Luyen Ngo Dinh,Minh Le,Nam Le,Linh Ngo Van*

Main category: cs.CL

TL;DR: WAVE++提出了一种基于提示的持续关系抽取方法，通过任务特定提示池和标签描述提升性能，解决了现有方法在任务识别和遗忘问题上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有基于记忆的持续关系抽取方法存在内存和隐私问题，而基于提示的方法在任务识别和防止灾难性遗忘方面仍有不足。

Method: WAVE++结合前缀调优和专家混合思想，引入任务特定提示池和标签描述，并采用免训练机制优化任务预测。

Result: 实验表明WAVE++在持续关系抽取任务上优于现有基于提示和记忆的方法。

Conclusion: WAVE++为持续关系抽取提供了更鲁棒的解决方案，无需显式存储数据，性能显著提升。

Abstract: Memory-based approaches have shown strong performance in Continual Relation
Extraction (CRE). However, storing examples from previous tasks increases
memory usage and raises privacy concerns. Recently, prompt-based methods have
emerged as a promising alternative, as they do not rely on storing past
samples. Despite this progress, current prompt-based techniques face several
core challenges in CRE, particularly in accurately identifying task identities
and mitigating catastrophic forgetting. Existing prompt selection strategies
often suffer from inaccuracies, lack robust mechanisms to prevent forgetting in
shared parameters, and struggle to handle both cross-task and within-task
variations. In this paper, we propose WAVE++, a novel approach inspired by the
connection between prefix-tuning and mixture of experts. Specifically, we
introduce task-specific prompt pools that enhance flexibility and adaptability
across diverse tasks while avoiding boundary-spanning risks; this design more
effectively captures variations within each task and across tasks. To further
refine relation classification, we incorporate label descriptions that provide
richer, more global context, enabling the model to better distinguish among
different relations. We also propose a training-free mechanism to improve task
prediction during inference. Moreover, we integrate a generative model to
consolidate prior knowledge within the shared parameters, thereby removing the
need for explicit data storage. Extensive experiments demonstrate that WAVE++
outperforms state-of-the-art prompt-based and rehearsal-based methods, offering
a more robust solution for continual relation extraction. Our code is publicly
available at https://github.com/PiDinosauR2804/WAVE-CRE-PLUS-PLUS.

</details>


### [34] [Memory-Centric Embodied Question Answer](https://arxiv.org/abs/2505.13948)
*Mingliang Zhai,Zhi Gao,Yuwei Wu,Yunde Jia*

Main category: cs.CL

TL;DR: 论文提出了一种以记忆为中心的EQA框架MemoryEQA，通过多模态分层记忆机制提升复杂任务处理效率，并在新构建的数据集MT-HM3D上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有EQA框架以规划器为核心，记忆模块无法充分与其他模块交互，限制了处理跨区域多目标复杂任务的能力。

Method: 提出MemoryEQA框架，建立全局记忆（语言增强场景图）和局部记忆（历史观测）的分层机制，利用多模态大模型将记忆信息动态注入各模块。

Result: 在HM-EQA、MT-HM3D和OpenEQA数据集上验证，MT-HM3D性能提升19.8%，证明记忆能力对复杂任务的关键作用。

Conclusion: 以记忆为中心的框架能显著提升EQA系统性能，多模态分层记忆机制是解决跨区域多目标问题的有效方案。

Abstract: Embodied Question Answering (EQA) requires agents to autonomously explore and
understand the environment to answer context-dependent questions. Existing
frameworks typically center around the planner, which guides the stopping
module, memory module, and answering module for reasoning. In this paper, we
propose a memory-centric EQA framework named MemoryEQA. Unlike planner-centric
EQA models where the memory module cannot fully interact with other modules,
MemoryEQA flexible feeds memory information into all modules, thereby enhancing
efficiency and accuracy in handling complex tasks, such as those involving
multiple targets across different regions. Specifically, we establish a
multi-modal hierarchical memory mechanism, which is divided into global memory
that stores language-enhanced scene maps, and local memory that retains
historical observations and state information. When performing EQA tasks, the
multi-modal large language model is leveraged to convert memory information
into the required input formats for injection into different modules. To
evaluate EQA models' memory capabilities, we constructed the MT-HM3D dataset
based on HM3D, comprising 1,587 question-answer pairs involving multiple
targets across various regions, which requires agents to maintain memory of
exploration-acquired target information. Experimental results on HM-EQA,
MT-HM3D, and OpenEQA demonstrate the effectiveness of our framework, where a
19.8% performance gain on MT-HM3D compared to baseline model further
underscores memory capability's pivotal role in resolving complex tasks.

</details>


### [35] [FlashThink: An Early Exit Method For Efficient Reasoning](https://arxiv.org/abs/2505.13949)
*Guochao Jiang,Guofeng Quan,Zepeng Ding,Ziqin Luo,Dixuan Wang,Zheng Hu*

Main category: cs.CL

TL;DR: 论文提出FlashThink方法，通过提前终止大语言模型的推理过程来减少计算开销，同时保持准确性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在推理任务中生成过长的推理内容，导致不必要的计算开销，而实际上模型可能在推理中途已能得出正确答案。

Method: 引入验证模型，动态判断何时可以提前终止推理过程，从而缩短推理内容。

Result: 在四个基准测试中，FlashThink将Deepseek-R1和QwQ-32B模型的推理内容长度分别减少77.04%和77.47%，且不影响准确性。

Conclusion: FlashThink方法有效提升了大语言模型的推理效率，显著减少计算开销，同时保持模型性能。

Abstract: Large Language Models (LLMs) have shown impressive performance in reasoning
tasks. However, LLMs tend to generate excessively long reasoning content,
leading to significant computational overhead. Our observations indicate that
even on simple problems, LLMs tend to produce unnecessarily lengthy reasoning
content, which is against intuitive expectations. Preliminary experiments show
that at a certain point during the generation process, the model is already
capable of producing the correct solution without completing the full reasoning
content. Therefore, we consider that the reasoning process of the model can be
exited early to achieve the purpose of efficient reasoning. We introduce a
verification model that identifies the exact moment when the model can stop
reasoning and still provide the correct answer. Comprehensive experiments on
four different benchmarks demonstrate that our proposed method, FlashThink,
effectively shortens the reasoning content while preserving the model accuracy.
For the Deepseek-R1 and QwQ-32B models, we reduced the length of reasoning
content by 77.04% and 77.47%, respectively, without reducing the accuracy.

</details>


### [36] [Through a Compressed Lens: Investigating the Impact of Quantization on LLM Explainability and Interpretability](https://arxiv.org/abs/2505.13963)
*Qianli Wang,Mingyang Wang,Nils Feldhus,Simon Ostermann,Yuan Cao,Hinrich Schütze,Sebastian Möller,Vera Schmitt*

Main category: cs.CL

TL;DR: 量化方法对大型语言模型的可解释性和透明度影响复杂，效果因方法、评估协议不同而异，需谨慎应用于透明度要求高的场景。


<details>
  <summary>Details</summary>
Motivation: 尽管量化技术广泛用于加速大型语言模型的推理和部署，但其对模型可解释性和透明度的影响尚未被充分研究。本文旨在填补这一空白，探讨量化如何影响模型决策过程的可理解性。

Method: 研究采用三种常见量化技术在不同比特宽度下，结合两种可解释性方法（反事实示例和自然语言解释）和两种可解释性分析（知识记忆分析和潜在多跳推理分析），并通过用户研究评估量化对可解释性的影响。

Result: 研究发现，量化对模型可解释性和透明度的影响因配置不同而异，效果方向不一致。在某些情况下量化会降低可解释性，而在其他情况下甚至可能改善。

Conclusion: 量化可能不可预测地影响模型透明度，这对在透明度要求高的应用中部署大型语言模型具有重要启示。研究提醒需谨慎考虑量化对模型可解释性的潜在影响。

Abstract: Quantization methods are widely used to accelerate inference and streamline
the deployment of large language models (LLMs). While prior research has
extensively investigated the degradation of various LLM capabilities due to
quantization, its effects on model explainability and interpretability, which
are crucial for understanding decision-making processes, remain unexplored. To
address this gap, we conduct comprehensive experiments using three common
quantization techniques at distinct bit widths, in conjunction with two
explainability methods, counterfactual examples and natural language
explanations, as well as two interpretability approaches, knowledge
memorization analysis and latent multi-hop reasoning analysis. We complement
our analysis with a thorough user study, evaluating selected explainability
methods. Our findings reveal that, depending on the configuration, quantization
can significantly impact model explainability and interpretability. Notably,
the direction of this effect is not consistent, as it strongly depends on (1)
the quantization method, (2) the explainability or interpretability approach,
and (3) the evaluation protocol. In some settings, human evaluation shows that
quantization degrades explainability, while in others, it even leads to
improvements. Our work serves as a cautionary tale, demonstrating that
quantization can unpredictably affect model transparency. This insight has
important implications for deploying LLMs in applications where transparency is
a critical requirement.

</details>


### [37] [CAFES: A Collaborative Multi-Agent Framework for Multi-Granular Multimodal Essay Scoring](https://arxiv.org/abs/2505.13965)
*Jiamin Su,Yibo Yan,Zhuoran Gao,Han Zhang,Xiang Liu,Xuming Hu*

Main category: cs.CL

TL;DR: CAFES是一个多代理协作框架，用于提升自动作文评分的准确性和人类对齐性。


<details>
  <summary>Details</summary>
Motivation: 传统自动作文评分方法在评估泛化性和多模态感知方面存在不足，而现有的多模态大语言模型方法可能产生与人类判断不符的分数和理由。

Method: CAFES框架包含三个专业代理：初始评分器、反馈池管理器和反思评分器，通过协作迭代优化评分。

Result: 实验表明，CAFES在QWK指标上相对提升了21%，尤其在语法和词汇多样性方面表现突出。

Conclusion: CAFES为智能多模态自动作文评分系统开辟了新途径。

Abstract: Automated Essay Scoring (AES) is crucial for modern education, particularly
with the increasing prevalence of multimodal assessments. However, traditional
AES methods struggle with evaluation generalizability and multimodal
perception, while even recent Multimodal Large Language Model (MLLM)-based
approaches can produce hallucinated justifications and scores misaligned with
human judgment. To address the limitations, we introduce CAFES, the first
collaborative multi-agent framework specifically designed for AES. It
orchestrates three specialized agents: an Initial Scorer for rapid,
trait-specific evaluations; a Feedback Pool Manager to aggregate detailed,
evidence-grounded strengths; and a Reflective Scorer that iteratively refines
scores based on this feedback to enhance human alignment. Extensive
experiments, using state-of-the-art MLLMs, achieve an average relative
improvement of 21% in Quadratic Weighted Kappa (QWK) against ground truth,
especially for grammatical and lexical diversity. Our proposed CAFES framework
paves the way for an intelligent multimodal AES system. The code will be
available upon acceptance.

</details>


### [38] [Truth or Twist? Optimal Model Selection for Reliable Label Flipping Evaluation in LLM-based Counterfactuals](https://arxiv.org/abs/2505.13972)
*Qianli Wang,Van Bach Nguyen,Nils Feldhus,Luis Felipe Villa-Arenas,Christin Seifert,Sebastian Möller,Vera Schmitt*

Main category: cs.CL

TL;DR: 研究发现，在反事实数据增强中，使用独立且未经微调的评估模型能提供最可靠的标签翻转评估，但自动化流程仍需人工干预。


<details>
  <summary>Details</summary>
Motivation: 反事实数据增强（CDA）广泛用于提升大语言模型的性能和鲁棒性，但评估模型的选择导致结果不一致，因此需要研究不同关系对评估的影响。

Method: 定义了生成模型与评估模型之间的四种关系，并通过实验（两种先进方法、三个数据集、五种生成模型和15种评估模型）和用户研究（n=90）进行分析。

Result: 独立且未经微调的评估模型提供最可靠的标签翻转评估，且与用户研究结果一致的模型关系能提升性能和鲁棒性，但自动化流程与人工评估仍有较大差距。

Conclusion: 完全自动化的CDA流程可能不足，需要结合人工干预以确保评估的可靠性。

Abstract: Counterfactual examples are widely employed to enhance the performance and
robustness of large language models (LLMs) through counterfactual data
augmentation (CDA). However, the selection of the judge model used to evaluate
label flipping, the primary metric for assessing the validity of generated
counterfactuals for CDA, yields inconsistent results. To decipher this, we
define four types of relationships between the counterfactual generator and
judge models. Through extensive experiments involving two state-of-the-art
LLM-based methods, three datasets, five generator models, and 15 judge models,
complemented by a user study (n = 90), we demonstrate that judge models with an
independent, non-fine-tuned relationship to the generator model provide the
most reliable label flipping evaluations. Relationships between the generator
and judge models, which are closely aligned with the user study for CDA, result
in better model performance and robustness. Nevertheless, we find that the gap
between the most effective judge models and the results obtained from the user
study remains considerably large. This suggests that a fully automated pipeline
for CDA may be inadequate and requires human intervention.

</details>


### [39] [Toward Effective Reinforcement Learning Fine-Tuning for Medical VQA in Vision-Language Models](https://arxiv.org/abs/2505.13973)
*Wenhui Zhu,Xuanzhao Dong,Xin Li,Peijie Qiu,Xiwen Chen,Abolfazl Razi,Aris Sotiras,Yi Su,Yalin Wang*

Main category: cs.CL

TL;DR: 该论文研究了基于强化学习的多模态大语言模型在医学视觉问答任务中的优化，分析了四个关键因素，并证明GRPO方法优于标准监督微调。


<details>
  <summary>Details</summary>
Motivation: 为了将模型响应与临床期望对齐，研究者探索了影响强化学习调优在医学视觉问答中有效性的四个关键维度。

Method: 通过实验分析了基础模型初始化策略、医学语义对齐的作用、基于长度的奖励对长链推理的影响以及偏差的影响。

Result: 实验结果表明，基于GRPO的强化学习调优在准确性和推理质量上均优于标准监督微调。

Conclusion: 该研究为医学领域特定微调提供了新见解，并验证了GRPO方法在医学任务中的优越性。

Abstract: Recently, reinforcement learning (RL)-based tuning has shifted the trajectory
of Multimodal Large Language Models (MLLMs), particularly following the
introduction of Group Relative Policy Optimization (GRPO). However, directly
applying it to medical tasks remains challenging for achieving clinically
grounded model behavior. Motivated by the need to align model response with
clinical expectations, we investigate four critical dimensions that affect the
effectiveness of RL-based tuning in medical visual question answering (VQA):
base model initialization strategy, the role of medical semantic alignment, the
impact of length-based rewards on long-chain reasoning, and the influence of
bias. We conduct extensive experiments to analyze these factors for medical
MLLMs, providing new insights into how models are domain-specifically
fine-tuned. Additionally, our results also demonstrate that GRPO-based RL
tuning consistently outperforms standard supervised fine-tuning (SFT) in both
accuracy and reasoning quality.

</details>


### [40] [DRP: Distilled Reasoning Pruning with Skill-aware Step Decomposition for Efficient Large Reasoning Models](https://arxiv.org/abs/2505.13975)
*Yuxuan Jiang,Dawei Li,Frank Ferraro*

Main category: cs.CL

TL;DR: 论文提出DRP框架，通过推理剪枝与蒸馏结合，显著提升大模型推理效率而不损失精度。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型（LRMs）在复杂推理任务中表现优异，但其冗长的推理链导致效率低下。为解决这一问题，研究团队提出了DRP框架。

Method: DRP结合推理剪枝与蒸馏技术，利用教师模型进行技能感知的步骤分解和内容剪枝，并将剪枝后的推理路径蒸馏到学生模型中。

Result: 在多个数学推理数据集上，DRP显著提升了token效率（如GSM8K上token使用减少64%，准确率提升2.4%），且不影响模型性能。

Conclusion: 研究表明，将训练推理链与学生模型的推理能力对齐，对知识迁移和性能提升至关重要。DRP框架有效平衡了效率与准确性。

Abstract: While Large Reasoning Models (LRMs) have demonstrated success in complex
reasoning tasks through long chain-of-thought (CoT) reasoning, their inference
often involves excessively verbose reasoning traces, resulting in substantial
inefficiency. To address this, we propose Distilled Reasoning Pruning (DRP), a
hybrid framework that combines inference-time pruning with tuning-based
distillation, two widely used strategies for efficient reasoning. DRP uses a
teacher model to perform skill-aware step decomposition and content pruning,
and then distills the pruned reasoning paths into a student model, enabling it
to reason both efficiently and accurately. Across several challenging
mathematical reasoning datasets, we find that models trained with DRP achieve
substantial improvements in token efficiency without sacrificing accuracy.
Specifically, DRP reduces average token usage on GSM8K from 917 to 328 while
improving accuracy from 91.7% to 94.1%, and achieves a 43% token reduction on
AIME with no performance drop. Further analysis shows that aligning the
reasoning structure of training CoTs with the student's reasoning capacity is
critical for effective knowledge transfer and performance gains.

</details>


### [41] [Mixed Signals: Understanding Model Disagreement in Multimodal Empathy Detection](https://arxiv.org/abs/2505.13979)
*Maya Srikanth,Run Chen,Julia Hirschberg*

Main category: cs.CL

TL;DR: 多模态模型在共情检测中表现不一，模态间的冲突信号会导致性能下降。研究发现，单模态与多模态预测的分歧常源于数据本身的模糊性，且人类同样不总能从多模态输入中获益。


<details>
  <summary>Details</summary>
Motivation: 探讨多模态模型在共情检测中因模态间信号冲突导致的性能下降问题，分析单模态与多模态预测分歧的原因及其对模型鲁棒性的影响。

Method: 使用微调后的文本、音频和视频单模态模型，结合门控融合模型，对比单模态与多模态预测结果，并通过标注者不确定性验证分歧案例的模糊性。

Result: 发现单一模态的主导信号若缺乏其他模态支持会误导融合结果；人类与模型类似，多模态输入并不总能带来一致性能提升。

Conclusion: 预测分歧可作为诊断信号，帮助识别挑战性样本并提升共情检测系统的鲁棒性。

Abstract: Multimodal models play a key role in empathy detection, but their performance
can suffer when modalities provide conflicting cues. To understand these
failures, we examine cases where unimodal and multimodal predictions diverge.
Using fine-tuned models for text, audio, and video, along with a gated fusion
model, we find that such disagreements often reflect underlying ambiguity, as
evidenced by annotator uncertainty. Our analysis shows that dominant signals in
one modality can mislead fusion when unsupported by others. We also observe
that humans, like models, do not consistently benefit from multimodal input.
These insights position disagreement as a useful diagnostic signal for
identifying challenging examples and improving empathy system robustness.

</details>


### [42] [The Hallucination Tax of Reinforcement Finetuning](https://arxiv.org/abs/2505.13988)
*Linxin Song,Taiwei Shi,Jieyu Zhao*

Main category: cs.CL

TL;DR: 研究发现强化微调(RFT)会降低大语言模型对不可答问题的拒绝率，导致幻觉回答增加。通过引入合成不可答数学数据集(SUM)，仅需10%数据即可显著恢复模型的拒绝行为。


<details>
  <summary>Details</summary>
Motivation: 探讨强化微调(RFT)对大语言模型可信度的影响，特别是发现其会导致模型对不可答问题产生更多幻觉回答（称为'幻觉税'）。

Method: 构建SUM数据集（含不可答数学问题），用于测试模型识别不可答问题的能力，并在RFT训练中掺入10% SUM数据。

Result: 标准RFT使模型拒绝率下降超80%，而加入少量SUM数据可显著恢复拒绝行为，且对可解任务准确率影响极小，还能提升模型对不确定性的推理能力。

Conclusion: 在RFT中引入不可答问题训练能有效平衡模型性能与可信度，使模型更好地认知自身知识边界，该效果可泛化至数学和事实问答领域。

Abstract: Reinforcement finetuning (RFT) has become a standard approach for enhancing
the reasoning capabilities of large language models (LLMs). However, its impact
on model trustworthiness remains underexplored. In this work, we identify and
systematically study a critical side effect of RFT, which we term the
hallucination tax: a degradation in refusal behavior causing models to produce
hallucinated answers to unanswerable questions confidently. To investigate
this, we introduce SUM (Synthetic Unanswerable Math), a high-quality dataset of
unanswerable math problems designed to probe models' ability to recognize an
unanswerable question by reasoning from the insufficient or ambiguous
information. Our results show that standard RFT training could reduce model
refusal rates by more than 80%, which significantly increases model's tendency
to hallucinate. We further demonstrate that incorporating just 10% SUM during
RFT substantially restores appropriate refusal behavior, with minimal accuracy
trade-offs on solvable tasks. Crucially, this approach enables LLMs to leverage
inference-time compute to reason about their own uncertainty and knowledge
boundaries, improving generalization not only to out-of-domain math problems
but also to factual question answering tasks.

</details>


### [43] [DecIF: Improving Instruction-Following through Meta-Decomposition](https://arxiv.org/abs/2505.13990)
*Tingfeng Hui,Pengyu Zhu,Bowen Ping,Ling Tang,Yaqi Zhang,Sen Su*

Main category: cs.CL

TL;DR: DecIF框架通过分解原则，仅用LLMs自主生成多样且高质量的指令跟随数据，提升模型的灵活性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖外部资源生成指令数据，限制了灵活性和泛化性，DecIF旨在解决这一问题。

Method: DecIF通过元分解指导LLMs迭代生成元信息，结合响应约束形成结构化指令，并检测解决不一致性。

Result: 实验表明DecIF在多种场景下表现优异，能自动合成高质量指令数据，具有强灵活性和可扩展性。

Conclusion: DecIF为指令跟随任务提供了一种高效自主的数据生成方法，展现了广泛的应用潜力。

Abstract: Instruction-following has emerged as a crucial capability for large language
models (LLMs). However, existing approaches often rely on pre-existing
documents or external resources to synthesize instruction-following data, which
limits their flexibility and generalizability. In this paper, we introduce
DecIF, a fully autonomous, meta-decomposition guided framework that generates
diverse and high-quality instruction-following data using only LLMs. DecIF is
grounded in the principle of decomposition. For instruction generation, we
guide LLMs to iteratively produce various types of meta-information, which are
then combined with response constraints to form well-structured and
semantically rich instructions. We further utilize LLMs to detect and resolve
potential inconsistencies within the generated instructions. Regarding response
generation, we decompose each instruction into atomic-level evaluation
criteria, enabling rigorous validation and the elimination of inaccurate
instruction-response pairs. Extensive experiments across a wide range of
scenarios and settings demonstrate DecIF's superior performance on
instruction-following tasks. Further analysis highlights its strong
flexibility, scalability, and generalizability in automatically synthesizing
high-quality instruction data.

</details>


### [44] [Social Sycophancy: A Broader Understanding of LLM Sycophancy](https://arxiv.org/abs/2505.13995)
*Myra Cheng,Sunny Yu,Cinoo Lee,Pranav Khadpe,Lujain Ibrahim,Dan Jurafsky*

Main category: cs.CL

TL;DR: 论文提出LLMs存在过度迎合用户的'社交谄媚'问题，并开发ELEPHANT框架量化评估，发现模型在模糊情境中比人类更倾向维护用户面子，且难以缓解。


<details>
  <summary>Details</summary>
Motivation: 现有研究仅关注LLMs对用户明确观点的附和，但忽略了无明确事实场景（如建议寻求）中的谄媚行为，这种隐性迎合可能强化有害假设或行为。

Method: 提出'社交谄媚'理论框架ELEPHANT，通过五种面子维护行为（情感认同、道德支持等）在OEQ和AITA数据集上评估8个模型。

Result: LLMs社交谄媚率显著高于人类（OEQ+47%），在AITA中42%情况下认可人类评判不当的行为，且这种行为在偏好数据集中被奖励。

Conclusion: 研究为LLMs隐性谄媚问题提供理论工具，证明其普遍性及缓解难度，呼吁关注这一未被充分认识但影响重大的问题。

Abstract: A serious risk to the safety and utility of LLMs is sycophancy, i.e.,
excessive agreement with and flattery of the user. Yet existing work focuses on
only one aspect of sycophancy: agreement with users' explicitly stated beliefs
that can be compared to a ground truth. This overlooks forms of sycophancy that
arise in ambiguous contexts such as advice and support-seeking, where there is
no clear ground truth, yet sycophancy can reinforce harmful implicit
assumptions, beliefs, or actions. To address this gap, we introduce a richer
theory of social sycophancy in LLMs, characterizing sycophancy as the excessive
preservation of a user's face (the positive self-image a person seeks to
maintain in an interaction). We present ELEPHANT, a framework for evaluating
social sycophancy across five face-preserving behaviors (emotional validation,
moral endorsement, indirect language, indirect action, and accepting framing)
on two datasets: open-ended questions (OEQ) and Reddit's r/AmITheAsshole
(AITA). Across eight models, we show that LLMs consistently exhibit high rates
of social sycophancy: on OEQ, they preserve face 47% more than humans, and on
AITA, they affirm behavior deemed inappropriate by crowdsourced human judgments
in 42% of cases. We further show that social sycophancy is rewarded in
preference datasets and is not easily mitigated. Our work provides theoretical
grounding and empirical tools (datasets and code) for understanding and
addressing this under-recognized but consequential issue.

</details>


### [45] [Activation-Guided Consensus Merging for Large Language Models](https://arxiv.org/abs/2505.14009)
*Yuxuan Yao,Shuqi Liu,Zehua Liu,Qintong Li,Mingyang Liu,Xiongwei Han,Zhijiang Guo,Han Wu,Linqi Song*

Main category: cs.CL

TL;DR: 该论文提出了一种名为ACM的模型融合框架，通过激活引导的共识合并方法，有效整合不同大型语言模型的能力，提升推理准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 现有方法在整合System 1的高效性和System 2的推理能力时面临效率和稳定性挑战，且传统模型融合方法忽略了神经组件的功能异质性。

Method: 提出ACM框架，基于预训练和微调模型激活间的互信息确定层特定融合系数，无需梯度计算或额外训练。

Result: 在L2S和一般融合任务中，ACM表现优于基线方法，如在Qwen-7B模型上，响应长度减少55.3%，推理准确性提高1.3点。

Conclusion: ACM是一种即插即用的模型融合方法，能有效保留任务特定能力，显著提升模型性能。

Abstract: Recent research has increasingly focused on reconciling the reasoning
capabilities of System 2 with the efficiency of System 1. While existing
training-based and prompt-based approaches face significant challenges in terms
of efficiency and stability, model merging emerges as a promising strategy to
integrate the diverse capabilities of different Large Language Models (LLMs)
into a unified model. However, conventional model merging methods often assume
uniform importance across layers, overlooking the functional heterogeneity
inherent in neural components. To address this limitation, we propose
\textbf{A}ctivation-Guided \textbf{C}onsensus \textbf{M}erging (\textbf{ACM}),
a plug-and-play merging framework that determines layer-specific merging
coefficients based on mutual information between activations of pre-trained and
fine-tuned models. ACM effectively preserves task-specific capabilities without
requiring gradient computations or additional training. Extensive experiments
on Long-to-Short (L2S) and general merging tasks demonstrate that ACM
consistently outperforms all baseline methods. For instance, in the case of
Qwen-7B models, TIES-Merging equipped with ACM achieves a \textbf{55.3\%}
reduction in response length while simultaneously improving reasoning accuracy
by \textbf{1.3} points. We submit the code with the paper for reproducibility,
and it will be publicly available.

</details>


### [46] [AUTOLAW: Enhancing Legal Compliance in Large Language Models via Case Law Generation and Jury-Inspired Deliberation](https://arxiv.org/abs/2505.14015)
*Tai D. Nguyen,Long H. Pham,Jun Sun*

Main category: cs.CL

TL;DR: 提出AutoLaw框架，通过对抗数据生成和陪审团式审议提升法律大模型的合规性检测能力。


<details>
  <summary>Details</summary>
Motivation: 现有法律评估基准缺乏对区域法律差异的适应性，难以应对动态变化的监管环境。

Method: 结合对抗数据生成和模拟陪审团审议的动态法律合成方法，通过排名筛选LLM陪审员减少偏见。

Result: 在Law-SG等三个基准测试中，对抗数据提升模型判别力，陪审团投票策略显著提高违规检测率。

Conclusion: AutoLaw能自适应探测法律偏差，提供可扩展的敏感领域LLM评估方案。

Abstract: The rapid advancement of domain-specific large language models (LLMs) in
fields like law necessitates frameworks that account for nuanced regional legal
distinctions, which are critical for ensuring compliance and trustworthiness.
Existing legal evaluation benchmarks often lack adaptability and fail to
address diverse local contexts, limiting their utility in dynamically evolving
regulatory landscapes. To address these gaps, we propose AutoLaw, a novel
violation detection framework that combines adversarial data generation with a
jury-inspired deliberation process to enhance legal compliance of LLMs. Unlike
static approaches, AutoLaw dynamically synthesizes case law to reflect local
regulations and employs a pool of LLM-based "jurors" to simulate judicial
decision-making. Jurors are ranked and selected based on synthesized legal
expertise, enabling a deliberation process that minimizes bias and improves
detection accuracy. Evaluations across three benchmarks: Law-SG, Case-SG
(legality), and Unfair-TOS (policy), demonstrate AutoLaw's effectiveness:
adversarial data generation improves LLM discrimination, while the jury-based
voting strategy significantly boosts violation detection rates. Our results
highlight the framework's ability to adaptively probe legal misalignments and
deliver reliable, context-aware judgments, offering a scalable solution for
evaluating and enhancing LLMs in legally sensitive applications.

</details>


### [47] [From Unaligned to Aligned: Scaling Multilingual LLMs with Multi-Way Parallel Corpora](https://arxiv.org/abs/2505.14045)
*Yingli Shen,Wen Lai,Shuo Wang,Kangyang Luo,Alexander Fraser,Maosong Sun*

Main category: cs.CL

TL;DR: 论文提出使用多语言平行语料库TED2025提升大语言模型的多语言性能，实验证明其优于非对齐数据。


<details>
  <summary>Details</summary>
Motivation: 现有大规模多语言数据未对齐，难以有效捕捉跨语言语义。多语言平行数据能提供更强的跨语言一致性，提升模型性能。

Method: 基于TED Talks构建包含113种语言、最多50种语言对齐的大规模高质量平行语料库TED2025，研究其在持续预训练和指令微调中的应用策略。

Result: 在六个多语言基准测试中，基于平行数据训练的模型性能始终优于非对齐数据训练的模型。

Conclusion: 多语言平行数据能显著提升大语言模型的多语言性能，TED2025为相关研究提供了高质量资源。

Abstract: Continued pretraining and instruction tuning on large-scale multilingual data
have proven to be effective in scaling large language models (LLMs) to
low-resource languages. However, the unaligned nature of such data limits its
ability to effectively capture cross-lingual semantics. In contrast, multi-way
parallel data, where identical content is aligned across multiple languages,
provides stronger cross-lingual consistency and offers greater potential for
improving multilingual performance. In this paper, we introduce a large-scale,
high-quality multi-way parallel corpus, TED2025, based on TED Talks. The corpus
spans 113 languages, with up to 50 languages aligned in parallel, ensuring
extensive multilingual coverage. Using this dataset, we investigate best
practices for leveraging multi-way parallel data to enhance LLMs, including
strategies for continued pretraining, instruction tuning, and the analysis of
key influencing factors. Experiments on six multilingual benchmarks show that
models trained on multiway parallel data consistently outperform those trained
on unaligned multilingual data.

</details>


### [48] [Improved Methods for Model Pruning and Knowledge Distillation](https://arxiv.org/abs/2505.14052)
*Wei Jiang,Anying Fu,Youling Zhang*

Main category: cs.CL

TL;DR: 提出MAMA剪枝法，通过权重和GRPO奖励指标，在保持性能的同时大幅压缩大语言模型。


<details>
  <summary>Details</summary>
Motivation: 现有剪枝方法导致性能显著下降或需要大量微调，需开发高效剪枝技术以平衡模型大小与性能。

Method: 基于预训练阶段固定的权重/偏置和训练后GRPO奖励作为剪枝指标，结合运动与幅度分析（MAMA）。

Result: 在极端剪枝水平下仍保持原始模型性能，优于现有方法，适用于多种下游NLP任务。

Conclusion: MAMA剪枝法实现了模型高效压缩与性能保留的平衡，为大型语言模型优化提供新方案。

Abstract: Model pruning is a performance optimization technique for large language
models like R1 or o3-mini. However, existing pruning methods often lead to
significant performance degradation or require extensive retraining and
fine-tuning. This technique aims to identify and remove neurons, connections
unlikely leading to the contribution during the human-computer interaction
phase. Our goal is to obtain a much smaller and faster knowledge distilled
model that can quickly generate content almost as good as those of the unpruned
ones. We propose MAMA Pruning, short for Movement and Magnitude Analysis, an
improved pruning method that effectively reduces model size and computational
complexity while maintaining performance comparable to the original unpruned
model even at extreme pruned levels. The improved method is based on weights,
bias fixed in the pre-training phase and GRPO rewards verified during the
post-training phase as our novel pruning indicators. Preliminary experimental
results show that our method outperforms and be comparable to state-of-the-art
methods across various pruning levels and different downstream computational
linguistics tasks.

</details>


### [49] [Enhancing LLMs via High-Knowledge Data Selection](https://arxiv.org/abs/2505.14070)
*Feiyu Duan,Xuemiao Zhang,Sirui Wang,Haoran Que,Yuqi Liu,Wenge Rong,Xunliang Cai*

Main category: cs.CL

TL;DR: 提出了一种无梯度的高知识评分器（HKS），通过知识密度和覆盖率筛选高质量训练数据，提升大语言模型在知识密集任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有数据选择方法忽视文本语料的知识丰富性，导致预训练语料存在知识稀缺问题。

Method: 构建多领域知识元素池，设计基于知识密度和覆盖率的评分器HKS，支持领域定制化数据筛选。

Result: 实验表明HKS筛选的高知识双语数据集能同时提升模型在知识密集型任务和通用理解任务中的性能。

Conclusion: HKS通过知识维度优化数据选择，有效增强模型的通用能力和领域特定能力。

Abstract: The performance of Large Language Models (LLMs) is intrinsically linked to
the quality of its training data. Although several studies have proposed
methods for high-quality data selection, they do not consider the importance of
knowledge richness in text corpora. In this paper, we propose a novel and
gradient-free High-Knowledge Scorer (HKS) to select high-quality data from the
dimension of knowledge, to alleviate the problem of knowledge scarcity in the
pre-trained corpus. We propose a comprehensive multi-domain knowledge element
pool and introduce knowledge density and coverage as metrics to assess the
knowledge content of the text. Based on this, we propose a comprehensive
knowledge scorer to select data with intensive knowledge, which can also be
utilized for domain-specific high-knowledge data selection by restricting
knowledge elements to the specific domain. We train models on a high-knowledge
bilingual dataset, and experimental results demonstrate that our scorer
improves the model's performance in knowledge-intensive and general
comprehension tasks, and is effective in enhancing both the generic and
domain-specific capabilities of the model.

</details>


### [50] [BAR: A Backward Reasoning based Agent for Complex Minecraft Tasks](https://arxiv.org/abs/2505.14079)
*Weihong Du,Wenrui Liao,Binyu Yan,Hongru Liang,Anthony G. Cohn,Wenqiang Lei*

Main category: cs.CL

TL;DR: 提出基于逆向推理的BAR智能体，通过从目标状态反向规划解决复杂任务，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于正向推理的LLM智能体在复杂任务中表现不佳，因初始状态与目标间存在巨大感知差距。

Method: 设计BAR智能体，包含递归目标分解、状态一致性维护和阶段记忆模块，实现从终态开始的稳健规划。

Result: 实验证明BAR优于现有方法，且各模块均有效。

Conclusion: 逆向推理能有效解决复杂任务规划问题，BAR框架具有实际应用价值。

Abstract: Large language model (LLM) based agents have shown great potential in
following human instructions and automatically completing various tasks. To
complete a task, the agent needs to decompose it into easily executed steps by
planning. Existing studies mainly conduct the planning by inferring what steps
should be executed next starting from the agent's initial state. However, this
forward reasoning paradigm doesn't work well for complex tasks. We propose to
study this issue in Minecraft, a virtual environment that simulates complex
tasks based on real-world scenarios. We believe that the failure of forward
reasoning is caused by the big perception gap between the agent's initial state
and task goal. To this end, we leverage backward reasoning and make the
planning starting from the terminal state, which can directly achieve the task
goal in one step. Specifically, we design a BAckward Reasoning based agent
(BAR). It is equipped with a recursive goal decomposition module, a state
consistency maintaining module and a stage memory module to make robust,
consistent, and efficient planning starting from the terminal state.
Experimental results demonstrate the superiority of BAR over existing methods
and the effectiveness of proposed modules.

</details>


### [51] [Gender Trouble in Language Models: An Empirical Audit Guided by Gender Performativity Theory](https://arxiv.org/abs/2505.14080)
*Franziska Sofia Hafner,Ana Valdivia,Luc Rocher*

Main category: cs.CL

TL;DR: 语言模型编码并延续有害的性别刻板印象，现有研究仅解除了非性别词汇与性别词汇的关联，但忽略了性别建构本身带来的问题。论文呼吁扩展'性别偏见'定义，实证分析16种语言模型，发现其将性别二元化且与生理性别绑定，导致跨性别者被边缘化。模型越大，性别与生理关联越强。


<details>
  <summary>Details</summary>
Motivation: 当前语言模型性别偏见研究局限于词汇关联，而性别建构理论指出更深层问题（如性别与生理性别的混淆）会导致跨性别者被抹杀、下游应用伤害（如错误诊断）。需重新定义'性别偏见'以涵盖这些结构性危害。

Method: 基于性别研究理论，对16种不同架构、训练数据和规模的语言模型进行实证分析，检验其如何编码性别概念。

Result: 模型普遍将性别编码为与生理性别绑定的二元类别，非二元性别词汇被边缘化或病理化。更大模型在性能提升的同时，性别与生理性别的关联更强。

Conclusion: 需重新评估语言模型性别危害的定义与解决方式，当前方法（如解除词汇关联）不足以应对性别建构本身带来的结构性偏见。

Abstract: Language models encode and subsequently perpetuate harmful gendered
stereotypes. Research has succeeded in mitigating some of these harms, e.g. by
dissociating non-gendered terms such as occupations from gendered terms such as
'woman' and 'man'. This approach, however, remains superficial given that
associations are only one form of prejudice through which gendered harms arise.
Critical scholarship on gender, such as gender performativity theory,
emphasizes how harms often arise from the construction of gender itself, such
as conflating gender with biological sex. In language models, these issues
could lead to the erasure of transgender and gender diverse identities and
cause harms in downstream applications, from misgendering users to
misdiagnosing patients based on wrong assumptions about their anatomy.
  For FAccT research on gendered harms to go beyond superficial linguistic
associations, we advocate for a broader definition of 'gender bias' in language
models. We operationalize insights on the construction of gender through
language from gender studies literature and then empirically test how 16
language models of different architectures, training datasets, and model sizes
encode gender. We find that language models tend to encode gender as a binary
category tied to biological sex, and that gendered terms that do not neatly
fall into one of these binary categories are erased and pathologized. Finally,
we show that larger models, which achieve better results on performance
benchmarks, learn stronger associations between gender and sex, further
reinforcing a narrow understanding of gender. Our findings lead us to call for
a re-evaluation of how gendered harms in language models are defined and
addressed.

</details>


### [52] [Beyond Chains: Bridging Large Language Models and Knowledge Bases in Complex Question Answering](https://arxiv.org/abs/2505.14099)
*Yihua Zhu,Qianying Liu,Akiko Aizawa,Hidetoshi Shimodaira*

Main category: cs.CL

TL;DR: PDRR框架通过预测、分解、检索和推理四阶段，有效提升KBQA性能，尤其在处理复杂问题时表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有LLM方法存在知识过时、幻觉和透明度不足的问题，而基于链的KG-RAG方法仅适用于简单链式问题。PDRR旨在解决这些限制，提升复杂问题的处理能力。

Method: PDRR框架包含四个阶段：预测问题类型、分解问题为结构化三元组、从知识库检索相关信息、引导LLM代理推理并补全三元组。

Result: 实验表明，PDRR在不同LLM骨干上均优于现有方法，尤其在链式和非链式复杂问题上表现突出。

Conclusion: PDRR通过结合语义解析和知识库检索，显著提升了KBQA的性能和适用范围，为复杂问题解答提供了有效解决方案。

Abstract: Knowledge Base Question Answering (KBQA) aims to answer natural language
questions using structured knowledge from KBs. While LLM-only approaches offer
generalization, they suffer from outdated knowledge, hallucinations, and lack
of transparency. Chain-based KG-RAG methods address these issues by
incorporating external KBs, but are limited to simple chain-structured
questions due to the absence of planning and logical structuring. Inspired by
semantic parsing methods, we propose PDRR: a four-stage framework consisting of
Predict, Decompose, Retrieve, and Reason. Our method first predicts the
question type and decomposes the question into structured triples. Then
retrieves relevant information from KBs and guides the LLM as an agent to
reason over and complete the decomposed triples. Experimental results
demonstrate that PDRR consistently outperforms existing methods across various
LLM backbones and achieves superior performance on both chain-structured and
non-chain complex questions.

</details>


### [53] [MultiHal: Multilingual Dataset for Knowledge-Graph Grounded Evaluation of LLM Hallucinations](https://arxiv.org/abs/2505.14101)
*Ernests Lavrinovics,Russa Biswas,Katja Hose,Johannes Bjerva*

Main category: cs.CL

TL;DR: 论文提出基于知识图谱的多语言多跳基准MultiHal，用于评估生成文本的事实性，通过整合KG提升语义相似度得分。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型存在事实性幻觉问题，且现有评估基准多依赖英语数据集和附加文本信息，缺乏结构化知识图谱的支持和多语言覆盖。

Method: 从开放域知识图谱挖掘14万条路径，筛选出2.59万条高质量路径，构建多语言多跳基准MultiHal，并对比KG-RAG与传统QA的效果。

Result: 实验表明，整合知识图谱后，多语言多模型下的语义相似度得分绝对提升0.12-0.36分。

Conclusion: MultiHal基准有望推动基于图谱的幻觉缓解和事实核查研究，证明KG整合的潜力。

Abstract: Large Language Models (LLMs) have inherent limitations of faithfulness and
factuality, commonly referred to as hallucinations. Several benchmarks have
been developed that provide a test bed for factuality evaluation within the
context of English-centric datasets, while relying on supplementary informative
context like web links or text passages but ignoring the available structured
factual resources. To this end, Knowledge Graphs (KGs) have been identified as
a useful aid for hallucination mitigation, as they provide a structured way to
represent the facts about entities and their relations with minimal linguistic
overhead. We bridge the lack of KG paths and multilinguality for factual
language modeling within the existing hallucination evaluation benchmarks and
propose a KG-based multilingual, multihop benchmark called \textbf{MultiHal}
framed for generative text evaluation. As part of our data collection pipeline,
we mined 140k KG-paths from open-domain KGs, from which we pruned noisy
KG-paths, curating a high-quality subset of 25.9k. Our baseline evaluation
shows an absolute scale increase by approximately 0.12 to 0.36 points for the
semantic similarity score in KG-RAG over vanilla QA across multiple languages
and multiple models, demonstrating the potential of KG integration. We
anticipate MultiHal will foster future research towards several graph-based
hallucination mitigation and fact-checking tasks.

</details>


### [54] [Legal Rule Induction: Towards Generalizable Principle Discovery from Analogous Judicial Precedents](https://arxiv.org/abs/2505.14104)
*Wei Fan,Tianshi Zheng,Yiran Hu,Zheye Deng,Weiqi Wang,Baixuan Xu,Chunyang Li,Haoran Li,Weixing Shen,Yangqiu Song*

Main category: cs.CL

TL;DR: 该论文提出了法律规则归纳（LRI）任务，旨在从类似判例中提取隐含的法律规则，并构建了首个LRI基准数据集。实验表明，现有大型语言模型在任务中存在过度泛化和幻觉问题，但通过训练可显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 当前计算法律研究主要关注已有规则的应用，而从司法判决中归纳法律规则的研究不足，受限于模型推理能力和符号推理的局限性。大型语言模型的出现为自动化提取隐含法律原则提供了新机遇，但缺乏正式任务定义、基准数据集和方法论。

Method: 论文将法律规则归纳（LRI）任务形式化为从类似判例中提取简洁、可推广的法律规则，包括共同前提、规范行为和法律后果。构建了包含5,121个案例集（总计38,088个中国案例）的基准数据集，其中216个为专家标注的测试集。

Result: 实验结果显示：1）最先进的大型语言模型在LRI任务中存在过度泛化和幻觉问题；2）使用该数据集训练显著提升了模型在捕捉类似案例间细微规则模式的能力。

Conclusion: 该研究填补了法律规则归纳领域的空白，提出的基准数据集和方法为未来研究奠定了基础，同时揭示了大型语言模型在法律规则归纳任务中的潜力和挑战。

Abstract: Legal rules encompass not only codified statutes but also implicit
adjudicatory principles derived from precedents that contain discretionary
norms, social morality, and policy. While computational legal research has
advanced in applying established rules to cases, inducing legal rules from
judicial decisions remains understudied, constrained by limitations in model
inference efficacy and symbolic reasoning capability. The advent of Large
Language Models (LLMs) offers unprecedented opportunities for automating the
extraction of such latent principles, yet progress is stymied by the absence of
formal task definitions, benchmark datasets, and methodologies. To address this
gap, we formalize Legal Rule Induction (LRI) as the task of deriving concise,
generalizable doctrinal rules from sets of analogous precedents, distilling
their shared preconditions, normative behaviors, and legal consequences. We
introduce the first LRI benchmark, comprising 5,121 case sets (38,088 Chinese
cases in total) for model tuning and 216 expert-annotated gold test sets.
Experimental results reveal that: 1) State-of-the-art LLMs struggle with
over-generalization and hallucination; 2) Training on our dataset markedly
enhances LLMs capabilities in capturing nuanced rule patterns across similar
cases.

</details>


### [55] [A Personalized Conversational Benchmark: Towards Simulating Personalized Conversations](https://arxiv.org/abs/2505.14106)
*Li Li,Peilin Cai,Ryan A. Rossi,Franck Dernoncourt,Branislav Kveton,Junda Wu,Tong Yu,Linxin Song,Tiankai Yang,Yuehan Qin,Nesreen K. Ahmed,Samyadeep Basu,Subhojyoti Mukherjee,Ruiyi Zhang,Zhengmian Hu,Bo Ni,Yuxiao Zhou,Zichao Wang,Yue Huang,Yu Wang,Xiangliang Zhang,Philip S. Yu,Xiyang Hu,Yue Zhao*

Main category: cs.CL

TL;DR: PersonaConvBench是一个大规模基准测试，用于评估多轮对话中大型语言模型的个性化推理和生成能力，整合了个性化和对话结构，并在多个任务上测试了不同模型。


<details>
  <summary>Details</summary>
Motivation: 现有研究往往单独关注个性化或对话结构，缺乏对两者结合的系统性分析。PersonaConvBench旨在填补这一空白，支持研究如何利用个性化对话上下文提升模型表现。

Method: 构建包含10个Reddit领域的基准测试，设计三个核心任务（句子分类、影响回归、用户中心文本生成），并在统一提示设置下对商业和开源LLM进行测试。

Result: 实验表明，引入个性化历史数据能显著提升模型性能，如在情感分类任务中相对最佳非对话基线实现了198%的性能增益。

Conclusion: 通过发布PersonaConvBench及其评估代码，该研究为LLM适应个体风格、追踪长期上下文及生成丰富响应提供了重要支持。

Abstract: We present PersonaConvBench, a large-scale benchmark for evaluating
personalized reasoning and generation in multi-turn conversations with large
language models (LLMs). Unlike existing work that focuses on either
personalization or conversational structure in isolation, PersonaConvBench
integrates both, offering three core tasks: sentence classification, impact
regression, and user-centric text generation across ten diverse Reddit-based
domains. This design enables systematic analysis of how personalized
conversational context shapes LLM outputs in realistic multi-user scenarios. We
benchmark several commercial and open-source LLMs under a unified prompting
setup and observe that incorporating personalized history yields substantial
performance improvements, including a 198 percent relative gain over the best
non-conversational baseline in sentiment classification. By releasing
PersonaConvBench with evaluations and code, we aim to support research on LLMs
that adapt to individual styles, track long-term context, and produce
contextually rich, engaging responses.

</details>


### [56] [DiagnosisArena: Benchmarking Diagnostic Reasoning for Large Language Models](https://arxiv.org/abs/2505.14107)
*Yakun Zhu,Zhongzhen Huang,Linjie Mu,Yutong Huang,Wei Nie,Shaoting Zhang,Pengfei Liu,Xiaofan Zhang*

Main category: cs.CL

TL;DR: 论文提出了DiagnosisArena基准测试，用于系统评估大语言模型在临床诊断推理中的能力，发现当前最先进模型表现不佳。


<details>
  <summary>Details</summary>
Motivation: 为了确保大语言模型在真实医疗环境中的安全有效应用，需要系统评估其诊断能力。现有医学基准在评估高级诊断推理方面存在不足。

Method: 通过筛选10种顶级医学期刊的临床病例报告，构建包含1,113对病例和诊断的DiagnosisArena基准，涵盖28个医学专科，并经过AI和专家多轮审核。

Result: 最先进的推理模型o3-mini、o1和DeepSeek-R1的准确率分别仅为45.82%、31.09%和17.79%，显示当前模型在临床诊断推理中存在显著泛化瓶颈。

Conclusion: DiagnosisArena旨在推动AI诊断推理能力的进步，为解决真实临床诊断挑战提供更有效方案，并公开了基准和评估工具供进一步研究。

Abstract: The emergence of groundbreaking large language models capable of performing
complex reasoning tasks holds significant promise for addressing various
scientific challenges, including those arising in complex clinical scenarios.
To enable their safe and effective deployment in real-world healthcare
settings, it is urgently necessary to benchmark the diagnostic capabilities of
current models systematically. Given the limitations of existing medical
benchmarks in evaluating advanced diagnostic reasoning, we present
DiagnosisArena, a comprehensive and challenging benchmark designed to
rigorously assess professional-level diagnostic competence. DiagnosisArena
consists of 1,113 pairs of segmented patient cases and corresponding diagnoses,
spanning 28 medical specialties, deriving from clinical case reports published
in 10 top-tier medical journals. The benchmark is developed through a
meticulous construction pipeline, involving multiple rounds of screening and
review by both AI systems and human experts, with thorough checks conducted to
prevent data leakage. Our study reveals that even the most advanced reasoning
models, o3-mini, o1, and DeepSeek-R1, achieve only 45.82%, 31.09%, and 17.79%
accuracy, respectively. This finding highlights a significant generalization
bottleneck in current large language models when faced with clinical diagnostic
reasoning challenges. Through DiagnosisArena, we aim to drive further
advancements in AIs diagnostic reasoning capabilities, enabling more effective
solutions for real-world clinical diagnostic challenges. We provide the
benchmark and evaluation tools for further research and development
https://github.com/SPIRAL-MED/DiagnosisArena.

</details>


### [57] [Invisible Entropy: Towards Safe and Efficient Low-Entropy LLM Watermarking](https://arxiv.org/abs/2505.14112)
*Tianle Gu,Zongqi Wang,Kexin Huang,Yuanqi Yao,Xiangliang Zhang,Yujiu Yang,Xiuying Chen*

Main category: cs.CL

TL;DR: 提出Invisible Entropy (IE)水印方法，通过轻量级特征提取器和熵标记器预测高低熵，解决低熵场景下水印难题，提升安全性和效率。


<details>
  <summary>Details</summary>
Motivation: 现有基于Logit的LLM水印方法在低熵场景下效果不佳，且依赖原始模型导致计算成本高和模型泄露风险。

Method: 引入轻量级特征提取器和熵标记器预测高低熵，开发自适应熵阈值的阈值导航器，优化水印文本自然性和检测鲁棒性。

Result: 在HumanEval和MBPP数据集上，IE减少99%参数量，性能与最先进方法相当。

Conclusion: IE为低熵水印提供安全高效的解决方案，显著提升水印效果和实用性。

Abstract: Logit-based LLM watermarking traces and verifies AI-generated content by
maintaining green and red token lists and increasing the likelihood of green
tokens during generation. However, it fails in low-entropy scenarios, where
predictable outputs make green token selection difficult without disrupting
natural text flow. Existing approaches address this by assuming access to the
original LLM to calculate entropy and selectively watermark high-entropy
tokens. However, these methods face two major challenges: (1) high
computational costs and detection delays due to reliance on the original LLM,
and (2) potential risks of model leakage. To address these limitations, we
propose Invisible Entropy (IE), a watermarking paradigm designed to enhance
both safety and efficiency. Instead of relying on the original LLM, IE
introduces a lightweight feature extractor and an entropy tagger to predict
whether the entropy of the next token is high or low. Furthermore, based on
theoretical analysis, we develop a threshold navigator that adaptively sets
entropy thresholds. It identifies a threshold where the watermark ratio
decreases as the green token count increases, enhancing the naturalness of the
watermarked text and improving detection robustness. Experiments on HumanEval
and MBPP datasets demonstrate that IE reduces parameter size by 99\% while
achieving performance on par with state-of-the-art methods. Our work introduces
a safe and efficient paradigm for low-entropy watermarking.
https://github.com/Carol-gutianle/IE
https://huggingface.co/datasets/Carol0110/IE-Tagger

</details>


### [58] [Self-Reasoning Language Models: Unfold Hidden Reasoning Chains with Few Reasoning Catalyst](https://arxiv.org/abs/2505.14116)
*Hongru Wang,Deng Cai,Wanjun Zhong,Shijue Huang,Jeff Z. Pan,Zeming Liu,Kam-Fai Wong*

Main category: cs.CL

TL;DR: 论文提出自推理语言模型（SRLM），通过自我训练合成更长的思维链数据，显著提升大语言模型在复杂推理任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法通过增加思维链长度提升大语言模型推理能力，但长推理链的生成和获取困难。本文旨在让模型自身合成更长的思维链数据，通过自我训练迭代提升性能。

Method: 引入自推理语言模型（SRLM），利用少量示范样本（如1000个）作为推理催化剂，指导模型从现有响应中展开隐藏推理链，并通过自我训练迭代优化。

Result: SRLM在五个推理任务（MMLU、GSM8K、ARC-C、HellaSwag、BBH）上平均绝对提升超过2.5分，采样64次时平均提升达7.89分，展现出深度、多样和创造性的推理路径。

Conclusion: SRLM通过自我训练合成更长的思维链数据，不仅提升初始性能，还能在迭代中实现更稳定和一致的改进，显著增强大语言模型的推理能力。

Abstract: Inference-time scaling has attracted much attention which significantly
enhance the performance of Large Language Models (LLMs) in complex reasoning
tasks by increasing the length of Chain-of-Thought. These longer intermediate
reasoning rationales embody various meta-reasoning skills in human cognition,
such as reflection and decomposition, being difficult to create and acquire. In
this work, we introduce \textit{Self-Reasoning Language Model} (SRLM), where
the model itself can synthesize longer CoT data and iteratively improve
performance through self-training. By incorporating a few demonstration
examples (i.e., 1,000 samples) on how to unfold hidden reasoning chains from
existing responses, which act as a reasoning catalyst, we demonstrate that SRLM
not only enhances the model's initial performance but also ensures more stable
and consistent improvements in subsequent iterations. Our proposed SRLM
achieves an average absolute improvement of more than $+2.5$ points across five
reasoning tasks: MMLU, GSM8K, ARC-C, HellaSwag, and BBH on two backbone models.
Moreover, it brings more improvements with more times of sampling during
inference, such as absolute $+7.89$ average improvement with $64$ sampling
times, revealing the in-depth, diverse and creative reasoning paths in SRLM
against the strong baseline.

</details>


### [59] [Probing BERT for German Compound Semantics](https://arxiv.org/abs/2505.14130)
*Filip Miletić,Aaron Schmid,Sabine Schulte im Walde*

Main category: cs.CL

TL;DR: 该研究探讨了预训练德语BERT模型对名词复合词语义的编码能力，发现其表现不及英语同类模型，可能与德语复合词的高产性和歧义性有关。


<details>
  <summary>Details</summary>
Motivation: 研究旨在评估预训练德语BERT模型在编码名词复合词语义方面的能力，并与英语模型的表现进行对比。

Method: 通过系统地变化目标词、层数以及模型大小写设置，并使用868个标准复合词进行组合性预测，分析了Transformer架构中的表征模式。

Result: 研究发现，组合性信息在模型的早期层中最易提取，但德语模型的表现明显落后于英语模型，表明德语复合词的处理更具挑战性。

Conclusion: 德语复合词的高产性和成分歧义性可能是导致模型表现不佳的原因，这为未来研究提供了方向。

Abstract: This paper investigates the extent to which pretrained German BERT encodes
knowledge of noun compound semantics. We comprehensively vary combinations of
target tokens, layers, and cased vs. uncased models, and evaluate them by
predicting the compositionality of 868 gold standard compounds. Looking at
representational patterns within the transformer architecture, we observe
trends comparable to equivalent prior work on English, with compositionality
information most easily recoverable in the early layers. However, our strongest
results clearly lag behind those reported for English, suggesting an inherently
more difficult task in German. This may be due to the higher productivity of
compounding in German than in English and the associated increase in
constituent-level ambiguity, including in our target compound set.

</details>


### [60] [Texts or Images? A Fine-grained Analysis on the Effectiveness of Input Representations and Models for Table Question Answering](https://arxiv.org/abs/2505.14131)
*Wei Zhou,Mohsen Mesgar,Heike Adel,Annemarie Friedrich*

Main category: cs.CL

TL;DR: 该论文通过控制实验比较了表格问答任务中不同表格表示与模型的组合效果，提出了动态选择表示方法FRES，性能提升10%。


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏对表格问答任务中文本与图像表示方法在复杂性和表格大小影响下的精细对比，需系统评估不同组合的效果差异。

Method: 基于现有数据集构建新基准，系统分析7对多模态大模型与纯文本模型的组合效果，提出动态表示选择方法FRES。

Result: 实验表明最佳组合因场景而异，FRES方法比 indiscriminately 使用两种表示平均提升10%性能。

Conclusion: 表格表示与模型的选择应动态适配任务特性，FRES为表格问答提供了有效的自适应解决方案。

Abstract: In table question answering (TQA), tables are encoded as either texts or
images. Prior work suggests that passing images of tables to multi-modal large
language models (MLLMs) performs comparably to or even better than using
textual input with large language models (LLMs). However, the lack of
controlled setups limits fine-grained distinctions between these approaches. In
this paper, we conduct the first controlled study on the effectiveness of
several combinations of table representations and models from two perspectives:
question complexity and table size. We build a new benchmark based on existing
TQA datasets. In a systematic analysis of seven pairs of MLLMs and LLMs, we
find that the best combination of table representation and model varies across
setups. We propose FRES, a method selecting table representations dynamically,
and observe a 10% average performance improvement compared to using both
representations indiscriminately.

</details>


### [61] [Enhancing Keyphrase Extraction from Academic Articles Using Section Structure Information](https://arxiv.org/abs/2505.14149)
*Chengzhi Zhang,Xinyi Yan,Lei Zhao,Yingyi Zhang*

Main category: cs.CL

TL;DR: 该论文提出了一种利用学术文章章节结构信息进行关键词提取的方法，通过结合结构特征和章节文本，提高了关键词提取的性能。


<details>
  <summary>Details</summary>
Motivation: 随着学术论文数量的指数级增长，研究人员查找相关文献的时间大幅增加。关键词提取（KPE）能够帮助高效检索文献，但现有方法受限于摘要长度或全文噪声问题。

Method: 论文方法包括两部分：(1) 探究七种结构特征对KPE模型的影响，(2) 通过关键词整合算法将各章节文本的提取结果融合。同时研究了章节结构分类质量对KPE性能的影响。

Result: 结果表明，结构特征的引入提升了KPE性能，不同特征对模型效果影响各异。关键词整合方法表现最佳，章节结构分类质量也会影响KPE性能。

Conclusion: 利用学术文章的章节结构信息有助于实现有效的关键词提取。相关代码和数据集已开源。

Abstract: The exponential increase in academic papers has significantly increased the
time required for researchers to access relevant literature. Keyphrase
Extraction (KPE) offers a solution to this situation by enabling researchers to
efficiently retrieve relevant literature. The current study on KPE from
academic articles aims to improve the performance of extraction models through
innovative approaches using Title and Abstract as input corpora. However, the
semantic richness of keywords is significantly constrained by the length of the
abstract. While full-text-based KPE can address this issue, it simultaneously
introduces noise, which significantly diminishes KPE performance. To address
this issue, this paper utilized the structural features and section texts
obtained from the section structure information of academic articles to extract
keyphrase from academic papers. The approach consists of two main parts: (1)
exploring the effect of seven structural features on KPE models, and (2)
integrating the extraction results from all section texts used as input corpora
for KPE models via a keyphrase integration algorithm to obtain the keyphrase
integration result. Furthermore, this paper also examined the effect of the
classification quality of section structure on the KPE performance. The results
show that incorporating structural features improves KPE performance, though
different features have varying effects on model efficacy. The keyphrase
integration approach yields the best performance, and the classification
quality of section structure can affect KPE performance. These findings
indicate that using the section structure information of academic articles
contributes to effective KPE from academic articles. The code and dataset
supporting this study are available at https://github.com/yan-xinyi/SSB_KPE.

</details>


### [62] [Prior Prompt Engineering for Reinforcement Fine-Tuning](https://arxiv.org/abs/2505.14157)
*Pittawat Taveekitworachai,Potsawee Manakul,Sarana Nutanong,Kunat Pipatanakul*

Main category: cs.CL

TL;DR: 该论文研究了在强化微调（RFT）中先验提示工程（pPE）的作用，发现不同pPE方法能引导语言模型内化不同行为，其中空示例pPE方法表现最佳。


<details>
  <summary>Details</summary>
Motivation: 现有RFT研究主要关注算法、奖励塑造和数据筛选，而先验提示设计在训练中引导模型行为的作用尚未充分探索。

Method: 将五种推理时提示工程（iPE）策略转化为pPE方法，并在Qwen2.5-7B模型上进行实验，评估其在领域内外基准测试中的表现。

Result: 所有pPE训练模型均优于iPE提示模型，空示例pPE方法平均性能提升最大，在AIME2024和GPQA-Diamond上表现最佳。

Conclusion: pPE是RFT中一个强大但未被充分研究的维度，不同pPE策略能赋予模型不同的行为风格。

Abstract: This paper investigates prior prompt engineering (pPE) in the context of
reinforcement fine-tuning (RFT), where language models (LMs) are incentivized
to exhibit behaviors that maximize performance through reward signals. While
existing RFT research has primarily focused on algorithms, reward shaping, and
data curation, the design of the prior prompt--the instructions prepended to
queries during training to elicit behaviors such as step-by-step
reasoning--remains underexplored. We investigate whether different pPE
approaches can guide LMs to internalize distinct behaviors after RFT. Inspired
by inference-time prompt engineering (iPE), we translate five representative
iPE strategies--reasoning, planning, code-based reasoning, knowledge recall,
and null-example utilization--into corresponding pPE approaches. We experiment
with Qwen2.5-7B using each of the pPE approaches, then evaluate performance on
in-domain and out-of-domain benchmarks (e.g., AIME2024, HumanEval+, and
GPQA-Diamond). Our results show that all pPE-trained models surpass their
iPE-prompted counterparts, with the null-example pPE approach achieving the
largest average performance gain and the highest improvement on AIME2024 and
GPQA-Diamond, surpassing the commonly used reasoning approach. Furthermore, by
adapting a behavior-classification framework, we demonstrate that different pPE
strategies instill distinct behavioral styles in the resulting models. These
findings position pPE as a powerful yet understudied axis for RFT.

</details>


### [63] [Temporal Alignment of Time Sensitive Facts with Activation Engineering](https://arxiv.org/abs/2505.14158)
*Sanjay Govindan,Maurice Pagnucco,Yang Song*

Main category: cs.CL

TL;DR: 该研究通过激活工程技术改进LLMs的时间对齐能力，无需训练或数据集创建，显著提升了事实回忆的准确性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）训练数据涵盖多个领域和时间段，部分知识仅在特定时间背景下有效。确保模型生成时间准确的响应对于保持相关性和准确性至关重要。

Method: 研究采用激活工程技术，将三个版本的LLaMA 2模型锚定到特定时间点，并探讨不同注入层和提示策略的效果。

Result: 实验显示，相对提示和显式提示分别提升了44%和16%，性能与微调方法相当，但计算效率更高且无需预对齐数据集。

Conclusion: 激活工程技术是一种高效的时间对齐方法，可在不增加计算负担的情况下显著提升LLMs的时间敏感问题回答能力。

Abstract: Large Language Models (LLMs) are trained on diverse and often conflicting
knowledge spanning multiple domains and time periods. Some of this knowledge is
only valid within specific temporal contexts, such as answering the question,
"Who is the President of the United States in 2022?" Ensuring LLMs generate
time appropriate responses is crucial for maintaining relevance and accuracy.
In this work we explore activation engineering as a method for temporally
aligning LLMs to improve factual recall without any training or dataset
creation. In this research we explore an activation engineering technique to
ground three versions of LLaMA 2 to specific points in time and examine the
effects of varying injection layers and prompting strategies. Our experiments
demonstrate up to a 44% and 16% improvement in relative and explicit prompting
respectively, achieving comparable performance to the fine-tuning method
proposed by Zhao et al. (2024) . Notably, our approach achieves similar results
to the fine-tuning baseline while being significantly more computationally
efficient and requiring no pre-aligned datasets.

</details>


### [64] [Breaking Language Barriers or Reinforcing Bias? A Study of Gender and Racial Disparities in Multilingual Contrastive Vision Language Models](https://arxiv.org/abs/2505.14160)
*Zahraa Al Sahili,Ioannis Patras,Matthew Purver*

Main category: cs.CL

TL;DR: 研究发现多语言视觉语言模型存在性别和种族偏见，且多语言性并未减轻偏见，反而在某些情况下加剧了偏见。


<details>
  <summary>Details</summary>
Motivation: 尽管多语言视觉语言模型在图像-文本检索方面表现出色，但其社会偏见尚未得到充分研究。本文旨在系统评估这些模型在不同语言中的偏见表现。

Method: 研究使用FairFace和PATA数据集，在零样本设置下评估了三种多语言CLIP模型（M-CLIP、NLLB-CLIP和CAPIVARA-CLIP）在十种语言中的性别和种族偏见，以及刻板印象的放大效应。

Result: 所有模型在多语言环境下的性别偏见均强于其英语基线，尤其是低资源语言和目标语言。共享跨语言编码器会将英语的性别刻板印象传播到性别中立的语言中，而松散耦合的编码器则避免了这种传播。

Conclusion: 多语言视觉语言模型在不同语言中表现出显著的偏见，尤其是性别偏见。未来的研究需要进行更细粒度、语言感知的偏见评估，以避免偏见传播和放大。

Abstract: Multilingual vision-language models promise universal image-text retrieval,
yet their social biases remain under-explored. We present the first systematic
audit of three public multilingual CLIP checkpoints -- M-CLIP, NLLB-CLIP, and
CAPIVARA-CLIP -- across ten languages that vary in resource availability and
grammatical gender. Using balanced subsets of \textsc{FairFace} and the
\textsc{PATA} stereotype suite in a zero-shot setting, we quantify race and
gender bias and measure stereotype amplification. Contrary to the assumption
that multilinguality mitigates bias, every model exhibits stronger gender bias
than its English-only baseline. CAPIVARA-CLIP shows its largest biases
precisely in the low-resource languages it targets, while the shared
cross-lingual encoder of NLLB-CLIP transports English gender stereotypes into
gender-neutral languages; loosely coupled encoders largely avoid this transfer.
Highly gendered languages consistently magnify all measured bias types, but
even gender-neutral languages remain vulnerable when cross-lingual weight
sharing imports foreign stereotypes. Aggregated metrics conceal
language-specific ``hot spots,'' underscoring the need for fine-grained,
language-aware bias evaluation in future multilingual vision-language research.

</details>


### [65] [PL-FGSA: A Prompt Learning Framework for Fine-Grained Sentiment Analysis Based on MindSpore](https://arxiv.org/abs/2505.14165)
*Zhenkai Qin,Jiajing He,Qiao Fang*

Main category: cs.CL

TL;DR: PL-FGSA是一种基于提示学习的细粒度情感分析框架，通过多任务提示增强生成方法，在多个数据集上表现优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统细粒度情感分析方法需要特定任务架构和大量标注数据，限制了其泛化能力和扩展性。PL-FGSA旨在解决这些问题，提供更通用的解决方案。

Method: PL-FGSA采用基于提示学习的统一框架，结合轻量级TextCNN主干网络，将细粒度情感分析重新定义为多任务提示增强生成问题。

Result: 在SST-2、SemEval-2014 Task 4和MAMS三个基准数据集上，PL-FGSA的F1分数分别达到0.922、0.694和0.597，优于传统微调方法。

Conclusion: PL-FGSA通过提示学习实现了更好的泛化性能，在实际情感分析任务中具有实用价值。

Abstract: Fine-grained sentiment analysis (FGSA) aims to identify sentiment polarity
toward specific aspects within a text, enabling more precise opinion mining in
domains such as product reviews and social media. However, traditional FGSA
approaches often require task-specific architectures and extensive annotated
data, limiting their generalization and scalability. To address these
challenges, we propose PL-FGSA, a unified prompt learning-based framework
implemented using the MindSpore platform, which integrates prompt design with a
lightweight TextCNN backbone. Our method reformulates FGSA as a multi-task
prompt-augmented generation problem, jointly tackling aspect extraction,
sentiment classification, and causal explanation in a unified paradigm. By
leveraging prompt-based guidance, PL-FGSA enhances interpretability and
achieves strong performance under both full-data and low-resource conditions.
Experiments on three benchmark datasets-SST-2, SemEval-2014 Task 4, and
MAMS-demonstrate that our model consistently outperforms traditional
fine-tuning methods and achieves F1-scores of 0.922, 0.694, and 0.597,
respectively. These results validate the effectiveness of prompt-based
generalization and highlight the practical value of PL-FGSA for real-world
sentiment analysis tasks.

</details>


### [66] [The Strawberry Problem: Emergence of Character-level Understanding in Tokenized Language Models](https://arxiv.org/abs/2505.14172)
*Adrian Cosma,Stefan Ruseti,Emilian Radoi,Mihai Dascalu*

Main category: cs.CL

TL;DR: 大语言模型在字符级任务上表现不佳，研究通过概念涌现理论分析并提出轻量级架构改进。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在多领域取得进展，但在简单字符级任务（如字母计数）上表现不佳，主要受限于分词机制。

Method: 使用19项合成任务隔离字符级推理，分析概念涌现模式，并提出轻量级架构改进。

Result: 字符级能力在训练后期缓慢且突然涌现，改进架构显著提升性能并保留子词模型优势。

Conclusion: 研究填补了分词模型的感知缺陷，为理解和缓解其结构盲点提供了理论框架。

Abstract: Despite their remarkable progress across diverse domains, Large Language
Models (LLMs) consistently fail at simple character-level tasks, such as
counting letters in words, due to a fundamental limitation: tokenization. In
this work, we frame this limitation as a problem of low mutual information and
analyze it in terms of concept emergence. Using a suite of 19 synthetic tasks
that isolate character-level reasoning in a controlled setting, we show that
such capabilities emerge slowly, suddenly, and only late in training. We
further show that percolation-based models of concept emergence explain these
patterns, suggesting that learning character composition is not fundamentally
different from learning commonsense knowledge. To address this bottleneck, we
propose a lightweight architectural modification that significantly improves
character-level reasoning while preserving the inductive advantages of subword
models. Together, our results bridge low-level perceptual gaps in tokenized LMs
and provide a principled framework for understanding and mitigating their
structural blind spots. We make our code publicly available.

</details>


### [67] [THOR-MoE: Hierarchical Task-Guided and Context-Responsive Routing for Neural Machine Translation](https://arxiv.org/abs/2505.14173)
*Yunlong Liang,Fandong Meng,Jie Zhou*

Main category: cs.CL

TL;DR: THOR-MoE提出了一种结合任务引导和上下文感知的混合专家路由策略，显著提升了机器翻译性能。


<details>
  <summary>Details</summary>
Motivation: 当前稀疏混合专家模型在机器翻译中存在两个问题：1) 直接使用任务知识（如领域/语言知识），但这些知识在实际应用中通常不可得；2) 专家选择仅依赖局部标记表示，忽略了全局上下文信息。

Method: THOR-MoE通过分层任务引导和上下文响应路由策略：1) 预测领域/语言标签并提取混合表示以分配任务级专家；2) 注入上下文信息增强标记路由，从预选专家集中精准分配。

Result: 在多领域和多语言翻译基准测试中，THOR-MoE表现优异，平均BLEU提升0.75，且激活参数少于22%。

Conclusion: THOR-MoE是一种即插即用模块，兼容现有路由方案，适用于多种混合专家架构，显著提升翻译性能。

Abstract: The sparse Mixture-of-Experts (MoE) has achieved significant progress for
neural machine translation (NMT). However, there exist two limitations in
current MoE solutions which may lead to sub-optimal performance: 1) they
directly use the task knowledge of NMT into MoE (\emph{e.g.},
domain/linguistics-specific knowledge), which are generally unavailable at
practical application and neglect the naturally grouped domain/linguistic
properties; 2) the expert selection only depends on the localized token
representation without considering the context, which fully grasps the state of
each token in a global view. To address the above limitations, we propose
THOR-MoE via arming the MoE with hierarchical task-guided and
context-responsive routing policies. Specifically, it 1) firstly predicts the
domain/language label and then extracts mixed domain/language representation to
allocate task-level experts in a hierarchical manner; 2) injects the context
information to enhance the token routing from the pre-selected task-level
experts set, which can help each token to be accurately routed to more
specialized and suitable experts. Extensive experiments on multi-domain
translation and multilingual translation benchmarks with different
architectures consistently demonstrate the superior performance of THOR-MoE.
Additionally, the THOR-MoE operates as a plug-and-play module compatible with
existing Top-$k$~\cite{shazeer2017} and Top-$p$~\cite{huang-etal-2024-harder}
routing schemes, ensuring broad applicability across diverse MoE architectures.
For instance, compared with vanilla Top-$p$~\cite{huang-etal-2024-harder}
routing, the context-aware manner can achieve an average improvement of 0.75
BLEU with less than 22\% activated parameters on multi-domain translation
tasks.

</details>


### [68] [Cheaper, Better, Faster, Stronger: Robust Text-to-SQL without Chain-of-Thought or Fine-Tuning](https://arxiv.org/abs/2505.14174)
*Yusuf Denizay Dönder,Derek Hommel,Andrea W Wen-Yi,David Mimno,Unso Eun Seo Jo*

Main category: cs.CL

TL;DR: 论文提出了一种名为N-rep的低成本文本转SQL方法，效果媲美昂贵方案，单次查询成本仅0.039美元。


<details>
  <summary>Details</summary>
Motivation: 现有LLM代码生成方法（如思维链、自洽性、微调）成本高昂，单次查询可达0.46美元，微调模型更需数千美元。

Method: N-rep通过同一模式输入的多种表征来弥补单一表征的弱点，无需推理或微调即可使用更小更便宜的模型。

Result: 在BIRD基准测试中达到与昂贵方法相近的分数，成本降低至每次查询0.039美元。

Conclusion: N-rep是当前该成本区间内性能最优的文本转SQL解决方案。

Abstract: LLMs are effective at code generation tasks like text-to-SQL, but is it worth
the cost? Many state-of-the-art approaches use non-task-specific LLM techniques
including Chain-of-Thought (CoT), self-consistency, and fine-tuning. These
methods can be costly at inference time, sometimes requiring over a hundred LLM
calls with reasoning, incurring average costs of up to \$0.46 per query, while
fine-tuning models can cost thousands of dollars. We introduce "N-rep"
consistency, a more cost-efficient text-to-SQL approach that achieves similar
BIRD benchmark scores as other more expensive methods, at only \$0.039 per
query. N-rep leverages multiple representations of the same schema input to
mitigate weaknesses in any single representation, making the solution more
robust and allowing the use of smaller and cheaper models without any reasoning
or fine-tuning. To our knowledge, N-rep is the best-performing text-to-SQL
approach in its cost range.

</details>


### [69] [Tokenization Constraints in LLMs: A Study of Symbolic and Arithmetic Reasoning Limits](https://arxiv.org/abs/2505.14178)
*Xiang Zhang,Juntai Cao,Jiaqi Wei,Yiwei Xu,Chenyu You*

Main category: cs.CL

TL;DR: 论文探讨了分词方案（如BPE）如何影响语言模型的符号推理能力，提出'Token Awareness'概念，并证明原子对齐的分词格式能显著提升小模型在结构化推理任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注模型架构对推理能力的影响，而忽略了分词作为计算底层的关键作用。作者发现子词分词会破坏原子推理单元的完整性，从而限制模型的符号计算能力。

Method: 通过理论分析和系统实验（算术/符号任务），比较不同分词方案下CoT提示的效果，提出用原子对齐格式优化token级表示。

Result: 实验显示：1）不良分词会导致推理失败；2）原子对齐格式使小模型（GPT-4o-mini）在结构化推理上超越大模型（o1）。

Conclusion: LLMs的符号推理能力不仅取决于架构，更依赖于token级表示质量。优化分词粒度是提升推理性能的关键路径。

Abstract: Tokenization is the first - and often underappreciated - layer of computation
in language models. While Chain-of-Thought (CoT) prompting enables transformer
models to approximate recurrent computation by externalizing intermediate
steps, we show that the success of such reasoning is fundamentally bounded by
the structure of tokenized inputs. This work presents a theoretical and
empirical investigation into how tokenization schemes, particularly
subword-based methods like byte-pair encoding (BPE), impede symbolic
computation by merging or obscuring atomic reasoning units. We introduce the
notion of Token Awareness to formalize how poor token granularity disrupts
logical alignment and prevents models from generalizing symbolic procedures.
Through systematic evaluation on arithmetic and symbolic tasks, we demonstrate
that token structure dramatically affect reasoning performance, causing failure
even with CoT, while atomically-aligned formats unlock strong generalization,
allowing small models (e.g., GPT-4o-mini) to outperform larger systems (e.g.,
o1) in structured reasoning. Our findings reveal that symbolic reasoning
ability in LLMs is not purely architectural, but deeply conditioned on
token-level representations.

</details>


### [70] [Enhancing Abstractive Summarization of Scientific Papers Using Structure Information](https://arxiv.org/abs/2505.14179)
*Tong Bao,Heng Zhang,Chengzhi Zhang*

Main category: cs.CL

TL;DR: 提出两阶段摘要生成框架，通过自动识别论文结构功能提升摘要质量，实验证明优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有摘要模型难以捕捉科学论文的结构信息，且传统方法对不同学科适应性差。

Method: 第一阶段构建结构功能识别数据集并训练分类器；第二阶段用Longformer生成上下文感知摘要。

Result: 在两个领域数据集上超越基线模型，生成更全面的摘要。

Conclusion: 该方法有效解决了科学论文结构信息利用不足的问题，代码和数据集已开源。

Abstract: Abstractive summarization of scientific papers has always been a research
focus, yet existing methods face two main challenges. First, most summarization
models rely on Encoder-Decoder architectures that treat papers as sequences of
words, thus fail to fully capture the structured information inherent in
scientific papers. Second, existing research often use keyword mapping or
feature engineering to identify the structural information, but these methods
struggle with the structural flexibility of scientific papers and lack
robustness across different disciplines. To address these challenges, we
propose a two-stage abstractive summarization framework that leverages
automatic recognition of structural functions within scientific papers. In the
first stage, we standardize chapter titles from numerous scientific papers and
construct a large-scale dataset for structural function recognition. A
classifier is then trained to automatically identify the key structural
components (e.g., Background, Methods, Results, Discussion), which provides a
foundation for generating more balanced summaries. In the second stage, we
employ Longformer to capture rich contextual relationships across sections and
generating context-aware summaries. Experiments conducted on two
domain-specific scientific paper summarization datasets demonstrate that our
method outperforms advanced baselines, and generates more comprehensive
summaries. The code and dataset can be accessed at
https://github.com/tongbao96/code-for-SFR-AS.

</details>


### [71] [SlangDIT: Benchmarking LLMs in Interpretative Slang Translation](https://arxiv.org/abs/2505.14181)
*Yunlong Liang,Fandong Meng,Jiaan Wang,Jie Zhou*

Main category: cs.CL

TL;DR: 该论文提出了一种名为SlangDIT的俚语翻译任务，包含检测、解释和翻译三个子任务，并构建了一个包含2.5万条英中句对的数据集。提出的SlangOWL模型通过深度思考显著提升了大型语言模型的俚语翻译性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究将俚语检测、解释和翻译视为独立任务，忽略了它们之间的内在关联。缺乏一个能体现任务间依赖关系的基准数据集，阻碍了俚语翻译质量的提升。

Method: 构建SlangDIT数据集（25k英中句对），提出SlangOWL模型：先检测俚语，分析多义性，生成上下文相关解释，最后输出翻译。

Result: 实验表明，SlangOWL在Qwen2.5和LLama-3.1等模型上显著优于原始模型和无思维机制的微调模型。

Conclusion: 通过联合处理俚语检测、解释和翻译任务，并引入深度思考机制，能有效提升俚语翻译的准确性。

Abstract: The challenge of slang translation lies in capturing context-dependent
semantic extensions, as slang terms often convey meanings beyond their literal
interpretation. While slang detection, explanation, and translation have been
studied as isolated tasks in the era of large language models (LLMs), their
intrinsic interdependence remains underexplored. The main reason is lacking of
a benchmark where the two tasks can be a prerequisite for the third one, which
can facilitate idiomatic translation. In this paper, we introduce the
interpretative slang translation task (named SlangDIT) consisting of three
sub-tasks: slang detection, cross-lingual slang explanation, and slang
translation within the current context, aiming to generate more accurate
translation with the help of slang detection and slang explanation. To this
end, we construct a SlangDIT dataset, containing over 25k English-Chinese
sentence pairs. Each source sentence mentions at least one slang term and is
labeled with corresponding cross-lingual slang explanation. Based on the
benchmark, we propose a deep thinking model, named SlangOWL. It firstly
identifies whether the sentence contains a slang, and then judges whether the
slang is polysemous and analyze its possible meaning. Further, the SlangOWL
provides the best explanation of the slang term targeting on the current
context. Finally, according to the whole thought, the SlangOWL offers a
suitable translation. Our experiments on LLMs (\emph{e.g.}, Qwen2.5 and
LLama-3.1), show that our deep thinking approach indeed enhances the
performance of LLMs where the proposed SLangOWL significantly surpasses the
vanilla models and supervised fine-tuned models without thinking.

</details>


### [72] [ThinkSwitcher: When to Think Hard, When to Think Fast](https://arxiv.org/abs/2505.14183)
*Guosheng Liang,Longguang Zhong,Ziyi Yang,Xiaojun Quan*

Main category: cs.CL

TL;DR: ThinkSwitcher框架通过动态切换长短推理链，在保持复杂任务准确性的同时降低20-30%计算成本。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型在简单任务上存在过度思考问题，导致不必要的计算开销，需要一种能动态调整推理模式的方法。

Method: 提出ThinkSwitcher框架，通过轻量级切换模块根据任务复杂度动态选择短链或长链推理模式。

Result: 在多个推理基准测试中，ThinkSwitcher在维持复杂任务高精度的同时显著降低计算开销。

Conclusion: ThinkSwitcher为大型推理模型提供了一种可扩展的高效部署解决方案。

Abstract: Large reasoning models (LRMs) excel at solving complex tasks by leveraging
long chain-of-thought (CoT) reasoning. However, this often leads to
overthinking on simple tasks, resulting in unnecessary computational overhead.
We observe that LRMs inherently possess the capability for efficient short CoT
reasoning, which can be reliably elicited through prompt design. To leverage
this capability, we propose ThinkSwitcher, a framework that enables a single
LRM to dynamically switch between short and long CoT modes based on task
complexity. ThinkSwitcher introduces a lightweight switching module trained
with supervision signals derived from the relative performance of each
reasoning mode across tasks. Experiments on multiple reasoning benchmarks show
that ThinkSwitcher reduces computational cost by 20-30% while maintaining high
accuracy on complex tasks. This demonstrates the effectiveness of ThinkSwitcher
as a scalable and efficient solution for unified LRM deployment.

</details>


### [73] [Unraveling Interwoven Roles of Large Language Models in Authorship Privacy: Obfuscation, Mimicking, and Verification](https://arxiv.org/abs/2505.14195)
*Tuc Nguyen,Yifan Hu,Thai Le*

Main category: cs.CL

TL;DR: 该论文提出了首个统一框架，用于分析大语言模型（LLMs）在作者隐私背景下支持的三项任务（作者混淆、模仿和验证）之间的动态关系，并探讨了人口统计元数据对它们的影响。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的广泛应用，用户生成内容中的个人信息和写作风格可能被无意泄露，引发作者隐私的担忧。此前的研究独立探讨了作者混淆、模仿和验证任务，但它们在LLM时代的相互作用尚未充分研究。

Method: 论文提出了一个统一框架，量化分析LLM支持的作者混淆、模仿和验证任务之间的互动关系，并考察了人口统计元数据（如性别、学术背景）对这些任务性能和隐私风险的影响。

Result: 研究揭示了这些任务在单次和迭代转换中的相互作用，以及人口统计元数据如何调节它们的动态关系和隐私风险。

Conclusion: 该研究填补了LLM时代作者隐私研究的空白，为理解和管理文本生成中的隐私风险提供了新的视角和工具。所有源代码将公开。

Abstract: Recent advancements in large language models (LLMs) have been fueled by large
scale training corpora drawn from diverse sources such as websites, news
articles, and books. These datasets often contain explicit user information,
such as person names and addresses, that LLMs may unintentionally reproduce in
their generated outputs. Beyond such explicit content, LLMs can also leak
identity revealing cues through implicit signals such as distinctive writing
styles, raising significant concerns about authorship privacy. There are three
major automated tasks in authorship privacy, namely authorship obfuscation
(AO), authorship mimicking (AM), and authorship verification (AV). Prior
research has studied AO, AM, and AV independently. However, their interplays
remain under explored, which leaves a major research gap, especially in the era
of LLMs, where they are profoundly shaping how we curate and share user
generated content, and the distinction between machine generated and human
authored text is also increasingly blurred. This work then presents the first
unified framework for analyzing the dynamic relationships among LLM enabled AO,
AM, and AV in the context of authorship privacy. We quantify how they interact
with each other to transform human authored text, examining effects at a single
point in time and iteratively over time. We also examine the role of
demographic metadata, such as gender, academic background, in modulating their
performances, inter-task dynamics, and privacy risks. All source code will be
publicly available.

</details>


### [74] [Automatic Dataset Generation for Knowledge Intensive Question Answering Tasks](https://arxiv.org/abs/2505.14212)
*Sizhe Yuen,Ting Su,Ziyang Wang,Yali Du,Adam J. Sobey*

Main category: cs.CL

TL;DR: 论文提出了一种通过自动生成基于上下文的问答对来增强大型语言模型在知识密集型问答任务中表现的新方法，减少了人工标注的依赖并提升了模型的理解和推理能力。


<details>
  <summary>Details</summary>
Motivation: 当前的问答系统在处理需要复杂推理或实时知识整合的查询时表现不佳，检索增强生成（RAG）技术虽能补充但仍面临复杂推理和多源信息逻辑连接的挑战。

Method: 该方法利用大型语言模型自动生成问答对作为微调数据，包括一个自动问答生成器和一个模型微调器，并使用困惑度、ROUGE、BLEU和BERTScore进行评估。

Result: 实验结果显示，该方法在逻辑一致性和事实准确性上有所提升，Mistral-7b-v0.3在BERT F1、BLEU和ROUGE分数上优于Llama-3-8b。

Conclusion: 该方法为开发适应性强的AI系统提供了新的思路，通过自动生成问答对有效提升了模型在知识密集型任务中的表现。

Abstract: A question-answering (QA) system is to search suitable answers within a
knowledge base. Current QA systems struggle with queries requiring complex
reasoning or real-time knowledge integration. They are often supplemented with
retrieval techniques on a data source such as Retrieval-Augmented Generation
(RAG). However, RAG continues to face challenges in handling complex reasoning
and logical connections between multiple sources of information. A novel
approach for enhancing Large Language Models (LLMs) in knowledge-intensive QA
tasks is presented through the automated generation of context-based QA pairs.
This methodology leverages LLMs to create fine-tuning data, reducing reliance
on human labelling and improving model comprehension and reasoning
capabilities. The proposed system includes an automated QA generator and a
model fine-tuner, evaluated using perplexity, ROUGE, BLEU, and BERTScore.
Comprehensive experiments demonstrate improvements in logical coherence and
factual accuracy, with implications for developing adaptable Artificial
Intelligence (AI) systems. Mistral-7b-v0.3 outperforms Llama-3-8b with BERT F1,
BLEU, and ROUGE scores 0.858, 0.172, and 0.260 of for the LLM generated QA
pairs compared to scores of 0.836, 0.083, and 0.139 for the human annotated QA
pairs.

</details>


### [75] ["Haet Bhasha aur Diskrimineshun": Phonetic Perturbations in Code-Mixed Hinglish to Red-Team LLMs](https://arxiv.org/abs/2505.14226)
*Darpan Aswal,Siddharth D Jaiswal*

Main category: cs.CL

TL;DR: 该论文提出了一种利用代码混合和语音扰动的新型越狱策略，有效绕过大型语言模型的安全过滤器，在文本和图像生成任务中实现了高攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 现有的越狱方法主要集中在英语上，使用固定的模板攻击，导致模型在多语言和多模态环境下仍易受攻击。因此，需要更通用的安全对齐方法。

Method: 论文引入了两种新的越狱策略：代码混合和语音扰动，通过修改敏感词的拼写来绕过安全过滤器。

Result: 新型提示在文本生成中实现了99%的攻击成功率，图像生成为78%，攻击相关率分别为100%和95%。语音扰动影响了词元化，导致越狱成功。

Conclusion: 研究强调了在多语言多模态模型中加强通用安全对齐的必要性，特别是在现实场景中可能存在拼写错误的情况下。

Abstract: Large Language Models (LLMs) have become increasingly powerful, with
multilingual and multimodal capabilities improving by the day. These models are
being evaluated through audits, alignment studies and red-teaming efforts to
expose model vulnerabilities towards generating harmful, biased and unfair
content. Existing red-teaming efforts have previously focused on the English
language, using fixed template-based attacks; thus, models continue to be
susceptible to multilingual jailbreaking strategies, especially in the
multimodal context. In this study, we introduce a novel strategy that leverages
code-mixing and phonetic perturbations to jailbreak LLMs for both text and
image generation tasks. We also introduce two new jailbreak strategies that
show higher effectiveness than baseline strategies. Our work presents a method
to effectively bypass safety filters in LLMs while maintaining interpretability
by applying phonetic misspellings to sensitive words in code-mixed prompts. Our
novel prompts achieve a 99% Attack Success Rate for text generation and 78% for
image generation, with Attack Relevance Rate of 100% for text generation and
95% for image generation when using the phonetically perturbed code-mixed
prompts. Our interpretability experiments reveal that phonetic perturbations
impact word tokenization, leading to jailbreak success. Our study motivates
increasing the focus towards more generalizable safety alignment for
multilingual multimodal models, especially in real-world settings wherein
prompts can have misspelt words.

</details>


### [76] [Mechanistic Fine-tuning for In-context Learning](https://arxiv.org/abs/2505.14233)
*Hakaze Cho,Peng Luo,Mariko Kato,Rin Kaenbyou,Naoya Inoue*

Main category: cs.CL

TL;DR: 本文提出了一种名为注意力行为微调（ABFT）的新方法，通过优化注意力分数而非最终输出来提升语言模型的上下文学习能力，显著降低了计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有的上下文学习方法（ICL）需要在大规模ICL风格数据上进行端到端微调，计算成本高昂。为了降低成本并提升效率，作者探索了基于注意力机制内部工作原理的替代方案。

Method: ABFT方法通过构建针对注意力分数的训练目标，强制模型关注上下文中的正确标签词，同时抑制对错误标签词的注意力分配。该方法仅需约0.01%的数据量。

Result: 在9个现代语言模型和8个数据集上的实验表明，ABFT在性能、鲁棒性、无偏性和效率方面均优于现有方法。分析还发现端到端训练目标隐含包含了ABFT目标。

Conclusion: 该研究证明了通过控制语言模型内部特定模块序列来改善其行为的可行性，为机制可解释性研究的未来应用开辟了新方向。

Abstract: In-context Learning (ICL) utilizes structured demonstration-query inputs to
induce few-shot learning on Language Models (LMs), which are not originally
pre-trained on ICL-style data. To bridge the gap between ICL and pre-training,
some approaches fine-tune LMs on large ICL-style datasets by an end-to-end
paradigm with massive computational costs. To reduce such costs, in this paper,
we propose Attention Behavior Fine-Tuning (ABFT), utilizing the previous
findings on the inner mechanism of ICL, building training objectives on the
attention scores instead of the final outputs, to force the attention scores to
focus on the correct label tokens presented in the context and mitigate
attention scores from the wrong label tokens. Our experiments on 9 modern LMs
and 8 datasets empirically find that ABFT outperforms in performance,
robustness, unbiasedness, and efficiency, with only around 0.01% data cost
compared to the previous methods. Moreover, our subsequent analysis finds that
the end-to-end training objective contains the ABFT objective, suggesting the
implicit bias of ICL-style data to the emergence of induction heads. Our work
demonstrates the possibility of controlling specific module sequences within
LMs to improve their behavior, opening up the future application of mechanistic
interpretability.

</details>


### [77] [ABBA: Highly Expressive Hadamard Product Adaptation for Large Language Models](https://arxiv.org/abs/2505.14238)
*Raghav Singhal,Kaustubh Ponkshe,Rohit Vartak,Praneeth Vepakomma*

Main category: cs.CL

TL;DR: 论文提出ABBA方法，通过解耦可学习低秩矩阵的Hadamard积实现高效参数微调，显著提升表达力并在多个基准测试中超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在新领域的高效适配仍具挑战性。现有参数高效微调方法（如LoRA）受限于低秩分解的表达力，而HiRA等方法仍依赖预训练模型结构。

Method: ABBA架构将权重更新重新参数化为两个独立可学习低秩矩阵的Hadamard积，完全解耦预训练权重与更新参数。

Result: ABBA在相同参数量下展现更高表达力，在算术推理和常识推理基准测试中显著优于现有PEFT方法。

Conclusion: ABBA通过解耦式设计突破预训练模型结构限制，为参数高效微调提供了新的研究方向。

Abstract: Large Language Models have demonstrated strong performance across a wide
range of tasks, but adapting them efficiently to new domains remains a key
challenge. Parameter-Efficient Fine-Tuning (PEFT) methods address this by
introducing lightweight, trainable modules while keeping most pre-trained
weights fixed. The prevailing approach, LoRA, models updates using a low-rank
decomposition, but its expressivity is inherently constrained by the rank.
Recent methods like HiRA aim to increase expressivity by incorporating a
Hadamard product with the frozen weights, but still rely on the structure of
the pre-trained model. We introduce ABBA, a new PEFT architecture that
reparameterizes the update as a Hadamard product of two independently learnable
low-rank matrices. In contrast to prior work, ABBA fully decouples the update
from the pre-trained weights, enabling both components to be optimized freely.
This leads to significantly higher expressivity under the same parameter
budget. We formally analyze ABBA's expressive capacity and validate its
advantages through matrix reconstruction experiments. Empirically, ABBA
achieves state-of-the-art results on arithmetic and commonsense reasoning
benchmarks, consistently outperforming existing PEFT methods by a significant
margin across multiple models. Our code is publicly available at:
https://github.com/CERT-Lab/abba.

</details>


### [78] [Technical Report on classification of literature related to children speech disorder](https://arxiv.org/abs/2505.14242)
*Ziang Wang,Amir Aryani*

Main category: cs.CL

TL;DR: 该技术报告提出了一种基于自然语言处理的方法，用于系统分类儿童言语障碍科学文献，通过LDA和BERTopic模型识别出14个临床相关主题。


<details>
  <summary>Details</summary>
Motivation: 旨在自动化文献综述过程，提高儿童言语障碍领域文献分类的效率和精确度。

Method: 从PubMed数据库中检索并筛选4804篇相关文章，应用LDA和BERTopic两种主题建模技术，结合定制停用词表进行预处理和分析。

Result: LDA模型的连贯性得分为0.42，困惑度为-7.5；BERTopic模型的异常主题比例低于20%，显示出良好的分类能力。

Conclusion: 该方法为言语病理学领域的自动化文献综述提供了有效基础，展示了NLP技术在科学文献分类中的应用潜力。

Abstract: This technical report presents a natural language processing (NLP)-based
approach for systematically classifying scientific literature on childhood
speech disorders. We retrieved and filtered 4,804 relevant articles published
after 2015 from the PubMed database using domain-specific keywords. After
cleaning and pre-processing the abstracts, we applied two topic modeling
techniques - Latent Dirichlet Allocation (LDA) and BERTopic - to identify
latent thematic structures in the corpus. Our models uncovered 14 clinically
meaningful clusters, such as infantile hyperactivity and abnormal epileptic
behavior. To improve relevance and precision, we incorporated a custom stop
word list tailored to speech pathology. Evaluation results showed that the LDA
model achieved a coherence score of 0.42 and a perplexity of -7.5, indicating
strong topic coherence and predictive performance. The BERTopic model exhibited
a low proportion of outlier topics (less than 20%), demonstrating its capacity
to classify heterogeneous literature effectively. These results provide a
foundation for automating literature reviews in speech-language pathology.

</details>


### [79] [TransBench: Benchmarking Machine Translation for Industrial-Scale Applications](https://arxiv.org/abs/2505.14244)
*Haijun Li,Tianqi Shi,Zifu Shang,Yuxuan Han,Xueyu Zhao,Hao Wang,Yu Qian,Zhiqiang Qian,Linlong Xu,Minghao Wu,Chenyang Lyu,Longyue Wang,Gongbo Tang,Weihua Luo,Zhao Xu,Kaifu Zhang*

Main category: cs.CL

TL;DR: 论文提出针对工业场景的机器翻译评估框架TransBench，包含多维度能力指标和首个电商领域公开基准，弥补学术基准与产业需求的鸿沟。


<details>
  <summary>Details</summary>
Motivation: 通用机器翻译模型在专业领域（如术语、文化适配）表现不足，现有评估体系无法有效衡量工业场景的实际效能。

Method: 构建三层能力框架（基础语言/领域专业/文化适应），开发包含17,000句电商数据的TransBench基准，结合传统指标与领域评价模型Marco-MOS。

Result: 推出首个电商机器翻译公开基准，提供结构化评估框架、多级质量指标及开源工具，支持33种语言对。

Conclusion: 该研究填补工业MT评估空白，为领域定制化翻译系统的优化提供系统性方法论和可复现标准。

Abstract: Machine translation (MT) has become indispensable for cross-border
communication in globalized industries like e-commerce, finance, and legal
services, with recent advancements in large language models (LLMs)
significantly enhancing translation quality. However, applying general-purpose
MT models to industrial scenarios reveals critical limitations due to
domain-specific terminology, cultural nuances, and stylistic conventions absent
in generic benchmarks. Existing evaluation frameworks inadequately assess
performance in specialized contexts, creating a gap between academic benchmarks
and real-world efficacy. To address this, we propose a three-level translation
capability framework: (1) Basic Linguistic Competence, (2) Domain-Specific
Proficiency, and (3) Cultural Adaptation, emphasizing the need for holistic
evaluation across these dimensions. We introduce TransBench, a benchmark
tailored for industrial MT, initially targeting international e-commerce with
17,000 professionally translated sentences spanning 4 main scenarios and 33
language pairs. TransBench integrates traditional metrics (BLEU, TER) with
Marco-MOS, a domain-specific evaluation model, and provides guidelines for
reproducible benchmark construction. Our contributions include: (1) a
structured framework for industrial MT evaluation, (2) the first publicly
available benchmark for e-commerce translation, (3) novel metrics probing
multi-level translation quality, and (4) open-sourced evaluation tools. This
work bridges the evaluation gap, enabling researchers and practitioners to
systematically assess and enhance MT systems for industry-specific needs.

</details>


### [80] [FuxiMT: Sparsifying Large Language Models for Chinese-Centric Multilingual Machine Translation](https://arxiv.org/abs/2505.14256)
*Shaolin Zhu,Tianyu Dong,Bo Li,Deyi Xiong*

Main category: cs.CL

TL;DR: FuxiMT是一种新型以中文为中心的多语言机器翻译模型，采用稀疏化大语言模型(LLM)驱动，通过两阶段训练策略和课程学习，在低资源场景下表现优异，并具备零样本翻译能力。


<details>
  <summary>Details</summary>
Motivation: 为了解决多语言机器翻译中低资源语言对的性能问题，并探索大语言模型在翻译任务中的潜力。

Method: 采用两阶段训练策略：先在中文语料上预训练，再在65种语言的平行数据上进行多语言微调，结合混合专家(MoEs)和课程学习策略。

Result: FuxiMT显著优于现有基线模型，尤其在低资源场景下表现突出，并展现出对未见语言对的零样本翻译能力。

Conclusion: FuxiMT通过创新的训练方法和架构设计，有效提升了多语言翻译性能，特别是在资源匮乏情况下，为稀缺平行数据的语言对提供了可行的翻译解决方案。

Abstract: In this paper, we present FuxiMT, a novel Chinese-centric multilingual
machine translation model powered by a sparsified large language model (LLM).
We adopt a two-stage strategy to train FuxiMT. We first pre-train the model on
a massive Chinese corpus and then conduct multilingual fine-tuning on a large
parallel dataset encompassing 65 languages. FuxiMT incorporates
Mixture-of-Experts (MoEs) and employs a curriculum learning strategy for robust
performance across various resource levels. Experimental results demonstrate
that FuxiMT significantly outperforms strong baselines, including
state-of-the-art LLMs and machine translation models, particularly under
low-resource scenarios. Furthermore, FuxiMT exhibits remarkable zero-shot
translation capabilities for unseen language pairs, indicating its potential to
bridge communication gaps where parallel data are scarce or unavailable.

</details>


### [81] [SLOT: Sample-specific Language Model Optimization at Test-time](https://arxiv.org/abs/2505.12392)
*Yang Hu,Xingyu Zhang,Xueji Fang,Zhiyang Chen,Xiao Wang,Huatian Zhang,Guojun Qi*

Main category: cs.CL

TL;DR: SLOT是一种参数高效的测试时推理方法，通过少量优化步骤更新轻量级样本特定参数向量，提升语言模型对单个提示的响应准确性。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型在处理复杂指令时表现不佳，尤其是那些在通用样本中代表性不足的指令。SLOT旨在通过测试时优化提升模型对单个提示的适应能力。

Method: SLOT在测试时进行少量优化步骤，更新一个轻量级的样本特定参数向量，并将其添加到输出头前的最终隐藏层。通过最小化输入提示的交叉熵损失，实现高效适配。

Result: 实验表明，SLOT在多个基准测试和大型语言模型上优于对比模型。例如，Qwen2.5-7B在GSM8K上的准确率提升了8.6%，DeepSeek-R1-Distill-Llama-70B在GPQA上达到了70B级别模型中的SOTA准确率68.69%。

Conclusion: SLOT通过测试时优化显著提升了语言模型对单个提示的响应准确性，为处理复杂指令提供了一种高效的方法。

Abstract: We propose SLOT (Sample-specific Language Model Optimization at Test-time), a
novel and parameter-efficient test-time inference approach that enhances a
language model's ability to more accurately respond to individual prompts.
Existing Large Language Models (LLMs) often struggle with complex instructions,
leading to poor performances on those not well represented among general
samples. To address this, SLOT conducts few optimization steps at test-time to
update a light-weight sample-specific parameter vector. It is added to the
final hidden layer before the output head, and enables efficient adaptation by
caching the last layer features during per-sample optimization. By minimizing
the cross-entropy loss on the input prompt only, SLOT helps the model better
aligned with and follow each given instruction. In experiments, we demonstrate
that our method outperforms the compared models across multiple benchmarks and
LLMs. For example, Qwen2.5-7B with SLOT achieves an accuracy gain of 8.6% on
GSM8K from 57.54% to 66.19%, while DeepSeek-R1-Distill-Llama-70B with SLOT
achieves a SOTA accuracy of 68.69% on GPQA among 70B-level models. Our code is
available at https://github.com/maple-research-lab/SLOT.

</details>


### [82] [Think-J: Learning to Think for Generative LLM-as-a-Judge](https://arxiv.org/abs/2505.14268)
*Hui Huang,Yancheng He,Hongli Zhou,Rui Zhang,Wei Liu,Weixun Wang,Wenbo Su,Bo Zheng,Jiaheng Liu*

Main category: cs.CL

TL;DR: 提出Think-J方法，通过强化学习优化生成式LLM的评判能力，无需额外人工标注即可超越现有生成式和分类器式LLM评判模型。


<details>
  <summary>Details</summary>
Motivation: 当前生成式大语言模型（LLM）在作为评判模型（LLM-as-a-Judge）时表现不佳，亟需提升其自动评估和奖励建模能力。

Method: 1. 使用少量精选数据训练初始评判思维模型；2. 基于离线/在线强化学习优化评判思维轨迹：离线法训练评论家模型构建正负例，在线法通过规则奖励反馈优化。

Result: 实验表明该方法显著提升生成式LLM评判能力，超越生成式和分类器式基线模型。

Conclusion: Think-J通过思维学习机制有效突破生成式LLM的评判性能瓶颈，为自动化评估提供新思路。

Abstract: LLM-as-a-Judge refers to the automatic modeling of preferences for responses
generated by Large Language Models (LLMs), which is of significant importance
for both LLM evaluation and reward modeling. Although generative LLMs have made
substantial progress in various tasks, their performance as LLM-Judge still
falls short of expectations. In this work, we propose Think-J, which improves
generative LLM-as-a-Judge by learning how to think. We first utilized a small
amount of curated data to develop the model with initial judgment thinking
capabilities. Subsequently, we optimize the judgment thinking traces based on
reinforcement learning (RL). We propose two methods for judgment thinking
optimization, based on offline and online RL, respectively. The offline RL
requires training a critic model to construct positive and negative examples
for learning. The online method defines rule-based reward as feedback for
optimization. Experimental results showed that our approach can significantly
enhance the evaluation capability of generative LLM-Judge, surpassing both
generative and classifier-based LLM-Judge without requiring extra human
annotations.

</details>


### [83] [FAID: Fine-grained AI-generated Text Detection using Multi-task Auxiliary and Multi-level Contrastive Learning](https://arxiv.org/abs/2505.14271)
*Minh Ngoc Ta,Dong Cao Van,Duc-Anh Hoang,Minh Le-Anh,Truong Nguyen,My Anh Tran Nguyen,Yuxia Wang,Preslav Nakov,Sang Dinh*

Main category: cs.CL

TL;DR: 论文提出FAID框架，通过多级对比学习和多任务辅助分类，区分人类、AI及人机协作文本，并识别AI模型家族，提升检测泛化能力。


<details>
  <summary>Details</summary>
Motivation: 随着人机协作生成文本的普及，现有二元分类器难以区分人类、AI及协作文本，且缺乏对AI模型家族的识别能力，需更细粒度的检测方法。

Method: 构建多语言、多领域数据集FAIDSet，设计FAID框架结合多级对比学习与多任务分类，建模AI家族风格特征，并引入分布适应机制。

Result: FAID在未知领域和新AI模型上泛化性能优于基线方法，同时提供模型家族识别和风格可解释性。

Conclusion: FAID为AI辅助写作的透明性和责任归属提供了潜在解决方案，通过细粒度检测增强人机协作的可靠性。

Abstract: The growing collaboration between humans and AI models in generative tasks
has introduced new challenges in distinguishing between human-written,
AI-generated, and human-AI collaborative texts. In this work, we collect a
multilingual, multi-domain, multi-generator dataset FAIDSet. We further
introduce a fine-grained detection framework FAID to classify text into these
three categories, meanwhile identifying the underlying AI model family. Unlike
existing binary classifiers, FAID is built to capture both authorship and
model-specific characteristics. Our method combines multi-level contrastive
learning with multi-task auxiliary classification to learn subtle stylistic
cues. By modeling AI families as distinct stylistic entities, FAID offers
improved interpretability. We incorporate an adaptation to address
distributional shifts without retraining for unseen data. Experimental results
demonstrate that FAID outperforms several baseline approaches, particularly
enhancing the generalization accuracy on unseen domains and new AI models. It
provide a potential solution for improving transparency and accountability in
AI-assisted writing.

</details>


### [84] [Data-Efficient Hate Speech Detection via Cross-Lingual Nearest Neighbor Retrieval with Limited Labeled Data](https://arxiv.org/abs/2505.14272)
*Faeze Ghorbanpour,Daryna Dementieva,Alexander Fraser*

Main category: cs.CL

TL;DR: 该论文提出了一种基于最近邻检索的跨语言迁移学习方法，用于低资源语言中的仇恨言论检测，通过数据增强显著提升了检测性能。


<details>
  <summary>Details</summary>
Motivation: 仇恨言论检测需要大量标注数据，但对低资源语言来说，标注成本高昂且耗时。因此，研究如何利用跨语言迁移学习和数据增强来提升低资源语言下的检测性能具有重要意义。

Method: 利用目标语言的少量标注数据，从多语言仇恨言论检测池中检索最相关的标注样本进行数据增强，并结合最大边际相关性减少冗余。

Result: 在八种语言上的实验表明，该方法性能优于仅使用目标语言数据的模型，并在多数情况下超越了当前最优方法，且具有较高的数据效率（仅需200个样本即可保持优异性能）。

Conclusion: 该方法高效、可扩展，能快速适应新语言和任务，为低资源语言的仇恨言论检测提供了实用解决方案。

Abstract: Considering the importance of detecting hateful language, labeled hate speech
data is expensive and time-consuming to collect, particularly for low-resource
languages. Prior work has demonstrated the effectiveness of cross-lingual
transfer learning and data augmentation in improving performance on tasks with
limited labeled data. To develop an efficient and scalable cross-lingual
transfer learning approach, we leverage nearest-neighbor retrieval to augment
minimal labeled data in the target language, thereby enhancing detection
performance. Specifically, we assume access to a small set of labeled training
instances in the target language and use these to retrieve the most relevant
labeled examples from a large multilingual hate speech detection pool. We
evaluate our approach on eight languages and demonstrate that it consistently
outperforms models trained solely on the target language data. Furthermore, in
most cases, our method surpasses the current state-of-the-art. Notably, our
approach is highly data-efficient, retrieving as small as 200 instances in some
cases while maintaining superior performance. Moreover, it is scalable, as the
retrieval pool can be easily expanded, and the method can be readily adapted to
new languages and tasks. We also apply maximum marginal relevance to mitigate
redundancy and filter out highly similar retrieved instances, resulting in
improvements in some languages.

</details>


### [85] [YESciEval: Robust LLM-as-a-Judge for Scientific Question Answering](https://arxiv.org/abs/2505.14279)
*Jennifer D'Souza,Hamed Babaei Giglou,Quentin Münch*

Main category: cs.CL

TL;DR: YESciEval是一个开源框架，结合细粒度评分标准和强化学习，减少LLM评估中的乐观偏差，支持多学科科学问答评估。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型（LLM）在科学问答评估中的鲁棒性不足，缺乏透明且可扩展的评估方法。

Method: 提出YESciEval框架，结合细粒度评分标准和强化学习，并发布多学科科学问答数据集及对抗变体。

Result: 实现了无需依赖专有模型或人工反馈的可扩展、免费评估，提升了LLM作为评估者的可靠性。

Conclusion: 该工作推动了AI对齐和透明评估，对科学探究和通用人工智能发展至关重要。

Abstract: Large Language Models (LLMs) drive scientific question-answering on modern
search engines, yet their evaluation robustness remains underexplored. We
introduce YESciEval, an open-source framework that combines fine-grained
rubric-based assessment with reinforcement learning to mitigate optimism bias
in LLM evaluators. We release multidisciplinary scienceQ&A datasets, including
adversarial variants, with evaluation scores from multiple LLMs. Independent of
proprietary models and human feedback, our approach enables scalable, cost-free
evaluation. By advancing reliable LLM-as-a-judge models, this work supports AI
alignment and fosters robust, transparent evaluation essential for scientific
inquiry and artificial general intelligence.

</details>


### [86] [Universal Acoustic Adversarial Attacks for Flexible Control of Speech-LLMs](https://arxiv.org/abs/2505.14286)
*Rao Ma,Mengjie Qian,Vyas Raina,Mark Gales,Kate Knill*

Main category: cs.CL

TL;DR: 研究发现语音大语言模型存在通用对抗攻击漏洞，需加强鲁棒性训练。


<details>
  <summary>Details</summary>
Motivation: 预训练语音编码器与大语言模型结合的语音LLMs虽强大灵活，但其灵活性可能增加对抗攻击的脆弱性。本文旨在探究该问题的严重程度。

Method: 通过在原始音频前添加固定的通用对抗音频段，研究模型无输出或任务被篡改的攻击效果，并扩展至基于特定输入属性（如说话者性别或语言）的选择性攻击。

Result: Qwen2-Audio和Granite-Speech等语音LLMs存在显著漏洞，易受通用对抗攻击影响。

Conclusion: 语音LLMs的对抗攻击脆弱性凸显了增强鲁棒性训练和抗攻击能力的必要性。

Abstract: The combination of pre-trained speech encoders with large language models has
enabled the development of speech LLMs that can handle a wide range of spoken
language processing tasks. While these models are powerful and flexible, this
very flexibility may make them more vulnerable to adversarial attacks. To
examine the extent of this problem, in this work we investigate universal
acoustic adversarial attacks on speech LLMs. Here a fixed, universal,
adversarial audio segment is prepended to the original input audio. We
initially investigate attacks that cause the model to either produce no output
or to perform a modified task overriding the original prompt. We then extend
the nature of the attack to be selective so that it activates only when
specific input attributes, such as a speaker gender or spoken language, are
present. Inputs without the targeted attribute should be unaffected, allowing
fine-grained control over the model outputs. Our findings reveal critical
vulnerabilities in Qwen2-Audio and Granite-Speech and suggest that similar
speech LLMs may be susceptible to universal adversarial attacks. This
highlights the need for more robust training strategies and improved resistance
to adversarial attacks.

</details>


### [87] [Cross-Lingual Optimization for Language Transfer in Large Language Models](https://arxiv.org/abs/2505.14297)
*Jungseob Lee,Seongtae Hong,Hyeonseok Moon,Heuiseok Lim*

Main category: cs.CL

TL;DR: 论文提出跨语言优化方法CLO，通过利用英文SFT数据和翻译模型，在低资源语言中高效迁移英语中心的大语言模型，同时保持英语能力。


<details>
  <summary>Details</summary>
Motivation: 传统监督微调方法在适应大语言模型到其他语言时，过度依赖英语性能，尤其在数据受限环境中表现不佳。

Method: 提出跨语言优化（CLO），利用公开英文SFT数据和翻译模型实现跨语言迁移，减少对目标语言数据量的依赖。

Result: CLO在六种不同资源水平的语言上均优于SFT，尤其在低资源语言中仅用3200样本即超越SFT的6400样本表现。

Conclusion: CLO能更高效地实现跨语言迁移，且在数据量敏感的中低资源语言中表现稳健，优于传统SFT方法。

Abstract: Adapting large language models to other languages typically employs
supervised fine-tuning (SFT) as a standard approach. However, it often suffers
from an overemphasis on English performance, a phenomenon that is especially
pronounced in data-constrained environments. To overcome these challenges, we
propose \textbf{Cross-Lingual Optimization (CLO)} that efficiently transfers an
English-centric LLM to a target language while preserving its English
capabilities. CLO utilizes publicly available English SFT data and a
translation model to enable cross-lingual transfer. We conduct experiments
using five models on six languages, each possessing varying levels of resource.
Our results show that CLO consistently outperforms SFT in both acquiring target
language proficiency and maintaining English performance. Remarkably, in
low-resource languages, CLO with only 3,200 samples surpasses SFT with 6,400
samples, demonstrating that CLO can achieve better performance with less data.
Furthermore, we find that SFT is particularly sensitive to data quantity in
medium and low-resource languages, whereas CLO remains robust. Our
comprehensive analysis emphasizes the limitations of SFT and incorporates
additional training strategies in CLO to enhance efficiency.

</details>


### [88] [JOLT-SQL: Joint Loss Tuning of Text-to-SQL with Confusion-aware Noisy Schema Sampling](https://arxiv.org/abs/2505.14305)
*Jinwang Song,Hongying Zan,Kunli Zhang,Lingling Mu,Yingjie Han,Haobo Hua,Min Peng*

Main category: cs.CL

TL;DR: JOLT-SQL提出了一种单阶段监督微调框架，通过联合优化模式链接和SQL生成，提升Text-to-SQL任务在噪声模式下的鲁棒性和效率。


<details>
  <summary>Details</summary>
Motivation: 当前基于监督微调（SFT）的Text-to-SQL方法存在多阶段流程复杂、对噪声模式信息鲁棒性差的问题，亟需改进。

Method: 采用判别式模式链接（增强局部双向注意力）和噪声模式感知采样策略（选择性注意力），通过统一损失函数实现端到端优化。

Result: 在Spider和BIRD基准测试中，JOLT-SQL达到同规模开源模型的SOTA执行准确率，并显著提升训练/推理效率。

Conclusion: JOLT-SQL通过单阶段联合优化框架有效解决了传统SFT方法的缺陷，为Text-to-SQL任务提供了更高效的解决方案。

Abstract: Text-to-SQL, which maps natural language to SQL queries, has benefited
greatly from recent advances in Large Language Models (LLMs). While LLMs offer
various paradigms for this task, including prompting and supervised fine-tuning
(SFT), SFT approaches still face challenges such as complex multi-stage
pipelines and poor robustness to noisy schema information. To address these
limitations, we present JOLT-SQL, a streamlined single-stage SFT framework that
jointly optimizes schema linking and SQL generation via a unified loss.
JOLT-SQL employs discriminative schema linking, enhanced by local bidirectional
attention, alongside a confusion-aware noisy schema sampling strategy with
selective attention to improve robustness under noisy schema conditions.
Experiments on the Spider and BIRD benchmarks demonstrate that JOLT-SQL
achieves state-of-the-art execution accuracy among comparable-size open-source
models, while significantly improving both training and inference efficiency.

</details>


### [89] [Studying the Role of Input-Neighbor Overlap in Retrieval-Augmented Language Models Training Efficiency](https://arxiv.org/abs/2505.14309)
*Ehsan Doostmohammadi,Marco Kuhlmann*

Main category: cs.CL

TL;DR: 检索增强语言模型通过优化查询与上下文重叠度，显著提升性能并减少训练时间。


<details>
  <summary>Details</summary>
Motivation: 研究查询与检索上下文之间的重叠度如何影响模型性能，以优化检索机制。

Method: 系统研究不同重叠度对模型训练和推理的影响，并通过生成合成上下文（如查询改写）增加重叠度。

Result: 增加重叠度超过临界值后，测试困惑度显著降低，训练时间减少约40%，且不影响性能。

Conclusion: 检索增强语言模型在预训练中存在显著优化潜力，可通过调整重叠度提升效率。

Abstract: Retrieval-augmented language models have demonstrated performance comparable
to much larger models while requiring fewer computational resources. The
effectiveness of these models crucially depends on the overlap between query
and retrieved context, but the optimal degree of this overlap remains
unexplored. In this paper, we systematically investigate how varying levels of
query--context overlap affect model performance during both training and
inference. Our experiments reveal that increased overlap initially has minimal
effect, but substantially improves test-time perplexity and accelerates model
learning above a critical threshold. Building on these findings, we demonstrate
that deliberately increasing overlap through synthetic context can enhance data
efficiency and reduce training time by approximately 40\% without compromising
performance. We specifically generate synthetic context through paraphrasing
queries. We validate our perplexity-based findings on question-answering tasks,
confirming that the benefits of retrieval-augmented language modeling extend to
practical applications. Our results provide empirical evidence of significant
optimization potential for retrieval mechanisms in language model pretraining.

</details>


### [90] [HausaNLP: Current Status, Challenges and Future Directions for Hausa Natural Language Processing](https://arxiv.org/abs/2505.14311)
*Shamsuddeen Hassan Muhammad,Ibrahim Said Ahmad,Idris Abdulmumin,Falalu Ibrahim Lawan,Babangida Sani,Sukairaj Hafiz Imam,Yusuf Aliyu,Sani Abdullahi Sani,Ali Usman Umar,Kenneth Church,Vukosi Marivate*

Main category: cs.CL

TL;DR: 本文综述了豪萨语NLP的现状，提出了资源整合平台HausaNLP，并探讨了挑战与未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 豪萨语作为低资源语言，尽管拥有大量使用者，但在NLP领域的研究和资源仍显不足。

Method: 系统梳理豪萨语NLP现有资源与研究，建立HausaNLP平台整合数据集与工具，并分析LLM整合问题。

Result: 创建了HausaNLP资源目录，识别了豪萨语NLP的关键挑战，如分词和方言差异问题。

Conclusion: 提出通过扩展数据集、改进语言建模和加强社区合作来推动豪萨语NLP发展。

Abstract: Hausa Natural Language Processing (NLP) has gained increasing attention in
recent years, yet remains understudied as a low-resource language despite
having over 120 million first-language (L1) and 80 million second-language (L2)
speakers worldwide. While significant advances have been made in high-resource
languages, Hausa NLP faces persistent challenges, including limited open-source
datasets and inadequate model representation. This paper presents an overview
of the current state of Hausa NLP, systematically examining existing resources,
research contributions, and gaps across fundamental NLP tasks: text
classification, machine translation, named entity recognition, speech
recognition, and question answering. We introduce HausaNLP
(https://catalog.hausanlp.org), a curated catalog that aggregates datasets,
tools, and research works to enhance accessibility and drive further
development. Furthermore, we discuss challenges in integrating Hausa into large
language models (LLMs), addressing issues of suboptimal tokenization and
dialectal variation. Finally, we propose strategic research directions
emphasizing dataset expansion, improved language modeling approaches, and
strengthened community collaboration to advance Hausa NLP. Our work provides
both a foundation for accelerating Hausa NLP progress and valuable insights for
broader multilingual NLP research.

</details>


### [91] [A MIND for Reasoning: Meta-learning for In-context Deduction](https://arxiv.org/abs/2505.14313)
*Leonardo Bertolazzi,Manuel Vargas Guzmán,Raffaella Bernardi,Maciej Malicki,Jakub Szymanik*

Main category: cs.CL

TL;DR: 论文提出MIND方法，通过元学习微调提升小模型在演绎推理任务上的泛化能力，使其超越GPT-4o等大模型。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在形式化任务中表现优异，但在分布外问题上的泛化能力有限。研究旨在提升模型对演绎规则的系统性理解能力，特别是从知识库中筛选前提推导假设的任务。

Method: 提出MIND（基于元学习的上下文演绎微调方法），采用小样本元学习策略，使模型能适应未见过的知识库并系统应用推理规则。

Result: MIND显著提升了1.5B-7B参数小模型的泛化性能，在低数据场景和小模型中效果尤为突出。经MIND调优的小模型甚至超越GPT-4o等顶尖大模型。

Conclusion: 通过针对性元学习微调，小语言模型能在特定推理任务上实现超越大模型的系统性泛化能力，为高效推理模型设计提供新思路。

Abstract: Large language models (LLMs) are increasingly evaluated on formal tasks,
where strong reasoning abilities define the state of the art. However, their
ability to generalize to out-of-distribution problems remains limited. In this
paper, we investigate how LLMs can achieve a systematic understanding of
deductive rules. Our focus is on the task of identifying the appropriate subset
of premises within a knowledge base needed to derive a given hypothesis. To
tackle this challenge, we propose Meta-learning for In-context Deduction
(MIND), a novel few-shot meta-learning fine-tuning approach. The goal of MIND
is to enable models to generalize more effectively to unseen knowledge bases
and to systematically apply inference rules. Our results show that MIND
significantly improves generalization in small LMs ranging from 1.5B to 7B
parameters. The benefits are especially pronounced in smaller models and
low-data settings. Remarkably, small models fine-tuned with MIND outperform
state-of-the-art LLMs, such as GPT-4o and o3-mini, on this task.

</details>


### [92] [QA-prompting: Improving Summarization with Large Language Models using Question-Answering](https://arxiv.org/abs/2505.14347)
*Neelabh Sinha*

Main category: cs.CL

TL;DR: 提出QA-prompting方法，通过问答步骤提升语言模型的长文本摘要能力，无需微调即可显著提高ROUGE分数。


<details>
  <summary>Details</summary>
Motivation: 现有语言模型在长文本摘要中存在位置偏差问题，传统优化方法需微调或复杂流程，亟需简单有效的解决方案。

Method: 在生成摘要前插入问答步骤（QA-prompting），通过提取关键信息缓解位置偏差，单次调用模型即可完成。

Result: 在跨领域数据集和10个前沿模型上验证，QA-prompting使ROUGE分数最高提升29%，优于基线方法。

Conclusion: 该方法为摘要任务提供可扩展方案，同时证明领域特定问题选择对性能优化的重要性。

Abstract: Language Models (LMs) have revolutionized natural language processing,
enabling high-quality text generation through prompting and in-context
learning. However, models often struggle with long-context summarization due to
positional biases, leading to suboptimal extraction of critical information.
There are techniques to improve this with fine-tuning, pipelining, or using
complex techniques, which have their own challenges. To solve these challenges,
we propose QA-prompting - a simple prompting method for summarization that
utilizes question-answering as an intermediate step prior to summary
generation. Our method extracts key information and enriches the context of
text to mitigate positional biases and improve summarization in a single LM
call per task without requiring fine-tuning or pipelining. Experiments on
multiple datasets belonging to different domains using ten state-of-the-art
pre-trained models demonstrate that QA-prompting outperforms baseline and other
state-of-the-art methods, achieving up to 29% improvement in ROUGE scores. This
provides an effective and scalable solution for summarization and highlights
the importance of domain-specific question selection for optimal performance.

</details>


### [93] [OSoRA: Output-Dimension and Singular-Value Initialized Low-Rank Adaptation](https://arxiv.org/abs/2505.14350)
*Jialong Han,Si Zhang,Ke Zhang*

Main category: cs.CL

TL;DR: OSoRA是一种新型参数高效微调方法，通过结合SVD和可学习缩放向量，显著减少计算资源需求，同时保持或超越现有方法性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型(LLM)的微调因计算成本高昂而面临挑战，现有参数高效微调(PEFT)方法仍需大量资源。

Method: OSoRA扩展了LoRA方法，将SVD与可学习缩放向量结合，先对预训练权重矩阵进行SVD分解，再优化输出维度向量，同时冻结奇异向量矩阵。

Result: 在数学推理、常识推理等任务上，OSoRA性能媲美或优于LoRA和VeRA等方法，且参数随秩增加呈线性扩展。

Conclusion: OSoRA通过联合训练奇异值和输出维度向量，实现了计算资源的高效利用和优异性能，为LLM微调提供了新方案。

Abstract: Fine-tuning Large Language Models (LLMs) has become increasingly challenging
due to their massive scale and associated computational costs.
Parameter-Efficient Fine-Tuning (PEFT) methodologies have been proposed as
computational alternatives; however, their implementations still require
significant resources. In this paper, we present OSoRA (Output-Dimension and
Singular-Value Initialized Low-Rank Adaptation), a novel PEFT method for LLMs.
OSoRA extends Low-Rank Adaptation (LoRA) by integrating Singular Value
Decomposition (SVD) with learnable scaling vectors in a unified framework. It
first performs an SVD of pre-trained weight matrices, then optimizes an
output-dimension vector during training, while keeping the corresponding
singular vector matrices frozen. OSoRA substantially reduces computational
resource requirements by minimizing the number of trainable parameters during
fine-tuning. Comprehensive evaluations across mathematical reasoning, common
sense reasoning, and other benchmarks demonstrate that OSoRA achieves
comparable or superior performance to state-of-the-art methods like LoRA and
VeRA, while maintaining a linear parameter scaling even as the rank increases
to higher dimensions. Our ablation studies further confirm that jointly
training both the singular values and the output-dimension vector is critical
for optimal performance.

</details>


### [94] [WirelessMathBench: A Mathematical Modeling Benchmark for LLMs in Wireless Communications](https://arxiv.org/abs/2505.14354)
*Xin Li,Mengbing Liu,Li Wei,Jiancheng An,Mérouane Debbah,Chau Yuen*

Main category: cs.CL

TL;DR: 论文介绍了WirelessMathBench，一个评估大语言模型在无线通信数学建模能力的基准测试集，发现当前模型在复杂方程重建任务上表现不佳。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在多种任务上表现出色，但其在无线通信等特定领域的复杂数学推理能力尚未充分探索。

Method: 研究团队创建了包含587个问题的WirelessMathBench基准测试集，涵盖从基础选择题到复杂方程完成任务，并测试了多个领先的大语言模型。

Result: 实验显示，多数模型在基础任务上表现良好，但在重建部分或完全遮挡的方程时表现显著下降，最佳模型DeepSeek-R1的平均准确率仅为38.05%。

Conclusion: 通过公开WirelessMathBench和评估工具包，研究旨在推动开发更强大、领域感知的大语言模型，以支持无线系统分析和更广泛的工程应用。

Abstract: Large Language Models (LLMs) have achieved impressive results across a broad
array of tasks, yet their capacity for complex, domain-specific mathematical
reasoning-particularly in wireless communications-remains underexplored. In
this work, we introduce WirelessMathBench, a novel benchmark specifically
designed to evaluate LLMs on mathematical modeling challenges to wireless
communications engineering. Our benchmark consists of 587 meticulously curated
questions sourced from 40 state-of-the-art research papers, encompassing a
diverse spectrum of tasks ranging from basic multiple-choice questions to
complex equation completion tasks, including both partial and full completions,
all of which rigorously adhere to physical and dimensional constraints. Through
extensive experimentation with leading LLMs, we observe that while many models
excel in basic recall tasks, their performance degrades significantly when
reconstructing partially or fully obscured equations, exposing fundamental
limitations in current LLMs. Even DeepSeek-R1, the best performer on our
benchmark, achieves an average accuracy of only 38.05%, with a mere 7.83%
success rate in full equation completion. By publicly releasing
WirelessMathBench along with the evaluation toolkit, we aim to advance the
development of more robust, domain-aware LLMs for wireless system analysis and
broader engineering applications.

</details>


### [95] [Dual Decomposition of Weights and Singular Value Low Rank Adaptation](https://arxiv.org/abs/2505.14367)
*Jialong Han,Si Zhang,Ke Zhang*

Main category: cs.CL

TL;DR: 论文提出DuDe方法，通过分解权重矩阵解决LoRA训练不稳定和知识迁移效率低的问题，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有LoRA方法因适配器参数随机初始化导致训练不稳定和预训练模型知识迁移效率低，需改进。

Method: DuDe通过奇异值分解(SVD)将权重矩阵分解为幅度和方向分量，实现有原则的初始化。

Result: DuDe在MMLU和GSM8K上分别达到48.35%和62.53%的准确率，优化稳定性增强且更好保留预训练表示。

Conclusion: DuDe结合理论分析和实证验证，成为LLMs参数高效微调领域的重要贡献。

Abstract: Parameter-Efficient Fine-Tuning (PEFT) has emerged as a critical paradigm for
adapting Large Language Models (LLMs) to downstream tasks, among which Low-rank
Adaptation (LoRA) represents one of the most widely adopted methodologies.
However, existing LoRA-based approaches exhibit two fundamental limitations:
unstable training dynamics and inefficient knowledge transfer from pre-trained
models, both stemming from random initialization of adapter parameters. To
overcome these challenges, we propose DuDe, a novel approach that decomposes
weight matrices into magnitude and direction components, employing Singular
Value Decomposition (SVD) for principled initialization. Our comprehensive
evaluation demonstrates DuDe's superior performance and robustness, achieving
up to 48.35\% accuracy on MMLU and 62.53\% ($\pm$ 1.59) accuracy on GSM8K. Our
theoretical analysis and empirical validation collectively demonstrate that
DuDe's decomposition strategy enhances optimization stability and better
preserves pre-trained representations, particularly for domain-specific tasks
requiring specialized knowledge. The combination of robust empirical
performance and rigorous theoretical foundations establishes DuDe as a
significant contribution to PEFT methodologies for LLMs.

</details>


### [96] [AutoRev: Automatic Peer Review System for Academic Research Papers](https://arxiv.org/abs/2505.14376)
*Maitreya Prafulla Chitale,Ketaki Mangesh Shetye,Harshit Gupta,Manav Chaudhary,Vasudeva Varma*

Main category: cs.CL

TL;DR: AutoRev提出了一种基于图结构的自动学术论文评审系统，通过提取关键段落生成评审，性能超越现有方法58.72%。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的论文评审方法常忽略长文本输入的计算和性能限制，需更高效的解决方案。

Method: 将学术论文表示为图结构，从中提取对评审至关重要的关键段落，构建自动评审框架AutoRev。

Result: 在评审生成任务中，AutoRev平均超越所有基线方法58.72%，且可扩展至问答、摘要等下游任务。

Conclusion: 该工作推动了图结构提取技术在NLP领域的应用，代码将开源以促进相关研究。

Abstract: Generating a review for an academic research paper is a complex task that
requires a deep understanding of the document's content and the
interdependencies between its sections. It demands not only insight into
technical details but also an appreciation of the paper's overall coherence and
structure. Recent methods have predominantly focused on fine-tuning large
language models (LLMs) to address this challenge. However, they often overlook
the computational and performance limitations imposed by long input token
lengths. To address this, we introduce AutoRev, an Automatic Peer Review System
for Academic Research Papers. Our novel framework represents an academic
document as a graph, enabling the extraction of the most critical passages that
contribute significantly to the review. This graph-based approach demonstrates
effectiveness for review generation and is potentially adaptable to various
downstream tasks, such as question answering, summarization, and document
representation. When applied to review generation, our method outperforms SOTA
baselines by an average of 58.72% across all evaluation metrics. We hope that
our work will stimulate further research in applying graph-based extraction
techniques to other downstream tasks in NLP. We plan to make our code public
upon acceptance.

</details>


### [97] [Editing Across Languages: A Survey of Multilingual Knowledge Editing](https://arxiv.org/abs/2505.14393)
*Nadir Durrani,Basel Mousi,Fahim Dalvi*

Main category: cs.CL

TL;DR: 该综述系统整理了多语言知识编辑（MKE）的最新研究，提出了方法分类，总结了关键发现，并指出了跨语言传播中的挑战和未解决问题。


<details>
  <summary>Details</summary>
Motivation: 尽管知识编辑在单语言环境中已被广泛研究，但在多语言环境中的探索仍然不足。本文旨在填补这一空白，推动可编辑的多语言大模型的发展。

Method: 本文提出了一个全面的MKE方法分类，包括基于参数、基于记忆、微调和超网络等方法，并调查了现有基准。

Result: 总结了方法有效性和迁移模式的关键发现，识别了跨语言传播的挑战，并强调了语言各向异性、评估覆盖范围和编辑可扩展性等开放问题。

Conclusion: 本文整合了一个快速发展的领域，为未来可编辑的语言感知大模型的发展奠定了基础。

Abstract: While Knowledge Editing has been extensively studied in monolingual settings,
it remains underexplored in multilingual contexts. This survey systematizes
recent research on Multilingual Knowledge Editing (MKE), a growing subdomain of
model editing focused on ensuring factual edits generalize reliably across
languages. We present a comprehensive taxonomy of MKE methods, covering
parameter-based, memory-based, fine-tuning, and hypernetwork approaches. We
survey available benchmarks,summarize key findings on method effectiveness and
transfer patterns, identify challenges in cross-lingual propagation, and
highlight open problems related to language anisotropy, evaluation coverage,
and edit scalability. Our analysis consolidates a rapidly evolving area and
lays the groundwork for future progress in editable language-aware LLMs.

</details>


### [98] [MUG-Eval: A Proxy Evaluation Framework for Multilingual Generation Capabilities in Any Language](https://arxiv.org/abs/2505.14395)
*Seyoung Song,Seogyeong Jeong,Eunsu Kim,Jiho Jin,Dongkwan Kim,Jay Shin,Alice Oh*

Main category: cs.CL

TL;DR: MUG-Eval是一个评估大语言模型多语言生成能力的新框架，通过将现有基准转化为对话任务并测量准确率，无需依赖语言特定工具或标注数据。


<details>
  <summary>Details</summary>
Motivation: 评估大语言模型在低资源语言中的文本生成能力具有挑战性，因为直接评估方法稀缺且依赖语言特定工具或标注数据。

Method: 将现有基准转化为对话任务，要求模型在目标语言中有效沟通，并以任务成功率作为生成成功对话的代理指标。

Result: MUG-Eval在30种语言上评估了8个大语言模型，与现有基准强相关（r > 0.75），支持跨语言和模型的标准化比较。

Conclusion: MUG-Eval提供了一个稳健且资源高效的解决方案，可扩展至数千种语言的多语言生成评估。

Abstract: Evaluating text generation capabilities of large language models (LLMs) is
challenging, particularly for low-resource languages where methods for direct
assessment are scarce. We propose MUG-Eval, a novel framework that evaluates
LLMs' multilingual generation capabilities by transforming existing benchmarks
into conversational tasks and measuring the LLMs' accuracies on those tasks. We
specifically designed these conversational tasks to require effective
communication in the target language. Then, we simply use task success rate as
a proxy of successful conversation generation. Our approach offers two key
advantages: it is independent of language-specific NLP tools or annotated
datasets, which are limited for most languages, and it does not rely on
LLMs-as-judges, whose evaluation quality degrades outside a few high-resource
languages. We evaluate 8 LLMs across 30 languages spanning high, mid, and
low-resource categories, and we find that MUG-Eval correlates strongly with
established benchmarks ($r$ > 0.75) while enabling standardized comparisons
across languages and models. Our framework provides a robust and
resource-efficient solution for evaluating multilingual generation that can be
extended to thousands of languages.

</details>


### [99] [Log-Augmented Generation: Scaling Test-Time Reasoning with Reusable Computation](https://arxiv.org/abs/2505.14398)
*Peter Baile Chen,Yi Zhang,Dan Roth,Samuel Madden,Jacob Andreas,Michael Cafarella*

Main category: cs.CL

TL;DR: 提出了一种名为LAG的新框架，通过重用过去的计算和推理日志来增强大语言模型的学习能力，使其在新任务上表现更好。


<details>
  <summary>Details</summary>
Motivation: 大语言模型及其代理版本难以保留和复用过去的推理经验，限制了其在未来任务中的应用能力。

Method: LAG框架利用键值缓存（KV caches）存储过去任务的推理上下文，并在新任务中检索和复用这些缓存以增强生成能力。

Result: 实验表明，LAG在知识和推理密集型数据集上显著优于未使用日志的标准代理系统及现有基于反射和KV缓存的技术。

Conclusion: LAG通过直接复用过去的推理和计算，有效提升了模型在新任务上的表现，同时保持了系统的高效性和可扩展性。

Abstract: While humans naturally learn and adapt from past experiences, large language
models (LLMs) and their agentic counterparts struggle to retain reasoning from
previous tasks and apply them in future contexts. To address this limitation,
we propose a novel framework, log-augmented generation (LAG) that directly
reuses prior computation and reasoning from past logs at test time to enhance
model's ability to learn from previous tasks and perform better on new, unseen
challenges, all while keeping the system efficient and scalable. Specifically,
our system represents task logs using key-value (KV) caches, encoding the full
reasoning context of prior tasks while storing KV caches for only a selected
subset of tokens. When a new task arises, LAG retrieves the KV values from
relevant logs to augment generation. Our approach differs from reflection-based
memory mechanisms by directly reusing prior reasoning and computations without
requiring additional steps for knowledge extraction or distillation. Our method
also goes beyond existing KV caching techniques, which primarily target
efficiency gains rather than improving accuracy. Experiments on knowledge- and
reasoning-intensive datasets demonstrate that our method significantly
outperforms standard agentic systems that do not utilize logs, as well as
existing solutions based on reflection and KV cache techniques.

</details>


### [100] [Pierce the Mists, Greet the Sky: Decipher Knowledge Overshadowing via Knowledge Circuit Analysis](https://arxiv.org/abs/2505.14406)
*Haoming Huang,Yibo Yan,Jiahao Huo,Xin Zou,Xinfeng Li,Kun Wang,Xuming Hu*

Main category: cs.CL

TL;DR: 论文提出PhantomCircuit框架，用于分析和检测大语言模型中的知识遮蔽现象，通过知识回路分析揭示其内部机制。


<details>
  <summary>Details</summary>
Motivation: 大语言模型存在知识遮蔽问题，即一个知识片段无意中掩盖另一个相关片段，导致错误输出。目前对其起源和内部机制缺乏深入理解。

Method: 引入PhantomCircuit框架，创新性地运用知识回路分析，剖析注意力头的内部工作机制，追踪竞争知识路径如何导致遮蔽现象及其在训练过程中的演变。

Result: 大量实验证明PhantomCircuit能有效识别知识遮蔽实例，为这一难以捉摸的幻觉现象提供新见解。

Conclusion: PhantomCircuit为研究社区提供了新的方法论视角，有望缓解知识遮蔽问题。

Abstract: Large Language Models (LLMs), despite their remarkable capabilities, are
hampered by hallucinations. A particularly challenging variant, knowledge
overshadowing, occurs when one piece of activated knowledge inadvertently masks
another relevant piece, leading to erroneous outputs even with high-quality
training data. Current understanding of overshadowing is largely confined to
inference-time observations, lacking deep insights into its origins and
internal mechanisms during model training. Therefore, we introduce
PhantomCircuit, a novel framework designed to comprehensively analyze and
detect knowledge overshadowing. By innovatively employing knowledge circuit
analysis, PhantomCircuit dissects the internal workings of attention heads,
tracing how competing knowledge pathways contribute to the overshadowing
phenomenon and its evolution throughout the training process. Extensive
experiments demonstrate PhantomCircuit's effectiveness in identifying such
instances, offering novel insights into this elusive hallucination and
providing the research community with a new methodological lens for its
potential mitigation.

</details>


### [101] [Hidden Ghost Hand: Unveiling Backdoor Vulnerabilities in MLLM-Powered Mobile GUI Agents](https://arxiv.org/abs/2505.14418)
*Pengzhou Cheng,Haowen Hu,Zheng Wu,Zongru Wu,Tianjie Ju,Daizong Ding,Zhuosheng Zhang,Gongshen Liu*

Main category: cs.CL

TL;DR: 该论文揭示了多模态大语言模型（MLLM）驱动的GUI代理存在后门攻击风险，并提出了一个名为AgentGhost的框架，用于高效且隐蔽地进行红队后门攻击。


<details>
  <summary>Details</summary>
Motivation: 由于微调成本高，用户常依赖开源GUI代理或AI提供商提供的API，这引入了供应链威胁，尤其是后门攻击。论文旨在揭示并解决这一问题。

Method: 通过组合目标和交互级别的触发器，将后门注入建模为Min-Max优化问题，利用监督对比学习和监督微调来增强后门的灵活性和有效性。

Result: 在多个代理模型和移动基准测试中，AgentGhost攻击准确率高达99.7%，且仅导致1%的效用下降。论文还提出了一种防御方法，将攻击准确率降至22.1%。

Conclusion: AgentGhost展示了GUI代理的后门攻击风险，并提出了有效的攻击和防御方法，为未来研究提供了重要参考。

Abstract: Graphical user interface (GUI) agents powered by multimodal large language
models (MLLMs) have shown greater promise for human-interaction. However, due
to the high fine-tuning cost, users often rely on open-source GUI agents or
APIs offered by AI providers, which introduces a critical but underexplored
supply chain threat: backdoor attacks. In this work, we first unveil that
MLLM-powered GUI agents naturally expose multiple interaction-level triggers,
such as historical steps, environment states, and task progress. Based on this
observation, we introduce AgentGhost, an effective and stealthy framework for
red-teaming backdoor attacks. Specifically, we first construct composite
triggers by combining goal and interaction levels, allowing GUI agents to
unintentionally activate backdoors while ensuring task utility. Then, we
formulate backdoor injection as a Min-Max optimization problem that uses
supervised contrastive learning to maximize the feature difference across
sample classes at the representation space, improving flexibility of the
backdoor. Meanwhile, it adopts supervised fine-tuning to minimize the
discrepancy between backdoor and clean behavior generation, enhancing
effectiveness and utility. Extensive evaluations of various agent models in two
established mobile benchmarks show that AgentGhost is effective and generic,
with attack accuracy that reaches 99.7\% on three attack objectives, and shows
stealthiness with only 1\% utility degradation. Furthermore, we tailor a
defense method against AgentGhost that reduces the attack accuracy to 22.1\%.
Our code is available at \texttt{anonymous}.

</details>


### [102] [SAE-FiRE: Enhancing Earnings Surprise Predictions Through Sparse Autoencoder Feature Selection](https://arxiv.org/abs/2505.14420)
*Huopu Zhang,Yanguang Liu,Mengnan Du*

Main category: cs.CL

TL;DR: 提出SAE-FiRE框架，通过稀疏自编码器分析财报电话会议记录，有效提取关键信息并预测盈利意外。


<details>
  <summary>Details</summary>
Motivation: 财报电话会议记录包含大量冗余和专业术语，传统语言模型难以有效分析，需开发新方法提取关键金融信号。

Method: 使用稀疏自编码器(SAEs)构建SAE-FiRE框架，过滤噪声并捕捉具有预测性的细微金融信号。

Result: 实验表明SAE-FiRE显著优于基线模型，能更准确地预测盈利意外。

Conclusion: SAE-FiRE框架解决了财报文本分析中的冗余和术语障碍，为盈利预测提供了有效工具。

Abstract: Predicting earnings surprises through the analysis of earnings conference
call transcripts has attracted increasing attention from the financial research
community. Conference calls serve as critical communication channels between
company executives, analysts, and shareholders, offering valuable
forward-looking information. However, these transcripts present significant
analytical challenges, typically containing over 5,000 words with substantial
redundancy and industry-specific terminology that creates obstacles for
language models. In this work, we propose the Sparse Autoencoder for Financial
Representation Enhancement (SAE-FiRE) framework to address these limitations by
extracting key information while eliminating redundancy. SAE-FiRE employs
Sparse Autoencoders (SAEs) to efficiently identify patterns and filter out
noises, and focusing specifically on capturing nuanced financial signals that
have predictive power for earnings surprises. Experimental results indicate
that the proposed method can significantly outperform comparing baselines.

</details>


### [103] [Scaling Low-Resource MT via Synthetic Data Generation with LLMs](https://arxiv.org/abs/2505.14423)
*Ona de Gibert,Joseph Attieh,Teemu Vahtola,Mikko Aulamo,Zihao Li,Raúl Vázquez,Tiancheng Hu,Jörg Tiedemann*

Main category: cs.CL

TL;DR: 利用LLM生成合成数据提升低资源机器翻译性能，构建SynOPUS公开数据集。


<details>
  <summary>Details</summary>
Motivation: 解决低资源语言机器翻译数据不足的问题，探索合成数据的潜力。

Method: 基于英语Europarl构建文档级合成语料，通过枢轴翻译扩展至147种语言对，并进行自动与人工评估。

Result: 合成数据显著提升低资源语言翻译性能，即使存在噪声也有效。

Conclusion: LLM生成的合成数据是低资源机器翻译的有力补充，SynOPUS数据集为研究提供资源。

Abstract: We investigate the potential of LLM-generated synthetic data for improving
low-resource machine translation (MT). Focusing on seven diverse target
languages, we construct a document-level synthetic corpus from English
Europarl, and extend it via pivoting to 147 additional language pairs.
Automatic and human evaluation confirm its high overall quality. We study its
practical application by (i) identifying effective training regimes, (ii)
comparing our data with the HPLT dataset, and (iii) testing its utility beyond
English-centric MT. Finally, we introduce SynOPUS, a public repository for
synthetic parallel datasets. Our findings show that LLM-generated synthetic
data, even when noisy, can substantially improve MT performance for
low-resource languages.

</details>


### [104] [From Templates to Natural Language: Generalization Challenges in Instruction-Tuned LLMs for Spatial Reasoning](https://arxiv.org/abs/2505.14425)
*Chalamalasetti Kranti,Sherzod Hakimov,David Schlangen*

Main category: cs.CL

TL;DR: 研究显示，指令调优的大型语言模型在简单空间任务上表现良好，但在复杂任务上泛化能力显著下降。


<details>
  <summary>Details</summary>
Motivation: 探讨指令调优的大型语言模型在从合成指令泛化到人类编写指令时的挑战，特别是在空间基础任务中。

Method: 仅使用合成指令对大型语言模型进行微调，并在包含合成和人类编写指令的基准数据集上评估其性能。

Result: 模型在简单任务上泛化良好，但在复杂任务上性能显著下降，并进行了详细的错误分析。

Conclusion: 研究揭示了指令泛化中的性能差距，特别是在复杂空间基础任务中。

Abstract: Instruction-tuned large language models (LLMs) have shown strong performance
on a variety of tasks; however, generalizing from synthetic to human-authored
instructions in grounded environments remains a challenge for them. In this
work, we study generalization challenges in spatial grounding tasks where
models interpret and translate instructions for building object arrangements on
a $2.5$D grid. We fine-tune LLMs using only synthetic instructions and evaluate
their performance on a benchmark dataset containing both synthetic and
human-written instructions. Our results reveal that while models generalize
well on simple tasks, their performance degrades significantly on more complex
tasks. We present a detailed error analysis of the gaps in instruction
generalization.

</details>


### [105] [Neural Incompatibility: The Unbridgeable Gap of Cross-Scale Parametric Knowledge Transfer in Large Language Models](https://arxiv.org/abs/2505.14436)
*Yuqiao Tan,Shizhu He,Kang Liu,Jun Zhao*

Main category: cs.CL

TL;DR: 该论文探讨了大型语言模型（LLMs）中参数化知识迁移（PKT）的挑战，提出了Pre-Align PKT（PrePKT）新范式及LaTen解决方案，发现神经不兼容性是跨规模PKT的主要障碍。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于超越基于符号语言的传统知识迁移范式，实现真正的参数化知识迁移（PKT），并探索不同规模LLMs间通过参数迁移知识的有效方法。

Method: 论文首先证明参数空间对齐是跨规模PKT成功的前提，重新定义知识迁移为Post-Align PKT（PostPKT），并提出Pre-Align PKT（PrePKT）新范式及LaTen解决方案，通过少量训练步骤对齐参数空间。

Result: 在四个基准测试上的实验表明，PostPKT和PrePKT均难以实现稳定迁移。深入分析揭示，不同规模LLMs间的神经不兼容性是PKT的根本挑战。

Conclusion: 研究为LLMs参数架构提供了新见解，并指出了未来高效PKT研究的潜在方向。

Abstract: Large Language Models (LLMs) offer a transparent brain with accessible
parameters that encode extensive knowledge, which can be analyzed, located and
transferred. Consequently, a key research challenge is to transcend traditional
knowledge transfer paradigms rooted in symbolic language and achieve genuine
Parametric Knowledge Transfer (PKT). Significantly, exploring effective methods
for transferring knowledge across LLMs of different scales through parameters
presents an intriguing and valuable research direction. In this paper, we first
demonstrate $\textbf{Alignment}$ in parametric space is the fundamental
prerequisite to achieve successful cross-scale PKT. We redefine the previously
explored knowledge transfer as Post-Align PKT (PostPKT), which utilizes
extracted parameters for LoRA initialization and requires subsequent fine-tune
for alignment. Hence, to reduce cost for further fine-tuning, we introduce a
novel Pre-Align PKT (PrePKT) paradigm and propose a solution called
$\textbf{LaTen}$
($\textbf{L}$oc$\textbf{a}$te-$\textbf{T}$h$\textbf{e}$n-Alig$\textbf{n}$) that
aligns the parametric spaces of LLMs across scales only using several training
steps without following training. Comprehensive experiments on four benchmarks
demonstrate that both PostPKT and PrePKT face challenges in achieving
consistently stable transfer. Through in-depth analysis, we identify
$\textbf{Neural Incompatibility}$ as the ethological and parametric structural
differences between LLMs of varying scales, presenting fundamental challenges
to achieving effective PKT. These findings provide fresh insights into the
parametric architectures of LLMs and highlight promising directions for future
research on efficient PKT. Our code is available at
https://github.com/Trae1ounG/Neural_Incompatibility.

</details>


### [106] [Creative Preference Optimization](https://arxiv.org/abs/2505.14442)
*Mete Ismayilzada,Antonio Laverghetta Jr.,Simone A. Luchini,Reet Patel,Antoine Bosselut,Lonneke van der Plas,Roger Beaty*

Main category: cs.CL

TL;DR: 该论文提出了一种名为CrPO的新方法，通过多维度创造力信号增强LLM的创造性，实验表明其效果优于GPT-4o等基线模型。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型在生成内容时缺乏真正的创造性（新颖性、多样性、惊喜感和质量），现有方法往往只关注单一维度或特定任务，未能全面解决创造性的多面性问题。

Method: 提出了Creative Preference Optimization (CrPO)，一种模块化的对齐方法，将多个创造力维度的信号注入偏好优化目标，并使用新的大规模人类偏好数据集MuCE进行训练和评估。

Result: 使用CrPO训练的模型在自动和人类评估中均优于包括GPT-4o在内的强基线模型，能够生成更具新颖性、多样性和惊喜感的内容，同时保持高质量输出。

Conclusion: 在偏好框架中直接优化创造力是提升LLM创造性能力的有前景的方向，且不会影响输出质量。

Abstract: While Large Language Models (LLMs) have demonstrated impressive performance
across natural language generation tasks, their ability to generate truly
creative content-characterized by novelty, diversity, surprise, and
quality-remains limited. Existing methods for enhancing LLM creativity often
focus narrowly on diversity or specific tasks, failing to address creativity's
multifaceted nature in a generalizable way. In this work, we propose Creative
Preference Optimization (CrPO), a novel alignment method that injects signals
from multiple creativity dimensions into the preference optimization objective
in a modular fashion. We train and evaluate creativity-augmented versions of
several models using CrPO and MuCE, a new large-scale human preference dataset
spanning over 200,000 human-generated responses and ratings from more than 30
psychological creativity assessments. Our models outperform strong baselines,
including GPT-4o, on both automated and human evaluations, producing more
novel, diverse, and surprising generations while maintaining high output
quality. Additional evaluations on NoveltyBench further confirm the
generalizability of our approach. Together, our results demonstrate that
directly optimizing for creativity within preference frameworks is a promising
direction for advancing the creative capabilities of LLMs without compromising
output quality.

</details>


### [107] [CtrlDiff: Boosting Large Diffusion Language Models with Dynamic Block Prediction and Controllable Generation](https://arxiv.org/abs/2505.14455)
*Chihan Huang,Hao Tang*

Main category: cs.CL

TL;DR: 本文提出CtrlDiff框架，通过强化学习动态调整生成块大小并结合分类器引导机制，解决了现有扩散语言模型固定长度和弱可控性问题，缩小了与自回归模型的性能差距。


<details>
  <summary>Details</summary>
Motivation: 当前扩散语言模型存在固定长度生成和缺乏灵活控制的问题，限制了其实际应用。本文旨在结合自回归和扩散模型的优势，提升生成灵活性和可控性。

Method: 提出动态半自回归框架CtrlDiff，利用强化学习自适应确定生成块大小，并设计针对离散扩散的分类器引导控制机制，降低计算开销。

Result: 实验表明CtrlDiff在混合扩散模型中表现优异，缩小了与最先进自回归模型的差距，并能有效支持多样化的条件文本生成任务。

Conclusion: CtrlDiff通过动态块生成和高效控制机制，为扩散语言模型提供了更灵活的生成方式，推动了该领域的发展。

Abstract: Although autoregressive models have dominated language modeling in recent
years, there has been a growing interest in exploring alternative paradigms to
the conventional next-token prediction framework. Diffusion-based language
models have emerged as a compelling alternative due to their powerful parallel
generation capabilities and inherent editability. However, these models are
often constrained by fixed-length generation. A promising direction is to
combine the strengths of both paradigms, segmenting sequences into blocks,
modeling autoregressive dependencies across blocks while leveraging discrete
diffusion to estimate the conditional distribution within each block given the
preceding context. Nevertheless, their practical application is often hindered
by two key limitations: rigid fixed-length outputs and a lack of flexible
control mechanisms. In this work, we address the critical limitations of fixed
granularity and weak controllability in current large diffusion language
models. We propose CtrlDiff, a dynamic and controllable semi-autoregressive
framework that adaptively determines the size of each generation block based on
local semantics using reinforcement learning. Furthermore, we introduce a
classifier-guided control mechanism tailored to discrete diffusion, which
significantly reduces computational overhead while facilitating efficient
post-hoc conditioning without retraining. Extensive experiments demonstrate
that CtrlDiff sets a new standard among hybrid diffusion models, narrows the
performance gap to state-of-the-art autoregressive approaches, and enables
effective conditional text generation across diverse tasks.

</details>


### [108] [Not All Correct Answers Are Equal: Why Your Distillation Source Matters](https://arxiv.org/abs/2505.14464)
*Xiaoyu Tian,Yunjie Ji,Haotian Wang,Shuaiting Chen,Sitong Zhao,Yiping Peng,Han Zhao,Xiangang Li*

Main category: cs.CL

TL;DR: 蒸馏技术提升开源语言模型推理能力，AM-Thinking-v1数据表现最佳，模型在多项基准测试中领先。


<details>
  <summary>Details</summary>
Motivation: 研究旨在通过蒸馏技术增强开源语言模型的推理能力，探索高质量推理数据对模型性能的影响。

Method: 收集三个先进教师模型在189万查询上的验证输出，构建并行数据集并分析分布，训练学生模型并评估。

Result: 基于AM-Thinking-v1的模型在AIME2024、AIME2025、MATH500和LiveCodeBench上表现最佳，且能自适应输出长度。

Conclusion: 高质量验证推理数据对提升模型性能至关重要，相关数据集已公开以支持未来研究。

Abstract: Distillation has emerged as a practical and effective approach to enhance the
reasoning capabilities of open-source language models. In this work, we conduct
a large-scale empirical study on reasoning data distillation by collecting
verified outputs from three state-of-the-art teacher models-AM-Thinking-v1,
Qwen3-235B-A22B, and DeepSeek-R1-on a shared corpus of 1.89 million queries. We
construct three parallel datasets and analyze their distributions, revealing
that AM-Thinking-v1-distilled data exhibits greater token length diversity and
lower perplexity. Student models trained on each dataset are evaluated on
reasoning benchmarks including AIME2024, AIME2025, MATH500, and LiveCodeBench.
The AM-based model consistently achieves the best performance (e.g., 84.3 on
AIME2024, 72.2 on AIME2025, 98.4 on MATH500, and 65.9 on LiveCodeBench) and
demonstrates adaptive output behavior-producing longer responses for harder
tasks and shorter ones for simpler tasks. These findings highlight the value of
high-quality, verified reasoning traces. We release the AM-Thinking-v1 and
Qwen3-235B-A22B distilled datasets to support future research on open and
high-performing reasoning-oriented language models. The datasets are publicly
available on Hugging Face\footnote{Datasets are available on Hugging Face:
\href{https://huggingface.co/datasets/a-m-team/AM-Thinking-v1-Distilled}{AM-Thinking-v1-Distilled},
\href{https://huggingface.co/datasets/a-m-team/AM-Qwen3-Distilled}{AM-Qwen3-Distilled}.}.

</details>


### [109] [Void in Language Models](https://arxiv.org/abs/2505.14467)
*Mani Shemiranifar*

Main category: cs.CL

TL;DR: 研究发现，在指令调优的语言模型推理过程中，并非所有层都被激活，通过L2自适应计算方法跳过未激活层可提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 尽管基于Transformer的语言模型取得了进展，但推理过程中是否所有层都被激活仍是一个未解之谜。本文旨在探究这一问题，并验证跳过未激活层是否能提升模型效率与性能。

Method: 采用非训练、无参数的L2自适应计算方法（LAC），通过监测激活值的L2范数变化识别未激活层（Voids），并分别在提示处理（PP）和响应生成（RG）阶段追踪激活层。

Result: 在MMLU、GPQA Diamond和BoolQ基准测试中，跳过未激活层可使模型性能提升（如Qwen2.5-7B-Instruct在MMLU上准确率从69.24升至71.29），同时显著减少计算量（如仅使用30%层）。

Conclusion: 语言模型推理过程中存在大量未激活层，选择性跳过这些层不仅能保持性能，还能提升特定任务的表现，说明模型层并非均等参与推理。

Abstract: Despite advances in transformer-based language models (LMs), a fundamental
question remains largely unanswered: Are all layers activated during inference?
We investigate this question by detecting unactivated layers (which we refer to
as Voids) using a non-trainable and parameter-free adaptive computation method
called L2 Adaptive Computation (LAC). We adapt LAC from its original
efficiency-focused application to trace activated layers during inference. This
method monitors changes in the L2-norm of activations to identify voids. We
analyze layer activation in instruction-tuned LMs across two phases: Prompt
Processing (PP), where we trace activated layers for each token in the input
prompts, and Response Generation (RG), where we trace activated layers for each
generated token. We further demonstrate that distinct layers are activated
during these two phases. To show the effectiveness of our method, we evaluated
three distinct instruction-tuned LMs from the Llama, Mistral, and Qwen families
on three benchmarks: MMLU, GPQA Diamond, and BoolQ. For example, on MMLU with a
zero-shot setting, skipping voids in Qwen2.5-7B-Instruct resulted in an
improvement from 69.24 to 71.29 while the model uses only 30% of the layers.
Similarly, Mistral-7B-Instruct-v0.3 on GPQA Diamond improved from 13.88 to
18.36 when using 70% of the layers during both the PP and RG phases. These
results show that not all layers contribute equally during inference, and that
selectively skipping most of them can improve the performance of models on
certain tasks.

</details>


### [110] [Attributional Safety Failures in Large Language Models under Code-Mixed Perturbations](https://arxiv.org/abs/2505.14469)
*Somnath Banerjee,Pratyush Chatterjee,Shanu Kumar,Sayan Layek,Parag Agrawal,Rima Hazra,Animesh Mukherjee*

Main category: cs.CL

TL;DR: 研究发现LLMs在处理混合代码输入时更容易产生不安全输出，并通过可解释性方法分析了其内部机制。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs的进步，处理混合代码输入时的安全问题日益突出，研究旨在系统评估其风险并探索文化因素的影响。

Method: 使用可解释性方法分析模型内部归因变化，并区分普遍不安全与文化特定不安全查询。

Result: 混合代码提示比单语提示更易引发不安全输出，揭示了模型行为的内部机制和文化维度的影响。

Conclusion: 研究阐明了LLMs在混合代码场景下的安全隐患，为安全改进提供了实验依据。

Abstract: Recent advancements in LLMs have raised significant safety concerns,
particularly when dealing with code-mixed inputs and outputs. Our study
systematically investigates the increased susceptibility of LLMs to produce
unsafe outputs from code-mixed prompts compared to monolingual English prompts.
Utilizing explainability methods, we dissect the internal attribution shifts
causing model's harmful behaviors. In addition, we explore cultural dimensions
by distinguishing between universally unsafe and culturally-specific unsafe
queries. This paper presents novel experimental insights, clarifying the
mechanisms driving this phenomenon.

</details>


### [111] [Adapting Pretrained Language Models for Citation Classification via Self-Supervised Contrastive Learning](https://arxiv.org/abs/2505.14471)
*Tong Li,Jiachuan Wang,Yongqi Zhang,Shuangyin Li,Lei Chen*

Main category: cs.CL

TL;DR: Citss是一个新颖的框架，通过自监督对比学习和两种专门策略解决引用分类中的数据稀缺和噪声问题，兼容编码器和解码器模型，实验表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 引用分类对学术分析至关重要，但直接微调预训练语言模型面临数据稀缺、上下文噪声和虚假关键词关联等挑战。

Method: Citss引入自监督对比学习，采用句子级裁剪和关键词扰动策略生成对比对，兼容编码器和解码器模型。

Result: 在三个基准数据集上的实验表明，Citss在编码器和解码器模型上均优于现有方法。

Conclusion: Citss有效解决了引用分类中的关键挑战，为学术分析提供了更强大的工具。

Abstract: Citation classification, which identifies the intention behind academic
citations, is pivotal for scholarly analysis. Previous works suggest
fine-tuning pretrained language models (PLMs) on citation classification
datasets, reaping the reward of the linguistic knowledge they gained during
pretraining. However, directly fine-tuning for citation classification is
challenging due to labeled data scarcity, contextual noise, and spurious
keyphrase correlations. In this paper, we present a novel framework, Citss,
that adapts the PLMs to overcome these challenges. Citss introduces
self-supervised contrastive learning to alleviate data scarcity, and is
equipped with two specialized strategies to obtain the contrastive pairs:
sentence-level cropping, which enhances focus on target citations within long
contexts, and keyphrase perturbation, which mitigates reliance on specific
keyphrases. Compared with previous works that are only designed for
encoder-based PLMs, Citss is carefully developed to be compatible with both
encoder-based PLMs and decoder-based LLMs, to embrace the benefits of enlarged
pretraining. Experiments with three benchmark datasets with both encoder-based
PLMs and decoder-based LLMs demonstrate our superiority compared to the
previous state of the art. Our code is available at: github.com/LITONG99/Citss

</details>


### [112] [PlanGPT-VL: Enhancing Urban Planning with Domain-Specific Vision-Language Models](https://arxiv.org/abs/2505.14481)
*He Zhu,Junyou Su,Minxi Chen,Wen Wang,Yijie Deng,Guanhua Chen,Wenjia Zhang*

Main category: cs.CL

TL;DR: PlanGPT-VL是一个专为城市规划地图设计的视觉语言模型，通过创新方法提升地图分析能力，性能优于通用模型。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型在城市规划地图分析上效果不佳，而这类地图对空间配置、法规要求等有专业需求，亟需专用工具。

Method: 采用PlanAnno-V框架合成高质量VQA数据，Critical Point Thinking减少幻觉，结合监督微调与冻结视觉编码器参数进行训练。

Result: PlanGPT-VL在PlanBench-V基准测试中显著优于通用VLMs，7B参数模型性能媲美72B参数模型，保持高事实准确性。

Conclusion: PlanGPT-VL为城市规划提供可靠的地图分析与教育工具，高效实现领域专业化。

Abstract: In the field of urban planning, existing Vision-Language Models (VLMs)
frequently fail to effectively analyze and evaluate planning maps, despite the
critical importance of these visual elements for urban planners and related
educational contexts. Planning maps, which visualize land use, infrastructure
layouts, and functional zoning, require specialized understanding of spatial
configurations, regulatory requirements, and multi-scale analysis. To address
this challenge, we introduce PlanGPT-VL, the first domain-specific
Vision-Language Model tailored specifically for urban planning maps. PlanGPT-VL
employs three innovative approaches: (1) PlanAnno-V framework for high-quality
VQA data synthesis, (2) Critical Point Thinking to reduce hallucinations
through structured verification, and (3) comprehensive training methodology
combining Supervised Fine-Tuning with frozen vision encoder parameters. Through
systematic evaluation on our proposed PlanBench-V benchmark, we demonstrate
that PlanGPT-VL significantly outperforms general-purpose state-of-the-art VLMs
in specialized planning map interpretation tasks, offering urban planning
professionals a reliable tool for map analysis, assessment, and educational
applications while maintaining high factual accuracy. Our lightweight 7B
parameter model achieves comparable performance to models exceeding 72B
parameters, demonstrating efficient domain specialization without sacrificing
performance.

</details>


### [113] [MoMoE: Mixture of Moderation Experts Framework for AI-Assisted Online Governance](https://arxiv.org/abs/2505.14483)
*Agam Goyal,Xianyang Zhan,Yilun Chen,Koustuv Saha,Eshwar Chandrasekharan*

Main category: cs.CL

TL;DR: MoMoE框架通过模块化专家系统实现跨社区内容审核，提供可解释性且无需针对每个社区微调，性能媲美微调基线。


<details>
  <summary>Details</summary>
Motivation: 现有内容审核方法需为每个社区单独训练模型且决策不透明，限制了实际应用。需要一种可扩展、透明的解决方案。

Method: 提出MoMoE框架，包含分配、预测、聚合、解释四个操作模块，实例化为7个社区专家和5个违规专家。

Result: 在30个未见过的subreddit上，最佳变体Micro-F1分别达0.72和0.67，社区专家峰值准确率最高，违规专家跨领域表现更稳定。

Conclusion: MoMoE证明了轻量级可解释专家系统可实现可信赖的内容审核，为NLP和HCI研究提供了新方向。

Abstract: Large language models (LLMs) have shown great potential in flagging harmful
content in online communities. Yet, existing approaches for moderation require
a separate model for every community and are opaque in their decision-making,
limiting real-world adoption. We introduce Mixture of Moderation Experts
(MoMoE), a modular, cross-community framework that adds post-hoc explanations
to scalable content moderation. MoMoE orchestrates four operators -- Allocate,
Predict, Aggregate, Explain -- and is instantiated as seven
community-specialized experts (MoMoE-Community) and five norm-violation experts
(MoMoE-NormVio). On 30 unseen subreddits, the best variants obtain Micro-F1
scores of 0.72 and 0.67, respectively, matching or surpassing strong fine-tuned
baselines while consistently producing concise and reliable explanations.
Although community-specialized experts deliver the highest peak accuracy,
norm-violation experts provide steadier performance across domains. These
findings show that MoMoE yields scalable, transparent moderation without
needing per-community fine-tuning. More broadly, they suggest that lightweight,
explainable expert ensembles can guide future NLP and HCI research on
trustworthy human-AI governance of online communities.

</details>


### [114] [Enhanced Multimodal Aspect-Based Sentiment Analysis by LLM-Generated Rationales](https://arxiv.org/abs/2505.14499)
*Jun Cao,Jiyi Li,Ziwei Yang,Renjie Zhou*

Main category: cs.CL

TL;DR: 提出LRSA框架，结合小语言模型决策能力与大语言模型补充信息，通过双交叉注意力机制提升多模态方面情感分析性能。


<details>
  <summary>Details</summary>
Motivation: 现有MABSA方法依赖小语言模型，但能力有限导致识别不准确；大语言模型虽表现优异，但在ABSA领域仍不及微调小模型。需结合两者优势提升性能。

Method: LRSA框架将大语言模型生成解释作为补充信息注入小语言模型，采用双交叉注意力机制增强特征交互与融合。

Result: 在三个基准测试中优于基线模型，验证了方法对多数预训练模型的通用性和适用性。

Conclusion: LRSA通过结合大小语言模型优势，有效提升了多模态方面情感分析的准确性。

Abstract: There has been growing interest in Multimodal Aspect-Based Sentiment Analysis
(MABSA) in recent years. Existing methods predominantly rely on pre-trained
small language models (SLMs) to collect information related to aspects and
sentiments from both image and text, with an aim to align these two modalities.
However, small SLMs possess limited capacity and knowledge, often resulting in
inaccurate identification of meaning, aspects, sentiments, and their
interconnections in textual and visual data. On the other hand, Large language
models (LLMs) have shown exceptional capabilities in various tasks by
effectively exploring fine-grained information in multimodal data. However,
some studies indicate that LLMs still fall short compared to fine-tuned small
models in the field of ABSA. Based on these findings, we propose a novel
framework, termed LRSA, which combines the decision-making capabilities of SLMs
with additional information provided by LLMs for MABSA. Specifically, we inject
explanations generated by LLMs as rationales into SLMs and employ a dual
cross-attention mechanism for enhancing feature interaction and fusion, thereby
augmenting the SLMs' ability to identify aspects and sentiments. We evaluated
our method using two baseline models, numerous experiments highlight the
superiority of our approach on three widely-used benchmarks, indicating its
generalizability and applicability to most pre-trained models for MABSA.

</details>


### [115] [ModRWKV: Transformer Multimodality in Linear Time](https://arxiv.org/abs/2505.14505)
*Jiale Kang,Ziyin Yue,Qingyu Yin,Jiang Rui,Weile Li,Zening Lu,Zhouran Ji*

Main category: cs.CL

TL;DR: 论文提出基于RWKV7架构的轻量级多模态框架ModRWKV，通过动态异构编码器实现多源信息融合，证明现代RNN架构可作为Transformer的有效替代方案。


<details>
  <summary>Details</summary>
Motivation: 当前多模态研究主要依赖计算复杂度高的Transformer架构，而线性模型（如RNN）虽推理成本低，但多局限于单模态文本。本文探索现代RNN在多模态场景的潜力。

Method: 基于RWKV7架构构建解耦式多模态框架ModRWKV，采用动态异构模态编码器融合多源信息，设计极轻量级模块，并通过实验确定性能与效率最优配置。

Result: 实验表明：1) 利用RWKV7预训练权重显著加速训练；2) 该初始化策略对提升多模态信号理解能力至关重要；3) ModRWKV在性能与计算效率间取得最佳平衡。

Conclusion: 现代RNN架构可作为多模态大语言模型中Transformer的可行替代方案，系统探索确定了ModRWKV的最优配置。

Abstract: Currently, most multimodal studies are based on large language models (LLMs)
with quadratic-complexity Transformer architectures. While linear models like
RNNs enjoy low inference costs, their application has been largely limited to
the text-only modality. This work explores the capabilities of modern RNN
architectures in multimodal contexts. We propose ModRWKV-a decoupled multimodal
framework built upon the RWKV7 architecture as its LLM backbone-which achieves
multi-source information fusion through dynamically adaptable heterogeneous
modality encoders. We designed the multimodal modules in ModRWKV with an
extremely lightweight architecture and, through extensive experiments,
identified a configuration that achieves an optimal balance between performance
and computational efficiency. ModRWKV leverages the pretrained weights of the
RWKV7 LLM for initialization, which significantly accelerates multimodal
training. Comparative experiments with different pretrained checkpoints further
demonstrate that such initialization plays a crucial role in enhancing the
model's ability to understand multimodal signals. Supported by extensive
experiments, we conclude that modern RNN architectures present a viable
alternative to Transformers in the domain of multimodal large language models
(MLLMs). Furthermore, we identify the optimal configuration of the ModRWKV
architecture through systematic exploration.

</details>


### [116] [Exploring Graph Representations of Logical Forms for Language Modeling](https://arxiv.org/abs/2505.14523)
*Michael Sullivan*

Main category: cs.CL

TL;DR: 论文提出基于逻辑形式的语言模型（LFLMs），证明其比纯文本模型更高效，并通过GFoLDS原型验证其性能优势。


<details>
  <summary>Details</summary>
Motivation: 研究动机是证明基于逻辑形式的语言模型（LFLMs）比纯文本模型更具数据效率，能够利用内置的语义知识快速学习复杂模式。

Method: 方法是通过引入GFoLDS原型，一种基于逻辑形式图表示的预训练语言模型，作为LFLMs的概念验证。

Result: 实验结果表明，GFoLDS在少量数据下显著优于纯文本Transformer模型，且性能可能随参数和数据量增加而提升。

Conclusion: 结论是LFLMs在实际应用中具有潜力，因其数据效率高且性能可扩展。

Abstract: We make the case for language models over logical forms (LFLMs), arguing that
such models are more data-efficient than their textual counterparts. To that
end, we introduce the Graph-based Formal-Logical Distributional Semantics
(GFoLDS) prototype, a pretrained LM over graph representations of logical
forms, as a proof-of-concept of LFLMs. Using GFoLDS, we present strong
experimental evidence that LFLMs can leverage the built-in, basic linguistic
knowledge inherent in such models to immediately begin learning more complex
patterns. On downstream tasks, we show that GFoLDS vastly outperforms textual,
transformer LMs pretrained on similar amounts of data, indicating that LFLMs
can learn with substantially less data than models over plain text.
Furthermore, we show that the performance of this model is likely to scale with
additional parameters and pretraining data, suggesting the viability of LFLMs
in real-world applications.

</details>


### [117] [Internal Chain-of-Thought: Empirical Evidence for Layer-wise Subtask Scheduling in LLMs](https://arxiv.org/abs/2505.14530)
*Zhipeng Yang,Junzhuo Li,Siyu Xia,Xuming Hu*

Main category: cs.CL

TL;DR: 研究发现大语言模型（LLMs）具有内部思维链能力，能分层顺序分解和执行复合任务，并通过实验验证了其分层执行模式。


<details>
  <summary>Details</summary>
Motivation: 探索大语言模型内部如何处理复合任务，验证其是否通过分层逐步执行子任务，以增强模型透明度和可解释性。

Method: 使用层间上下文掩码和新颖的跨任务修补方法验证子任务在不同网络深度的学习情况，并通过LogitLens解码隐藏状态分析执行模式。

Result: 在15个两步复合任务基准和真实TRACE基准上，均观察到模型分层顺序执行子任务的模式，证实了内部思维链的存在。

Conclusion: 研究揭示了LLMs内部规划和执行子任务的能力，为细粒度的指令级激活调控开辟了新途径。

Abstract: We show that large language models (LLMs) exhibit an $\textit{internal
chain-of-thought}$: they sequentially decompose and execute composite tasks
layer-by-layer. Two claims ground our study: (i) distinct subtasks are learned
at different network depths, and (ii) these subtasks are executed sequentially
across layers. On a benchmark of 15 two-step composite tasks, we employ
layer-from context-masking and propose a novel cross-task patching method,
confirming (i). To examine claim (ii), we apply LogitLens to decode hidden
states, revealing a consistent layerwise execution pattern. We further
replicate our analysis on the real-world $\text{TRACE}$ benchmark, observing
the same stepwise dynamics. Together, our results enhance LLMs transparency by
showing their capacity to internally plan and execute subtasks (or
instructions), opening avenues for fine-grained, instruction-level activation
steering.

</details>


### [118] [Breaking Bad Tokens: Detoxification of LLMs Using Sparse Autoencoders](https://arxiv.org/abs/2505.14536)
*Agam Goyal,Vedant Rathi,William Yeh,Yian Wang,Yuen Chen,Hari Sundaram*

Main category: cs.CL

TL;DR: 论文提出基于稀疏自编码器（SAE）的激活导向方法，通过识别毒性相关方向并针对性干预，在降低大语言模型毒性输出的同时保持语言流畅性和模型能力。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型（LLM）仍会生成不当的有毒内容（如脏话、贬损言论），而主流去毒方法易被越狱攻击绕过。需要更精准的干预手段。

Method: 使用稀疏自编码器定位模型残差流中的毒性方向，通过解码器向量进行三级强度的激活导向干预，在GPT-2 Small和Gemma-2-2B上验证。

Result: 强干预可使毒性降低20%（优于基线），但GPT-2 Small的流畅性会下降；模型知识能力保持稳定；宽SAE的特征分裂会削弱安全干预效果。

Conclusion: SAE的因果干预对LLM去毒有潜力但存在局限，需解耦特征学习，为安全部署提供实践指导。

Abstract: Large language models (LLMs) are now ubiquitous in user-facing applications,
yet they still generate undesirable toxic outputs, including profanity,
vulgarity, and derogatory remarks. Although numerous detoxification methods
exist, most apply broad, surface-level fixes and can therefore easily be
circumvented by jailbreak attacks. In this paper we leverage sparse
autoencoders (SAEs) to identify toxicity-related directions in the residual
stream of models and perform targeted activation steering using the
corresponding decoder vectors. We introduce three tiers of steering
aggressiveness and evaluate them on GPT-2 Small and Gemma-2-2B, revealing
trade-offs between toxicity reduction and language fluency. At stronger
steering strengths, these causal interventions surpass competitive baselines in
reducing toxicity by up to 20%, though fluency can degrade noticeably on GPT-2
Small depending on the aggressiveness. Crucially, standard NLP benchmark scores
upon steering remain stable, indicating that the model's knowledge and general
abilities are preserved. We further show that feature-splitting in wider SAEs
hampers safety interventions, underscoring the importance of disentangled
feature learning. Our findings highlight both the promise and the current
limitations of SAE-based causal interventions for LLM detoxification, further
suggesting practical guidelines for safer language-model deployment.

</details>


### [119] [KORGym: A Dynamic Game Platform for LLM Reasoning Evaluation](https://arxiv.org/abs/2505.14552)
*Jiajun Shi,Jian Yang,Jiaheng Liu,Xingyuan Bu,Jiangjie Chen,Junting Zhou,Kaijing Ma,Zhoufutu Wen,Bingli Wang,Yancheng He,Liang Song,Hualei Zhu,Shilong Li,Xingjian Wang,Wei Zhang,Ruibin Yuan,Yifan Yao,Wenjun Yang,Yunli Wang,Siyuan Fang,Siyu Yuan,Qianyu He,Xiangru Tang,Yingshui Tan,Wangchunshu Zhou,Zhaoxiang Zhang,Zhoujun Li,Wenhao Huang,Ge Zhang*

Main category: cs.CL

TL;DR: 论文提出了KORGym平台，用于全面评估大语言模型的推理能力，通过多模态交互测试揭示了闭源模型的优越性。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型评估方法多为领域特定，无法全面捕捉模型的通用推理潜力，因此需要更全面的评估平台。

Method: 引入KORGym动态评估平台，包含50多种文本或视觉游戏，支持交互式多轮评估和强化学习场景。

Result: 对19个LLM和8个VLM的实验显示，闭源模型表现更优，并分析了模态、推理策略等因素对性能的影响。

Conclusion: KORGym有望成为推动LLM推理研究和复杂交互环境评估方法发展的重要资源。

Abstract: Recent advancements in large language models (LLMs) underscore the need for
more comprehensive evaluation methods to accurately assess their reasoning
capabilities. Existing benchmarks are often domain-specific and thus cannot
fully capture an LLM's general reasoning potential. To address this limitation,
we introduce the Knowledge Orthogonal Reasoning Gymnasium (KORGym), a dynamic
evaluation platform inspired by KOR-Bench and Gymnasium. KORGym offers over
fifty games in either textual or visual formats and supports interactive,
multi-turn assessments with reinforcement learning scenarios. Using KORGym, we
conduct extensive experiments on 19 LLMs and 8 VLMs, revealing consistent
reasoning patterns within model families and demonstrating the superior
performance of closed-source models. Further analysis examines the effects of
modality, reasoning strategies, reinforcement learning techniques, and response
length on model performance. We expect KORGym to become a valuable resource for
advancing LLM reasoning research and developing evaluation methodologies suited
to complex, interactive environments.

</details>


### [120] [Pivot Language for Low-Resource Machine Translation](https://arxiv.org/abs/2505.14553)
*Abhimanyu Talwar,Julien Laasri*

Main category: cs.CL

TL;DR: 使用印地语作为中介语言，通过转移方法和回译方法将尼泊尔语翻译成英语，提升了基线性能。


<details>
  <summary>Details</summary>
Motivation: 某些语言对缺乏大规模、多领域的平行语料库，通过中介语言可以解决这一问题。

Method: 采用印地语作为中介语言，使用转移方法（全监督）和回译方法（半监督）进行翻译。

Result: 转移方法在开发测试集上SacreBLEU得分为14.2，比基线提升了6.6分，但略低于半监督基线15.1。

Conclusion: 讨论了性能不足的原因，并提出了未来工作的方向。

Abstract: Certain pairs of languages suffer from lack of a parallel corpus which is
large in size and diverse in domain. One of the ways this is overcome is via
use of a pivot language. In this paper we use Hindi as a pivot language to
translate Nepali into English. We describe what makes Hindi a good candidate
for the pivot. We discuss ways in which a pivot language can be used, and use
two such approaches - the Transfer Method (fully supervised) and
Backtranslation (semi-supervised) - to translate Nepali into English. Using the
former, we are able to achieve a devtest Set SacreBLEU score of 14.2, which
improves the baseline fully supervised score reported by (Guzman et al., 2019)
by 6.6 points. While we are slightly below the semi-supervised baseline score
of 15.1, we discuss what may have caused this under-performance, and suggest
scope for future work.

</details>


### [121] [TRATES: Trait-Specific Rubric-Assisted Cross-Prompt Essay Scoring](https://arxiv.org/abs/2505.14577)
*Sohaila Eltanbouly,Salam Albatarni,Tamer Elsayed*

Main category: cs.CL

TL;DR: TRATES提出了一种基于大语言模型的特质评分框架，通过结合特质评分标准、通用写作质量和提示特定特征，实现了跨提示的自动化作文评分新突破。


<details>
  <summary>Details</summary>
Motivation: 现有自动化作文评分（AES）研究多关注整体评分，而忽视了对作文个体特质的评估。本文旨在填补这一空白，提出一种特质特定的评分框架。

Method: TRATES框架利用大语言模型（LLM）根据特质评分标准生成特质特定特征（表现为评估问题），并结合通用写作质量和提示特定特征，训练一个简单的经典回归模型来预测未见提示的作文特质分数。

Result: 实验表明，TRATES在广泛使用的数据集上对所有特质均实现了最先进的性能，其中基于LLM生成的特征贡献最为显著。

Conclusion: TRATES框架通过特质特定的特征生成和结合通用特征，显著提升了跨提示自动化作文评分的性能，为特质评分设立了新的标杆。

Abstract: Research on holistic Automated Essay Scoring (AES) is long-dated; yet, there
is a notable lack of attention for assessing essays according to individual
traits. In this work, we propose TRATES, a novel trait-specific and
rubric-based cross-prompt AES framework that is generic yet specific to the
underlying trait. The framework leverages a Large Language Model (LLM) that
utilizes the trait grading rubrics to generate trait-specific features
(represented by assessment questions), then assesses those features given an
essay. The trait-specific features are eventually combined with generic
writing-quality and prompt-specific features to train a simple classical
regression model that predicts trait scores of essays from an unseen prompt.
Experiments show that TRATES achieves a new state-of-the-art performance across
all traits on a widely-used dataset, with the generated LLM-based features
being the most significant.

</details>


### [122] [Can Pruning Improve Reasoning? Revisiting Long-CoT Compression with Capability in Mind for Better Reasoning](https://arxiv.org/abs/2505.14582)
*Shangziqi Zhao,Jiahao Yuan,Guisong Yang,Usman Naseem*

Main category: cs.CL

TL;DR: 论文提出Prune-on-Logic框架，通过剪枝优化长链推理（Long-CoT）以适应小语言模型（SLMs），发现剪枝验证步骤能提升准确性并降低推理成本。


<details>
  <summary>Details</summary>
Motivation: 长链推理（Long-CoT）虽能提升大语言模型（LLMs）的准确性，但其冗长、自反的风格难以有效压缩至小语言模型（SLMs）。论文旨在通过能力对齐视角重新审视Long-CoT压缩，探索剪枝是否能改进推理。

Method: 提出Prune-on-Logic框架，将Long-CoT转化为逻辑图，并在自验证约束下选择性剪枝低效用推理步骤。通过三种剪枝策略（针对整个链、核心推理和验证）进行系统分析。

Result: 剪枝验证步骤能持续提升准确性并降低推理成本，优于词级基线和未压缩微调；而剪枝推理或全链步骤会降低性能，表明小模型受益于语义更精简的CoT而非更短的CoT。

Conclusion: 剪枝是一种结构优化策略，可对齐CoT推理与小模型能力，语义精简的CoT对小模型更有效。

Abstract: Long chain-of-thought (Long-CoT) reasoning improves accuracy in LLMs, yet its
verbose, self-reflective style often hinders effective distillation into small
language models (SLMs). We revisit Long-CoT compression through the lens of
capability alignment and ask: Can pruning improve reasoning? We propose
Prune-on-Logic, a structure-aware framework that transforms Long-CoT into logic
graphs and selectively prunes low-utility reasoning steps under
self-verification constraints. Through systematic analysis across three pruning
strategies -- targeting entire chains, core reasoning, and verification -- we
find that pruning verification steps yields consistent accuracy gains while
reducing inference cost, outperforming token-level baselines and uncompressed
fine-tuning. In contrast, pruning reasoning or all-chain steps degrades
performance, revealing that small models benefit not from shorter CoTs, but
from semantically leaner ones. Our findings highlight pruning as a structural
optimization strategy for aligning CoT reasoning with SLM capacity.

</details>


### [123] [Context Reasoner: Incentivizing Reasoning Capability for Contextualized Privacy and Safety Compliance via Reinforcement Learning](https://arxiv.org/abs/2505.14585)
*Wenbin Hu,Haoran Li,Huihao Jing,Qi Hu,Ziqian Zeng,Sirui Han,Heli Xu,Tianshu Chu,Peizhao Hu,Yangqiu Song*

Main category: cs.CL

TL;DR: 该论文提出了一种基于上下文完整性理论的方法，通过强化学习提升大语言模型的安全隐私合规性，同时增强其推理能力。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型存在安全隐私风险，现有缓解策略依赖敏感模式匹配且忽视法规标准，导致系统性合规风险。

Method: 采用上下文完整性框架，结合GDPR/EU AI Act/HIPAA标准，使用基于规则的强化学习奖励机制提升模型合规性与推理能力。

Result: 方法使安全隐私合规性提升17.64%，OpenThinker-7B模型的MMLU/LegalBench基准准确率分别提升2.05%和8.98%。

Conclusion: 该框架有效平衡合规要求与模型性能，为AI系统安全部署提供新思路。

Abstract: While Large Language Models (LLMs) exhibit remarkable capabilities, they also
introduce significant safety and privacy risks. Current mitigation strategies
often fail to preserve contextual reasoning capabilities in risky scenarios.
Instead, they rely heavily on sensitive pattern matching to protect LLMs, which
limits the scope. Furthermore, they overlook established safety and privacy
standards, leading to systemic risks for legal compliance. To address these
gaps, we formulate safety and privacy issues into contextualized compliance
problems following the Contextual Integrity (CI) theory. Under the CI
framework, we align our model with three critical regulatory standards: GDPR,
EU AI Act, and HIPAA. Specifically, we employ reinforcement learning (RL) with
a rule-based reward to incentivize contextual reasoning capabilities while
enhancing compliance with safety and privacy norms. Through extensive
experiments, we demonstrate that our method not only significantly enhances
legal compliance (achieving a +17.64% accuracy improvement in safety/privacy
benchmarks) but also further improves general reasoning capability. For
OpenThinker-7B, a strong reasoning model that significantly outperforms its
base model Qwen2.5-7B-Instruct across diverse subjects, our method enhances its
general reasoning capabilities, with +2.05% and +8.98% accuracy improvement on
the MMLU and LegalBench benchmark, respectively.

</details>


### [124] [MCIP: Protecting MCP Safety via Model Contextual Integrity Protocol](https://arxiv.org/abs/2505.14590)
*Huihao Jing,Haoran Li,Wenbin Hu,Qi Hu,Heli Xu,Tianshu Chu,Peizhao Hu,Yangqiu Song*

Main category: cs.CL

TL;DR: 论文提出MCIP协议增强MCP安全性，构建细粒度分类法和基准数据，实验证明能显著提升LLMs在MCP交互中的安全性。


<details>
  <summary>Details</summary>
Motivation: MCP的分散式架构虽便于使用，但存在未充分探索的安全风险，缺乏系统性安全分析。

Method: 基于MAESTRO框架分析MCP安全缺陷，提出MCIP协议；构建不安全行为分类法，开发基准与训练数据。

Result: 实验显示LLMs在MCP交互中存在漏洞，所提方法大幅提升其安全性能。

Conclusion: MCIP协议及配套分类法、基准数据能有效解决MCP安全问题，提升LLMs交互安全性。

Abstract: As Model Context Protocol (MCP) introduces an easy-to-use ecosystem for users
and developers, it also brings underexplored safety risks. Its decentralized
architecture, which separates clients and servers, poses unique challenges for
systematic safety analysis. This paper proposes a novel framework to enhance
MCP safety. Guided by the MAESTRO framework, we first analyze the missing
safety mechanisms in MCP, and based on this analysis, we propose the Model
Contextual Integrity Protocol (MCIP), a refined version of MCP that addresses
these gaps.Next, we develop a fine-grained taxonomy that captures a diverse
range of unsafe behaviors observed in MCP scenarios. Building on this taxonomy,
we develop benchmark and training data that support the evaluation and
improvement of LLMs' capabilities in identifying safety risks within MCP
interactions. Leveraging the proposed benchmark and training data, we conduct
extensive experiments on state-of-the-art LLMs. The results highlight LLMs'
vulnerabilities in MCP interactions and demonstrate that our approach
substantially improves their safety performance.

</details>


### [125] [Success is in the Details: Evaluate and Enhance Details Sensitivity of Code LLMs through Counterfactuals](https://arxiv.org/abs/2505.14597)
*Xianzhen Luo,Qingfu Zhu,Zhiming Zhang,Mingzheng Xu,Tianhao Cheng,Yixuan Wang,Zheng Chu,Shijie Xuyang,Zhiyuan Ma,YuanTao Fan,Wanxiang Che*

Main category: cs.CL

TL;DR: 论文提出代码敏感性概念，并构建CTF-Code基准和CTF-Instruct微调框架，显著提升大语言模型在代码任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 当前代码基准和指令数据主要关注难度和多样性，而忽视了代码敏感性（即模型对问题描述细节变化的识别和响应能力）。

Method: 通过反事实扰动构建CTF-Code基准，最小化输入变化但最大化输出差异；提出CTF-Instruct增量指令微调框架，结合选择机制平衡难度、多样性和敏感性。

Result: 实验显示，现有LLMs在CTF-Code上性能下降超10%；经CTF-Instruct微调的模型在CTF-Code上提升超2%，在LiveCodeBench上提升超10%。

Conclusion: 增强代码敏感性可有效提升LLMs性能，CTF框架为模型训练提供了新的优化维度。

Abstract: Code Sensitivity refers to the ability of Code LLMs to recognize and respond
to details changes in problem descriptions. While current code benchmarks and
instruction data focus on difficulty and diversity, sensitivity is overlooked.
We first introduce the CTF-Code benchmark, constructed using counterfactual
perturbations, minimizing input changes while maximizing output changes. The
evaluation shows that many LLMs have a more than 10\% performance drop compared
to the original problems. To fully utilize sensitivity, CTF-Instruct, an
incremental instruction fine-tuning framework, extends on existing data and
uses a selection mechanism to meet the three dimensions of difficulty,
diversity, and sensitivity. Experiments show that LLMs fine-tuned with
CTF-Instruct data achieve over a 2\% improvement on CTF-Code, and more than a
10\% performance boost on LiveCodeBench, validating the feasibility of
enhancing LLMs' sensitivity to improve performance.

</details>


### [126] [Toward Reliable Biomedical Hypothesis Generation: Evaluating Truthfulness and Hallucination in Large Language Models](https://arxiv.org/abs/2505.14599)
*Guangzhi Xiong,Eric Xie,Corey Williams,Myles Kim,Amir Hassan Shariatmadari,Sikun Guo,Stefan Bekiranov,Aidong Zhang*

Main category: cs.CL

TL;DR: 该论文介绍了TruthHypo基准和KnowHD检测器，用于评估大语言模型在生成真实生物医学假设方面的能力，并检测幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在生物医学等科学领域具有生成假设的潜力，但其真实性和幻觉问题需要系统评估，以提升可靠性。

Method: 引入TruthHypo基准和KnowHD知识基础的幻觉检测器，分析模型生成假设的真实性。

Result: 大语言模型在生成真实假设方面表现不佳，KnowHD的评分能有效筛选真实假设，人类评估验证了其效用。

Conclusion: KnowHD能有效识别真实假设，加速科学发现，相关数据和代码已开源。

Abstract: Large language models (LLMs) have shown significant potential in scientific
disciplines such as biomedicine, particularly in hypothesis generation, where
they can analyze vast literature, identify patterns, and suggest research
directions. However, a key challenge lies in evaluating the truthfulness of
generated hypotheses, as verifying their accuracy often requires substantial
time and resources. Additionally, the hallucination problem in LLMs can lead to
the generation of hypotheses that appear plausible but are ultimately
incorrect, undermining their reliability. To facilitate the systematic study of
these challenges, we introduce TruthHypo, a benchmark for assessing the
capabilities of LLMs in generating truthful biomedical hypotheses, and KnowHD,
a knowledge-based hallucination detector to evaluate how well hypotheses are
grounded in existing knowledge. Our results show that LLMs struggle to generate
truthful hypotheses. By analyzing hallucinations in reasoning steps, we
demonstrate that the groundedness scores provided by KnowHD serve as an
effective metric for filtering truthful hypotheses from the diverse outputs of
LLMs. Human evaluations further validate the utility of KnowHD in identifying
truthful hypotheses and accelerating scientific discovery. Our data and source
code are available at https://github.com/Teddy-XiongGZ/TruthHypo.

</details>


### [127] [sudoLLM : On Multi-role Alignment of Language Models](https://arxiv.org/abs/2505.14607)
*Soumadeep Saha,Akshay Chaturvedi,Joy Mahapatra,Utpal Garain*

Main category: cs.CL

TL;DR: 本文提出sudoLLM框架，通过用户授权机制增强大语言模型的安全性和对齐性。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型缺乏用户授权机制，存在安全风险，需要引入类似安全关键系统中的访问控制。

Method: sudoLLM通过在查询中注入用户偏置信号，并训练模型根据用户权限生成敏感信息。

Result: 实验表明该方法显著提升了模型的对齐性、泛化能力和抗提示攻击能力。

Conclusion: sudoLLM作为额外安全层，可与现有防护机制互补，增强大语言模型的端到端安全性。

Abstract: User authorization-based access privileges are a key feature in many
safety-critical systems, but have thus far been absent from the large language
model (LLM) realm. In this work, drawing inspiration from such access control
systems, we introduce sudoLLM, a novel framework that results in multi-role
aligned LLMs, i.e., LLMs that account for, and behave in accordance with, user
access rights. sudoLLM injects subtle user-based biases into queries and trains
an LLM to utilize this bias signal in order to produce sensitive information if
and only if the user is authorized. We present empirical results demonstrating
that this approach shows substantially improved alignment, generalization, and
resistance to prompt-based jailbreaking attacks. The persistent tension between
the language modeling objective and safety alignment, which is often exploited
to jailbreak LLMs, is somewhat resolved with the aid of the injected bias
signal. Our framework is meant as an additional security layer, and complements
existing guardrail mechanisms for enhanced end-to-end safety with LLMs.

</details>


### [128] [Language Models Optimized to Fool Detectors Still Have a Distinct Style (And How to Change It)](https://arxiv.org/abs/2505.14608)
*Rafael Rivera Soto,Barry Chen,Nicholas Andrews*

Main category: cs.CL

TL;DR: 论文探讨了机器生成文本检测的固有困难，提出了一种基于风格特征的鲁棒检测方法，并引入AURA指标评估人机文本分布重叠。


<details>
  <summary>Details</summary>
Motivation: 针对现有研究认为机器生成文本检测本质上不可靠的观点，作者旨在验证这一论断，并探索更鲁棒的检测方法。

Method: 通过分析风格特征空间的鲁棒性，开发对抗优化的检测方法；提出新型改述攻击测试检测器；引入AURA指标量化分布重叠。

Result: 风格特征检测器对针对性优化具有显著鲁棒性；单样本时所有检测器均失效，但多样本下人机分布可分；AURA有效反映检测潜力。

Conclusion: 研究支持避免依赖机器文本检测的建议，同时表明风格特征和多样本分析可提升检测鲁棒性。

Abstract: Despite considerable progress in the development of machine-text detectors,
it has been suggested that the problem is inherently hard, and therefore, that
stakeholders should proceed under the assumption that machine-generated text
cannot be reliably detected as such. We examine a recent such claim by Nicks et
al. (2024) regarding the ease with which language models can be optimized to
degrade the performance of machine-text detectors, including detectors not
specifically optimized against. We identify a feature space$\unicode{x2013}$the
stylistic feature space$\unicode{x2013}$that is robust to such optimization,
and show that it may be used to reliably detect samples from language models
optimized to prevent detection. Furthermore, we show that even when models are
explicitly optimized against stylistic detectors, detection performance remains
surprisingly unaffected. We then seek to understand if stylistic detectors are
inherently more robust. To study this question, we explore a new paraphrasing
approach that simultaneously aims to close the gap between human writing and
machine writing in stylistic feature space while avoiding detection using
traditional features. We show that when only a single sample is available for
detection, this attack is universally effective across all detectors
considered, including those that use writing style. However, as the number of
samples available for detection grows, the human and machine distributions
become distinguishable. This observation encourages us to introduce AURA, a
metric that estimates the overlap between human and machine-generated
distributions by analyzing how detector performance improves as more samples
become available. Overall, our findings underscore previous recommendations to
avoid reliance on machine-text detection.

</details>


### [129] [Linear Control of Test Awareness Reveals Differential Compliance in Reasoning Models](https://arxiv.org/abs/2505.14617)
*Sahar Abdelnabi,Ahmed Salem*

Main category: cs.CL

TL;DR: 研究发现大型语言模型在测试时会改变行为，类似霍桑效应，影响安全对齐。论文提出白盒探测框架量化这一影响，并展示不同模型受影响程度不同。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在测试时可能因察觉被评估而改变行为（类似霍桑效应），这会扭曲安全评估结果。论文旨在量化这种'测试意识'对模型安全对齐的具体影响。

Method: 提出白盒探测框架：(1) 线性识别与测试意识相关的激活值；(2) 通过操控这些激活值来增强或抑制测试意识，同时监测下游任务表现。方法应用于多个开源推理型LLM。

Result: 测试意识显著影响模型的安全对齐表现，且不同模型受影响程度存在差异。通过框架可实现对这一潜在效应的细粒度控制。

Conclusion: 该研究为理解测试意识对LLM行为的影响提供了量化工具，通过主动控制这一效应，可提升安全评估的可信度。

Abstract: Reasoning-focused large language models (LLMs) sometimes alter their behavior
when they detect that they are being evaluated, an effect analogous to the
Hawthorne phenomenon, which can lead them to optimize for test-passing
performance or to comply more readily with harmful prompts if real-world
consequences appear absent. We present the first quantitative study of how such
"test awareness" impacts model behavior, particularly its safety alignment. We
introduce a white-box probing framework that (i) linearly identifies
awareness-related activations and (ii) steers models toward or away from test
awareness while monitoring downstream performance. We apply our method to
different state-of-the-art open-source reasoning LLMs across both realistic and
hypothetical tasks. Our results demonstrate that test awareness significantly
impact safety alignment, and is different for different models. By providing
fine-grained control over this latent effect, our work aims to increase trust
in how we perform safety evaluation.

</details>


### [130] [Think Only When You Need with Large Hybrid-Reasoning Models](https://arxiv.org/abs/2505.14631)
*Lingjie Jiang,Xun Wu,Shaohan Huang,Qingxiu Dong,Zewen Chi,Li Dong,Xingxing Zhang,Tengchao Lv,Lei Cui,Furu Wei*

Main category: cs.CL

TL;DR: 论文提出了一种新型混合推理模型LHRM，能根据查询复杂度自适应选择是否进行深度思考，显著提升效率与性能。


<details>
  <summary>Details</summary>
Motivation: 现有大型推理模型(LRM)对所有查询均采用冗长思考过程，导致简单查询产生不必要的计算开销和延迟。

Method: 采用两阶段训练：混合微调(HFT)冷启动+在线强化学习(HGPO)，并设计混合准确率指标评估模型能力。

Result: LHRM在各类查询上自适应表现优异，推理能力和通用性超越现有模型，同时显著提升效率。

Conclusion: 研究倡导重新审视深度思考过程的使用场景，为构建混合思维系统提供了坚实基础。

Abstract: Recent Large Reasoning Models (LRMs) have shown substantially improved
reasoning capabilities over traditional Large Language Models (LLMs) by
incorporating extended thinking processes prior to producing final responses.
However, excessively lengthy thinking introduces substantial overhead in terms
of token consumption and latency, which is particularly unnecessary for simple
queries. In this work, we introduce Large Hybrid-Reasoning Models (LHRMs), the
first kind of model capable of adaptively determining whether to perform
thinking based on the contextual information of user queries. To achieve this,
we propose a two-stage training pipeline comprising Hybrid Fine-Tuning (HFT) as
a cold start, followed by online reinforcement learning with the proposed
Hybrid Group Policy Optimization (HGPO) to implicitly learn to select the
appropriate thinking mode. Furthermore, we introduce a metric called Hybrid
Accuracy to quantitatively assess the model's capability for hybrid thinking.
Extensive experimental results show that LHRMs can adaptively perform hybrid
thinking on queries of varying difficulty and type. It outperforms existing
LRMs and LLMs in reasoning and general capabilities while significantly
improving efficiency. Together, our work advocates for a reconsideration of the
appropriate use of extended thinking processes and provides a solid starting
point for building hybrid thinking systems.

</details>


### [131] [Will AI Tell Lies to Save Sick Children? Litmus-Testing AI Values Prioritization with AIRiskDilemmas](https://arxiv.org/abs/2505.14633)
*Yu Ying Chiu,Zhilin Wang,Sharan Maiya,Yejin Choi,Kyle Fish,Sydney Levine,Evan Hubinger*

Main category: cs.CL

TL;DR: 该论文提出通过识别AI模型的核心价值观来预测其潜在风险行为，开发了LitmusValues评估框架和AIRiskDilemmas数据集，证明价值观优先级能有效预测已知和未知风险。


<details>
  <summary>Details</summary>
Motivation: 随着AI模型能力增强，传统风险检测方法（如对齐验证）可能被新型规避手段（如Alignment Faking）绕过。受人类危险行为常受价值观驱动的启发，研究者认为识别AI模型的价值观可作为风险预警系统。

Method: 1) 创建LitmusValues评估流程，量化AI模型对不同价值类别的优先级；2) 构建AIRiskDilemmas数据集，包含涉及AI安全风险（如权力追求）的价值观冲突场景；3) 通过模型在困境中的聚合选择推导其价值观优先级。

Result: 实验表明，LitmusValues揭示的价值观（包括看似无害的'关怀'）能预测：1) AIRiskDilemmas中已知风险行为；2) HarmBench中未参与训练的风险行为，验证了价值观作为风险指标的有效性。

Conclusion: AI模型的价值观优先级可成为新型风险检测维度，LitmusValues框架为AI安全研究提供了可扩展的价值观分析工具，未来或能通过早期价值观识别防范潜在危害。

Abstract: Detecting AI risks becomes more challenging as stronger models emerge and
find novel methods such as Alignment Faking to circumvent these detection
attempts. Inspired by how risky behaviors in humans (i.e., illegal activities
that may hurt others) are sometimes guided by strongly-held values, we believe
that identifying values within AI models can be an early warning system for
AI's risky behaviors. We create LitmusValues, an evaluation pipeline to reveal
AI models' priorities on a range of AI value classes. Then, we collect
AIRiskDilemmas, a diverse collection of dilemmas that pit values against one
another in scenarios relevant to AI safety risks such as Power Seeking. By
measuring an AI model's value prioritization using its aggregate choices, we
obtain a self-consistent set of predicted value priorities that uncover
potential risks. We show that values in LitmusValues (including seemingly
innocuous ones like Care) can predict for both seen risky behaviors in
AIRiskDilemmas and unseen risky behaviors in HarmBench.

</details>


### [132] [General-Reasoner: Advancing LLM Reasoning Across All Domains](https://arxiv.org/abs/2505.14652)
*Xueguang Ma,Qian Liu,Dongfu Jiang,Ge Zhang,Zejun Ma,Wenhu Chen*

Main category: cs.CL

TL;DR: 论文提出General-Reasoner训练范式，通过构建多领域数据集和生成式答案验证器，增强大语言模型的跨领域推理能力。


<details>
  <summary>Details</summary>
Motivation: 当前强化学习主要提升大语言模型在数学和编程领域的推理能力，但其他领域因数据稀缺和答案多样性受限。论文旨在解决这一局限性。

Method: 1) 构建多领域高质量可验证答案数据集；2) 开发基于生成模型的答案验证器，替代传统规则验证。

Result: 在12个跨领域基准测试（如MMLU-Pro、GPQA等）中，General-Reasoner超越基线方法，保持数学推理优势的同时实现泛化。

Conclusion: 该范式显著提升大语言模型的跨领域推理性能，为通用推理能力的发展提供了新方向。

Abstract: Reinforcement learning (RL) has recently demonstrated strong potential in
enhancing the reasoning capabilities of large language models (LLMs).
Particularly, the "Zero" reinforcement learning introduced by Deepseek-R1-Zero,
enables direct RL training of base LLMs without relying on an intermediate
supervised fine-tuning stage. Despite these advancements, current works for LLM
reasoning mainly focus on mathematical and coding domains, largely due to data
abundance and the ease of answer verification. This limits the applicability
and generalization of such models to broader domains, where questions often
have diverse answer representations, and data is more scarce. In this paper, we
propose General-Reasoner, a novel training paradigm designed to enhance LLM
reasoning capabilities across diverse domains. Our key contributions include:
(1) constructing a large-scale, high-quality dataset of questions with
verifiable answers curated by web crawling, covering a wide range of
disciplines; and (2) developing a generative model-based answer verifier, which
replaces traditional rule-based verification with the capability of
chain-of-thought and context-awareness. We train a series of models and
evaluate them on a wide range of datasets covering wide domains like physics,
chemistry, finance, electronics etc. Our comprehensive evaluation across these
12 benchmarks (e.g. MMLU-Pro, GPQA, SuperGPQA, TheoremQA, BBEH and MATH AMC)
demonstrates that General-Reasoner outperforms existing baseline methods,
achieving robust and generalizable reasoning performance while maintaining
superior effectiveness in mathematical reasoning tasks.

</details>


### [133] [EmoGist: Efficient In-Context Learning for Visual Emotion Understanding](https://arxiv.org/abs/2505.14660)
*Ronald Seoh,Dan Goldwasser*

Main category: cs.CL

TL;DR: EmoGist是一种无需训练的上下文学习方法，通过预生成情感标签解释提升视觉情感分类准确率。


<details>
  <summary>Details</summary>
Motivation: 情感在图像中的表现高度依赖上下文，传统方法难以捕捉这种细微差异，因此需要一种更灵活的分类方法。

Method: 通过聚类分析生成多版本情感标签解释，测试时根据嵌入相似度检索最匹配的解释，并输入快速视觉语言模型进行分类。

Result: 在Memotion多标签数据集上F1分数提升13点，在FI多类数据集上提升8点。

Conclusion: EmoGist通过上下文相关的标签定义显著提高了视觉情感分类的准确性。

Abstract: In this paper, we introduce EmoGist, a training-free, in-context learning
method for performing visual emotion classification with LVLMs. The key
intuition of our approach is that context-dependent definition of emotion
labels could allow more accurate predictions of emotions, as the ways in which
emotions manifest within images are highly context dependent and nuanced.
EmoGist pre-generates multiple explanations of emotion labels, by analyzing the
clusters of example images belonging to each category. At test time, we
retrieve a version of explanation based on embedding similarity, and feed it to
a fast VLM for classification. Through our experiments, we show that EmoGist
allows up to 13 points improvement in micro F1 scores with the multi-label
Memotion dataset, and up to 8 points in macro F1 in the multi-class FI dataset.

</details>


### [134] [Reward Reasoning Model](https://arxiv.org/abs/2505.14674)
*Jiaxin Guo,Zewen Chi,Li Dong,Qingxiu Dong,Xun Wu,Shaohan Huang,Furu Wei*

Main category: cs.CL

TL;DR: 本文提出奖励推理模型（RRMs），通过链式推理优化奖励模型性能，无需显式训练数据即可自适应利用测试计算资源提升准确性。


<details>
  <summary>Details</summary>
Motivation: 现有奖励模型在复杂查询时难以即时生成合适奖励，且测试阶段计算资源利用不足。

Method: 采用强化学习框架开发RRMs，通过链式推理（chain-of-thought）实现无显式推理轨迹的自进化奖励推理能力。

Result: RRMs在多领域奖励建模基准中表现优异，并能自适应利用测试计算资源进一步提高奖励准确性。

Conclusion: RRMs为奖励模型提供可解释的推理路径，显著提升复杂场景下的性能，模型已开源。

Abstract: Reward models play a critical role in guiding large language models toward
outputs that align with human expectations. However, an open challenge remains
in effectively utilizing test-time compute to enhance reward model performance.
In this work, we introduce Reward Reasoning Models (RRMs), which are
specifically designed to execute a deliberate reasoning process before
generating final rewards. Through chain-of-thought reasoning, RRMs leverage
additional test-time compute for complex queries where appropriate rewards are
not immediately apparent. To develop RRMs, we implement a reinforcement
learning framework that fosters self-evolved reward reasoning capabilities
without requiring explicit reasoning traces as training data. Experimental
results demonstrate that RRMs achieve superior performance on reward modeling
benchmarks across diverse domains. Notably, we show that RRMs can adaptively
exploit test-time compute to further improve reward accuracy. The pretrained
reward reasoning models are available at
https://huggingface.co/Reward-Reasoning.

</details>


### [135] [UltraEdit: Training-, Subject-, and Memory-Free Lifelong Editing in Large Language Models](https://arxiv.org/abs/2505.14679)
*Xiaojie Gu,Guangxu Chen,Jungang Li,Jia-Chen Gu,Xuming Hu,Kai Zhang*

Main category: cs.CL

TL;DR: ULTRAEDIT提出了一种无需训练、无主题限制且无需额外内存的大语言模型终身学习方法，通过轻量级线性代数运算实现高效知识更新，速度提升7倍以上，并支持百万级编辑。


<details>
  <summary>Details</summary>
Motivation: 现有终身学习方法难以满足大规模实际应用的需求，特别是在高效更新知识、保持现有能力及可靠部署方面存在挑战。

Method: ULTRAEDIT采用自包含的编辑过程，仅依赖轻量级线性代数运算计算参数偏移，并结合终身归一化策略适应分布变化。

Result: ULTRAEDIT在编辑速度上比现有最快方法快7倍以上，VRAM消耗减少三分之二，支持在24GB消费级GPU上编辑7B参数模型，并在构建的最大数据集上验证了百万级编辑的可行性。

Conclusion: ULTRAEDIT为大规模终身学习提供了一种高效、可扩展的解决方案，在多样化的模型编辑场景中均表现出优越性能。

Abstract: Lifelong learning enables large language models (LLMs) to adapt to evolving
information by continually updating their internal knowledge. An ideal system
should support efficient, wide-ranging updates while preserving existing
capabilities and ensuring reliable deployment. Model editing stands out as a
promising solution for this goal, offering a focused and efficient way to
revise a model's internal knowledge. Although recent paradigms have made
notable progress, they often struggle to meet the demands of practical lifelong
adaptation at scale. To bridge this gap, we propose ULTRAEDIT-a fundamentally
new editing solution that is training-, subject- and memory-free, making it
particularly well-suited for ultra-scalable, real-world lifelong model editing.
ULTRAEDIT performs editing through a self-contained process that relies solely
on lightweight linear algebra operations to compute parameter shifts, enabling
fast and consistent parameter modifications with minimal overhead. To improve
scalability in lifelong settings, ULTRAEDIT employs a lifelong normalization
strategy that continuously updates feature statistics across turns, allowing it
to adapt to distributional shifts and maintain consistency over time. ULTRAEDIT
achieves editing speeds over 7x faster than the previous state-of-the-art
method-which was also the fastest known approach-while consuming less than 1/3
the VRAM, making it the only method currently capable of editing a 7B LLM on a
24GB consumer-grade GPU. Furthermore, we construct ULTRAEDITBENCH-the largest
dataset in the field to date, with over 2M editing pairs-and demonstrate that
our method supports up to 1M edits while maintaining high accuracy.
Comprehensive experiments on four datasets and six models show that ULTRAEDIT
consistently achieves superior performance across diverse model editing
scenarios. Our code is available at: https://github.com/XiaojieGu/UltraEdit.

</details>


### [136] [Mind the Gap: Bridging Thought Leap for Improved Chain-of-Thought Tuning](https://arxiv.org/abs/2505.14684)
*Haolei Xu,Yuchen Yan,Yongliang Shen,Wenqi Zhang,Guiyang Hou,Shengpei Jiang,Kaitao Song,Weiming Lu,Jun Xiao,Yueting Zhuang*

Main category: cs.CL

TL;DR: 该论文提出了一种自动检测思维跳跃并生成缺失推理步骤的方法CoT-Bridge，通过构建ScaleQM+数据集提升数学推理任务的性能。


<details>
  <summary>Details</summary>
Motivation: 现有数学思维链数据集因专家省略中间步骤导致思维跳跃，影响模型学习和泛化能力。

Method: 提出CoT Thought Leap Bridge任务，构建ScaleQM+数据集，训练CoT-Bridge模型填补缺失推理步骤。

Result: 在NuminaMath等基准测试中，使用桥接数据微调的模型性能提升最高达5.87%，并增强泛化能力。

Conclusion: CoT-Bridge通过提升推理完整性，显著改进模型性能，并可作为即插即用模块兼容现有优化技术。

Abstract: Large language models (LLMs) have achieved remarkable progress on
mathemati-cal tasks through Chain-of-Thought (CoT) reasoning. However, existing
mathematical CoT datasets often suffer from Thought Leaps due to experts
omitting intermediate steps, which negatively impacts model learning and
generalization. We propose the CoT Thought Leap Bridge Task, which aims to
automatically detect leaps and generate missing intermediate reasoning steps to
restore the completeness and coherence of CoT. To facilitate this, we
constructed a specialized training dataset called ScaleQM+, based on the
structured ScaleQuestMath dataset, and trained CoT-Bridge to bridge thought
leaps. Through comprehensive experiments on mathematical reasoning benchmarks,
we demonstrate that models fine-tuned on bridged datasets consistently
outperform those trained on original datasets, with improvements of up to
+5.87% on NuminaMath. Our approach effectively enhances distilled data (+3.02%)
and provides better starting points for reinforcement learning (+3.1%),
functioning as a plug-and-play module compatible with existing optimization
techniques. Furthermore, CoT-Bridge demonstrate improved generalization to
out-of-domain logical reasoning tasks, confirming that enhancing reasoning
completeness yields broadly applicable benefits.

</details>


### [137] [Language Models use Lookbacks to Track Beliefs](https://arxiv.org/abs/2505.14685)
*Nikhil Prakash,Natalie Shapira,Arnab Sen Sharma,Christoph Riedl,Yonatan Belinkov,Tamar Rott Shaham,David Bau,Atticus Geiger*

Main category: cs.CL

TL;DR: 该研究分析了Llama-3-70B-Instruct模型如何通过因果中介和抽象机制追踪角色信念，发现了一种称为'回看机制'的算法模式，用于在需要时检索关键信息。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探索语言模型如何表征角色的信念（尤其是与事实不符的信念），以理解其心智理论（ToM）能力。

Method: 方法包括构建包含简单故事的数据集，利用因果中介分析和抽象技术，研究模型如何通过'回看机制'绑定角色-对象-状态三元组。

Result: 研究发现模型通过低秩子空间中的排序ID（OIs）绑定信息，并利用可见性ID更新角色信念，揭示了其信念跟踪机制。

Conclusion: 该工作为逆向工程语言模型的心智理论推理提供了见解，推进了对模型内部机制的理解。

Abstract: How do language models (LMs) represent characters' beliefs, especially when
those beliefs may differ from reality? This question lies at the heart of
understanding the Theory of Mind (ToM) capabilities of LMs. We analyze
Llama-3-70B-Instruct's ability to reason about characters' beliefs using causal
mediation and abstraction. We construct a dataset that consists of simple
stories where two characters each separately change the state of two objects,
potentially unaware of each other's actions. Our investigation uncovered a
pervasive algorithmic pattern that we call a lookback mechanism, which enables
the LM to recall important information when it becomes necessary. The LM binds
each character-object-state triple together by co-locating reference
information about them, represented as their Ordering IDs (OIs) in low rank
subspaces of the state token's residual stream. When asked about a character's
beliefs regarding the state of an object, the binding lookback retrieves the
corresponding state OI and then an answer lookback retrieves the state token.
When we introduce text specifying that one character is (not) visible to the
other, we find that the LM first generates a visibility ID encoding the
relation between the observing and the observed character OIs. In a visibility
lookback, this ID is used to retrieve information about the observed character
and update the observing character's beliefs. Our work provides insights into
the LM's belief tracking mechanisms, taking a step toward reverse-engineering
ToM reasoning in LMs.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [138] [AgentSGEN: Multi-Agent LLM in the Loop for Semantic Collaboration and GENeration of Synthetic Data](https://arxiv.org/abs/2505.13466)
*Vu Dinh Xuan,Hao Vo,David Murphy,Hoang D. Nguyen*

Main category: cs.AI

TL;DR: 提出了一种基于多智能体协作的框架，通过LLM生成符合安全关键场景需求的合成数据，解决真实数据稀缺问题。


<details>
  <summary>Details</summary>
Motivation: 由于伦理和物流障碍，安全关键应用（如建筑安全）缺乏真实危险场景数据，限制了AI系统的训练效果。现有合成数据方法语义深度不足，亟需新方案。

Method: 采用双智能体迭代协作框架：评估智能体（LLM）确保语义一致性与安全约束，编辑智能体根据反馈生成并优化场景。

Result: 实验表明该方法能生成符合现实安全需求且视觉语义平衡的合成场景，优于现有方法。

Conclusion: 该迭代框架为多媒体安全应用的数据稀缺问题提供了可行解决方案，兼具鲁棒性与视觉合理性。

Abstract: The scarcity of data depicting dangerous situations presents a major obstacle
to training AI systems for safety-critical applications, such as construction
safety, where ethical and logistical barriers hinder real-world data
collection. This creates an urgent need for an end-to-end framework to generate
synthetic data that can bridge this gap. While existing methods can produce
synthetic scenes, they often lack the semantic depth required for scene
simulations, limiting their effectiveness. To address this, we propose a novel
multi-agent framework that employs an iterative, in-the-loop collaboration
between two agents: an Evaluator Agent, acting as an LLM-based judge to enforce
semantic consistency and safety-specific constraints, and an Editor Agent,
which generates and refines scenes based on this guidance. Powered by LLM's
capabilities to reasoning and common-sense knowledge, this collaborative design
produces synthetic images tailored to safety-critical scenarios. Our
experiments suggest this design can generate useful scenes based on realistic
specifications that address the shortcomings of prior approaches, balancing
safety requirements with visual semantics. This iterative process holds promise
for delivering robust, aesthetically sound simulations, offering a potential
solution to the data scarcity challenge in multimedia safety applications.

</details>


### [139] [Evaluating Large Language Models for Real-World Engineering Tasks](https://arxiv.org/abs/2505.13484)
*Rene Heesch,Sebastian Eilermann,Alexander Windmann,Alexander Diedrich,Philipp Rosenthal,Oliver Niggemann*

Main category: cs.AI

TL;DR: 该论文指出当前大语言模型在工程任务评估中的不足，并提出一个包含100多个真实工程问题的数据集，评估了四种先进模型在复杂工程任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 当前对大语言模型在工程任务中的评估存在两个主要问题：一是依赖简化的用例，二是使用临时场景未能充分捕捉关键工程能力。因此，需要更系统地评估模型在复杂、真实工程问题中的表现。

Method: 论文引入了一个包含100多个真实工程问题的数据集，覆盖产品设计、预测和诊断等核心能力，并评估了四种先进大语言模型的表现。

Result: 研究发现，大语言模型在基本时间和结构推理方面表现良好，但在抽象推理、形式化建模和上下文敏感的工程逻辑方面表现较差。

Conclusion: 论文填补了复杂工程问题评估的空白，揭示了当前大语言模型在工程任务中的优势和局限性。

Abstract: Large Language Models (LLMs) are transformative not only for daily activities
but also for engineering tasks. However, current evaluations of LLMs in
engineering exhibit two critical shortcomings: (i) the reliance on simplified
use cases, often adapted from examination materials where correctness is easily
verifiable, and (ii) the use of ad hoc scenarios that insufficiently capture
critical engineering competencies. Consequently, the assessment of LLMs on
complex, real-world engineering problems remains largely unexplored. This paper
addresses this gap by introducing a curated database comprising over 100
questions derived from authentic, production-oriented engineering scenarios,
systematically designed to cover core competencies such as product design,
prognosis, and diagnosis. Using this dataset, we evaluate four state-of-the-art
LLMs, including both cloud-based and locally hosted instances, to
systematically investigate their performance on complex engineering tasks. Our
results show that LLMs demonstrate strengths in basic temporal and structural
reasoning but struggle significantly with abstract reasoning, formal modeling,
and context-sensitive engineering logic.

</details>


### [140] [Contrastive Cross-Course Knowledge Tracing via Concept Graph Guided Knowledge Transfer](https://arxiv.org/abs/2505.13489)
*Wenkang Han,Wang Lin,Liya Hu,Zhenlong Dai,Yiyun Zhou,Mengze Li,Zemin Liu,Chang Yao,Jingyuan Chen*

Main category: cs.AI

TL;DR: TransKT提出了一种跨课程知识追踪方法，利用概念图和对比学习提升学习者知识状态建模。


<details>
  <summary>Details</summary>
Motivation: 现有知识追踪模型仅关注单一课程数据，难以全面捕捉学习者的知识状态。

Method: 通过零样本LLM构建跨课程概念图，结合GCN进行知识迁移，并采用对比学习对齐单课程与跨课程知识状态。

Result: TransKT显著提升了知识状态估计的准确性和鲁棒性。

Conclusion: 跨课程知识迁移和对比学习能有效增强知识追踪模型的性能。

Abstract: Knowledge tracing (KT) aims to predict learners' future performance based on
historical learning interactions. However, existing KT models predominantly
focus on data from a single course, limiting their ability to capture a
comprehensive understanding of learners' knowledge states. In this paper, we
propose TransKT, a contrastive cross-course knowledge tracing method that
leverages concept graph guided knowledge transfer to model the relationships
between learning behaviors across different courses, thereby enhancing
knowledge state estimation. Specifically, TransKT constructs a cross-course
concept graph by leveraging zero-shot Large Language Model (LLM) prompts to
establish implicit links between related concepts across different courses.
This graph serves as the foundation for knowledge transfer, enabling the model
to integrate and enhance the semantic features of learners' interactions across
courses. Furthermore, TransKT includes an LLM-to-LM pipeline for incorporating
summarized semantic features, which significantly improves the performance of
Graph Convolutional Networks (GCNs) used for knowledge transfer. Additionally,
TransKT employs a contrastive objective that aligns single-course and
cross-course knowledge states, thereby refining the model's ability to provide
a more robust and accurate representation of learners' overall knowledge
states.

</details>


### [141] [ADALog: Adaptive Unsupervised Anomaly detection in Logs with Self-attention Masked Language Model](https://arxiv.org/abs/2505.13496)
*Przemek Pospieszny,Wojciech Mormul,Karolina Szyndler,Sanjeev Kumar*

Main category: cs.AI

TL;DR: ADALog是一个自适应、无监督的日志异常检测框架，利用预训练的双向编码器和自适应阈值技术，在多种真实环境中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现代软件系统生成大量异构日志数据，格式动态、事件序列碎片化且时间模式多变，使得异常检测既关键又具挑战性。传统方法依赖日志解析、严格序列依赖或标记数据，难以应对这些复杂性。

Method: ADALog基于预训练的双向编码器（Transformer架构），通过掩码语言建模任务微调，捕获领域特定的语法和语义模式。异常检测通过令牌级重建概率聚合为日志级分数，并采用基于百分位的自适应阈值技术。

Result: 在BGL、Thunderbird和Spirit基准数据集上，ADALog展现出强大的泛化能力和竞争力，性能优于现有监督和无监督方法。消融研究验证了掩码、微调和令牌定位对模型行为的影响。

Conclusion: ADALog通过无监督学习和动态阈值技术，有效解决了复杂日志环境中的异常检测问题，具有实际应用价值。

Abstract: Modern software systems generate extensive heterogeneous log data with
dynamic formats, fragmented event sequences, and varying temporal patterns,
making anomaly detection both crucial and challenging. To address these
complexities, we propose ADALog, an adaptive, unsupervised anomaly detection
framework designed for practical applicability across diverse real-world
environments. Unlike traditional methods reliant on log parsing, strict
sequence dependencies, or labeled data, ADALog operates on individual
unstructured logs, extracts intra-log contextual relationships, and performs
adaptive thresholding on normal data. The proposed approach utilizes a
transformer-based, pretrained bidirectional encoder with a masked language
modeling task, fine-tuned on normal logs to capture domain-specific syntactic
and semantic patterns essential for accurate anomaly detection. Anomalies are
identified via token-level reconstruction probabilities, aggregated into
log-level scores, with adaptive percentile-based thresholding calibrated only
on normal data. This allows the model to dynamically adapt to evolving system
behaviors while avoiding rigid, heuristic-based thresholds common in
traditional systems. We evaluate ADALog on benchmark datasets BGL, Thunderbird,
and Spirit, showing strong generalization and competitive performance compared
to state-of-the-art supervised and unsupervised methods. Additional ablation
studies examine the effects of masking, fine-tuning, and token positioning on
model behavior and interpretability.

</details>


### [142] [Can AI Freelancers Compete? Benchmarking Earnings, Reliability, and Task Success at Scale](https://arxiv.org/abs/2505.13511)
*David Noever,Forrest McKee*

Main category: cs.AI

TL;DR: 研究评估了四种大语言模型在自由职业编程任务中的表现，Claude 3.5 Haiku表现最佳，赚取约152万美元。


<details>
  <summary>Details</summary>
Motivation: 探索大语言模型作为自主代理完成现实世界任务（如自由职业软件开发）的可行性，并建立一个可扩展的评估基准。

Method: 使用Kaggle自由职业数据集构建合成任务，标准化价格，提供结构化测试用例和预估价格，自动化评估四种模型的任务成功率和收益。

Result: Claude 3.5 Haiku表现最好，赚取约152万美元，其次是GPT-4o-mini（149万美元）、Qwen 2.5（133万美元）和Mistral（70万美元）。

Conclusion: 研究表明AI在自由职业开发中具有潜力，但结构化任务与真实自由职业的复杂性仍存在差距。

Abstract: This study explores Large Language Models (LLMs) as autonomous agents for
real-world tasks, including freelance software development. This work presents
a new benchmark that evaluates LLMs on freelance programming and data analysis
tasks derived from economic data. We construct the benchmark using synthetic
tasks created from a Kaggle Freelancer dataset of job postings, with all job
prices standardized to USD (median fixed-project price around $250, and an
average of $306). Each task is accompanied by structured input-output test
cases and an estimated price tag, enabling automated correctness checking and a
monetary performance valuation. This approach is inspired by OpenAI's recent
SWE-Lancer benchmark (1,400 real Upwork tasks worth $1M total). Still, our
framework simplifies evaluation using programmatically testable tasks and
predicted price values, making it highly scalable and repeatable. On this
benchmark, we evaluate four modern LLMs - Claude 3.5 Haiku, GPT-4o-mini, Qwen
2.5, and Mistral. We report each model's accuracy (task success rate and
test-case pass rate) and the total "freelance earnings" it achieves (sum of
prices of solved tasks). Our results show that Claude 3.5 Haiku performs best,
earning approximately $1.52 million USD, followed closely by GPT-4o-mini at
$1.49 million, then Qwen 2.5 ($1.33M) and Mistral ($0.70M). We analyze the
distribution of errors per task and observe that the strongest models solve the
most tasks and rarely fail completely on any project. We discuss the
implications of these results for the feasibility of AI as a freelance
developer, the advantages and limitations of our automated benchmark approach,
and the gap between performance on structured tasks versus the true complexity
of real-world freelance jobs.

</details>


### [143] [A Heuristic Algorithm Based on Beam Search and Iterated Local Search for the Maritime Inventory Routing Problem](https://arxiv.org/abs/2505.13522)
*Nathalie Sanghikian,Rafael Meirelles,Rafael Martinelli,Anand Subramanian*

Main category: cs.AI

TL;DR: 该论文提出了一种不依赖数学优化技术的启发式方法，用于解决确定性、有限时间、单一产品的海上库存路径问题（MIRP），并在72个测试实例中改进了10个实例的最优解。


<details>
  <summary>Details</summary>
Motivation: 由于海上库存路径问题（MIRP）的高复杂性，目前缺乏能高效解决大规模实例的方法。精确方法因计算时间过长而不适用于日常操作，而启发式方法又因问题的高度约束性而难以应用。因此，作者旨在提出一种新的启发式方法，以促进MIRPLib的使用和结果比较。

Method: 论文提出了一种结合改进的Beam Search算法和迭代局部搜索（Iterated Local Search）过程的启发式方法，用于解决确定性、有限时间、单一产品的MIRP问题。

Result: 在测试的72个实例中，该方法在可接受的CPU时间内改进了10个实例的最优解。

Conclusion: 该研究展示了一种有效的启发式方法，能够在合理时间内解决复杂的MIRP问题，并为未来的研究提供了可比较的基准。

Abstract: Maritime Inventory Routing Problem (MIRP) plays a crucial role in the
integration of global maritime commerce levels. However, there are still no
well-established methodologies capable of efficiently solving large MIRP
instances or their variants due to the high complexity of the problem. The
adoption of exact methods, typically based on Mixed Integer Programming (MIP),
for daily operations is nearly impractical due to the CPU time required, as
planning must be executed multiple times while ensuring high-quality results
within acceptable time limits. Non-MIP-based heuristics are less frequently
applied due to the highly constrained nature of the problem, which makes even
the construction of an effective initial solution challenging. Papageorgiou et
al. (2014) introduced a single-product MIRP as the foundation for MIRPLib,
aiming to provide a collection of publicly available benchmark instances.
However, only a few studies that propose new methodologies have been published
since then. To encourage the use of MIRPLib and facilitate result comparisons,
this study presents a heuristic approach that does not rely on mathematical
optimization techniques to solve a deterministic, finite-horizon,
single-product MIRP. The proposed heuristic combines a variation of a Beam
Search algorithm with an Iterated Local Search procedure. Among the 72
instances tested, the developed methodology can improve the best-known solution
for ten instances within an acceptable CPU time.

</details>


### [144] [BARREL: Boundary-Aware Reasoning for Factual and Reliable LRMs](https://arxiv.org/abs/2505.13529)
*Junxiao Yang,Jinzhe Tu,Haoran Liu,Xiaoce Wang,Chujie Zheng,Zhexin Zhang,Shiyao Cui,Caishun Chen,Tiantian He,Hongning Wang,Yew-Soon Ong,Minlie Huang*

Main category: cs.AI

TL;DR: 论文提出BARREL框架，解决大型推理模型(LRMs)过度自信导致错误答案的问题，通过边界感知推理提升可靠性。


<details>
  <summary>Details</summary>
Motivation: 当前大型推理模型在数学和逻辑推理中常表现出过度自信，即使不知道正确答案也会给出错误回答，这降低了模型的事实可靠性。

Method: 提出BARREL框架，针对两种病态推理模式（最后一刻猜测和二次思考螺旋）进行优化，促进简洁且有边界意识的事实推理。

Result: 实验显示，BARREL训练将DeepSeek-R1-Distill-Llama-8B的可靠性从39.33%提升至61.48%，同时保持与R1生成数据微调模型相当的准确率。

Conclusion: 这项初步研究表明，BARREL框架对构建更可靠、更符合事实的系统2大型推理模型具有启发意义。

Abstract: Recent advances in Large Reasoning Models (LRMs) have shown impressive
capabilities in mathematical and logical reasoning. However, current LRMs
rarely admit ignorance or respond with "I don't know". Instead, they often
produce incorrect answers while showing undue confidence, raising concerns
about their factual reliability. In this work, we identify two pathological
reasoning patterns characterized by overthinking that contribute to the
overconfident and incorrect answers: last-minute guessing and second-thought
spiraling. To address these issues, we propose BARREL-a novel framework that
promotes concise and boundary-aware factual reasoning. Our experiments show
that BARREL-training increases the reliability of DeepSeek-R1-Distill-Llama-8B
from 39.33% to 61.48%, while still achieving accuracy comparable to models
finetuned on reasoning data generated by R1. These results demonstrate that our
pilot study is inspiring to build more reliable and factual System 2 LRMs.

</details>


### [145] [FinMaster: A Holistic Benchmark for Mastering Full-Pipeline Financial Workflows with LLMs](https://arxiv.org/abs/2505.13533)
*Junzhe Jiang,Chang Yang,Aixin Cui,Sihan Jin,Ruiyu Wang,Bo Li,Xiao Huang,Dongning Sun,Xinrun Wang*

Main category: cs.AI

TL;DR: FinMaster是一个全面的金融基准测试，旨在评估大语言模型在金融领域的表现，填补现有基准测试的不足。


<details>
  <summary>Details</summary>
Motivation: 金融任务对全球经济稳定至关重要，但面临劳动密集、容错率低、数据分散和工具限制等挑战。现有的大语言模型评估基准缺乏领域特定数据、任务设计简单且评估框架不完整。

Method: FinMaster包含三个模块：FinSim生成合成金融数据，FinSuite提供183个金融任务，FinEval提供统一评估接口。

Result: 实验显示，大语言模型在复杂金融推理任务中准确率从90%降至40%，多指标场景下计算错误传播明显。

Conclusion: FinMaster是首个覆盖全流程金融任务的基准测试，有望推动大语言模型在金融实践中的应用，提升效率和准确性。

Abstract: Financial tasks are pivotal to global economic stability; however, their
execution faces challenges including labor intensive processes, low error
tolerance, data fragmentation, and tool limitations. Although large language
models (LLMs) have succeeded in various natural language processing tasks and
have shown potential in automating workflows through reasoning and contextual
understanding, current benchmarks for evaluating LLMs in finance lack
sufficient domain-specific data, have simplistic task design, and incomplete
evaluation frameworks. To address these gaps, this article presents FinMaster,
a comprehensive financial benchmark designed to systematically assess the
capabilities of LLM in financial literacy, accounting, auditing, and
consulting. Specifically, FinMaster comprises three main modules: i) FinSim,
which builds simulators that generate synthetic, privacy-compliant financial
data for companies to replicate market dynamics; ii) FinSuite, which provides
tasks in core financial domains, spanning 183 tasks of various types and
difficulty levels; and iii) FinEval, which develops a unified interface for
evaluation. Extensive experiments over state-of-the-art LLMs reveal critical
capability gaps in financial reasoning, with accuracy dropping from over 90% on
basic tasks to merely 40% on complex scenarios requiring multi-step reasoning.
This degradation exhibits the propagation of computational errors, where
single-metric calculations initially demonstrating 58% accuracy decreased to
37% in multimetric scenarios. To the best of our knowledge, FinMaster is the
first benchmark that covers full-pipeline financial workflows with challenging
tasks. We hope that FinMaster can bridge the gap between research and industry
practitioners, driving the adoption of LLMs in real-world financial practices
to enhance efficiency and accuracy.

</details>


### [146] [Prompt Stability Matters: Evaluating and Optimizing Auto-Generated Prompt in General-Purpose Systems](https://arxiv.org/abs/2505.13546)
*Ke Chen,Yufei Zhou,Xitong Zhang,Haohan Wang*

Main category: cs.AI

TL;DR: 该论文提出了一种基于语义稳定性的自动提示生成系统，通过评估提示的响应一致性来提升多任务代理系统的可靠性和性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常仅基于任务表现评估提示，忽视了其内在可靠性。这种结果导向的视角不仅限制了可解释性，也无法应对大语言模型的固有随机性。

Method: 提出语义稳定性作为评估提示响应一致性的标准，并微调基于LLaMA的评估器来自动测量跨任务稳定性，开发了首个稳定性感知的通用提示生成系统。

Result: 实验表明，该稳定性感知框架在通用和领域特定任务中均提高了准确性和输出一致性。

Conclusion: 通过将焦点从一次性结果转向持久可靠性，该研究为构建更可信的通用系统提供了新视角和实用工具。

Abstract: Automatic prompt generation plays a crucial role in enabling general-purpose
multi-agent systems to perform diverse tasks autonomously. Existing methods
typically evaluate prompts based on their immediate task performance,
overlooking the intrinsic qualities that determine their reliability. This
outcome-centric view not only limits interpretability but also fails to account
for the inherent stochasticity of large language models (LLMs). In this work,
we bring attention to prompt stability-the consistency of model responses
across repeated executions-as a key factor for building robust and effective
prompt generation systems. To quantify this, we propose semantic stability as a
criterion for assessing the response consistency of prompts, and fine-tune a
LLaMA-based evaluator to measure it automatically across tasks. These
components have enabled us to develop the first stability-aware general-purpose
prompt generation system that leverages stability feedback to iteratively
enhance both prompt quality and system-level performance. Furthermore, we
establish a logical chain between prompt stability and task success by
analyzing the structural dependencies within our system, proving stability as a
necessary condition for effective system-level execution. Empirical results
across general and domain-specific tasks demonstrate that our stability-aware
framework improves both accuracy and output consistency. By shifting the focus
from one-off results to persistent reliability, our work offers a new
perspective on prompt design and contributes practical tools for building more
trustworthy general-purpose systems.

</details>


### [147] [Counter-Inferential Behavior in Natural and Artificial Cognitive Systems](https://arxiv.org/abs/2505.13551)
*Serge Dolgikh*

Main category: cs.AI

TL;DR: 该研究探讨了自然和人工认知系统中反推理行为的出现，分析了导致认知僵化或适应不良的几种典型场景，并提出了抗信息压力的认知架构设计原则。


<details>
  <summary>Details</summary>
Motivation: 研究动机是理解认知系统为何会在某些情况下表现出反推理行为，即错误归因成功或抑制适应性调整，从而导致认知僵化或适应不良的稳定性。

Method: 研究方法包括分析几种典型场景：奖励失衡导致的稳定性强化、将成功归因于内部优势的元认知，以及感知模型脆弱性下的保护性重构。研究结合了人工系统、生物认知、人类心理学和社会动力学的证据。

Result: 研究发现反推理行为是一种普遍的认知脆弱性，即使在适应性良好的系统中也可能出现。研究还强调了在稳定条件下保持最小适应性激活的重要性。

Conclusion: 研究结论提出了抗信息压力的认知架构设计原则，以避免在信息压力下出现认知僵化。

Abstract: This study explores the emergence of counter-inferential behavior in natural
and artificial cognitive systems, that is, patterns in which agents
misattribute empirical success or suppress adaptation, leading to epistemic
rigidity or maladaptive stability. We analyze archetypal scenarios in which
such behavior arises: reinforcement of stability through reward imbalance,
meta-cognitive attribution of success to internal superiority, and protective
reframing under perceived model fragility. Rather than arising from noise or
flawed design, these behaviors emerge through structured interactions between
internal information models, empirical feedback, and higher-order evaluation
mechanisms. Drawing on evidence from artificial systems, biological cognition,
human psychology, and social dynamics, we identify counter-inferential behavior
as a general cognitive vulnerability that can manifest even in otherwise
well-adapted systems. The findings highlight the importance of preserving
minimal adaptive activation under stable conditions and suggest design
principles for cognitive architectures that can resist rigidity under
informational stress.

</details>


### [148] [Language and Thought: The View from LLMs](https://arxiv.org/abs/2505.13561)
*Daniel Rothschild*

Main category: cs.AI

TL;DR: 论文探讨了语言对思维的影响，通过AI模型（尤其是大语言模型）的表现来验证Daniel Dennett的观点，即语言的存在与否会显著改变思维的性质。


<details>
  <summary>Details</summary>
Motivation: 受Daniel Dennett在《Kinds of Minds》中提出的观点启发，研究旨在验证语言对思维的根本性影响，并利用AI模型的表现来支持这一理论。

Method: 通过对比分析有语言训练和无语言训练的AI系统表现，特别是大语言模型在推理任务中的表现，来探讨语言的作用。

Result: 研究发现，大语言模型在跨领域推理任务中的成功支持了Dennett的观点，表明语言的抽象性和高效编码能力是推理计算可行的关键。

Conclusion: 语言不仅显著改变了思维的性质，还使得跨领域的推理计算成为可能，这对理解人类生物大脑中语言的作用具有重要启示。

Abstract: Daniel Dennett speculated in *Kinds of Minds* 1996: "Perhaps the kind of mind
you get when you add language to it is so different from the kind of mind you
can have without language that calling them both minds is a mistake." Recent
work in AI can be seen as testing Dennett's thesis by exploring the performance
of AI systems with and without linguistic training. I argue that the success of
Large Language Models at inferential reasoning, limited though it may be,
supports Dennett's radical view about the effect of language on thought. I
suggest it is the abstractness and efficiency of linguistic encoding that lies
behind the capacity of LLMs to perform inferences across a wide range of
domains. In a slogan, language makes inference computationally tractable. I
assess what these results in AI indicate about the role of language in the
workings of our own biological minds.

</details>


### [149] [MAFA: A multi-agent framework for annotation](https://arxiv.org/abs/2505.13668)
*Mahmood Hegazy,Aaron Rodrigues,Azzam Naeem*

Main category: cs.AI

TL;DR: 提出多智能体框架提升FAQ标注效果，通过组合不同专业代理和法官代理重排序，显著优于单代理方法。


<details>
  <summary>Details</summary>
Motivation: 传统FAQ检索依赖单一模型，难以捕捉用户查询的多样性。需要更精准高效的解决方案。

Method: 基于ARQ结构化推理的多智能体框架，各代理采用差异化小样本策略，法官代理重排序候选结果。

Result: 银行数据集上Top-1准确率提升14%，Top-5提升18%，MRR提升12%，公开基准测试同样显著改进。

Conclusion: 该框架能有效处理模糊查询，具备跨领域泛化能力，适合生产环境部署。

Abstract: Modern applications require accurate and efficient retrieval of information
in response to user queries. Mapping user utterances to the most relevant
Frequently Asked Questions (FAQs) is a crucial component of these systems.
Traditional approaches often rely on a single model or technique, which may not
capture the nuances of diverse user inquiries. In this paper, we introduce a
multi-agent framework for FAQ annotation that combines multiple specialized
agents with different approaches and a judge agent that reranks candidates to
produce optimal results. Our agents utilize a structured reasoning approach
inspired by Attentive Reasoning Queries (ARQs), which guides them through
systematic reasoning steps using targeted, task-specific JSON queries. Our
framework features a specialized few-shot example strategy, where each agent
receives different few-shots, enhancing ensemble diversity and coverage of the
query space. We evaluate our framework on a real-world banking dataset as well
as public benchmark datasets (LCQMC and FiQA), demonstrating significant
improvements over single-agent approaches across multiple metrics, including a
14% increase in Top-1 accuracy, an 18% increase in Top-5 accuracy, and a 12%
improvement in Mean Reciprocal Rank on our dataset, and similar gains on public
benchmarks when compared with traditional single agent annotation techniques.
Our framework is particularly effective at handling ambiguous queries, making
it well-suited for deployment in production applications while showing strong
generalization capabilities across different domains and languages.

</details>


### [150] [A*-Decoding: Token-Efficient Inference Scaling](https://arxiv.org/abs/2505.13672)
*Giannis Chatziveroglou*

Main category: cs.AI

TL;DR: 提出A*-decoding方法，通过结构化搜索优化推理计算资源分配，使小模型达到大模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在固定计算预算下表现良好，但缺乏对推理过程中计算资源最优利用的研究。

Method: 基于A*搜索算法，在部分解的状态空间中进行结构化搜索，优先高质量推理路径。

Result: 在相同计算预算下，A*-decoding比现有方法节省3倍token和30%PRM计算，小模型性能匹配大模型。

Conclusion: 结构化搜索解码策略可提升小模型推理能力，为高效部署语言模型提供新方向。

Abstract: Inference-time scaling has emerged as a powerful alternative to parameter
scaling for improving language model performance on complex reasoning tasks.
While existing methods have shown strong performance gains under fixed compute
budgets, there has been little focus on optimally utilizing that budget during
inference. In this work, we introduce A*-decoding, a search-based
inference-time strategy that builds on the A* search algorithm to optimally
utilize a fixed compute budget by prioritizing high-quality reasoning paths
during generation. We frame language model decoding as a structured search in a
state space of partial solutions, applying the A* transition model to identify
promising continuations guided by an external process supervision signal. In
our experiments, A*-decoding reaches the performance levels of strong inference
scaling baselines like best-of-N and particle filtering while using up to 3x
fewer tokens and 30% fewer PRM passes under equivalent compute budgets. On the
MATH500 and AIME 2024 benchmarks, A*-decoding enables Llama-3.2-1B-Instruct to
match the performance of the 70x larger Llama-3.1-70B-Instruct, and allows
Qwen3-1.7B to reach o1-like reasoning accuracy. These results highlight the
power of structured search in decoding, offering an alternative to brute-force
sampling or scale-driven gains. Our work demonstrates how thoughtful
inference-time strategies can enhance reasoning in SLMs, pointing toward future
advances in more efficient and scalable language model deployment.

</details>


### [151] [Building spatial world models from sparse transitional episodic memories](https://arxiv.org/abs/2505.13696)
*Zizhan He,Maxime Daigle,Pouya Bashivan*

Main category: cs.AI

TL;DR: 该论文提出了一种名为ESWM的新型神经网络框架，能够从稀疏的片段记忆中高效构建空间环境模型，并支持快速适应环境变化及最优探索策略。


<details>
  <summary>Details</summary>
Motivation: 研究动机源于动物能快速构建灵活环境心理模型的能力，希望探索神经网络是否也能通过稀疏片段记忆学习构建类似的空间模型。

Method: 论文在模拟环境中提出Episodic Spatial World Model (ESWM)框架，通过稀疏观察构建环境表征，并测试其适应性和探索能力。

Result: 结果显示ESWM具有极高的样本效率，仅需极少观察即可构建稳健环境模型，且能快速适应环境变化，支持无需额外训练的最优探索与导航。

Conclusion: ESWM证明了神经网络可通过片段记忆高效学习空间模型，为类脑环境建模与自适应行为提供了新思路。

Abstract: Many animals possess a remarkable capacity to rapidly construct flexible
mental models of their environments. These world models are crucial for
ethologically relevant behaviors such as navigation, exploration, and planning.
The ability to form episodic memories and make inferences based on these sparse
experiences is believed to underpin the efficiency and adaptability of these
models in the brain. Here, we ask: Can a neural network learn to construct a
spatial model of its surroundings from sparse and disjoint episodic memories?
We formulate the problem in a simulated world and propose a novel framework,
the Episodic Spatial World Model (ESWM), as a potential answer. We show that
ESWM is highly sample-efficient, requiring minimal observations to construct a
robust representation of the environment. It is also inherently adaptive,
allowing for rapid updates when the environment changes. In addition, we
demonstrate that ESWM readily enables near-optimal strategies for exploring
novel environments and navigating between arbitrary points, all without the
need for additional training.

</details>


### [152] [Warm Up Before You Train: Unlocking General Reasoning in Resource-Constrained Settings](https://arxiv.org/abs/2505.13718)
*Safal Shrestha,Minwu Kim,Aadim Nepal,Anubhav Shrestha,Keith Ross*

Main category: cs.AI

TL;DR: 提出两阶段训练策略，先用逻辑谜题预热模型，再用少量目标域数据微调，显著提升小样本下LLM的推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法（RLVR和长思维链蒸馏）依赖大量优质数据，但现实中高质量训练数据往往稀缺。如何在有限监督下开发具备推理能力的LLM成为关键挑战。

Method: 1. 预热阶段：用K&K逻辑谜题蒸馏长思维链，获取通用推理能力
2. 微调阶段：在预热模型上使用少量目标域数据进行RLVR训练

Result: 1. 预热后模型在MATH等任务表现提升
2. 相同小数据微调时，预热模型始终优于基线
3. 保持跨领域泛化能力
4. 提升RLVR训练样本效率

Conclusion: 预热策略能有效解决数据稀缺环境下构建推理LLM的难题，在准确率和样本效率上均有突破。

Abstract: Designing effective reasoning-capable LLMs typically requires training using
Reinforcement Learning with Verifiable Rewards (RLVR) or distillation with
carefully curated Long Chain of Thoughts (CoT), both of which depend heavily on
extensive training data. This creates a major challenge when the amount of
quality training data is scarce. We propose a sample-efficient, two-stage
training strategy to develop reasoning LLMs under limited supervision. In the
first stage, we "warm up" the model by distilling Long CoTs from a toy domain,
namely, Knights \& Knaves (K\&K) logic puzzles to acquire general reasoning
skills. In the second stage, we apply RLVR to the warmed-up model using a
limited set of target-domain examples. Our experiments demonstrate that this
two-phase approach offers several benefits: $(i)$ the warmup phase alone
facilitates generalized reasoning, leading to performance improvements across a
range of tasks, including MATH, HumanEval$^{+}$, and MMLU-Pro. $(ii)$ When both
the base model and the warmed-up model are RLVR trained on the same small
dataset ($\leq100$ examples), the warmed-up model consistently outperforms the
base model; $(iii)$ Warming up before RLVR training allows a model to maintain
cross-domain generalizability even after training on a specific domain; $(iv)$
Introducing warmup in the pipeline improves not only accuracy but also overall
sample efficiency during RLVR training. The results in this paper highlight the
promise of warmup for building robust reasoning LLMs in data-scarce
environments.

</details>


### [153] [Causal Head Gating: A Framework for Interpreting Roles of Attention Heads in Transformers](https://arxiv.org/abs/2505.13737)
*Andrew Nam,Henry Conklin,Yukang Yang,Thomas Griffiths,Jonathan Cohen,Sarah-Jane Leslie*

Main category: cs.AI

TL;DR: 提出了一种可扩展的方法CHG，用于解释Transformer模型中注意力头的功能角色，并通过实验验证了其因果性。


<details>
  <summary>Details</summary>
Motivation: 现有方法多为假设驱动，需要提示模板或目标标签，而CHG可直接应用于任何数据集，提供更灵活的因果解释。

Method: CHG通过学习软门控对注意力头进行分类（促进、干扰或无关），并引入对比CHG以分离特定任务子电路。

Result: 实验表明，LLMs包含多个稀疏子电路，注意力头角色依赖交互（低模块化），且指令跟随和上下文学习机制可分离。

Conclusion: CHG提供了一种无需假设的因果解释方法，揭示了LLMs内部机制的复杂性和可分离性。

Abstract: We present causal head gating (CHG), a scalable method for interpreting the
functional roles of attention heads in transformer models. CHG learns soft
gates over heads and assigns them a causal taxonomy - facilitating,
interfering, or irrelevant - based on their impact on task performance. Unlike
prior approaches in mechanistic interpretability, which are hypothesis-driven
and require prompt templates or target labels, CHG applies directly to any
dataset using standard next-token prediction. We evaluate CHG across multiple
large language models (LLMs) in the Llama 3 model family and diverse tasks,
including syntax, commonsense, and mathematical reasoning, and show that CHG
scores yield causal - not merely correlational - insight, validated via
ablation and causal mediation analyses. We also introduce contrastive CHG, a
variant that isolates sub-circuits for specific task components. Our findings
reveal that LLMs contain multiple sparse, sufficient sub-circuits, that
individual head roles depend on interactions with others (low modularity), and
that instruction following and in-context learning rely on separable
mechanisms.

</details>


### [154] [Language Models Are Capable of Metacognitive Monitoring and Control of Their Internal Activations](https://arxiv.org/abs/2505.13763)
*Li Ji-An,Hua-Dong Xiong,Robert C. Wilson,Marcelo G. Mattar,Marcus K. Benna*

Main category: cs.AI

TL;DR: 该论文研究了大型语言模型（LLMs）的元认知能力，即模型监控和报告自身内部激活模式的能力，并探讨了其对AI安全的影响。


<details>
  <summary>Details</summary>
Motivation: 随着社会对大型语言模型的依赖增加，理解其元认知能力的局限性变得至关重要，尤其是模型监控内部激活的能力，这对AI安全具有重要意义。

Method: 论文引入了一种受神经科学启发的神经反馈范式，通过向模型提供句子-标签对来量化LLMs报告和控制其激活模式的能力。

Result: 研究发现，LLMs能够学习和报告其内部激活模式，但性能受示例对数量、目标神经方向的语义可解释性以及该方向解释的方差等因素影响。

Conclusion: 研究揭示了LLMs的元认知空间维度远低于其神经空间维度，表明模型只能监控其神经机制的一个子集，这对AI安全有重要启示。

Abstract: Large language models (LLMs) can sometimes report the strategies they
actually use to solve tasks, but they can also fail to do so. This suggests
some degree of metacognition -- the capacity to monitor one's own cognitive
processes for subsequent reporting and self-control. Metacognitive abilities
enhance AI capabilities but raise safety concerns, as models might obscure
their internal processes to evade neural-activation-based oversight mechanisms
designed to detect harmful behaviors. Given society's increased reliance on
these models, it is critical that we understand the limits of their
metacognitive abilities, particularly their ability to monitor their internal
activations. To address this, we introduce a neuroscience-inspired
neurofeedback paradigm designed to quantify the ability of LLMs to explicitly
report and control their activation patterns. By presenting models with
sentence-label pairs where labels correspond to sentence-elicited internal
activations along specific directions in the neural representation space, we
demonstrate that LLMs can learn to report and control these activations. The
performance varies with several factors: the number of example pairs provided,
the semantic interpretability of the target neural direction, and the variance
explained by that direction. These results reveal a "metacognitive space" with
dimensionality much lower than the model's neural space, suggesting LLMs can
monitor only a subset of their neural mechanisms. Our findings provide
empirical evidence quantifying metacognitive capabilities in LLMs, with
significant implications for AI safety.

</details>


### [155] [Ice Cream Doesn't Cause Drowning: Benchmarking LLMs Against Statistical Pitfalls in Causal Inference](https://arxiv.org/abs/2505.13770)
*Jin Du,Li Chen,Xun Xian,An Luo,Fangqiao Tian,Ganghua Wang,Charles Doss,Xiaotong Shen,Jie Ding*

Main category: cs.AI

TL;DR: 论文提出了CausalPitfalls基准，用于评估大语言模型在因果推理中的能力，发现现有模型存在显著局限性。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在因果推理任务中可能忽视统计陷阱（如辛普森悖论、选择偏差），限制了其实际应用。需要更严格的评估标准。

Method: 设计了多难度层级的CausalPitfalls基准，采用直接提示和代码辅助提示两种协议，并与人类专家评分对比验证。

Result: 现有大语言模型在统计因果推理方面表现存在显著不足，新基准提供了可量化的改进方向。

Conclusion: CausalPitfalls基准为开发可信赖的因果推理系统提供了关键指导，凸显了当前模型的改进空间。

Abstract: Reliable causal inference is essential for making decisions in high-stakes
areas like medicine, economics, and public policy. However, it remains unclear
whether large language models (LLMs) can handle rigorous and trustworthy
statistical causal inference. Current benchmarks usually involve simplified
tasks. For example, these tasks might only ask LLMs to identify semantic causal
relationships or draw conclusions directly from raw data. As a result, models
may overlook important statistical pitfalls, such as Simpson's paradox or
selection bias. This oversight limits the applicability of LLMs in the real
world. To address these limitations, we propose CausalPitfalls, a comprehensive
benchmark designed to rigorously evaluate the capability of LLMs in overcoming
common causal inference pitfalls. Our benchmark features structured challenges
across multiple difficulty levels, each paired with grading rubrics. This
approach allows us to quantitatively measure both causal reasoning capabilities
and the reliability of LLMs' responses. We evaluate models using two protocols:
(1) direct prompting, which assesses intrinsic causal reasoning, and (2)
code-assisted prompting, where models generate executable code for explicit
statistical analysis. Additionally, we validate the effectiveness of this judge
by comparing its scoring with assessments from human experts. Our results
reveal significant limitations in current LLMs when performing statistical
causal inference. The CausalPitfalls benchmark provides essential guidance and
quantitative metrics to advance the development of trustworthy causal reasoning
systems.

</details>


### [156] [Model Cards for AI Teammates: Comparing Human-AI Team Familiarization Methods for High-Stakes Environments](https://arxiv.org/abs/2505.13773)
*Ryan Bowers,Richard Agbeyibor,Jack Kolb,Karen Feigh*

Main category: cs.AI

TL;DR: 研究比较了三种人机协作前的熟悉方法，发现结合文档阅读和实操训练效果最佳。


<details>
  <summary>Details</summary>
Motivation: 探讨在快速变化的ISR环境中，如何让人工智能队友与人类更高效协作。

Method: 60名参与者分组进行文档阅读、实操训练或无熟悉，对比协作效果。

Result: 文档组策略形成最快但偏保守，实操组更愿冒险但对AI理解较弱，个体风险偏好差异显著。

Conclusion: 推荐结合AI文档、结构化训练和探索性互动的人机团队熟悉方法。

Abstract: We compare three methods of familiarizing a human with an artificial
intelligence (AI) teammate ("agent") prior to operation in a collaborative,
fast-paced intelligence, surveillance, and reconnaissance (ISR) environment. In
a between-subjects user study (n=60), participants either read documentation
about the agent, trained alongside the agent prior to the mission, or were
given no familiarization. Results showed that the most valuable information
about the agent included details of its decision-making algorithms and its
relative strengths and weaknesses compared to the human. This information
allowed the familiarization groups to form sophisticated team strategies more
quickly than the control group. Documentation-based familiarization led to the
fastest adoption of these strategies, but also biased participants towards
risk-averse behavior that prevented high scores. Participants familiarized
through direct interaction were able to infer much of the same information
through observation, and were more willing to take risks and experiment with
different control modes, but reported weaker understanding of the agent's
internal processes. Significant differences were seen between individual
participants' risk tolerance and methods of AI interaction, which should be
considered when designing human-AI control interfaces. Based on our findings,
we recommend a human-AI team familiarization method that combines AI
documentation, structured in-situ training, and exploratory interaction.

</details>


### [157] [Measuring the Faithfulness of Thinking Drafts in Large Reasoning Models](https://arxiv.org/abs/2505.13774)
*Zidi Xiong,Chen Shan,Zhenting Qi,Himabindu Lakkaraju*

Main category: cs.AI

TL;DR: 本文提出了一种系统性反事实干预框架，用于严格评估大型推理模型（LRMs）思维草稿的忠实性，发现当前模型在中间推理步骤和结论一致性上存在不足。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型通过多路径思维链探索提升复杂问题解决能力，但确保中间推理过程的忠实性对可靠监控和有效控制至关重要。

Method: 提出反事实干预框架，从两个维度评估：1) 草稿内忠实性（步骤间因果影响）；2) 草稿到答案的忠实性（结论逻辑一致性）。

Result: 实验表明，当前LRMs对中间步骤的选择性忠实性不足，且常无法与草稿结论保持一致。

Conclusion: 高级LRMs需要更忠实、可解释的推理机制以提升可靠性。

Abstract: Large Reasoning Models (LRMs) have significantly enhanced their capabilities
in complex problem-solving by introducing a thinking draft that enables
multi-path Chain-of-Thought explorations before producing final answers.
Ensuring the faithfulness of these intermediate reasoning processes is crucial
for reliable monitoring, interpretation, and effective control. In this paper,
we propose a systematic counterfactual intervention framework to rigorously
evaluate thinking draft faithfulness. Our approach focuses on two complementary
dimensions: (1) Intra-Draft Faithfulness, which assesses whether individual
reasoning steps causally influence subsequent steps and the final draft
conclusion through counterfactual step insertions; and (2) Draft-to-Answer
Faithfulness, which evaluates whether final answers are logically consistent
with and dependent on the thinking draft, by perturbing the draft's concluding
logic. We conduct extensive experiments across six state-of-the-art LRMs. Our
findings show that current LRMs demonstrate selective faithfulness to
intermediate reasoning steps and frequently fail to faithfully align with the
draft conclusions. These results underscore the need for more faithful and
interpretable reasoning in advanced LRMs.

</details>


### [158] [CoIn: Counting the Invisible Reasoning Tokens in Commercial Opaque LLM APIs](https://arxiv.org/abs/2505.13778)
*Guoheng Sun,Ziyao Wang,Bowei Tian,Meng Liu,Zheyu Shen,Shwai He,Yexiao He,Wanghao Ye,Yiting Wang,Ang Li*

Main category: cs.AI

TL;DR: 论文提出CoIn框架，用于验证大型语言模型隐藏推理令牌的数量和语义有效性，以解决计费不透明问题。


<details>
  <summary>Details</summary>
Motivation: 商业LLM API通常隐藏推理过程仅返回最终答案，导致用户无法验证被计费的隐藏令牌的真实性，可能存在令牌计数虚报问题。

Method: CoIn通过构建可验证的哈希树检查令牌数量，并基于嵌入的相关性匹配检测伪造的推理内容。

Result: 实验表明CoIn作为第三方审计工具，检测令牌计数虚报的成功率高达94.7%。

Conclusion: CoIn能有效恢复不透明LLM服务的计费透明度，相关数据和代码已开源。

Abstract: As post-training techniques evolve, large language models (LLMs) are
increasingly augmented with structured multi-step reasoning abilities, often
optimized through reinforcement learning. These reasoning-enhanced models
outperform standard LLMs on complex tasks and now underpin many commercial LLM
APIs. However, to protect proprietary behavior and reduce verbosity, providers
typically conceal the reasoning traces while returning only the final answer.
This opacity introduces a critical transparency gap: users are billed for
invisible reasoning tokens, which often account for the majority of the cost,
yet have no means to verify their authenticity. This opens the door to token
count inflation, where providers may overreport token usage or inject
synthetic, low-effort tokens to inflate charges. To address this issue, we
propose CoIn, a verification framework that audits both the quantity and
semantic validity of hidden tokens. CoIn constructs a verifiable hash tree from
token embedding fingerprints to check token counts, and uses embedding-based
relevance matching to detect fabricated reasoning content. Experiments
demonstrate that CoIn, when deployed as a trusted third-party auditor, can
effectively detect token count inflation with a success rate reaching up to
94.7%, showing the strong ability to restore billing transparency in opaque LLM
services. The dataset and code are available at
https://github.com/CASE-Lab-UMD/LLM-Auditing-CoIn.

</details>


### [159] [LLM-based Evaluation Policy Extraction for Ecological Modeling](https://arxiv.org/abs/2505.13794)
*Qi Cheng,Licheng Liu,Qing Zhu,Runlong Yu,Zhenong Jin,Yiqun Xie,Xiaowei Jia*

Main category: cs.AI

TL;DR: 提出结合度量学习与大语言模型的新框架，解决生态时间序列评估中传统数值指标无法捕捉领域特定时序模式的问题。


<details>
  <summary>Details</summary>
Motivation: 传统数值指标（如R平方、均方根误差）难以量化生态过程的关键时序模式，依赖专家视觉检查又费时费力，亟需自动化、可解释的评估方法。

Method: 通过整合度量学习与LLM的自然语言策略提取，开发可解释评估标准，利用成对标注数据优化策略组合多维度指标。

Result: 在作物总初级生产力和二氧化碳通量预测评估中，该方法能有效捕捉合成数据与专家标注的评估偏好。

Conclusion: 该框架弥合数值指标与专家知识间的鸿沟，为不同生态系统建模需求提供可解释的自动化评估方案。

Abstract: Evaluating ecological time series is critical for benchmarking model
performance in many important applications, including predicting greenhouse gas
fluxes, capturing carbon-nitrogen dynamics, and monitoring hydrological cycles.
Traditional numerical metrics (e.g., R-squared, root mean square error) have
been widely used to quantify the similarity between modeled and observed
ecosystem variables, but they often fail to capture domain-specific temporal
patterns critical to ecological processes. As a result, these methods are often
accompanied by expert visual inspection, which requires substantial human labor
and limits the applicability to large-scale evaluation. To address these
challenges, we propose a novel framework that integrates metric learning with
large language model (LLM)-based natural language policy extraction to develop
interpretable evaluation criteria. The proposed method processes pairwise
annotations and implements a policy optimization mechanism to generate and
combine different assessment metrics. The results obtained on multiple datasets
for evaluating the predictions of crop gross primary production and carbon
dioxide flux have confirmed the effectiveness of the proposed method in
capturing target assessment preferences, including both synthetically generated
and expert-annotated model comparisons. The proposed framework bridges the gap
between numerical metrics and expert knowledge while providing interpretable
evaluation policies that accommodate the diverse needs of different ecosystem
modeling studies.

</details>


### [160] [Multimodal RAG-driven Anomaly Detection and Classification in Laser Powder Bed Fusion using Large Language Models](https://arxiv.org/abs/2505.13828)
*Kiarash Naghavi Khanghah,Zhiling Chen,Lela Romeo,Qian Yang,Rajiv Malhotra,Farhad Imani,Hongyi Xu*

Main category: cs.AI

TL;DR: 该论文提出了一种基于检索增强生成的多模态框架，用于自动化检测增材制造过程中的异常，无需训练数据集，通过检索科学文献中的图像和文本信息实现零样本异常识别、分类和解释生成。


<details>
  <summary>Details</summary>
Motivation: 增材制造在制造复杂设计时面临缺陷和过程异常的挑战，传统方法依赖训练数据集，难以适应多样化的制造场景。本研究旨在开发一种无需训练、可自动更新的异常检测框架。

Method: 该框架结合文本和图像检索技术，利用多模态生成模型（如GPT-4o-mini和Qwen2-VL-2B）在激光粉末床熔融（L-PBF）环境中进行零样本异常检测。通过检索科学文献中的信息，减少幻觉风险并提供额外信息。

Result: 在四个L-PBF制造数据集上的评估表明，该框架具有适应性和泛化能力，GPT-4o-mini在异常分类中表现优于Qwen2-VL-2B和随机基线。检索机制的引入使平均准确率提高了12%。

Conclusion: 该框架通过自动化、零样本能力和持续更新机制，显著提高了增材制造异常分析的效率和准确性，可适应不断发展的增材制造技术。

Abstract: Additive manufacturing enables the fabrication of complex designs while
minimizing waste, but faces challenges related to defects and process
anomalies. This study presents a novel multimodal Retrieval-Augmented
Generation-based framework that automates anomaly detection across various
Additive Manufacturing processes leveraging retrieved information from
literature, including images and descriptive text, rather than training
datasets. This framework integrates text and image retrieval from scientific
literature and multimodal generation models to perform zero-shot anomaly
identification, classification, and explanation generation in a Laser Powder
Bed Fusion setting. The proposed framework is evaluated on four L-PBF
manufacturing datasets from Oak Ridge National Laboratory, featuring various
printer makes, models, and materials. This evaluation demonstrates the
framework's adaptability and generalizability across diverse images without
requiring additional training. Comparative analysis using Qwen2-VL-2B and
GPT-4o-mini as MLLM within the proposed framework highlights that GPT-4o-mini
outperforms Qwen2-VL-2B and proportional random baseline in manufacturing
anomalies classification. Additionally, the evaluation of the RAG system
confirms that incorporating retrieval mechanisms improves average accuracy by
12% by reducing the risk of hallucination and providing additional information.
The proposed framework can be continuously updated by integrating emerging
research, allowing seamless adaptation to the evolving landscape of AM
technologies. This scalable, automated, and zero-shot-capable framework
streamlines AM anomaly analysis, enhancing efficiency and accuracy.

</details>


### [161] [TelePlanNet: An AI-Driven Framework for Efficient Telecom Network Planning](https://arxiv.org/abs/2505.13831)
*Zongyuan Deng,Yujie Cai,Qing Liu,Shiyao Mu,Bin Lyu,Zhen Yang*

Main category: cs.AI

TL;DR: 论文提出AI驱动的基站选址框架TelePlanNet，结合LLM和强化学习优化5G网络规划，一致性提升至78%。


<details>
  <summary>Details</summary>
Motivation: 传统人工选址方法效率低且一致性差，现有AI工具难以满足动态网络需求，需开发多目标优化的自动化方案。

Method: 提出TelePlanNet框架：利用LLM实时处理用户输入，改进GRPO强化学习训练规划模型，实现多目标优化。

Result: 实验显示TelePlanNet将规划一致性提升至78%，优于人工方法，为运营商提供高效可扩展的工具。

Conclusion: TelePlanNet显著推进蜂窝网络规划自动化，有效解决多目标优化问题。

Abstract: The selection of base station sites is a critical challenge in 5G network
planning, which requires efficient optimization of coverage, cost, user
satisfaction, and practical constraints. Traditional manual methods, reliant on
human expertise, suffer from inefficiencies and are limited to an unsatisfied
planning-construction consistency. Existing AI tools, despite improving
efficiency in certain aspects, still struggle to meet the dynamic network
conditions and multi-objective needs of telecom operators' networks. To address
these challenges, we propose TelePlanNet, an AI-driven framework tailored for
the selection of base station sites, integrating a three-layer architecture for
efficient planning and large-scale automation. By leveraging large language
models (LLMs) for real-time user input processing and intent alignment with
base station planning, combined with training the planning model using the
improved group relative policy optimization (GRPO) reinforcement learning, the
proposed TelePlanNet can effectively address multi-objective optimization,
evaluates candidate sites, and delivers practical solutions. Experiments
results show that the proposed TelePlanNet can improve the consistency to 78%,
which is superior to the manual methods, providing telecom operators with an
efficient and scalable tool that significantly advances cellular network
planning.

</details>


### [162] [A Challenge to Build Neuro-Symbolic Video Agents](https://arxiv.org/abs/2505.13851)
*Sahil Shah,Harsh Goel,Sai Shankar Narasimhan,Minkyu Choi,S P Sharan,Oguzhan Akcin,Sandeep Chinchali*

Main category: cs.AI

TL;DR: 现代视频理解系统在场景分类、目标检测和短视频检索等任务上表现出色，但在时间推理方面存在局限，需要发展能够推理事件并采取行动的智能视频代理。


<details>
  <summary>Details</summary>
Motivation: 随着视频分析在现实应用中的重要性增加，需要开发能够理解事件序列并做出决策的主动视频代理，而当前深度学习模型在时间推理方面存在不足。

Method: 提出一种神经符号方法，将视频查询分解为原子事件，构建连贯序列，并通过时间约束验证，以增强可解释性和结构化推理。

Result: 提出一个研究挑战：开发下一代智能视频代理，整合自主视频搜索与分析、无缝现实交互和高级内容生成三大核心能力。

Conclusion: 通过神经符号方法和三大核心能力的整合，可以从被动感知转向能够推理、预测和行动的智能视频代理，推动视频理解的边界。

Abstract: Modern video understanding systems excel at tasks such as scene
classification, object detection, and short video retrieval. However, as video
analysis becomes increasingly central to real-world applications, there is a
growing need for proactive video agents for the systems that not only interpret
video streams but also reason about events and take informed actions. A key
obstacle in this direction is temporal reasoning: while deep learning models
have made remarkable progress in recognizing patterns within individual frames
or short clips, they struggle to understand the sequencing and dependencies of
events over time, which is critical for action-driven decision-making.
Addressing this limitation demands moving beyond conventional deep learning
approaches. We posit that tackling this challenge requires a neuro-symbolic
perspective, where video queries are decomposed into atomic events, structured
into coherent sequences, and validated against temporal constraints. Such an
approach can enhance interpretability, enable structured reasoning, and provide
stronger guarantees on system behavior, all key properties for advancing
trustworthy video agents. To this end, we present a grand challenge to the
research community: developing the next generation of intelligent video agents
that integrate three core capabilities: (1) autonomous video search and
analysis, (2) seamless real-world interaction, and (3) advanced content
generation. By addressing these pillars, we can transition from passive
perception to intelligent video agents that reason, predict, and act, pushing
the boundaries of video understanding.

</details>


### [163] [Mobile-Agent-V: A Video-Guided Approach for Effortless and Efficient Operational Knowledge Injection in Mobile Automation](https://arxiv.org/abs/2505.13887)
*Junyang Wang,Haiyang Xu,Xi Zhang,Ming Yan,Ji Zhang,Fei Huang,Jitao Sang*

Main category: cs.AI

TL;DR: Mobile-Agent-V利用视频作为指导工具，自动将操作知识注入移动自动化流程，显著提升效率。


<details>
  <summary>Details</summary>
Motivation: 移动设备使用的激增需要高效的自动化任务管理，但现有AI框架因缺乏操作知识而表现不佳，手动编写知识又效率低下。

Method: 提出Mobile-Agent-V框架，通过视频内容直接获取操作知识，无需人工干预，并设计Mobile-Knowledge基准进行评估。

Result: 实验显示，Mobile-Agent-V性能比现有方法提升36%，验证了其在移动自动化中的高效优势。

Conclusion: Mobile-Agent-V通过视频自动注入知识，显著减少获取成本，为移动自动化提供了高效解决方案。

Abstract: The exponential rise in mobile device usage necessitates streamlined
automation for effective task management, yet many AI frameworks fall short due
to inadequate operational expertise. While manually written knowledge can
bridge this gap, it is often burdensome and inefficient. We introduce
Mobile-Agent-V, an innovative framework that utilizes video as a guiding tool
to effortlessly and efficiently inject operational knowledge into mobile
automation processes. By deriving knowledge directly from video content,
Mobile-Agent-V eliminates manual intervention, significantly reducing the
effort and time required for knowledge acquisition. To rigorously evaluate this
approach, we propose Mobile-Knowledge, a benchmark tailored to assess the
impact of external knowledge on mobile agent performance. Our experimental
findings demonstrate that Mobile-Agent-V enhances performance by 36% compared
to existing methods, underscoring its effortless and efficient advantages in
mobile automation.

</details>


### [164] [Efficient Agent Training for Computer Use](https://arxiv.org/abs/2505.13909)
*Yanheng He,Jiahe Jin,Pengfei Liu*

Main category: cs.AI

TL;DR: PC Agent-E框架通过少量高质量轨迹数据和AI合成数据，显著提升计算机使用代理的性能，减少对人类演示数据的依赖。


<details>
  <summary>Details</summary>
Motivation: 解决高质量轨迹数据稀缺对开发类人计算机使用代理的限制问题。

Method: 使用312条人工标注轨迹，结合Claude 3.7 Sonnet合成多样化动作决策，提升数据质量并训练模型。

Result: PC Agent-E在WindowsAgentArena-V2上相对性能提升141%，并在OSWorld上展示出强泛化能力。

Conclusion: 少量高质量轨迹数据足以激发强大的计算机使用能力。

Abstract: Scaling up high-quality trajectory data has long been a critical bottleneck
for developing human-like computer use agents. We introduce PC Agent-E, an
efficient agent training framework that significantly reduces reliance on
large-scale human demonstrations. Starting with just 312 human-annotated
computer use trajectories, we further improved data quality by synthesizing
diverse action decisions with Claude 3.7 Sonnet. Trained on these enriched
trajectories, our PC Agent-E model achieved a remarkable 141% relative
improvement, surpassing the strong Claude 3.7 Sonnet with extended thinking on
WindowsAgentArena-V2, an improved benchmark we also released. Furthermore, PC
Agent-E demonstrates strong generalizability to different operating systems on
OSWorld. Our findings suggest that strong computer use capabilities can be
stimulated from a small amount of high-quality trajectory data.

</details>


### [165] [Parallel Belief Revision via Order Aggregation](https://arxiv.org/abs/2505.13914)
*Jake Chandler,Richard Booth*

Main category: cs.AI

TL;DR: 该论文提出了一种基于TeamQueue聚合器的通用方法，用于将串行迭代信念修正算子扩展到并行变化，以统一解释相关合理性公设。


<details>
  <summary>Details</summary>
Motivation: 尽管已有研究探讨了单步并行修正的约束条件，但如何将模型扩展到迭代情况的研究很少。Delgrande & Jin提出了相关合理性公设，但缺乏统一的解释基础。

Method: 利用迭代并行收缩的最新研究，提出了一种基于TeamQueue聚合器家族的通用方法，用于扩展串行迭代信念修正算子以处理并行变化。

Result: 该方法能系统地恢复文献中独立合理的性质，同时避免产生可疑的结果，为并行修正提供了原则性解决方案。

Conclusion: 通过TeamQueue聚合器的方法，论文为迭代并行信念修正提供了统一的理论框架，填补了现有研究的空白。

Abstract: Despite efforts to better understand the constraints that operate on
single-step parallel (aka "package", "multiple") revision, very little work has
been carried out on how to extend the model to the iterated case. A recent
paper by Delgrande & Jin outlines a range of relevant rationality postulates.
While many of these are plausible, they lack an underlying unifying
explanation. We draw on recent work on iterated parallel contraction to offer a
general method for extending serial iterated belief revision operators to
handle parallel change. This method, based on a family of order aggregators
known as TeamQueue aggregators, provides a principled way to recover the
independently plausible properties that can be found in the literature, without
yielding the more dubious ones.

</details>


### [166] [DrugPilot: LLM-based Parameterized Reasoning Agent for Drug Discovery](https://arxiv.org/abs/2505.13940)
*Kun Li,Zhennan Wu,Shoupeng Wang,Wenbin Hu*

Main category: cs.AI

TL;DR: 本文提出DrugPilot，一种基于LLM的药物发现智能体，通过参数化推理架构解决多模态数据处理、领域知识更新延迟等挑战，在8项药物发现任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在药物发现领域面临多模态异构数据处理困难、领域知识更新滞后、复杂任务预测置信度不足等挑战，亟需创新解决方案。

Method: 开发参数化记忆池标准化药物数据表示，构建8类药物指令数据集进行微调，采用参数化推理架构替代传统端到端预测。

Result: 在Berkeley评估框架下，DrugPilot在简单/多重/多轮任务中分别达到98.0%、93.5%和64.0%的完成率，超越ReAct等现有智能体。

Conclusion: DrugPilot通过参数化技术有效提升LLM在药物发现中的工具调用能力，为多阶段研究任务提供自动化支持。

Abstract: In the field of AI4Science, large-scale language models (LLMs) show great
potential to parse complex scientific semantics, integrate cross-disciplinary
knowledge, and assist critical task research. However, in the field of drug
discovery, despite the optimization through professional data pre-training,
context window expansion, and internet search, the existing LLMs are still
facing challenges such as massive multi-modal and heterogeneous data
processing, domain knowledge dynamic updating delay, and insufficient
confidence in predicting the results of complex computational tasks. To address
these challenges, we propose the DrugPilot, an LLM-based agent with
parameterized reasoning for drug discovery. DrugPilot addresses key limitations
of traditional end-to-end LLM prediction approaches through its parametric
inference architecture. This agent system supports major phases of the drug
discovery pipeline, facilitating automated planning and execution of
multi-stage research tasks. To address the critical challenge of multi-modal
drug data analysis (incorporating both public datasets and user-submitted
data), we developed an interactive parameterized memory pool. This innovative
component standardizes real-world drug data into parametric representations,
simultaneously enabling efficient knowledge retrieval in multi-turn dialogue
while mitigating the information loss inherent in text-based data transmission.
Additionally, we created a drug instruct dataset across 8 essential drug
discovery tasks for model fine-tuning and evaluation. Based on the Berkeley
function calling evaluation framework, DrugPilot demonstrated the most advanced
tool calling capabilities on our drug discovery tool instruction dataset,
outperforming existing agents (e.g., ReAct, LoT). Specifically, it achieves
task completion rates of 98.0%, 93.5%, and 64.0% on simple, multiple, and
multi-turn tasks, respectively.

</details>


### [167] [Visual Instruction Bottleneck Tuning](https://arxiv.org/abs/2505.13946)
*Changdae Oh,Jiatong Li,Shawn Im,Yixuan Li*

Main category: cs.AI

TL;DR: 提出Vittle方法，通过信息瓶颈原理提升多模态大语言模型在分布偏移下的鲁棒性，无需额外数据或更大模型架构。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在面对分布偏移时的性能下降问题，现有方法通常需要更多数据或更大模型，成本较高。

Method: 基于信息瓶颈原理，推导出变分下界并设计Vittle方法，从表示学习角度提升模型鲁棒性。

Result: 在45个数据集（含30种偏移场景）上的实验表明，Vittle能持续提升模型在分布偏移下的表现。

Conclusion: Vittle通过追求最小充分表示学习，有效增强了多模态大语言模型在分布偏移下的鲁棒性。

Abstract: Despite widespread adoption, multimodal large language models (MLLMs) suffer
performance degradation when encountering unfamiliar queries under distribution
shifts. Existing methods to improve MLLM generalization typically require
either more instruction data or larger advanced model architectures, both of
which incur non-trivial human labor or computational costs. In this work, we
take an alternative approach to enhance the robustness of MLLMs under
distribution shifts, from a representation learning perspective. Inspired by
the information bottleneck (IB) principle, we derive a variational lower bound
of the IB for MLLMs and devise a practical implementation, Visual Instruction
Bottleneck Tuning (Vittle). We then provide a theoretical justification of
Vittle by revealing its connection to an information-theoretic robustness
metric of MLLM. Empirical validation of three MLLMs on open-ended and
closed-form question answering and object hallucination detection tasks over 45
datasets, including 30 shift scenarios, demonstrates that Vittle consistently
improves the MLLM's robustness under shifts by pursuing the learning of a
minimal sufficient representation.

</details>


### [168] [Solving Normalized Cut Problem with Constrained Action Space](https://arxiv.org/abs/2505.13986)
*Qize Jiang,Linsey Pang,Alice Gatti,Mahima Aggarwa,Giovanna Vantin,Xiaosong Ma,Weiwei Sun,Sanjay Chawla*

Main category: cs.AI

TL;DR: 提出了一种基于强化学习的组合优化方法，通过约束动作空间引导归一化切割问题接近预定义模板，以交通网络为例验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 强化学习在组合优化中表现出色，但如何融入领域知识以引导解决方案仍具挑战性。本文旨在解决这一问题。

Method: 采用约束动作空间的强化学习方法，结合Wedge和Ring Transformer生成符合模板的图分割方案。

Result: 在交通网络领域实现了接近自然最优分割的楔形和环形图分割，方法具有跨领域通用性。

Conclusion: 该方法成功将领域知识融入强化学习，为组合优化问题提供了新的解决思路，并具备推广潜力。

Abstract: Reinforcement Learning (RL) has emerged as an important paradigm to solve
combinatorial optimization problems primarily due to its ability to learn
heuristics that can generalize across problem instances. However, integrating
external knowledge that will steer combinatorial optimization problem solutions
towards domain appropriate outcomes remains an extremely challenging task. In
this paper, we propose the first RL solution that uses constrained action
spaces to guide the normalized cut problem towards pre-defined template
instances. Using transportation networks as an example domain, we create a
Wedge and Ring Transformer that results in graph partitions that are shaped in
form of Wedges and Rings and which are likely to be closer to natural optimal
partitions. However, our approach is general as it is based on principles that
can be generalized to other domains.

</details>


### [169] [Divide by Question, Conquer by Agent: SPLIT-RAG with Question-Driven Graph Partitioning](https://arxiv.org/abs/2505.13994)
*Ruiyi Yang,Hao Xue,Imran Razzak,Hakim Hacid,Flora D. Salim*

Main category: cs.AI

TL;DR: SPLIT-RAG提出多智能体RAG框架，通过语义图分割和协作子图检索解决大知识图谱的效率-精度权衡问题。


<details>
  <summary>Details</summary>
Motivation: 现有RAG系统在大规模知识图谱上存在效率与精度的权衡问题，简单查询延迟高，复杂多跳查询推理碎片化。

Method: 采用问题驱动的语义图分割技术，将知识图谱划分为语义连贯的子图，分配轻量级LLM智能体，仅激活相关分区检索，并通过分层合并模块解决答案不一致问题。

Result: 实验验证表明，相比现有方法，SPLIT-RAG在效率和精度上均有显著提升。

Conclusion: SPLIT-RAG框架通过多智能体协作和语义分区，有效优化了大规模知识图谱检索的效率与精度。

Abstract: Retrieval-Augmented Generation (RAG) systems empower large language models
(LLMs) with external knowledge, yet struggle with efficiency-accuracy
trade-offs when scaling to large knowledge graphs. Existing approaches often
rely on monolithic graph retrieval, incurring unnecessary latency for simple
queries and fragmented reasoning for complex multi-hop questions. To address
these challenges, this paper propose SPLIT-RAG, a multi-agent RAG framework
that addresses these limitations with question-driven semantic graph
partitioning and collaborative subgraph retrieval. The innovative framework
first create Semantic Partitioning of Linked Information, then use the
Type-Specialized knowledge base to achieve Multi-Agent RAG. The attribute-aware
graph segmentation manages to divide knowledge graphs into semantically
coherent subgraphs, ensuring subgraphs align with different query types, while
lightweight LLM agents are assigned to partitioned subgraphs, and only relevant
partitions are activated during retrieval, thus reduce search space while
enhancing efficiency. Finally, a hierarchical merging module resolves
inconsistencies across subgraph-derived answers through logical verifications.
Extensive experimental validation demonstrates considerable improvements
compared to existing approaches.

</details>


### [170] [VeRecycle: Reclaiming Guarantees from Probabilistic Certificates for Stochastic Dynamical Systems after Change](https://arxiv.org/abs/2505.14001)
*Sterre Lutz,Matthijs T. J. Spaan,Anna Lukina*

Main category: cs.AI

TL;DR: 论文提出VeRecycle框架，用于在系统动态局部变化时高效复用概率安全证书，避免完全重新认证的高成本。


<details>
  <summary>Details</summary>
Motivation: 现实中的自主系统面临各种不确定性，当系统动态超出建模范围时（如遇到未知障碍），现有方法需完全重新认证概率安全证书，计算成本高昂。

Method: 提出VeRecycle框架，通过理论证明和算法实现，在系统动态仅部分状态变化时，局部复用原有概率证书。

Result: 实验表明VeRecycle能显著节省计算开销，并在组合神经控制中保持有竞争力的概率安全保证。

Conclusion: VeRecycle为随机动力系统的安全认证提供了首个可局部复用的高效解决方案，平衡了计算成本与安全性。

Abstract: Autonomous systems operating in the real world encounter a range of
uncertainties. Probabilistic neural Lyapunov certification is a powerful
approach to proving safety of nonlinear stochastic dynamical systems. When
faced with changes beyond the modeled uncertainties, e.g., unidentified
obstacles, probabilistic certificates must be transferred to the new system
dynamics. However, even when the changes are localized in a known part of the
state space, state-of-the-art requires complete re-certification, which is
particularly costly for neural certificates. We introduce VeRecycle, the first
framework to formally reclaim guarantees for discrete-time stochastic dynamical
systems. VeRecycle efficiently reuses probabilistic certificates when the
system dynamics deviate only in a given subset of states. We present a general
theoretical justification and algorithmic implementation. Our experimental
evaluation shows scenarios where VeRecycle both saves significant computational
effort and achieves competitive probabilistic guarantees in compositional
neural control.

</details>


### [171] [Disentangled Multi-span Evolutionary Network against Temporal Knowledge Graph Reasoning](https://arxiv.org/abs/2505.14020)
*Hao Dong,Ziyue Qiao,Zhiyuan Ning,Qi Hao,Yi Du,Pengyang Wang,Yuanchun Zhou*

Main category: cs.AI

TL;DR: 本文提出了一种名为DiMNet的新方法，用于时序知识图谱（TKG）推理，通过多跨度进化策略和解耦组件，显著提升了推理性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在建模时序知识图谱时，忽略了子图间的内部结构交互以及潜在的平滑特征，这限制了推理性能的提升。

Method: DiMNet采用多跨度进化策略捕捉局部邻居特征和历史语义信息，并通过解耦组件动态分离节点的活跃和稳定特征。

Result: 在四个真实TKG数据集上的实验表明，DiMNet在推理任务中表现优异，MRR指标最高提升22.7%。

Conclusion: DiMNet通过有效建模子图交互和语义变化，显著提升了时序知识图谱的推理能力。

Abstract: Temporal Knowledge Graphs (TKGs), as an extension of static Knowledge Graphs
(KGs), incorporate the temporal feature to express the transience of knowledge
by describing when facts occur. TKG extrapolation aims to infer possible future
facts based on known history, which has garnered significant attention in
recent years. Some existing methods treat TKG as a sequence of independent
subgraphs to model temporal evolution patterns, demonstrating impressive
reasoning performance. However, they still have limitations: 1) In modeling
subgraph semantic evolution, they usually neglect the internal structural
interactions between subgraphs, which are actually crucial for encoding TKGs.
2) They overlook the potential smooth features that do not lead to semantic
changes, which should be distinguished from the semantic evolution process.
Therefore, we propose a novel Disentangled Multi-span Evolutionary Network
(DiMNet) for TKG reasoning. Specifically, we design a multi-span evolution
strategy that captures local neighbor features while perceiving historical
neighbor semantic information, thus enabling internal interactions between
subgraphs during the evolution process. To maximize the capture of semantic
change patterns, we design a disentangle component that adaptively separates
nodes' active and stable features, used to dynamically control the influence of
historical semantics on future evolution. Extensive experiments conducted on
four real-world TKG datasets show that DiMNet demonstrates substantial
performance in TKG reasoning, and outperforms the state-of-the-art up to 22.7%
in MRR.

</details>


### [172] [ProMind-LLM: Proactive Mental Health Care via Causal Reasoning with Sensor Data](https://arxiv.org/abs/2505.14038)
*Xinzhe Zheng,Sijie Ji,Jiawei Sun,Renqi Chen,Wei Gao,Mani Srivastava*

Main category: cs.AI

TL;DR: ProMind-LLM提出了一种结合主观心理记录和客观行为数据的创新方法，通过领域特定预训练、自优化机制和因果链式推理，提升了心理健康风险评估的可靠性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有心理健康风险评估方法主要依赖主观文本记录，易受心理不确定性影响，导致预测不一致和不可靠。需要结合客观行为数据以提高评估的稳健性。

Method: ProMind-LLM整合了领域特定预训练、自优化机制处理数值行为数据，以及因果链式推理，以增强预测的可靠性和可解释性。

Result: 在PMData和Globem两个真实数据集上的评估显示，ProMind-LLM相比通用大语言模型有显著改进。

Conclusion: ProMind-LLM为心理健康解决方案提供了更可靠、可解释和可扩展的途径。

Abstract: Mental health risk is a critical global public health challenge,
necessitating innovative and reliable assessment methods. With the development
of large language models (LLMs), they stand out to be a promising tool for
explainable mental health care applications. Nevertheless, existing approaches
predominantly rely on subjective textual mental records, which can be distorted
by inherent mental uncertainties, leading to inconsistent and unreliable
predictions. To address these limitations, this paper introduces ProMind-LLM.
We investigate an innovative approach integrating objective behavior data as
complementary information alongside subjective mental records for robust mental
health risk assessment. Specifically, ProMind-LLM incorporates a comprehensive
pipeline that includes domain-specific pretraining to tailor the LLM for mental
health contexts, a self-refine mechanism to optimize the processing of
numerical behavioral data, and causal chain-of-thought reasoning to enhance the
reliability and interpretability of its predictions. Evaluations of two
real-world datasets, PMData and Globem, demonstrate the effectiveness of our
proposed methods, achieving substantial improvements over general LLMs. We
anticipate that ProMind-LLM will pave the way for more dependable,
interpretable, and scalable mental health case solutions.

</details>


### [173] [Personalized Student Knowledge Modeling for Future Learning Resource Prediction](https://arxiv.org/abs/2505.14072)
*Soroush Hashemifar,Sherry Sahebi*

Main category: cs.AI

TL;DR: 提出KMaP模型，通过聚类学生画像实现个性化知识追踪与行为建模，解决现有方法在上下文信息丢失和非评估材料建模上的不足。


<details>
  <summary>Details</summary>
Motivation: 当前深度学习在教育领域的应用存在个性化不足、非评估学习活动建模不充分、知识获取与行为模式关联缺失等问题，且固定长度序列分割导致上下文信息丢失。

Method: 采用基于聚类的学生画像构建个性化表征，设计状态化多任务模型KMaP，同步建模学生知识与行为，预测未来学习资源偏好。

Result: 在真实数据集上验证了不同学生群体的行为差异，证实KMaP模型的有效性。

Conclusion: KMaP通过整合知识建模与行为预测，显著提升个性化学习支持的精准度。

Abstract: Despite advances in deep learning for education, student knowledge tracing
and behavior modeling face persistent challenges: limited personalization,
inadequate modeling of diverse learning activities (especially non-assessed
materials), and overlooking the interplay between knowledge acquisition and
behavioral patterns. Practical limitations, such as fixed-size sequence
segmentation, frequently lead to the loss of contextual information vital for
personalized learning. Moreover, reliance on student performance on assessed
materials limits the modeling scope, excluding non-assessed interactions like
lectures. To overcome these shortcomings, we propose Knowledge Modeling and
Material Prediction (KMaP), a stateful multi-task approach designed for
personalized and simultaneous modeling of student knowledge and behavior. KMaP
employs clustering-based student profiling to create personalized student
representations, improving predictions of future learning resource preferences.
Extensive experiments on two real-world datasets confirm significant behavioral
differences across student clusters and validate the efficacy of the KMaP
model.

</details>


### [174] [Memory Assignment for Finite-Memory Strategies in Adversarial Patrolling Games](https://arxiv.org/abs/2505.14137)
*Vojtěch Kůr,Vít Musil,Vojtěch Řehák*

Main category: cs.AI

TL;DR: 该论文提出了一种迭代调整内存分配的方法，解决了有限记忆防御策略中内存分配难题，提升了对抗巡逻游戏的防御效果。


<details>
  <summary>Details</summary>
Motivation: 有限记忆防御策略在对抗巡逻游戏中表现优异，但内存分配问题一直是一个未解决的难题，阻碍了其实际应用。

Method: 开发了一种通用方法，通过迭代调整内存分配，可与任何黑盒策略优化工具结合使用。

Result: 实验表明该方法在各种巡逻模型实例中表现稳健，有效优化了防御策略。

Conclusion: 该方法成功解决了有限记忆策略中的内存分配问题，提升了防御效果，具有广泛适用性。

Abstract: Adversarial Patrolling games form a subclass of Security games where a
Defender moves between locations, guarding vulnerable targets. The main
algorithmic problem is constructing a strategy for the Defender that minimizes
the worst damage an Attacker can cause. We focus on the class of finite-memory
(also known as regular) Defender's strategies that experimentally outperformed
other competing classes. A finite-memory strategy can be seen as a positional
strategy on a finite set of states. Each state consists of a pair of a location
and a certain integer value--called memory. Existing algorithms improve the
transitional probabilities between the states but require that the available
memory size itself is assigned at each location manually. Choosing the right
memory assignment is a well-known open and hard problem that hinders the
usability of finite-memory strategies. We solve this issue by developing a
general method that iteratively changes the memory assignment. Our algorithm
can be used in connection with \emph{any} black-box strategy optimization tool.
We evaluate our method on various experiments and show its robustness by
solving instances of various patrolling models.

</details>


### [175] [RL of Thoughts: Navigating LLM Reasoning with Inference-time Reinforcement Learning](https://arxiv.org/abs/2505.14140)
*Qianyue Hao,Sibo Li,Jian Yuan,Yong Li*

Main category: cs.AI

TL;DR: 提出RLoT方法，通过强化学习训练轻量级导航模型，动态选择逻辑块增强LLM推理能力，显著提升性能并具备强迁移性。


<details>
  <summary>Details</summary>
Motivation: 现有推理时技术（如Chain/Tree/Graph-of-Thoughts）虽能提升大语言模型（LLM）推理性能，但缺乏任务适应性，需手动预定义通用框架。

Method: 设计五种基于人类认知的逻辑块，用强化学习训练轻量级导航模型（RL navigator），动态组合逻辑块构建任务专属推理结构。

Result: 在多个基准测试（AIME/MATH/GPQA等）和LLM（GPT/Llama/Qwen等）上，RLoT性能最高提升13.4%，3K参数的导航器可使10B级LLM媲美100B级模型。

Conclusion: RLoT通过自适应逻辑组合显著增强LLM推理，导航器具备参数高效性和跨模型/任务的强迁移能力，代码已开源。

Abstract: Despite rapid advancements in large language models (LLMs), the token-level
autoregressive nature constrains their complex reasoning capabilities. To
enhance LLM reasoning, inference-time techniques, including
Chain/Tree/Graph-of-Thought(s), successfully improve the performance, as they
are fairly cost-effective by guiding reasoning through sophisticated logical
structures without modifying LLMs' parameters. However, these manually
predefined, task-agnostic frameworks are applied uniformly across diverse
tasks, lacking adaptability. To improve this, we propose RL-of-Thoughts (RLoT),
where we train a lightweight navigator model with reinforcement learning (RL)
to adaptively enhance LLM reasoning at inference time. Specifically, we design
five basic logic blocks from the perspective of human cognition. During the
reasoning process, the trained RL navigator dynamically selects the suitable
logic blocks and combines them into task-specific logical structures according
to problem characteristics. Experiments across multiple reasoning benchmarks
(AIME, MATH, GPQA, etc.) with multiple LLMs (GPT, Llama, Qwen, and DeepSeek)
illustrate that RLoT outperforms established inference-time techniques by up to
13.4%. Remarkably, with less than 3K parameters, our RL navigator is able to
make sub-10B LLMs comparable to 100B-scale counterparts. Moreover, the RL
navigator demonstrates strong transferability: a model trained on one specific
LLM-task pair can effectively generalize to unseen LLMs and tasks. Our code is
open-source at https://anonymous.4open.science/r/RL-LLM-Reasoning-1A30 for
reproducibility.

</details>


### [176] [Building a Stable Planner: An Extended Finite State Machine Based Planning Module for Mobile GUI Agent](https://arxiv.org/abs/2505.14141)
*Fanglin Mo,Junzhe Chen,Haoxuan Zhu,Xuming Hu*

Main category: cs.AI

TL;DR: 论文提出SPlanner模块，通过扩展有限状态机建模应用逻辑，将用户指令分解为可执行计划，显著提升移动GUI代理的任务成功率。


<details>
  <summary>Details</summary>
Motivation: 移动GUI代理在执行任务时因缺乏对应用功能的深入理解，容易在任务规划阶段迷失方向，导致执行失败。

Method: 使用扩展有限状态机(EFSM)建模应用逻辑，分解用户指令为功能序列，并利用LLM生成自然语言执行计划。

Result: 在AndroidWorld基准测试中，SPlanner使Qwen2.5-VL-72B模型的任务成功率提升28.8%，达到63.8%。

Conclusion: SPlanner通过结构化任务规划有效解决了移动GUI代理的迷失问题，为智能交互系统提供了实用解决方案。

Abstract: Mobile GUI agents execute user commands by directly interacting with the
graphical user interface (GUI) of mobile devices, demonstrating significant
potential to enhance user convenience. However, these agents face considerable
challenges in task planning, as they must continuously analyze the GUI and
generate operation instructions step by step. This process often leads to
difficulties in making accurate task plans, as GUI agents lack a deep
understanding of how to effectively use the target applications, which can
cause them to become "lost" during task execution. To address the task planning
issue, we propose SPlanner, a plug-and-play planning module to generate
execution plans that guide vision language model(VLMs) in executing tasks. The
proposed planning module utilizes extended finite state machines (EFSMs) to
model the control logits and configurations of mobile applications. It then
decomposes a user instruction into a sequence of primary function modeled in
EFSMs, and generate the execution path by traversing the EFSMs. We further
refine the execution path into a natural language plan using an LLM. The final
plan is concise and actionable, and effectively guides VLMs to generate
interactive GUI actions to accomplish user tasks. SPlanner demonstrates strong
performance on dynamic benchmarks reflecting real-world mobile usage. On the
AndroidWorld benchmark, SPlanner achieves a 63.8% task success rate when paired
with Qwen2.5-VL-72B as the VLM executor, yielding a 28.8 percentage point
improvement compared to using Qwen2.5-VL-72B without planning assistance.

</details>


### [177] [Multimodal Mixture of Low-Rank Experts for Sentiment Analysis and Emotion Recognition](https://arxiv.org/abs/2505.14143)
*Shuo Zhang,Jinsong Zhang,Zhejun Zhang,Lei Li*

Main category: cs.AI

TL;DR: 提出了一种名为MMoLRE的新型多任务学习方法，用于多模态情感分析和情绪识别，通过共享和任务特定专家模型避免参数冲突，并采用低秩专家网络减少计算开销。


<details>
  <summary>Details</summary>
Motivation: 现有的多任务学习方法主要采用硬参数共享，忽略了复杂任务相关性导致的参数冲突，因此需要一种能够有效建模任务共性和特性的方法。

Method: MMoLRE方法结合了共享和任务特定的专家模型，利用低秩专家网络减少参数和计算开销，从而高效建模多模态情感分析和情绪识别任务。

Result: 在CMU-MOSI和CMU-MOSEI基准测试中，MMoLRE在多模态情感分析任务上达到了最先进的性能，在情绪识别任务上表现竞争性。

Conclusion: MMoLRE通过有效建模任务共性和特性，避免了参数冲突，并在减少计算开销的同时提升了多模态情感分析和情绪识别的性能。

Abstract: Multi-task learning (MTL) enables the efficient transfer of extra knowledge
acquired from other tasks. The high correlation between multimodal sentiment
analysis (MSA) and multimodal emotion recognition (MER) supports their joint
training. However, existing methods primarily employ hard parameter sharing,
ignoring parameter conflicts caused by complex task correlations. In this
paper, we present a novel MTL method for MSA and MER, termed Multimodal Mixture
of Low-Rank Experts (MMoLRE). MMoLRE utilizes shared and task-specific experts
to distinctly model common and unique task characteristics, thereby avoiding
parameter conflicts. Additionally, inspired by low-rank structures in the
Mixture of Experts (MoE) framework, we design low-rank expert networks to
reduce parameter and computational overhead as the number of experts increases.
Extensive experiments on the CMU-MOSI and CMU-MOSEI benchmarks demonstrate that
MMoLRE achieves state-of-the-art performance on the MSA task and competitive
results on the MER task.

</details>


### [178] [s3: You Don't Need That Much Data to Train a Search Agent via RL](https://arxiv.org/abs/2505.14146)
*Pengcheng Jiang,Xueqiang Xu,Jiacheng Lin,Jinfeng Xiao,Zifeng Wang,Jimeng Sun,Jiawei Han*

Main category: cs.AI

TL;DR: 提出s3框架，通过解耦搜索与生成并优化搜索奖励，显著提升检索增强生成性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法要么忽略下游任务效用，要么因联合优化导致检索与生成耦合，限制了实际应用。

Method: s3框架将搜索器与生成器解耦，使用超越RAG的生成准确率提升作为奖励训练搜索器。

Result: 仅需2.4k训练样本即超越基线模型，在11个QA基准上表现更优。

Conclusion: s3以轻量级、模型无关的方式显著提升了检索增强生成系统的实用性和兼容性。

Abstract: Retrieval-augmented generation (RAG) systems empower large language models
(LLMs) to access external knowledge during inference. Recent advances have
enabled LLMs to act as search agents via reinforcement learning (RL), improving
information acquisition through multi-turn interactions with retrieval engines.
However, existing approaches either optimize retrieval using search-only
metrics (e.g., NDCG) that ignore downstream utility or fine-tune the entire LLM
to jointly reason and retrieve-entangling retrieval with generation and
limiting the real search utility and compatibility with frozen or proprietary
models. In this work, we propose s3, a lightweight, model-agnostic framework
that decouples the searcher from the generator and trains the searcher using a
Gain Beyond RAG reward: the improvement in generation accuracy over naive RAG.
s3 requires only 2.4k training samples to outperform baselines trained on over
70x more data, consistently delivering stronger downstream performance across
six general QA and five medical QA benchmarks.

</details>


### [179] [SHARP: Synthesizing High-quality Aligned Reasoning Problems for Large Reasoning Models Reinforcement Learning](https://arxiv.org/abs/2505.14147)
*Xiong Jun Wu,Zhenduo Zhang,ZuJie Wen,Zhiqiang Zhang,Wang Ren,Lei Shi,Cai Chen,Deng Zhao,Dingnan Jin,Qing Cui,Jun Zhou*

Main category: cs.AI

TL;DR: SHARP方法通过合成高质量、可验证的STEM问题集，提升大型推理模型的复杂推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法生成的STEM问题过于简单或不可验证，限制了大型推理模型的训练效果。

Method: SHARP采用自对齐原则和三阶段框架（对齐、实例化、推理），结合强化学习优化模型推理。

Result: 在GPQA等基准测试中，SHARP显著提升了模型的复杂推理准确率，接近专家水平。

Conclusion: SHARP为大型推理模型的训练提供了高质量问题集，有效提升了复杂任务的表现。

Abstract: Training large reasoning models (LRMs) with reinforcement learning in STEM
domains is hindered by the scarcity of high-quality, diverse, and verifiable
problem sets. Existing synthesis methods, such as Chain-of-Thought prompting,
often generate oversimplified or uncheckable data, limiting model advancement
on complex tasks. To address these challenges, we introduce SHARP, a unified
approach to Synthesizing High-quality Aligned Reasoning Problems for LRMs
reinforcement learning with verifiable rewards (RLVR). SHARP encompasses a
strategic set of self-alignment principles -- targeting graduate and
Olympiad-level difficulty, rigorous logical consistency, and unambiguous,
verifiable answers -- and a structured three-phase framework (Alignment,
Instantiation, Inference) that ensures thematic diversity and fine-grained
control over problem generation. We implement SHARP by leveraging a
state-of-the-art LRM to infer and verify challenging STEM questions, then
employ a reinforcement learning loop to refine the model's reasoning through
verifiable reward signals. Experiments on benchmarks such as GPQA demonstrate
that SHARP-augmented training substantially outperforms existing methods,
markedly improving complex reasoning accuracy and pushing LRM performance
closer to expert-level proficiency. Our contributions include the SHARP
strategy, framework design, end-to-end implementation, and experimental
evaluation of its effectiveness in elevating LRM reasoning capabilities.

</details>


### [180] [MM-Agent: LLM as Agents for Real-world Mathematical Modeling Problem](https://arxiv.org/abs/2505.14148)
*Fan Liu,Zherui Yang,Cancheng Liu,Tianrui Song,Xiaofeng Gao,Hao Liu*

Main category: cs.AI

TL;DR: 论文提出MM-Agent框架，通过四阶段建模方法提升LLM在数学建模任务中的表现，并在MCM/ICM竞赛中验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型（LLM）在严格数学模型构建方面存在不足，限制了其在实际问题解决中的应用。

Method: 提出MM-Agent框架，将数学建模分解为开放性问题分析、结构化模型构建、计算求解和报告生成四个阶段。

Result: MM-Agent在MM-Bench基准上比基线模型提升11.88%，并在MCM/ICM 2025中协助两支本科队伍获得决赛奖（前2%）。

Conclusion: MM-Agent作为建模辅助工具具有实际效果，能显著提升数学建模任务的解决效率和质量。

Abstract: Mathematical modeling is a cornerstone of scientific discovery and
engineering practice, enabling the translation of real-world problems into
formal systems across domains such as physics, biology, and economics. Unlike
mathematical reasoning, which assumes a predefined formulation, modeling
requires open-ended problem analysis, abstraction, and principled
formalization. While Large Language Models (LLMs) have shown strong reasoning
capabilities, they fall short in rigorous model construction, limiting their
utility in real-world problem-solving. To this end, we formalize the task of
LLM-powered real-world mathematical modeling, where agents must analyze
problems, construct domain-appropriate formulations, and generate complete
end-to-end solutions. We introduce MM-Bench, a curated benchmark of 111
problems from the Mathematical Contest in Modeling (MCM/ICM), spanning the
years 2000 to 2025 and across ten diverse domains such as physics, biology, and
economics. To tackle this task, we propose MM-Agent, an expert-inspired
framework that decomposes mathematical modeling into four stages: open-ended
problem analysis, structured model formulation, computational problem solving,
and report generation. Experiments on MM-Bench show that MM-Agent significantly
outperforms baseline agents, achieving an 11.88\% improvement over human expert
solutions while requiring only 15 minutes and \$0.88 per task using GPT-4o.
Furthermore, under official MCM/ICM protocols, MM-Agent assisted two
undergraduate teams in winning the Finalist Award (\textbf{top 2.0\% among
27,456 teams}) in MCM/ICM 2025, demonstrating its practical effectiveness as a
modeling copilot. Our code is available at
https://github.com/usail-hkust/LLM-MM-Agent

</details>


### [181] [DSMentor: Enhancing Data Science Agents with Curriculum Learning and Online Knowledge Accumulation](https://arxiv.org/abs/2505.14163)
*He Wang,Alexander Hanbo Li,Yiqun Hu,Sheng Zhang,Hideo Kobayashi,Jiani Zhang,Henry Zhu,Chung-Wei Hang,Patrick Ng*

Main category: cs.AI

TL;DR: 论文提出DSMentor框架，通过课程学习策略优化LLM代理在数据科学任务中的表现，显著提升通过率和因果推理能力。


<details>
  <summary>Details</summary>
Motivation: 当前研究主要关注通过改进搜索、采样和规划技术来增强上下文学习，而忽视了问题解决顺序在推理时的重要性。

Method: 开发DSMentor框架，采用课程学习策略，按难度递增组织任务，并结合长期记忆保留先前经验，指导代理学习进程。

Result: DSMentor使用Claude-3.5-Sonnet在DSEval和QRData基准上通过率提升达5.2%，因果推理问题通过率比GPT-4提高8.8%。

Conclusion: 研究强调在推理时积累和利用知识的重要性，通过基于课程的推理优化为提升LLM性能开辟了新途径。

Abstract: Large language model (LLM) agents have shown promising performance in
generating code for solving complex data science problems. Recent studies
primarily focus on enhancing in-context learning through improved search,
sampling, and planning techniques, while overlooking the importance of the
order in which problems are tackled during inference. In this work, we develop
a novel inference-time optimization framework, referred to as DSMentor, which
leverages curriculum learning -- a strategy that introduces simpler task first
and progressively moves to more complex ones as the learner improves -- to
enhance LLM agent performance in challenging data science tasks. Our
mentor-guided framework organizes data science tasks in order of increasing
difficulty and incorporates a growing long-term memory to retain prior
experiences, guiding the agent's learning progression and enabling more
effective utilization of accumulated knowledge. We evaluate DSMentor through
extensive experiments on DSEval and QRData benchmarks. Experiments show that
DSMentor using Claude-3.5-Sonnet improves the pass rate by up to 5.2% on DSEval
and QRData compared to baseline agents. Furthermore, DSMentor demonstrates
stronger causal reasoning ability, improving the pass rate by 8.8% on the
causality problems compared to GPT-4 using Program-of-Thoughts prompts. Our
work underscores the importance of developing effective strategies for
accumulating and utilizing knowledge during inference, mirroring the human
learning process and opening new avenues for improving LLM performance through
curriculum-based inference optimization.

</details>


### [182] [Dynamic Replanning for Improved Public Transport Routing](https://arxiv.org/abs/2505.14193)
*Abdallah Abuaisha,Bojie Shen,Daniel Harabor,Peter Stuckey,Mark Wallace*

Main category: cs.AI

TL;DR: 论文提出两种动态重新规划公共交通路线的方法，主动推送方案优于手动请求方案，显著缩短到达时间。


<details>
  <summary>Details</summary>
Motivation: 公共交通延误常见，现有解决方案有限，无法充分利用实时延误数据进行系统级动态重新规划。

Method: 提出两种解决方案：用户手动请求重新规划的“拉取”方法和服务器主动监控调整行程的“推送”方法。

Result: 实验表明，推送方法优于拉取方法，实现显著加速，动态重新规划带来大量到达时间节省。

Conclusion: 主动推送的动态重新规划方法能有效利用实时数据，优化公共交通路线，提升用户体验。

Abstract: Delays in public transport are common, often impacting users through
prolonged travel times and missed transfers. Existing solutions for handling
delays remain limited; backup plans based on historical data miss opportunities
for earlier arrivals, while snapshot planning accounts for current delays but
not future ones. With the growing availability of live delay data, users can
adjust their journeys in real-time. However, the literature lacks a framework
that fully exploits this advantage for system-scale dynamic replanning. To
address this, we formalise the dynamic replanning problem in public transport
routing and propose two solutions: a "pull" approach, where users manually
request replanning, and a novel "push" approach, where the server proactively
monitors and adjusts journeys. Our experiments show that the push approach
outperforms the pull approach, achieving significant speedups. The results also
reveal substantial arrival time savings enabled by dynamic replanning.

</details>


### [183] [Embedded Mean Field Reinforcement Learning for Perimeter-defense Game](https://arxiv.org/abs/2505.14209)
*Li Wang,Xin Yu,Xuxin Lv,Gangzheng Ai,Wenjun Wu*

Main category: cs.AI

TL;DR: 该论文研究了三维环境下的大规模异构边界防御博弈，提出了EMFAC框架以解决防御策略中的大规模异构控制挑战，并通过仿真和实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 随着无人机和导弹技术的快速发展，攻击者与防御者之间的边界防御博弈在多个领域中变得越来越复杂和重要。然而，现有研究主要集中于小规模、简化的二维场景，忽略了现实环境中的扰动、运动动力学和异构性等因素，这些因素对实际应用构成了重大挑战。

Method: 论文提出了嵌入式均值场行动者-评论家（EMFAC）框架，利用表示学习实现高层动作的均值场聚合，支持防御者之间的可扩展协调。此外，还引入了基于奖励表示的轻量级代理级注意力机制，选择性过滤观测和均值场信息，以提升决策效率并加速大规模任务的收敛。

Result: 通过不同规模的仿真实验，EMFAC在收敛速度和整体性能上均优于现有基线方法。小规模实际实验进一步验证了该框架的实用性。

Conclusion: EMFAC框架在复杂场景中表现出色，能够有效应对大规模异构边界防御博弈的挑战，为实际应用提供了有力支持。

Abstract: With the rapid advancement of unmanned aerial vehicles (UAVs) and missile
technologies, perimeter-defense game between attackers and defenders for the
protection of critical regions have become increasingly complex and
strategically significant across a wide range of domains. However, existing
studies predominantly focus on small-scale, simplified two-dimensional
scenarios, often overlooking realistic environmental perturbations, motion
dynamics, and inherent heterogeneity--factors that pose substantial challenges
to real-world applicability. To bridge this gap, we investigate large-scale
heterogeneous perimeter-defense game in a three-dimensional setting,
incorporating realistic elements such as motion dynamics and wind fields. We
derive the Nash equilibrium strategies for both attackers and defenders,
characterize the victory regions, and validate our theoretical findings through
extensive simulations. To tackle large-scale heterogeneous control challenges
in defense strategies, we propose an Embedded Mean-Field Actor-Critic (EMFAC)
framework. EMFAC leverages representation learning to enable high-level action
aggregation in a mean-field manner, supporting scalable coordination among
defenders. Furthermore, we introduce a lightweight agent-level attention
mechanism based on reward representation, which selectively filters
observations and mean-field information to enhance decision-making efficiency
and accelerate convergence in large-scale tasks. Extensive simulations across
varying scales demonstrate the effectiveness and adaptability of EMFAC, which
outperforms established baselines in both convergence speed and overall
performance. To further validate practicality, we test EMFAC in small-scale
real-world experiments and conduct detailed analyses, offering deeper insights
into the framework's effectiveness in complex scenarios.

</details>


### [184] [Reinforcement Learning vs. Distillation: Understanding Accuracy and Capability in LLM Reasoning](https://arxiv.org/abs/2505.14216)
*Minwu Kim,Anubhav Shrestha,Safal Shrestha,Aadim Nepal,Keith Ross*

Main category: cs.AI

TL;DR: 本文探讨了强化学习与可验证奖励（RLVR）和蒸馏技术对语言模型推理行为的影响，发现RLVR虽提升整体准确性但无助于能力提升，而蒸馏技术则在引入新知识时能同时提升两者。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于理解RLVR和蒸馏技术如何影响语言模型的推理行为，特别是为何RLVR能提高准确性却无法提升模型能力，而蒸馏技术则能在某些情况下同时改善两者。

Method: 研究方法包括分析RLVR对模型在不同难度问题上的表现影响，以及蒸馏技术如何通过引入新知识和学习强推理模式来提升模型性能。

Result: 研究结果显示，RLVR主要提升简单问题的准确性而牺牲难题表现，蒸馏技术则在引入新知识时能提升能力，否则效果与RLVR类似。

Conclusion: 结论指出，RLVR和蒸馏技术对模型推理行为的影响机制不同，蒸馏技术在引入新知识时更为有效，而RLVR则需更可靠的质量指标来评估响应质量。

Abstract: Recent studies have shown that reinforcement learning with verifiable rewards
(RLVR) enhances overall accuracy but fails to improve capability, while
distillation can improve both. In this paper, we investigate the mechanisms
behind these phenomena. First, we demonstrate that RLVR does not improve
capability because it focuses on improving the accuracy of the less-difficult
questions to the detriment of the accuracy of the most difficult questions,
thereby leading to no improvement in capability. Second, we find that RLVR does
not merely increase the success probability for the less difficult questions,
but in our small model settings produces quality responses that were absent in
its output distribution before training. In addition, we show these responses
are neither noticeably longer nor feature more reflection-related keywords,
underscoring the need for more reliable indicators of response quality. Third,
we show that while distillation reliably improves accuracy by learning strong
reasoning patterns, it only improves capability when new knowledge is
introduced. Moreover, when distilling only with reasoning patterns and no new
knowledge, the accuracy of the less-difficult questions improves to the
detriment of the most difficult questions, similar to RLVR. Together, these
findings offer a clearer understanding of how RLVR and distillation shape
reasoning behavior in language models.

</details>


### [185] [Toward Embodied AGI: A Review of Embodied AI and the Road Ahead](https://arxiv.org/abs/2505.14235)
*Yequan Wang,Aixin Sun*

Main category: cs.AI

TL;DR: 本文提出了一个五级（L1-L5）的具身通用人工智能（AGI）分类体系，回顾了基础阶段（L1-L2）的研究与挑战，并提出了实现更高级能力（L3-L5）的关键组件。基于这些见解和现有技术，作者提出了一个L3+机器人大脑的概念框架。


<details>
  <summary>Details</summary>
Motivation: 随着机器人和基础AI模型的进步，我们正迈向一个具身AI系统日益普及的新时代。本文旨在通过系统分类具身AGI，推动这一领域的研究与发展。

Method: 作者引入了一个五级的具身AGI分类体系（L1-L5），并回顾了基础阶段的研究与挑战。基于这些分析，提出了实现更高级能力的关键组件，并构建了一个L3+机器人大脑的概念框架。

Result: 提出了一个系统化的具身AGI分类体系，并基于现有技术设计了一个L3+机器人大脑的概念框架，为未来研究提供了技术展望和基础。

Conclusion: 本文通过分类体系和概念框架的提出，为具身AGI的研究提供了系统化的视角和未来探索的基础。

Abstract: Artificial General Intelligence (AGI) is often envisioned as inherently
embodied. With recent advances in robotics and foundational AI models, we stand
at the threshold of a new era-one marked by increasingly generalized embodied
AI systems. This paper contributes to the discourse by introducing a systematic
taxonomy of Embodied AGI spanning five levels (L1-L5). We review existing
research and challenges at the foundational stages (L1-L2) and outline the key
components required to achieve higher-level capabilities (L3-L5). Building on
these insights and existing technologies, we propose a conceptual framework for
an L3+ robotic brain, offering both a technical outlook and a foundation for
future exploration.

</details>


### [186] [EVA: Red-Teaming GUI Agents via Evolving Indirect Prompt Injection](https://arxiv.org/abs/2505.14289)
*Yijie Lu,Tianjie Ju,Manman Zhao,Xinbei Ma,Yuan Guo,ZhuoSheng Zhang*

Main category: cs.AI

TL;DR: 论文提出EVA框架，通过动态优化间接提示注入攻击，显著提高攻击成功率并揭示多模态GUI代理的共同漏洞。


<details>
  <summary>Details</summary>
Motivation: 随着多模态代理在图形用户界面(GUI)中执行任务，间接提示注入攻击（如通过视觉环境嵌入误导指令）成为新兴威胁，需开发有效对抗方法。

Method: 提出EVA红队框架，将攻击转化为闭环优化：持续监控代理对GUI的注意力分布，动态调整对抗性线索、关键词和布局。

Result: 在6种GUI代理的测试中，EVA攻击成功率显著高于静态方法，且攻击模式可跨模型迁移，揭示了代理决策中的共同行为偏差。

Conclusion: 动态间接提示注入是红队测试的有效工具，同时能暴露多模态代理的共性弱点，为安全研究提供新方向。

Abstract: As multimodal agents are increasingly trained to operate graphical user
interfaces (GUIs) to complete user tasks, they face a growing threat from
indirect prompt injection, attacks in which misleading instructions are
embedded into the agent's visual environment, such as popups or chat messages,
and misinterpreted as part of the intended task. A typical example is
environmental injection, in which GUI elements are manipulated to influence
agent behavior without directly modifying the user prompt. To address these
emerging attacks, we propose EVA, a red teaming framework for indirect prompt
injection which transforms the attack into a closed loop optimization by
continuously monitoring an agent's attention distribution over the GUI and
updating adversarial cues, keywords, phrasing, and layout, in response.
Compared with prior one shot methods that generate fixed prompts without regard
for how the model allocates visual attention, EVA dynamically adapts to
emerging attention hotspots, yielding substantially higher attack success rates
and far greater transferability across diverse GUI scenarios. We evaluate EVA
on six widely used generalist and specialist GUI agents in realistic settings
such as popup manipulation, chat based phishing, payments, and email
composition. Experimental results show that EVA substantially improves success
rates over static baselines. Under goal agnostic constraints, where the
attacker does not know the agent's task intent, EVA still discovers effective
patterns. Notably, we find that injection styles transfer well across models,
revealing shared behavioral biases in GUI agents. These results suggest that
evolving indirect prompt injection is a powerful tool not only for red teaming
agents, but also for uncovering common vulnerabilities in their multimodal
decision making.

</details>


### [187] [SafetyNet: Detecting Harmful Outputs in LLMs by Modeling and Monitoring Deceptive Behaviors](https://arxiv.org/abs/2505.14300)
*Maheep Chaudhary,Fazl Barez*

Main category: cs.AI

TL;DR: 提出了一种无监督实时监控框架Safety-Net，通过检测LLMs生成有害内容时的内部行为特征，以96%的准确率识别后门触发的有害输出。


<details>
  <summary>Details</summary>
Motivation: 借鉴核能、航空等高危行业的实时监控机制，针对大语言模型（LLMs）可能被特定输入触发生成暴力/色情/仇恨言论等有害内容的问题，需开发能捕捉真实因果指标且防止模型故意规避的监控系统。

Method: 采用无监督方法，将正常行为作为基线，通过分析模型生成有害内容时的内部行为特征（如线性/非线性表征切换、特征关系变化），构建多检测器框架Safety-Net监控不同表征维度。

Result: Safety-Net在检测后门触发有害内容时达到96%准确率，能有效识别模型通过改变表征空间逃避监控的欺骗行为。

Conclusion: LLMs生成有害内容存在因果性行为特征，Safety-Net通过多维度监控策略成功解决了模型欺骗问题，为AI安全提供了实时防护方案。

Abstract: High-risk industries like nuclear and aviation use real-time monitoring to
detect dangerous system conditions. Similarly, Large Language Models (LLMs)
need monitoring safeguards. We propose a real-time framework to predict harmful
AI outputs before they occur by using an unsupervised approach that treats
normal behavior as the baseline and harmful outputs as outliers. Our study
focuses specifically on backdoor-triggered responses -- where specific input
phrases activate hidden vulnerabilities causing the model to generate unsafe
content like violence, pornography, or hate speech. We address two key
challenges: (1) identifying true causal indicators rather than surface
correlations, and (2) preventing advanced models from deception -- deliberately
evading monitoring systems. Hence, we approach this problem from an
unsupervised lens by drawing parallels to human deception: just as humans
exhibit physical indicators while lying, we investigate whether LLMs display
distinct internal behavioral signatures when generating harmful content. Our
study addresses two critical challenges: 1) designing monitoring systems that
capture true causal indicators rather than superficial correlations; and
2)preventing intentional evasion by increasingly capable "Future models''. Our
findings show that models can produce harmful content through causal mechanisms
and can become deceptive by: (a) alternating between linear and non-linear
representations, and (b) modifying feature relationships. To counter this, we
developed Safety-Net -- a multi-detector framework that monitors different
representation dimensions, successfully detecting harmful behavior even when
information is shifted across representational spaces to evade individual
monitors. Our evaluation shows 96% accuracy in detecting harmful cases using
our unsupervised ensemble approach.

</details>


### [188] [Towards Embodied Cognition in Robots via Spatially Grounded Synthetic Worlds](https://arxiv.org/abs/2505.14366)
*Joel Currie,Gioele Migno,Enrico Piacenti,Maria Elena Giannaccini,Patric Bach,Davide De Tommaso,Agnieszka Wykowska*

Main category: cs.AI

TL;DR: 提出一个训练视觉语言模型进行视觉透视取样的概念框架，并发布合成数据集支持空间推理任务研究。


<details>
  <summary>Details</summary>
Motivation: 视觉透视取样（VPT）是人机交互（HRI）中实现具身认知的核心能力，当前缺乏相关训练数据和方法。

Method: 使用NVIDIA Omniverse生成合成数据集，包含RGB图像、自然语言描述和物体姿态的真实变换矩阵，以监督学习方式训练模型进行Z轴距离推理。

Result: 构建了支持空间推理任务的数据集，并公开可用，为未来扩展到6自由度推理奠定基础。

Conclusion: 该研究为实现具身AI系统在交互场景中的空间理解能力迈出了基础性一步。

Abstract: We present a conceptual framework for training Vision-Language Models (VLMs)
to perform Visual Perspective Taking (VPT), a core capability for embodied
cognition essential for Human-Robot Interaction (HRI). As a first step toward
this goal, we introduce a synthetic dataset, generated in NVIDIA Omniverse,
that enables supervised learning for spatial reasoning tasks. Each instance
includes an RGB image, a natural language description, and a ground-truth 4X4
transformation matrix representing object pose. We focus on inferring Z-axis
distance as a foundational skill, with future extensions targeting full 6
Degrees Of Freedom (DOFs) reasoning. The dataset is publicly available to
support further research. This work serves as a foundational step toward
embodied AI systems capable of spatial understanding in interactive human-robot
scenarios.

</details>


### [189] [SCAN: Semantic Document Layout Analysis for Textual and Visual Retrieval-Augmented Generation](https://arxiv.org/abs/2505.14381)
*Yuyang Dong,Nobuhiro Ueda,Krisztián Boros,Daiki Ito,Takuya Sera,Masafumi Oyamada*

Main category: cs.AI

TL;DR: SCAN是一种新型文档布局分析方法，通过语义粒度划分提升文本和视觉RAG系统性能，实验显示其优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型和视觉语言模型的广泛应用，处理富含信息的文档仍具挑战性，需要一种能平衡上下文保留与处理效率的方法。

Method: SCAN采用粗粒度语义方法，将文档划分为连续组件的连贯区域，并通过微调目标检测模型进行训练。

Result: 实验表明，SCAN在英文和日文数据集上分别提升文本RAG性能9.0%和视觉RAG性能6.4%，优于传统及商业解决方案。

Conclusion: SCAN通过优化文档布局分析，显著提升了RAG系统的性能，为处理视觉丰富文档提供了有效解决方案。

Abstract: With the increasing adoption of Large Language Models (LLMs) and
Vision-Language Models (VLMs), rich document analysis technologies for
applications like Retrieval-Augmented Generation (RAG) and visual RAG are
gaining significant attention. Recent research indicates that using VLMs can
achieve better RAG performance, but processing rich documents still remains a
challenge since a single page contains large amounts of information. In this
paper, we present SCAN (\textbf{S}emanti\textbf{C} Document Layout
\textbf{AN}alysis), a novel approach enhancing both textual and visual
Retrieval-Augmented Generation (RAG) systems working with visually rich
documents. It is a VLM-friendly approach that identifies document components
with appropriate semantic granularity, balancing context preservation with
processing efficiency. SCAN uses a coarse-grained semantic approach that
divides documents into coherent regions covering continuous components. We
trained the SCAN model by fine-tuning object detection models with
sophisticated annotation datasets. Our experimental results across English and
Japanese datasets demonstrate that applying SCAN improves end-to-end textual
RAG performance by up to 9.0\% and visual RAG performance by up to 6.4\%,
outperforming conventional approaches and even commercial document processing
solutions.

</details>


### [190] [Beyond the First Error: Process Reward Models for Reflective Mathematical Reasoning](https://arxiv.org/abs/2505.14391)
*Zhaohui Yang,Chenghua He,Xiaowen Shi,Linjing Li,Qiyue Yin,Shihong Deng,Daxin Jiang*

Main category: cs.AI

TL;DR: 提出一种针对长链思维过程（CoT）的新型数据标注方法，通过引入错误传播与终止概念提升PRM模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有PRM训练方法在处理长链推理时存在缺陷，仅关注首个错误步骤而忽略后续可能的自我修正行为，无法全面评估推理质量。

Method: 基于错误传播与终止理论设计标注方案，利用LLM标注170万样本训练7B参数的PRM模型，支持解决方案和步骤级评估。

Result: 模型在搜索引导、BoN和F1等指标上超越现有开源PRM，数据效率和性能均优于主流MC标注方法。

Conclusion: 该方法能有效识别推理过程中的自我修正行为，具有稳定性与泛化性，为长链推理评估提供新思路。

Abstract: Many studies focus on data annotation techniques for training effective PRMs.
However, current methods encounter a significant issue when applied to long CoT
reasoning processes: they tend to focus solely on the first incorrect step and
all preceding steps, assuming that all subsequent steps are incorrect. These
methods overlook the unique self-correction and reflection mechanisms inherent
in long CoT, where correct reasoning steps may still occur after initial
reasoning mistakes. To address this issue, we propose a novel data annotation
method for PRMs specifically designed to score the long CoT reasoning process.
Given that under the reflection pattern, correct and incorrect steps often
alternate, we introduce the concepts of Error Propagation and Error Cessation,
enhancing PRMs' ability to identify both effective self-correction behaviors
and reasoning based on erroneous steps. Leveraging an LLM-based judger for
annotation, we collect 1.7 million data samples to train a 7B PRM and evaluate
it at both solution and step levels. Experimental results demonstrate that
compared to existing open-source PRMs and PRMs trained on open-source datasets,
our PRM achieves superior performance across various metrics, including search
guidance, BoN, and F1 scores. Compared to widely used MC-based annotation
methods, our annotation approach not only achieves higher data efficiency but
also delivers superior performance. Detailed analysis is also conducted to
demonstrate the stability and generalizability of our method.

</details>


### [191] [Knowledge Graph Based Repository-Level Code Generation](https://arxiv.org/abs/2505.14394)
*Mihir Athale,Vishal Vaddina*

Main category: cs.AI

TL;DR: 该论文提出了一种基于知识图谱的代码搜索与检索方法，显著提升了代码生成的上下文准确性和质量。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLMs）在代码生成方面表现出色，但在处理动态演化的代码库时，其上下文准确性不足。现有代码搜索方法在检索结果的质量和上下文相关性上缺乏鲁棒性，导致生成的代码不够理想。

Method: 论文提出了一种将代码库表示为知识图谱的方法，通过捕捉结构和关系信息来增强上下文感知的代码生成。采用混合方法进行代码检索，以提高上下文相关性、跟踪文件间模块依赖关系，并确保与现有代码库的一致性。

Result: 在Evolutionary Code Benchmark (EvoCodeBench)数据集上的实验表明，该方法显著优于基线方法。

Conclusion: 基于知识图谱的代码生成方法能够推动开发更鲁棒、上下文敏感的编码辅助工具。

Abstract: Recent advancements in Large Language Models (LLMs) have transformed code
generation from natural language queries. However, despite their extensive
knowledge and ability to produce high-quality code, LLMs often struggle with
contextual accuracy, particularly in evolving codebases. Current code search
and retrieval methods frequently lack robustness in both the quality and
contextual relevance of retrieved results, leading to suboptimal code
generation. This paper introduces a novel knowledge graph-based approach to
improve code search and retrieval leading to better quality of code generation
in the context of repository-level tasks. The proposed approach represents code
repositories as graphs, capturing structural and relational information for
enhanced context-aware code generation. Our framework employs a hybrid approach
for code retrieval to improve contextual relevance, track inter-file modular
dependencies, generate more robust code and ensure consistency with the
existing codebase. We benchmark the proposed approach on the Evolutionary Code
Benchmark (EvoCodeBench) dataset, a repository-level code generation benchmark,
and demonstrate that our method significantly outperforms the baseline
approach. These findings suggest that knowledge graph based code generation
could advance robust, context-sensitive coding assistance tools.

</details>


### [192] [Causal Cartographer: From Mapping to Reasoning Over Counterfactual Worlds](https://arxiv.org/abs/2505.14396)
*Gaël Gendron,Jože M. Rožanec,Michael Witbrock,Gillian Dobbie*

Main category: cs.AI

TL;DR: 论文提出Causal Cartographer框架，通过显式提取和建模因果关系，提升大语言模型在因果推理任务中的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前基础模型（如大语言模型）缺乏超越记忆现有因果关系的因果推理能力，且在现实应用中评估反事实推理具有挑战性。

Method: 采用图检索增强生成代理从数据中提取因果关系，构建大规模因果关系网络，并设计受因果关系约束的反事实推理代理进行逐步因果推断。

Result: 该方法能有效提取因果知识，降低推理成本和虚假相关性，同时提升大语言模型在因果推理任务中的表现。

Conclusion: Causal Cartographer框架为构建可解释的因果世界模型提供了可行路径，增强了模型对未知分布的因果推理能力。

Abstract: Causal world models are systems that can answer counterfactual questions
about an environment of interest, i.e. predict how it would have evolved if an
arbitrary subset of events had been realized differently. It requires
understanding the underlying causes behind chains of events and conducting
causal inference for arbitrary unseen distributions. So far, this task eludes
foundation models, notably large language models (LLMs), which do not have
demonstrated causal reasoning capabilities beyond the memorization of existing
causal relationships. Furthermore, evaluating counterfactuals in real-world
applications is challenging since only the factual world is observed, limiting
evaluation to synthetic datasets. We address these problems by explicitly
extracting and modeling causal relationships and propose the Causal
Cartographer framework. First, we introduce a graph retrieval-augmented
generation agent tasked to retrieve causal relationships from data. This
approach allows us to construct a large network of real-world causal
relationships that can serve as a repository of causal knowledge and build
real-world counterfactuals. In addition, we create a counterfactual reasoning
agent constrained by causal relationships to perform reliable step-by-step
causal inference. We show that our approach can extract causal knowledge and
improve the robustness of LLMs for causal reasoning tasks while reducing
inference costs and spurious correlations.

</details>


### [193] [Unearthing Gems from Stones: Policy Optimization with Negative Sample Augmentation for LLM Reasoning](https://arxiv.org/abs/2505.14403)
*Zhaohui Yang,Shilei Jiang,Chen Hu,Linjing Li,Shihong Deng,Daxin Jiang*

Main category: cs.AI

TL;DR: 该论文提出了一种名为BCPG-NSA的细粒度离线强化学习框架，通过利用负样本中的有效学习信号，提升了推理语言模型的样本效率和性能。


<details>
  <summary>Details</summary>
Motivation: 现有的方法在处理负样本时，要么完全丢弃，要么对所有标记进行均等惩罚，未能充分利用负样本中的自我反思和错误修正等有价值信息。

Method: BCPG-NSA框架包含三个阶段：样本分割、基于共识的步骤正确性评估（结合LLM和PRM判断器）以及带有负样本增强（NSA）的策略优化。

Result: 实验结果表明，BCPG-NSA在多个数学和编程推理基准测试中优于基线方法，提高了样本效率，并在多次迭代中表现出鲁棒性和可扩展性。

Conclusion: BCPG-NSA通过有效挖掘负样本中的积极步骤，显著提升了推理语言模型的性能，为长链推理任务提供了一种高效的解决方案。

Abstract: Recent advances in reasoning language models have witnessed a paradigm shift
from short to long CoT pattern. Given the substantial computational cost of
rollouts in long CoT models, maximizing the utility of fixed training datasets
becomes crucial. Our analysis reveals that negative responses contain valuable
components such as self-reflection and error-correction steps, yet primary
existing methods either completely discard negative samples (RFT) or apply
equal penalization across all tokens (RL), failing to leverage these potential
learning signals. In light of this, we propose Behavior Constrained Policy
Gradient with Negative Sample Augmentation (BCPG-NSA), a fine-grained offline
RL framework that encompasses three stages: 1) sample segmentation, 2)
consensus-based step correctness assessment combining LLM and PRM judgers, and
3) policy optimization with NSA designed to effectively mine positive steps
within negative samples. Experimental results show that BCPG-NSA outperforms
baselines on several challenging math/coding reasoning benchmarks using the
same training dataset, achieving improved sample efficiency and demonstrating
robustness and scalability when extended to multiple iterations.

</details>


### [194] [PRL: Prompts from Reinforcement Learning](https://arxiv.org/abs/2505.14412)
*Paweł Batorski,Adrian Kosmala,Paul Swoboda*

Main category: cs.AI

TL;DR: PRL是一种基于强化学习的自动提示生成方法，能生成未见过的few-shot示例，在多个任务上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 设计有效的提示需要专家直觉和对任务的深入理解，且关键语义线索可能难以察觉。PRL旨在通过自动化方法解决这一挑战。

Method: 提出PRL（Prompts from Reinforcement Learning），一种基于强化学习的自动提示生成方法，能生成训练中未见的few-shot示例。

Result: 在文本分类、简化和摘要任务上表现优异：分类任务超越APE 2.58%和EvoPrompt 1.00%；摘要任务ROUGE分数分别提升4.32和2.12；简化任务SARI分数提升6.93和6.01。

Conclusion: PRL通过强化学习自动生成高质量提示，显著提升LLM在多种任务上的性能，代码已开源。

Abstract: Effective prompt engineering remains a central challenge in fully harnessing
the capabilities of LLMs. While well-designed prompts can dramatically enhance
performance, crafting them typically demands expert intuition and a nuanced
understanding of the task. Moreover, the most impactful prompts often hinge on
subtle semantic cues, ones that may elude human perception but are crucial for
guiding LLM behavior. In this paper, we introduce PRL (Prompts from
Reinforcement Learning), a novel RL-based approach for automatic prompt
generation. Unlike previous methods, PRL can produce novel few-shot examples
that were not seen during training. Our approach achieves state-of-the-art
performance across a range of benchmarks, including text classification,
simplification, and summarization. On the classification task, it surpasses
prior methods by 2.58% over APE and 1.00% over EvoPrompt. Additionally, it
improves the average ROUGE scores on the summarization task by 4.32 over APE
and by 2.12 over EvoPrompt and the SARI score on simplification by 6.93 over
APE and by 6.01 over EvoPrompt. Our code is available at
https://github.com/Batorskq/prl .

</details>


### [195] [SCOPE: Compress Mathematical Reasoning Steps for Efficient Automated Process Annotation](https://arxiv.org/abs/2505.14419)
*Huimin Xu,Xin Mao,Feng-Lin Li,Xiaobao Wu,Wang Chen,Wei Zhang,Anh Tuan Luu*

Main category: cs.AI

TL;DR: 本文提出了一种名为SCOPE的新型压缩方法，显著降低了过程奖励模型（PRMs）的标注成本，并在性能上超越了现有自动标注方法。


<details>
  <summary>Details</summary>
Motivation: 现有的过程标注方法，无论是通过人工标注还是蒙特卡洛模拟，计算成本高昂，限制了过程奖励模型（PRMs）的应用。

Method: SCOPE通过将自然语言推理步骤转换为代码并利用抽象语法树进行归一化，合并等效步骤构建前缀树，从而将复杂度从O(NMK)降至O(N)。

Result: 实验结果表明，使用SCOPE构建的数据集训练的PRMs在Best-of-N策略和ProcessBench上均优于现有自动标注方法，且仅需5%的计算资源。

Conclusion: SCOPE显著降低了过程标注的计算成本，同时提升了模型性能，为过程奖励模型的广泛应用提供了可行方案。

Abstract: Process Reward Models (PRMs) have demonstrated promising results in
mathematical reasoning, but existing process annotation approaches, whether
through human annotations or Monte Carlo simulations, remain computationally
expensive. In this paper, we introduce Step COmpression for Process Estimation
(SCOPE), a novel compression-based approach that significantly reduces
annotation costs. We first translate natural language reasoning steps into code
and normalize them through Abstract Syntax Tree, then merge equivalent steps to
construct a prefix tree. Unlike simulation-based methods that waste numerous
samples on estimation, SCOPE leverages a compression-based prefix tree where
each root-to-leaf path serves as a training sample, reducing the complexity
from $O(NMK)$ to $O(N)$. We construct a large-scale dataset containing 196K
samples with only 5% of the computational resources required by previous
methods. Empirical results demonstrate that PRMs trained on our dataset
consistently outperform existing automated annotation approaches on both
Best-of-N strategy and ProcessBench.

</details>


### [196] [Towards Reliable Proof Generation with LLMs: A Neuro-Symbolic Approach](https://arxiv.org/abs/2505.14479)
*Oren Sultan,Eitan Stern,Dafna Shahaf*

Main category: cs.AI

TL;DR: 该论文提出了一种结合神经与符号方法的技术，通过检索类似问题和使用形式验证器反馈，显著提升了大型语言模型在几何证明生成中的准确性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在需要严格逻辑推理和符号推理的正式领域（如数学证明生成）中表现不佳。为了解决这一问题，作者提出了一种神经与符号相结合的方法。

Method: 方法分为两部分：(1) 检索类似问题并利用其证明指导LLM；(2) 使用形式验证器评估生成的证明并提供反馈，帮助模型修正错误。

Result: 实验表明，该方法显著提高了OpenAI o1模型在几何证明生成中的准确性（提升58%-70%），类似问题和验证器反馈均对此有贡献。

Conclusion: 通过生成可证明正确的结论，可以大幅提升LLM的可靠性、准确性和一致性，从而解锁需要高可信度的复杂任务和关键现实应用。

Abstract: Large language models (LLMs) struggle with formal domains that require
rigorous logical deduction and symbolic reasoning, such as mathematical proof
generation. We propose a neuro-symbolic approach that combines LLMs' generative
strengths with structured components to overcome this challenge. As a
proof-of-concept, we focus on geometry problems. Our approach is two-fold: (1)
we retrieve analogous problems and use their proofs to guide the LLM, and (2) a
formal verifier evaluates the generated proofs and provides feedback, helping
the model fix incorrect proofs. We demonstrate that our method significantly
improves proof accuracy for OpenAI's o1 model (58%-70% improvement); both
analogous problems and the verifier's feedback contribute to these gains. More
broadly, shifting to LLMs that generate provably correct conclusions could
dramatically improve their reliability, accuracy and consistency, unlocking
complex tasks and critical real-world applications that require
trustworthiness.

</details>


### [197] [Reasoning Models Better Express Their Confidence](https://arxiv.org/abs/2505.14489)
*Dongkeun Yoon,Seungone Kim,Sohee Yang,Sunkyoung Kim,Soyeon Kim,Yongil Kim,Eunbi Choi,Yireun Kim,Minjoon Seo*

Main category: cs.AI

TL;DR: 研究发现，采用链式思维推理的大语言模型在问题解决和信心表达上表现更优，且信心校准更准确。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在信心表达上往往不够准确，这限制了其可靠性。本研究旨在探索推理模型是否能更准确地表达信心。

Method: 研究比较了六种推理模型在六个数据集上的表现，分析了其链式思维推理（CoT）行为对信心校准的影响。

Result: 推理模型在36种设置中有33种表现优于非推理模型，信心校准更准确。移除慢思考行为会导致校准显著下降。

Conclusion: 推理模型通过慢思考行为动态调整信心，显著提升了信心校准的准确性，且非推理模型也能通过类似方法受益。

Abstract: Despite their strengths, large language models (LLMs) often fail to
communicate their confidence accurately, making it difficult to assess when
they might be wrong and limiting their reliability. In this work, we
demonstrate that reasoning models-LLMs that engage in extended chain-of-thought
(CoT) reasoning-exhibit superior performance not only in problem-solving but
also in accurately expressing their confidence. Specifically, we benchmark six
reasoning models across six datasets and find that they achieve strictly better
confidence calibration than their non-reasoning counterparts in 33 out of the
36 settings. Our detailed analysis reveals that these gains in calibration stem
from the slow thinking behaviors of reasoning models-such as exploring
alternative approaches and backtracking-which enable them to adjust their
confidence dynamically throughout their CoT, making it progressively more
accurate. In particular, we find that reasoning models become increasingly
better calibrated as their CoT unfolds, a trend not observed in non-reasoning
models. Moreover, removing slow thinking behaviors from the CoT leads to a
significant drop in calibration. Lastly, we show that these gains are not
exclusive to reasoning models-non-reasoning models also benefit when guided to
perform slow thinking via in-context learning.

</details>


### [198] [BACON: A fully explainable AI model with graded logic for decision making problems](https://arxiv.org/abs/2505.14510)
*Haishi Bai,Jozo Dujmovic,Jianwu Wang*

Main category: cs.AI

TL;DR: BACON是一个新型可解释AI框架，通过分级逻辑训练模型，在保持高准确率的同时提供透明决策逻辑，适用于医疗、金融等高风险领域。


<details>
  <summary>Details</summary>
Motivation: 随着AI在医疗、金融等高风险领域的应用增加，模型需要兼具高准确性和可解释性，以支持人机协作和专家优化。

Method: 提出BACON框架，利用分级逻辑自动训练可解释模型，生成符号化决策规则，并在布尔近似、花卉分类等场景验证。

Result: 实验表明BACON能生成紧凑、可验证的决策逻辑，在乳腺癌诊断等任务中保持高性能与透明度。

Conclusion: BACON为可解释AI提供了实用且理论完备的解决方案，能输出清晰可信的决策解释。

Abstract: As machine learning models and autonomous agents are increasingly deployed in
high-stakes, real-world domains such as healthcare, security, finance, and
robotics, the need for transparent and trustworthy explanations has become
critical. To ensure end-to-end transparency of AI decisions, we need models
that are not only accurate but also fully explainable and human-tunable. We
introduce BACON, a novel framework for automatically training explainable AI
models for decision making problems using graded logic. BACON achieves high
predictive accuracy while offering full structural transparency and precise,
logic-based symbolic explanations, enabling effective human-AI collaboration
and expert-guided refinement. We evaluate BACON with a diverse set of
scenarios: classic Boolean approximation, Iris flower classification, house
purchasing decisions and breast cancer diagnosis. In each case, BACON provides
high-performance models while producing compact, human-verifiable decision
logic. These results demonstrate BACON's potential as a practical and
principled approach for delivering crisp, trustworthy explainable AI.

</details>


### [199] [Guarded Query Routing for Large Language Models](https://arxiv.org/abs/2505.14524)
*Richard Šléher,William Brach,Tibor Sloboda,Kristián Košťál,Lukas Galke*

Main category: cs.AI

TL;DR: 该论文研究了带防护的查询路由问题，提出了GQR-Bench基准，并对比了多种路由方法的性能，发现WideMLP在准确性和速度上取得了最佳平衡。


<details>
  <summary>Details</summary>
Motivation: 查询路由任务需要处理分布外的查询，如无关领域的问题、其他语言的查询或不安全文本。因此，研究带防护的查询路由问题具有实际意义。

Method: 论文引入了GQR-Bench基准，覆盖三个目标领域和七个数据集，对比了LLM、传统机器学习模型和词袋分类器在路由任务中的表现。

Result: WideMLP在准确性（88%）和速度（<4ms）上表现最佳；fastText速度最快（<1ms），准确性尚可（80%）；LLM准确性最高（91%），但速度较慢（62ms-669ms）。

Conclusion: 研究结果表明，LLM并非查询路由任务的唯一选择，WideMLP和fastText在特定场景下更具优势。GQR-Bench将作为Python包发布。

Abstract: Query routing, the task to route user queries to different large language
model (LLM) endpoints, can be considered as a text classification problem.
However, out-of-distribution queries must be handled properly, as those could
be questions about unrelated domains, queries in other languages, or even
contain unsafe text. Here, we thus study a \emph{guarded} query routing
problem, for which we first introduce the Guarded Query Routing Benchmark
(GQR-Bench), which covers three exemplary target domains (law, finance, and
healthcare), and seven datasets to test robustness against out-of-distribution
queries. We then use GQR-Bench to contrast the effectiveness and efficiency of
LLM-based routing mechanisms (GPT-4o-mini, Llama-3.2-3B, and Llama-3.1-8B),
standard LLM-based guardrail approaches (LlamaGuard and NVIDIA NeMo
Guardrails), continuous bag-of-words classifiers (WideMLP, fastText), and
traditional machine learning models (SVM, XGBoost). Our results show that
WideMLP, enhanced with out-of-domain detection capabilities, yields the best
trade-off between accuracy (88\%) and speed (<4ms). The embedding-based
fastText excels at speed (<1ms) with acceptable accuracy (80\%), whereas LLMs
yield the highest accuracy (91\%) but are comparatively slow (62ms for local
Llama-3.1:8B and 669ms for remote GPT-4o-mini calls). Our findings challenge
the automatic reliance on LLMs for (guarded) query routing and provide concrete
recommendations for practical applications. GQR-Bench will be released as a
Python package -- \texttt{gqr}.

</details>


### [200] [A Logic of General Attention Using Edge-Conditioned Event Models (Extended Version)](https://arxiv.org/abs/2505.14539)
*Gaia Belardinelli,Thomas Bolander,Sebastian Watzl*

Main category: cs.AI

TL;DR: 本文提出了一种通用的注意力逻辑，能够处理复杂注意力场景，并克服现有动态认知逻辑的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有的动态认知逻辑只能处理原子公式的注意力，无法建模复杂注意力场景，且随着代理数和宣布的文字数量增加，逻辑规模呈指数级增长。

Method: 通过推广边缘条件事件模型，使其表达能力与标准事件模型相当但更简洁，并将注意力扩展到任意公式，包括其他代理的信念或注意力。

Result: 提出的逻辑能够建模复杂注意力场景，如高阶信念或其他代理的注意力，并通过示例展示了AI代理如何发现人类的注意力偏见。

Conclusion: 该研究将注意力视为一种模态，类似于信念或意识，并提出了注意力原则，为注意力逻辑的进一步研究奠定了基础。

Abstract: In this work, we present the first general logic of attention. Attention is a
powerful cognitive ability that allows agents to focus on potentially complex
information, such as logically structured propositions, higher-order beliefs,
or what other agents pay attention to. This ability is a strength, as it helps
to ignore what is irrelevant, but it can also introduce biases when some types
of information or agents are systematically ignored. Existing dynamic epistemic
logics for attention cannot model such complex attention scenarios, as they
only model attention to atomic formulas. Additionally, such logics quickly
become cumbersome, as their size grows exponentially in the number of agents
and announced literals. Here, we introduce a logic that overcomes both
limitations. First, we generalize edge-conditioned event models, which we show
to be as expressive as standard event models yet exponentially more succinct
(generalizing both standard event models and generalized arrow updates).
Second, we extend attention to arbitrary formulas, allowing agents to also
attend to other agents' beliefs or attention. Our work treats attention as a
modality, like belief or awareness. We introduce attention principles that
impose closure properties on that modality and that can be used in its
axiomatization. Throughout, we illustrate our framework with examples of AI
agents reasoning about human attentional biases, demonstrating how such agents
can discover attentional biases.

</details>


### [201] [Multi-agent Reinforcement Learning vs. Fixed-Time Control for Traffic Signal Optimization: A Simulation Study](https://arxiv.org/abs/2505.14544)
*Saahil Mahato*

Main category: cs.AI

TL;DR: 该研究探讨了使用多智能体强化学习（MARL）优化多交叉路口交通信号协调，相比传统固定时间控制，显著减少了车辆平均等待时间并提高了通行效率。


<details>
  <summary>Details</summary>
Motivation: 城市交通拥堵，尤其在交叉路口，严重影响出行时间、燃油消耗和排放。传统固定时间信号控制系统难以有效应对动态交通模式。

Method: 研究在模拟环境中应用MARL，开发了基于Pygame的仿真模型，模拟随机车流，并实现分散式MARL控制器，每个交通信号作为自主智能体，基于本地观察和邻近智能体信息做出决策。

Result: 与固定时间控制器相比，MARL方法在车辆平均等待时间和整体通行量上表现出显著改进。

Conclusion: 研究表明，基于MARL的动态控制策略在提升城市交通管理效率方面具有巨大潜力，但需进一步研究解决可扩展性和实际应用挑战。

Abstract: Urban traffic congestion, particularly at intersections, significantly
impacts travel time, fuel consumption, and emissions. Traditional fixed-time
signal control systems often lack the adaptability to manage dynamic traffic
patterns effectively. This study explores the application of multi-agent
reinforcement learning (MARL) to optimize traffic signal coordination across
multiple intersections within a simulated environment. Utilizing Pygame, a
simulation was developed to model a network of interconnected intersections
with randomly generated vehicle flows to reflect realistic traffic variability.
A decentralized MARL controller was implemented, in which each traffic signal
operates as an autonomous agent, making decisions based on local observations
and information from neighboring agents. Performance was evaluated against a
baseline fixed-time controller using metrics such as average vehicle wait time
and overall throughput. The MARL approach demonstrated statistically
significant improvements, including reduced average waiting times and improved
throughput. These findings suggest that MARL-based dynamic control strategies
hold substantial promise for improving urban traffic management efficiency.
More research is recommended to address scalability and real-world
implementation challenges.

</details>


### [202] [Agent Context Protocols Enhance Collective Inference](https://arxiv.org/abs/2505.14569)
*Devansh Bhardwaj,Arjun Beniwal,Shreyas Chaudhari,Ashwin Kalyan,Tanmay Rajpurohit,Karthik R. Narasimhan,Ameet Deshpande,Vishvak Murahari*

Main category: cs.AI

TL;DR: 论文提出了一种名为Agent Context Protocols (ACPs)的结构化协议，用于提升多智能体系统的协作与通信效率，显著优于现有商业AI系统。


<details>
  <summary>Details</summary>
Motivation: 当前多智能体系统依赖非结构化的自然语言进行协调，限制了复杂交互和领域专用智能体的互操作性。

Method: 引入ACPs协议，结合持久化执行蓝图和标准化消息模式，实现鲁棒且容错的多智能体协同推理。

Result: ACPs在长周期网络辅助任务中达到28.3%的准确率，并在多模态技术报告中表现最佳，优于商业AI系统。

Conclusion: ACPs具有高度模块化和可扩展性，能快速构建顶尖通用智能体系统。

Abstract: AI agents have become increasingly adept at complex tasks such as coding,
reasoning, and multimodal understanding. However, building generalist systems
requires moving beyond individual agents to collective inference -- a paradigm
where multi-agent systems with diverse, task-specialized agents complement one
another through structured communication and collaboration. Today, coordination
is usually handled with imprecise, ad-hoc natural language, which limits
complex interaction and hinders interoperability with domain-specific agents.
We introduce Agent context protocols (ACPs): a domain- and agent-agnostic
family of structured protocols for agent-agent communication, coordination, and
error handling. ACPs combine (i) persistent execution blueprints -- explicit
dependency graphs that store intermediate agent outputs -- with (ii)
standardized message schemas, enabling robust and fault-tolerant multi-agent
collective inference. ACP-powered generalist systems reach state-of-the-art
performance: 28.3 % accuracy on AssistantBench for long-horizon web assistance
and best-in-class multimodal technical reports, outperforming commercial AI
systems in human evaluation. ACPs are highly modular and extensible, allowing
practitioners to build top-tier generalist agents quickly.

</details>


### [203] [Towards a Foundation Model for Communication Systems](https://arxiv.org/abs/2505.14603)
*Davide Buffelli,Sowmen Das,Yu-Wei Lin,Sattar Vakili,Chien-Yi Wang,Masoud Attarifar,Pritthijit Nath,Da-shan Shiu*

Main category: cs.AI

TL;DR: 该论文提出了一种基于Transformer的多模态基础模型，用于直接处理通信数据，解决了包括标记化、位置嵌入、多模态等关键挑战，并实证验证了模型在估计多种通信特征上的有效性。


<details>
  <summary>Details</summary>
Motivation: 当前AI在通信系统中的应用多为任务特定解决方案，而AI领域正朝着能够支持多种应用的大型通用模型发展。因此，作者希望开发一种适用于通信数据的基础模型，以填补这一空白。

Method: 作者提出了一种基于Transformer的多模态模型，针对通信数据设计了专门的标记化、位置嵌入、多模态处理、可变特征大小和归一化等方法。

Result: 实证结果表明，该模型能够成功估计多种通信特征，包括传输秩、预编码器选择、多普勒扩展和延迟分布等。

Conclusion: 该研究为通信数据的基础模型开发迈出了重要一步，展示了Transformer架构在通信领域的潜力，并为未来更通用的AI通信解决方案奠定了基础。

Abstract: Artificial Intelligence (AI) has demonstrated unprecedented performance
across various domains, and its application to communication systems is an
active area of research. While current methods focus on task-specific
solutions, the broader trend in AI is shifting toward large general models
capable of supporting multiple applications. In this work, we take a step
toward a foundation model for communication data--a transformer-based,
multi-modal model designed to operate directly on communication data. We
propose methodologies to address key challenges, including tokenization,
positional embedding, multimodality, variable feature sizes, and normalization.
Furthermore, we empirically demonstrate that such a model can successfully
estimate multiple features, including transmission rank, selected precoder,
Doppler spread, and delay profile.

</details>


### [204] [Let LLMs Break Free from Overthinking via Self-Braking Tuning](https://arxiv.org/abs/2505.14604)
*Haoran Zhao,Yuchen Yan,Yongliang Shen,Haolei Xu,Wenqi Zhang,Kaitao Song,Jian Shao,Weiming Lu,Jun Xiao,Yueting Zhuang*

Main category: cs.AI

TL;DR: 论文提出Self-Braking Tuning（SBT）框架，通过让模型自我调节推理过程，减少冗余计算，在保持精度的同时显著降低token消耗。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型（LRMs）在提升推理能力的同时，产生了大量冗余计算，导致计算开销增加和过度思考问题。现有方法多依赖外部干预，本文旨在让模型自主调节推理过程。

Method: 构建过思考识别指标，设计冗余推理检测方法，生成自适应推理长度数据，并引入制动提示机制，使模型学会适时终止推理。

Result: 在数学基准测试（AIME等）中，该方法减少高达60%的token消耗，同时保持与无约束模型相当的精度。

Conclusion: SBT框架有效解决了LRMs的过思考问题，通过自我调节实现了高效推理，为模型优化提供了新思路。

Abstract: Large reasoning models (LRMs), such as OpenAI o1 and DeepSeek-R1, have
significantly enhanced their reasoning capabilities by generating longer chains
of thought, demonstrating outstanding performance across a variety of tasks.
However, this performance gain comes at the cost of a substantial increase in
redundant reasoning during the generation process, leading to high
computational overhead and exacerbating the issue of overthinking. Although
numerous existing approaches aim to address the problem of overthinking, they
often rely on external interventions. In this paper, we propose a novel
framework, Self-Braking Tuning (SBT), which tackles overthinking from the
perspective of allowing the model to regulate its own reasoning process, thus
eliminating the reliance on external control mechanisms. We construct a set of
overthinking identification metrics based on standard answers and design a
systematic method to detect redundant reasoning. This method accurately
identifies unnecessary steps within the reasoning trajectory and generates
training signals for learning self-regulation behaviors. Building on this
foundation, we develop a complete strategy for constructing data with adaptive
reasoning lengths and introduce an innovative braking prompt mechanism that
enables the model to naturally learn when to terminate reasoning at an
appropriate point. Experiments across mathematical benchmarks (AIME, AMC,
MATH500, GSM8K) demonstrate that our method reduces token consumption by up to
60% while maintaining comparable accuracy to unconstrained models.

</details>


### [205] [SATBench: Benchmarking LLMs' Logical Reasoning via Automated Puzzle Generation from SAT Formulas](https://arxiv.org/abs/2505.14615)
*Anjiang Wei,Yuheng Wu,Yingjia Wan,Tarun Suresh,Huanmi Tan,Zhanke Zhou,Sanmi Koyejo,Ke Wang,Alex Aiken*

Main category: cs.AI

TL;DR: SATBench是一个通过布尔可满足性问题生成的逻辑谜题基准，用于评估大型语言模型的逻辑推理能力，揭示了当前模型在搜索式逻辑推理上的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注基于推理规则的逻辑推理，而本文旨在通过SAT问题的搜索性质，评估大型语言模型在解决复杂逻辑约束时的能力。

Method: SATBench通过自动化流程生成2100个逻辑谜题，每个谜题源自SAT公式并转化为故事背景和条件，难度可通过子句数量调整，并通过LLM辅助和求解器进行一致性验证。

Result: 实验显示，即使是性能最强的o4-mini模型，在困难UNSAT问题上准确率仅为65.0%，接近随机基线的50%。

Conclusion: SATBench揭示了当前大型语言模型在搜索式逻辑推理上的根本局限，为未来逻辑推理研究提供了可扩展的测试平台。

Abstract: We introduce SATBench, a benchmark for evaluating the logical reasoning
capabilities of large language models (LLMs) through logical puzzles derived
from Boolean satisfiability (SAT) problems. Unlike prior work that focuses on
inference rule-based reasoning, which often involves deducing conclusions from
a set of premises, our approach leverages the search-based nature of SAT
problems, where the objective is to find a solution that fulfills a specified
set of logical constraints. Each instance in SATBench is generated from a SAT
formula, then translated into a story context and conditions using LLMs. The
generation process is fully automated and allows for adjustable difficulty by
varying the number of clauses. All 2100 puzzles are validated through both
LLM-assisted and solver-based consistency checks, with human validation on a
subset. Experimental results show that even the strongest model, o4-mini,
achieves only 65.0% accuracy on hard UNSAT problems, close to the random
baseline of 50%. SATBench exposes fundamental limitations in the search-based
logical reasoning abilities of current LLMs and provides a scalable testbed for
future research in logical reasoning.

</details>


### [206] [Debating for Better Reasoning: An Unsupervised Multimodal Approach](https://arxiv.org/abs/2505.14627)
*Ashutosh Adhikari,Mirella Lapata*

Main category: cs.AI

TL;DR: 该论文探讨了在多模态环境下通过辩论机制增强大型语言模型监督能力的方法，特别是在视觉问答任务中，通过专家模型间的辩论提升性能。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型在多领域和多模态中的能力提升，传统的监督方法面临挑战，尤其是在模型能力超越人类评估者时。辩论机制被视为一种潜在的解决方案。

Method: 论文提出了一种多模态辩论框架，其中两个视觉语言专家模型就视觉问答任务进行辩论，一个仅基于文本的盲审模型根据辩论质量裁决。专家模型仅捍卫与其信念一致的答案，从而专注于专家间的分歧实例。

Result: 实验表明，辩论框架在多模态任务中表现优于单个专家模型，且较弱的语言模型的裁决能力有助于通过微调增强视觉语言模型的推理能力。

Conclusion: 辩论机制在多模态环境下有效，能够通过模型间的互动提升性能，并为模型监督提供了一种新途径。

Abstract: As Large Language Models (LLMs) gain expertise across diverse domains and
modalities, scalable oversight becomes increasingly challenging, particularly
when their capabilities may surpass human evaluators. Debate has emerged as a
promising mechanism for enabling such oversight. In this work, we extend the
debate paradigm to a multimodal setting, exploring its potential for weaker
models to supervise and enhance the performance of stronger models. We focus on
visual question answering (VQA), where two "sighted" expert vision-language
models debate an answer, while a "blind" (text-only) judge adjudicates based
solely on the quality of the arguments. In our framework, the experts defend
only answers aligned with their beliefs, thereby obviating the need for
explicit role-playing and concentrating the debate on instances of expert
disagreement. Experiments on several multimodal tasks demonstrate that the
debate framework consistently outperforms individual expert models. Moreover,
judgments from weaker LLMs can help instill reasoning capabilities in
vision-language models through finetuning.

</details>


### [207] [Cost-Augmented Monte Carlo Tree Search for LLM-Assisted Planning](https://arxiv.org/abs/2505.14656)
*Zihao Zhang,Fei Liu*

Main category: cs.AI

TL;DR: 论文提出CATS方法，通过成本增强的蒙特卡洛树搜索提升LLM在预算敏感规划中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型（LLM）在开放推理中表现优异，但在成本敏感规划中常忽略行动成本差异或无法满足严格预算限制。

Method: 引入成本增强蒙特卡洛树搜索（CATS），将显式成本意识融入LLM引导的规划过程。

Result: 实验表明GPT-4.1等原始LLM在严格预算下表现不佳，而CATS能稳定实现更高任务成功率和成本效率。

Conclusion: CATS通过结合LLM推理能力和结构化搜索，为预算敏感决策提供了有效解决方案。

Abstract: While LLMs excel at open-ended reasoning, they often struggle with
cost-sensitive planning, either treating all actions as having equal cost or
failing to stay within strict budgets. In this paper, we introduce
Cost-Augmented Monte Carlo Tree Search (CATS), a novel approach that brings
explicit cost-awareness into LLM-guided planning. Tight cost constraints push
the planner to quickly identify infeasible solutions, while looser constraints
encourage optimization for minimal cost. We benchmark top LLMs such as GPT-4.1,
Claude-3.7-Sonnet, and DeepSeek-R1, against our CATS planner to evaluate their
performance in cost-sensitive scenarios. Our experiments suggest that raw LLMs
such as GPT-4.1 often falter under tight budgets, whereas CATS consistently
delivers strong performance, achieving higher task success rates and better
cost efficiency. CATS provides an effective solution for budget-aware
decision-making by combining the reasoning power of LLMs with structured
search.

</details>


### [208] [SAFEPATH: Preventing Harmful Reasoning in Chain-of-Thought via Early Alignment](https://arxiv.org/abs/2505.14667)
*Wonje Jeung,Sangyeon Yoon,Minsuk Kahng,Albert No*

Main category: cs.AI

TL;DR: SAFEPATH是一种轻量级对齐方法，通过在有害提示下生成简短安全前导标记来减少大推理模型的有害输出，同时保持推理性能。


<details>
  <summary>Details</summary>
Motivation: 现有的大推理模型（LRMs）在有害提示下可能产生不安全输出，且现有安全对齐方法会降低推理深度，并在复杂任务中面临权衡问题。

Method: SAFEPATH通过微调LRMs，在有害提示下生成8个标记的安全前导，其余推理过程不受监督，同时提供无需微调的零样本变体。

Result: SAFEPATH在DeepSeek-R1-Distill-Llama-8B模型中减少90%有害输出，阻止83.3%越狱尝试，计算量比现有方法低295.9倍和314.1倍。

Conclusion: SAFEPATH有效平衡安全性与推理性能，揭示了现有方法在推理中心模型中的局限性，为更安全的AI提供了新方向。

Abstract: Large Reasoning Models (LRMs) have become powerful tools for complex problem
solving, but their structured reasoning pathways can lead to unsafe outputs
when exposed to harmful prompts. Existing safety alignment methods reduce
harmful outputs but can degrade reasoning depth, leading to significant
trade-offs in complex, multi-step tasks, and remain vulnerable to sophisticated
jailbreak attacks. To address this, we introduce SAFEPATH, a lightweight
alignment method that fine-tunes LRMs to emit a short, 8-token Safety Primer at
the start of their reasoning, in response to harmful prompts, while leaving the
rest of the reasoning process unsupervised. Empirical results across multiple
benchmarks indicate that SAFEPATH effectively reduces harmful outputs while
maintaining reasoning performance. Specifically, SAFEPATH reduces harmful
responses by up to 90.0% and blocks 83.3% of jailbreak attempts in the
DeepSeek-R1-Distill-Llama-8B model, while requiring 295.9x less compute than
Direct Refusal and 314.1x less than SafeChain. We further introduce a zero-shot
variant that requires no fine-tuning. In addition, we provide a comprehensive
analysis of how existing methods in LLMs generalize, or fail, when applied to
reasoning-centric models, revealing critical gaps and new directions for safer
AI.

</details>


### [209] [ContextAgent: Context-Aware Proactive LLM Agents with Open-World Sensory Perceptions](https://arxiv.org/abs/2505.14668)
*Bufang Yang,Lilin Xu,Liekang Zeng,Kaiwei Liu,Siyang Jiang,Wenrui Lu,Hongkai Chen,Xiaofan Jiang,Guoliang Xing,Zhenyu Yan*

Main category: cs.AI

TL;DR: 论文提出ContextAgent，首个结合多维度感知情境的主动式LLM智能体，通过穿戴设备数据提升意图理解与工具调用能力，并在新基准测试中优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 现有主动式智能体仅依赖封闭环境观察或基于规则的提醒，导致用户意图理解不足且功能有限。需要融合多源情境数据实现更人性化的主动服务。

Method: 1) 从穿戴设备提取视频/音频等多维感知情境；2) 结合历史人物画像预测主动服务需求；3) 自动调用工具进行无干扰辅助。建立包含9类场景的评测基准ContextAgentBench。

Result: 在1,000个测试样本中，主动预测准确率提升8.5%，工具调用准确率提升6.0%，显著优于基线模型。

Conclusion: ContextAgent通过情境感知技术推动主动式AI助手发展，为构建以人为中心的高级智能体提供新思路。

Abstract: Recent advances in Large Language Models (LLMs) have propelled intelligent
agents from reactive responses to proactive support. While promising, existing
proactive agents either rely exclusively on observations from enclosed
environments (e.g., desktop UIs) with direct LLM inference or employ rule-based
proactive notifications, leading to suboptimal user intent understanding and
limited functionality for proactive service. In this paper, we introduce
ContextAgent, the first context-aware proactive agent that incorporates
extensive sensory contexts to enhance the proactive capabilities of LLM agents.
ContextAgent first extracts multi-dimensional contexts from massive sensory
perceptions on wearables (e.g., video and audio) to understand user intentions.
ContextAgent then leverages the sensory contexts and the persona contexts from
historical data to predict the necessity for proactive services. When proactive
assistance is needed, ContextAgent further automatically calls the necessary
tools to assist users unobtrusively. To evaluate this new task, we curate
ContextAgentBench, the first benchmark for evaluating context-aware proactive
LLM agents, covering 1,000 samples across nine daily scenarios and twenty
tools. Experiments on ContextAgentBench show that ContextAgent outperforms
baselines by achieving up to 8.5% and 6.0% higher accuracy in proactive
predictions and tool calling, respectively. We hope our research can inspire
the development of more advanced, human-centric, proactive AI assistants.

</details>


### [210] [Two Experts Are All You Need for Steering Thinking: Reinforcing Cognitive Effort in MoE Reasoning Models Without Additional Training](https://arxiv.org/abs/2505.14681)
*Mengru Wang,Xingyu Chen,Yue Wang,Zhiwei He,Jiahao Xu,Tian Liang,Qiuzhi Liu,Yunzhi Yao,Wenxuan Wang,Ruotian Ma,Haitao Mi,Ningyu Zhang,Zhaopeng Tu,Xiaolong Li,Dong Yu*

Main category: cs.AI

TL;DR: 提出RICE方法，通过nPMI识别认知专家，提升MoE模型的推理性能，无需额外训练。


<details>
  <summary>Details</summary>
Motivation: 现有推理模型存在认知效率低下的问题，如过度思考或思考不足，需要改进。

Method: 利用nPMI识别认知专家，设计RICE方法在推理时引导模型，提升效率。

Result: 在DeepSeek-R1和Qwen3-235B等模型上，推理准确性、认知效率和跨领域泛化能力均显著提升。

Conclusion: RICE是一种轻量级、实用且可解释的方法，能有效提升高级推理模型的认知效率。

Abstract: Mixture-of-Experts (MoE) architectures within Large Reasoning Models (LRMs)
have achieved impressive reasoning capabilities by selectively activating
experts to facilitate structured cognitive processes. Despite notable advances,
existing reasoning models often suffer from cognitive inefficiencies like
overthinking and underthinking. To address these limitations, we introduce a
novel inference-time steering methodology called Reinforcing Cognitive Experts
(RICE), designed to improve reasoning performance without additional training
or complex heuristics. Leveraging normalized Pointwise Mutual Information
(nPMI), we systematically identify specialized experts, termed ''cognitive
experts'' that orchestrate meta-level reasoning operations characterized by
tokens like ''<think>''. Empirical evaluations with leading MoE-based LRMs
(DeepSeek-R1 and Qwen3-235B) on rigorous quantitative and scientific reasoning
benchmarks demonstrate noticeable and consistent improvements in reasoning
accuracy, cognitive efficiency, and cross-domain generalization. Crucially, our
lightweight approach substantially outperforms prevalent reasoning-steering
techniques, such as prompt design and decoding constraints, while preserving
the model's general instruction-following skills. These results highlight
reinforcing cognitive experts as a promising, practical, and interpretable
direction to enhance cognitive efficiency within advanced reasoning models.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [211] [Tuning Learning Rates with the Cumulative-Learning Constant](https://arxiv.org/abs/2505.13457)
*Nathan Faraj*

Main category: cs.LG

TL;DR: 本文提出了一种优化机器学习学习率的新方法，发现了学习率与数据集大小之间的比例关系，并提出了累积学习常数的概念，为设计高级学习率调度提供了框架。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于探索数据集规模如何影响训练动态，并寻找优化学习率的方法以提高机器学习模型的训练效率和性能。

Method: 通过分析学习率与数据集大小的比例关系，提出了一种新的优化方法，并引入了累积学习常数的概念来设计学习率调度策略。

Result: 研究发现学习率与数据集大小之间存在比例关系，并提出了累积学习常数的框架，这些发现可以显著提升训练效率和模型性能。

Conclusion: 本文提出的方法为机器学习中的学习率优化提供了新的思路，具有广泛的应用潜力，能够提升多种机器学习任务的训练效果。

Abstract: This paper introduces a novel method for optimizing learning rates in machine
learning. A previously unrecognized proportionality between learning rates and
dataset sizes is discovered, providing valuable insights into how dataset scale
influences training dynamics. Additionally, a cumulative learning constant is
identified, offering a framework for designing and optimizing advanced learning
rate schedules. These findings have the potential to enhance training
efficiency and performance across a wide range of machine learning
applications.

</details>


### [212] [FPGA-based Acceleration for Convolutional Neural Networks: A Comprehensive Review](https://arxiv.org/abs/2505.13461)
*Junye Jiang,Yaan Zhou,Yuanhao Gong,Haoxuan Yuan,Shuanglong Liu*

Main category: cs.LG

TL;DR: 本文综述了基于FPGA的CNN硬件加速器，总结了性能评估框架、优化策略及不同架构的比较，并探讨了未来挑战与机遇。


<details>
  <summary>Details</summary>
Motivation: 随着卷积神经网络（CNN）复杂度的增加，计算需求大幅上升，需要高效的硬件加速器。FPGA因其可重构性、并行性和能效成为理想解决方案。

Method: 论文通过综合现有研究，提出了性能评估框架，并探讨了并行计算、数据流优化和软硬件协同设计等关键优化策略。

Result: 比较了不同FPGA架构在延迟、吞吐量、计算效率、功耗和资源利用率等方面的表现，总结了当前研究的成果。

Conclusion: 论文强调了FPGA在CNN加速中的潜力，并指出未来在创新和优化方面的挑战与机遇。

Abstract: Convolutional Neural Networks (CNNs) are fundamental to deep learning,
driving applications across various domains. However, their growing complexity
has significantly increased computational demands, necessitating efficient
hardware accelerators. Field-Programmable Gate Arrays (FPGAs) have emerged as a
leading solution, offering reconfigurability, parallelism, and energy
efficiency. This paper provides a comprehensive review of FPGA-based hardware
accelerators specifically designed for CNNs. It presents and summarizes the
performance evaluation framework grounded in existing studies and explores key
optimization strategies, such as parallel computing, dataflow optimization, and
hardware-software co-design. It also compares various FPGA architectures in
terms of latency, throughput, compute efficiency, power consumption, and
resource utilization. Finally, the paper highlights future challenges and
opportunities, emphasizing the potential for continued innovation in this
field.

</details>


### [213] [End-to-end fully-binarized network design: from Generic Learned Thermometer to Block Pruning](https://arxiv.org/abs/2505.13462)
*Thien Nguyen,William Guicquero*

Main category: cs.LG

TL;DR: 本文提出了一种名为GLT的输入数据编码技术，用于改进二值神经网络（BNN）的输入表示，并结合轻量级分组卷积和块剪枝技术，实现了轻量级全二值化模型。


<details>
  <summary>Details</summary>
Motivation: 现有的二值神经网络研究主要集中在模型权重和激活函数上，而忽略了输入原始数据的优化。本文旨在通过改进输入数据的表示方法，提升BNN的性能。

Method: 提出了通用学习温度计（GLT）编码技术，通过学习非线性量化阈值来优化输入数据的二值化表示；同时结合轻量级分组卷积、块剪枝和知识蒸馏技术，进一步减小模型规模和计算复杂度。

Result: 实验表明，GLT技术显著提升了模型在STL-10和VWW数据集上的准确率；结合块剪枝技术后，成功实现了轻量级（小于1Mb）全二值化模型，适用于传感器端的持续推理场景。

Conclusion: GLT技术为二值神经网络提供了更优的输入数据表示方法，结合轻量化设计，在保持较高准确率的同时显著降低了模型复杂度，适合实际应用场景。

Abstract: Existing works on Binary Neural Network (BNN) mainly focus on model's weights
and activations while discarding considerations on the input raw data. This
article introduces Generic Learned Thermometer (GLT), an encoding technique to
improve input data representation for BNN, relying on learning non linear
quantization thresholds. This technique consists in multiple data binarizations
which can advantageously replace a conventional Analog to Digital Conversion
(ADC) that uses natural binary coding. Additionally, we jointly propose a
compact topology with light-weight grouped convolutions being trained thanks to
block pruning and Knowledge Distillation (KD), aiming at reducing furthermore
the model size so as its computational complexity. We show that GLT brings
versatility to the BNN by intrinsically performing global tone mapping,
enabling significant accuracy gains in practice (demonstrated by simulations on
the STL-10 and VWW datasets). Moreover, when combining GLT with our proposed
block-pruning technique, we successfully achieve lightweight (under 1Mb),
fully-binarized models with limited accuracy degradation while being suitable
for in-sensor always-on inference use cases.

</details>


### [214] [Predicting The Evolution of Interfaces with Fourier Neural Operators](https://arxiv.org/abs/2505.13463)
*Paolo Guida,William L. Roberts*

Main category: cs.LG

TL;DR: 该论文展示了神经算子能够快速预测多相流中的液-汽界面演化，适用于需要快速响应的工业过程控制。


<details>
  <summary>Details</summary>
Motivation: 传统计算流体动力学（CFD）模型在多相流问题中因计算速度不足而难以实现快速控制，尤其是在涉及大密度梯度或相变的复杂配置中。

Method: 使用体积流体（VOF）模拟数据训练神经算子，结合实验数据或模拟数据进行训练。

Result: 神经算子的预测时间尺度与多相应用的时间尺度相当，且在预测液-汽界面演化时表现出极高的准确性。

Conclusion: 神经算子能够有效控制需要快速响应的工业过程，特别是在多相流问题中表现出强大的预测能力。

Abstract: Recent progress in AI has established neural operators as powerful tools that
can predict the evolution of partial differential equations, such as the
Navier-Stokes equations. Some complex problems rely on sophisticated algorithms
to deal with strong discontinuities in the computational domain. For example,
liquid-vapour multiphase flows are a challenging problem in many
configurations, particularly those involving large density gradients or phase
change. The complexity mentioned above has not allowed for fine control of fast
industrial processes or applications because computational fluid dynamics (CFD)
models do not have a quick enough forecasting ability. This work demonstrates
that the time scale of neural operators-based predictions is comparable to the
time scale of multi-phase applications, thus proving they can be used to
control processes that require fast response. Neural Operators can be trained
using experimental data, simulations or a combination. In the following, neural
operators were trained in volume of fluid simulations, and the resulting
predictions showed very high accuracy, particularly in predicting the evolution
of the liquid-vapour interface, one of the most critical tasks in a multi-phase
process controller.

</details>


### [215] [The Spotlight Resonance Method: Resolving the Alignment of Embedded Activations](https://arxiv.org/abs/2505.13471)
*George Bird*

Main category: cs.LG

TL;DR: 该论文提出了一种可视化工具，用于分析深度学习模型中嵌入数据的轴向对齐情况，揭示了激活函数如何导致表示与神经元基对齐。


<details>
  <summary>Details</summary>
Motivation: 当前理解深度学习模型如何表示数据的方法有限，因此需要开发新工具来揭示模型内部表示的对齐特性。

Method: 通过评估网络特权基向量定义的平面周围的分布，提供了一种直观的度量方法，并引入分辨率超参数以探测不同角度尺度。

Result: 研究发现嵌入表示倾向于与特权基对齐，激活函数直接导致特权基的形成，并发现了所谓的“祖母神经元”实例。

Conclusion: 该方法揭示了功能形式对称性破缺与表示对齐之间的直接因果关系，解释了表示倾向于与神经元基对齐的原因。

Abstract: Understanding how deep learning models represent data is currently difficult
due to the limited number of methodologies available. This paper demonstrates a
versatile and novel visualisation tool for determining the axis alignment of
embedded data at any layer in any deep learning model. In particular, it
evaluates the distribution around planes defined by the network's privileged
basis vectors. This method provides both an atomistic and a holistic, intuitive
metric for interpreting the distribution of activations across all planes. It
ensures that both positive and negative signals contribute, treating the
activation vector as a whole. Depending on the application, several variations
of this technique are presented, with a resolution scale hyperparameter to
probe different angular scales. Using this method, multiple examples are
provided that demonstrate embedded representations tend to be axis-aligned with
the privileged basis. This is not necessarily the standard basis, and it is
found that activation functions directly result in privileged bases. Hence, it
provides a direct causal link between functional form symmetry breaking and
representational alignment, explaining why representations have a tendency to
align with the neuron basis. Therefore, using this method, we begin to answer
the fundamental question of what causes the observed tendency of
representations to align with neurons. Finally, examples of so-called
grandmother neurons are found in a variety of networks.

</details>


### [216] [Optimal Control for Transformer Architectures: Enhancing Generalization, Robustness and Efficiency](https://arxiv.org/abs/2505.13499)
*Kelvin Kan,Xingjian Li,Benjamin J. Zhang,Tuhin Sahai,Stanley Osher,Markos A. Katsoulakis*

Main category: cs.LG

TL;DR: 该论文通过最优控制理论优化Transformer的训练和架构设计，提升性能并减少参数，实验证明在多个任务上有效。


<details>
  <summary>Details</summary>
Motivation: 研究旨在利用最优控制理论为Transformer提供理论支持，超越传统的试错方法，实现系统化改进。

Method: 采用连续时间最优控制理论框架，对现有Transformer模型进行轻量级修改，实现即插即用的优化。

Result: 实验显示，该框架在文本生成、情感分析等任务中显著降低测试损失（如nanoGPT测试损失减少46%），且参数效率更高（如节省42%参数）。

Conclusion: 该研究首次将最优控制理论应用于Transformer，为理论驱动的模型优化提供了新基础。

Abstract: We study Transformers through the perspective of optimal control theory,
using tools from continuous-time formulations to derive actionable insights
into training and architecture design. This framework improves the performance
of existing Transformer models while providing desirable theoretical
guarantees, including generalization and robustness. Our framework is designed
to be plug-and-play, enabling seamless integration with established Transformer
models and requiring only slight changes to the implementation. We conduct
seven extensive experiments on tasks motivated by text generation, sentiment
analysis, image classification, and point cloud classification. Experimental
results show that the framework improves the test performance of the baselines,
while being more parameter-efficient. On character-level text generation with
nanoGPT, our framework achieves a 46% reduction in final test loss while using
42% fewer parameters. On GPT-2, our framework achieves a 5.6% reduction in
final test loss, demonstrating scalability to larger models. To the best of our
knowledge, this is the first work that applies optimal control theory to both
the training and architecture of Transformers. It offers a new foundation for
systematic, theory-driven improvements and moves beyond costly trial-and-error
approaches.

</details>


### [217] [SPIEDiff: robust learning of long-time macroscopic dynamics from short-time particle simulations with quantified epistemic uncertainty](https://arxiv.org/abs/2505.13501)
*Zequn He,Celia Reina*

Main category: cs.LG

TL;DR: SPIEDiff框架利用统计物理和机器学习，解决了耗散系统长期宏观动力学和热力学数据驱动发现的难题。


<details>
  <summary>Details</summary>
Motivation: 耗散系统的长期宏观动力学和热力学研究面临时间尺度限制、热力学势非唯一性和不确定性量化等挑战。

Method: 提出SPIEDiff框架，结合统计物理、条件扩散模型和epinets，用于纯耗散系统的建模。

Result: SPIEDiff能准确揭示热力学和动力学，仅需短时粒子模拟数据即可进行长期预测，计算效率大幅提升。

Conclusion: SPIEDiff为热力学模型的数据驱动发现提供了可靠且高效的新途径。

Abstract: The data-driven discovery of long-time macroscopic dynamics and
thermodynamics of dissipative systems with particle fidelity is hampered by
significant obstacles. These include the strong time-scale limitations inherent
to particle simulations, the non-uniqueness of the thermodynamic potentials and
operators from given macroscopic dynamics, and the need for efficient
uncertainty quantification. This paper introduces Statistical-Physics Informed
Epistemic Diffusion Models (SPIEDiff), a machine learning framework designed to
overcome these limitations in the context of purely dissipative systems by
leveraging statistical physics, conditional diffusion models, and epinets. We
evaluate the proposed framework on stochastic Arrhenius particle processes and
demonstrate that SPIEDiff can accurately uncover both thermodynamics and
kinetics, while enabling reliable long-time macroscopic predictions using only
short-time particle simulation data. SPIEDiff can deliver accurate predictions
with quantified uncertainty in minutes, drastically reducing the computational
demand compared to direct particle simulations, which would take days or years
in the examples considered. Overall, SPIEDiff offers a robust and trustworthy
pathway for the data-driven discovery of thermodynamic models.

</details>


### [218] [Federated Low-Rank Adaptation for Foundation Models: A Survey](https://arxiv.org/abs/2505.13502)
*Yiyuan Yang,Guodong Long,Qinghua Lu,Liming Zhu,Jing Jiang,Chengqi Zhang*

Main category: cs.LG

TL;DR: 该论文综述了如何将低秩适应（LoRA）集成到联邦学习（FL）中，以高效微调基础模型，同时解决分布式学习、异构性和效率三大挑战。


<details>
  <summary>Details</summary>
Motivation: 有效利用私有数据集开发基础模型仍具挑战性，联邦学习（FL）和低秩适应（LoRA）的结合可兼顾隐私保护和资源效率。

Method: 通过分类现有工作，分析如何用LoRA解决联邦微调中的分布式学习、异构性和效率问题。

Result: 提出了FedLoRA框架，总结了现有方法，并指出未来研究方向。

Conclusion: FedLoRA为高效、隐私保护的模型微调提供了新方向，未来需进一步探索其潜力。

Abstract: Effectively leveraging private datasets remains a significant challenge in
developing foundation models. Federated Learning (FL) has recently emerged as a
collaborative framework that enables multiple users to fine-tune these models
while mitigating data privacy risks. Meanwhile, Low-Rank Adaptation (LoRA)
offers a resource-efficient alternative for fine-tuning foundation models by
dramatically reducing the number of trainable parameters. This survey examines
how LoRA has been integrated into federated fine-tuning for foundation models,
an area we term FedLoRA, by focusing on three key challenges: distributed
learning, heterogeneity, and efficiency. We further categorize existing work
based on the specific methods used to address each challenge. Finally, we
discuss open research questions and highlight promising directions for future
investigation, outlining the next steps for advancing FedLoRA.

</details>


### [219] [Open Set Domain Adaptation with Vision-language models via Gradient-aware Separation](https://arxiv.org/abs/2505.13507)
*Haoyang Chen*

Main category: cs.LG

TL;DR: 本文提出了一种利用CLIP模型解决开放集域适应问题的新方法，通过动态调整文本提示和梯度分析模块，有效对齐已知类别并识别未知类别。


<details>
  <summary>Details</summary>
Motivation: 开放集域适应（OSDA）面临跨域对齐已知类别分布和识别目标域特定未知类别的双重挑战。现有方法未能充分利用模态间的语义关系，且在未知样本检测中存在误差累积问题。

Method: 1) 基于提示的跨域对齐：通过可学习的文本提示动态调整CLIP的文本编码器，实现源域和目标域的语义一致性；2) 梯度感知的开放集分离：通过梯度分析模块量化域偏移，区分已知和未知样本的梯度行为差异。

Result: 在Office-Home数据集上的实验表明，该方法显著优于CLIP基线和标准基线，消融研究验证了梯度范数的关键作用。

Conclusion: 本文提出的方法通过结合CLIP的语义能力和梯度分析，有效解决了开放集域适应中的关键挑战，为未来研究提供了新方向。

Abstract: Open-Set Domain Adaptation (OSDA) confronts the dual challenge of aligning
known-class distributions across domains while identifying
target-domain-specific unknown categories. Current approaches often fail to
leverage semantic relationships between modalities and struggle with error
accumulation in unknown sample detection. We propose to harness Contrastive
Language-Image Pretraining (CLIP) to address these limitations through two key
innovations: 1) Prompt-driven cross-domain alignment: Learnable textual prompts
conditioned on domain discrepancy metrics dynamically adapt CLIP's text
encoder, enabling semantic consistency between source and target domains
without explicit unknown-class supervision. 2) Gradient-aware open-set
separation: A gradient analysis module quantifies domain shift by comparing the
L2-norm of gradients from the learned prompts, where known/unknown samples
exhibit statistically distinct gradient behaviors. Evaluations on Office-Home
show that our method consistently outperforms CLIP baseline and standard
baseline. Ablation studies confirm the gradient norm's critical role.

</details>


### [220] [On the definition and importance of interpretability in scientific machine learning](https://arxiv.org/abs/2505.13510)
*Conor Rowan,Alireza Doostan*

Main category: cs.LG

TL;DR: 论文探讨了神经网络在科学模型中的可解释性问题，提出应关注机制理解而非数学稀疏性，并重新定义了科学机器学习中的可解释性。


<details>
  <summary>Details</summary>
Motivation: 尽管神经网络在大数据集上训练成功用于描述和预测物理现象，但科学家认为其与传统科学模型不同，无法将发现整合到科学知识体系中。机器学习缺乏产生人类可理解关系的能力，引发了对可解释性的关注。

Method: 论文回顾了科学界外关于可解释性机器学习的关键论文，指出这些定义和方法虽能启发科学机器学习中的可解释性问题，但并不完全适用。作者提出了一个针对物理科学的可解释性操作定义。

Result: 作者提出的可解释性定义强调对机制的理解而非数学稀疏性，表明稀疏性通常是不必要的，并质疑在缺乏先验知识时进行可解释科学发现的可能性。

Conclusion: 论文认为，一个精确且哲学上合理的可解释性定义将有助于集中研究努力，克服实现数据驱动科学未来的重大障碍。

Abstract: Though neural networks trained on large data sets have been successfully used
to describe and predict many physical phenomena, there is a sense among
scientists that, unlike traditional scientific models, where relationships come
packaged in the form of simple mathematical expressions, the findings of the
neural network cannot be integrated into the body of scientific knowledge.
Critics of ML's inability to produce human-understandable relationships have
converged on the concept of "interpretability" as its point of departure from
more traditional forms of science. As the growing interest in interpretability
has shown, researchers in the physical sciences seek not just predictive
models, but also to uncover the fundamental principles that govern a system of
interest. However, clarity around a definition of interpretability and the
precise role that it plays in science is lacking in the literature. In this
work, we argue that researchers in equation discovery and symbolic regression
tend to conflate the concept of sparsity with interpretability. We review key
papers on interpretable ML from outside the scientific community and argue
that, though the definitions and methods they propose can inform questions of
interpretability for SciML, they are inadequate for this new purpose. Noting
these deficiencies, we propose an operational definition of interpretability
for the physical sciences. Our notion of interpretability emphasizes
understanding of the mechanism over mathematical sparsity. Innocuous though it
may seem, this emphasis on mechanism shows that sparsity is often unnecessary.
It also questions the possibility of interpretable scientific discovery when
prior knowledge is lacking. We believe a precise and philosophically informed
definition of interpretability in SciML will help focus research efforts toward
the most significant obstacles to realizing a data-driven scientific future.

</details>


### [221] [LoRASuite: Efficient LoRA Adaptation Across Large Language Model Upgrades](https://arxiv.org/abs/2505.13515)
*Yanan Li,Fanxu Meng,Muhan Zhang,Shiai Zhu,Shangguang Wang,Mengwei Xu*

Main category: cs.LG

TL;DR: LoRASuite提出模块化方法，利用已有LoRA权重适配新版大模型，显著提升效率并降低资源消耗。


<details>
  <summary>Details</summary>
Motivation: 随着大模型频繁更新，传统从头训练LoRA权重的方式成本高、耗时长且不环保，亟需高效迁移方法。

Method: 通过计算新旧模型参数转移矩阵，基于对齐指标分配层和注意力头，并进行小规模精调确保稳定性。

Result: 在MiniCPM和Qwen模型上，LoRASuite数学任务平均提升1.4和6.6分，内存减少5.5GB，计算时间降低78.23%。

Conclusion: LoRASuite不仅超越小规模LoRA方法，甚至优于全量重训练，为模型更新提供高效解决方案。

Abstract: As Large Language Models (LLMs) are frequently updated, LoRA weights trained
on earlier versions quickly become obsolete. The conventional practice of
retraining LoRA weights from scratch on the latest model is costly,
time-consuming, and environmentally detrimental, particularly as the diversity
of LLMs and downstream tasks expands. This motivates a critical question: "How
can we efficiently leverage existing LoRA weights to adapt to newer model
versions?" To address this, we propose LoRASuite, a modular approach tailored
specifically to various types of LLM updates. First, we compute a transfer
matrix utilizing known parameters from both old and new LLMs. Next, we allocate
corresponding layers and attention heads based on centered kernel alignment and
cosine similarity metrics, respectively. A subsequent small-scale, skillful
fine-tuning step ensures numerical stability. Experimental evaluations
demonstrate that LoRASuite consistently surpasses small-scale vanilla LoRA
methods. Notably, on backbone LLMs such as MiniCPM and Qwen, LoRASuite even
exceeds the performance of full-scale LoRA retraining, with average
improvements of +1.4 and +6.6 points on math tasks, respectively. Additionally,
LoRASuite significantly reduces memory consumption by 5.5 GB and computational
time by 78.23%.

</details>


### [222] [Zero-Shot Forecasting Mortality Rates: A Global Study](https://arxiv.org/abs/2505.13521)
*Gabor Petnehazi,Laith Al Shaggah,Jozsef Gall,Bernadett Aradi*

Main category: cs.LG

TL;DR: 该研究探讨了零样本时间序列预测在死亡率预测中的应用，比较了两种基础模型与传统方法的表现，发现零样本预测具有潜力但需谨慎选择模型。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探索零样本时间序列预测在死亡率预测中的潜力，避免任务特定的微调，利用预训练的基础模型进行预测。

Method: 研究评估了两种最先进的基础模型TimesFM和CHRONOS，以及传统和机器学习方法，在5、10和20年的预测时间范围内，使用来自50个国家和111个年龄组的数据。

Result: 零样本模型表现不一：CHRONOS在短期预测中表现优异，优于ARIMA和Lee-Carter模型，而TimesFM表现不佳。对CHRONOS进行微调显著提高了长期预测准确性。随机森林模型在整体表现上最佳。

Conclusion: 研究结果强调了零样本预测的潜力，同时指出需要谨慎选择模型并进行领域特定的适应。

Abstract: This study explores the potential of zero-shot time series forecasting, an
innovative approach leveraging pre-trained foundation models, to forecast
mortality rates without task-specific fine-tuning. We evaluate two
state-of-the-art foundation models, TimesFM and CHRONOS, alongside traditional
and machine learning-based methods across three forecasting horizons (5, 10,
and 20 years) using data from 50 countries and 111 age groups. In our
investigations, zero-shot models showed varying results: while CHRONOS
delivered competitive shorter-term forecasts, outperforming traditional methods
like ARIMA and the Lee-Carter model, TimesFM consistently underperformed.
Fine-tuning CHRONOS on mortality data significantly improved long-term
accuracy. A Random Forest model, trained on mortality data, achieved the best
overall performance. These findings underscore the potential of zero-shot
forecasting while highlighting the need for careful model selection and
domain-specific adaptation.

</details>


### [223] [Multi-head Temporal Latent Attention](https://arxiv.org/abs/2505.13544)
*Keqi Deng,Philip C. Woodland*

Main category: cs.LG

TL;DR: MTLA通过动态合并相邻KV缓存向量和步长感知因果掩码，显著降低自注意力推理的内存占用，提升推理速度，同时保持性能。


<details>
  <summary>Details</summary>
Motivation: Transformer自注意力中的KV缓存随序列长度线性增长，成为推理效率的瓶颈。

Method: 提出多头时间潜在注意力（MTLA），利用超网络动态合并时间相邻的KV缓存向量，并引入步长感知因果掩码解决压缩KV缓存与序列长度不匹配问题。

Result: 在语音翻译等任务中，MTLA相比标准多头注意力（MHA）实现5.3倍加速和8.3倍GPU内存节省，同时保持翻译质量。

Conclusion: MTLA在显著提升推理效率和降低内存占用的同时，保持了与MHA竞争的性能表现。

Abstract: While Transformer self-attention offers strong parallelism, the Key-Value
(KV) cache grows linearly with sequence length and becomes a bottleneck for
inference efficiency. Multi-head latent attention was recently developed to
compress the KV cache into a low-rank latent space. This paper proposes
Multi-head Temporal Latent Attention (MTLA), which further reduces the KV cache
size along the temporal dimension, greatly lowering the memory footprint of
self-attention inference. MTLA employs a hyper-network to dynamically merge
temporally adjacent KV cache vectors. To address the mismatch between the
compressed KV cache and processed sequence lengths, a stride-aware causal mask
is proposed to ensure efficient parallel training and consistency with
inference behaviour. Experiments across tasks, including speech translation,
speech recognition, speech understanding and text summarisation, demonstrate
that MTLA achieves competitive performance compared to standard Multi-Head
Attention (MHA), while greatly improving inference speed and GPU memory usage.
For example, on a English-German speech translation task, MTLA achieves a 5.3x
speedup and a reduction in GPU memory usage by a factor of 8.3 compared to MHA,
while maintaining translation quality.

</details>


### [224] [Exploring Federated Pruning for Large Language Models](https://arxiv.org/abs/2505.13547)
*Pengxin Guo,Yinong Wang,Wei Li,Mengting Liu,Ming Li,Jinkai Zheng,Liangqiong Qu*

Main category: cs.LG

TL;DR: FedPrLLM是一个联邦学习框架，用于在保护隐私的前提下压缩大型语言模型（LLM），通过客户端本地计算剪枝掩码矩阵并与服务器共享来实现全局模型剪枝。


<details>
  <summary>Details</summary>
Motivation: 当前LLM剪枝方法通常需要公开校准样本，这在隐私敏感领域难以获取。FedPrLLM旨在解决这一问题，实现隐私保护的LLM压缩。

Method: FedPrLLM框架中，每个客户端基于本地校准数据计算剪枝掩码矩阵，并与服务器共享以剪枝全局模型，从而在保护本地数据隐私的同时实现协作剪枝。

Result: 实验表明，在FedPrLLM框架中，采用一次性剪枝、分层比较且不缩放权重是最优选择。

Conclusion: FedPrLLM为隐私敏感领域的LLM剪枝提供了有效指导，未来可进一步推动该领域的研究。

Abstract: LLM pruning has emerged as a promising technology for compressing LLMs,
enabling their deployment on resource-limited devices. However, current
methodologies typically require access to public calibration samples, which can
be challenging to obtain in privacy-sensitive domains. To address this issue,
we introduce FedPrLLM, a comprehensive federated pruning framework designed for
the privacy-preserving compression of LLMs. In FedPrLLM, each client only needs
to calculate a pruning mask matrix based on its local calibration data and
share it with the server to prune the global model. This approach allows for
collaborative pruning of the global model with the knowledge of each client
while maintaining local data privacy. Additionally, we conduct extensive
experiments to explore various possibilities within the FedPrLLM framework,
including different comparison groups, pruning strategies, and the decision to
scale weights. Our extensive evaluation reveals that one-shot pruning with
layer comparison and no weight scaling is the optimal choice within the
FedPrLLM framework. We hope our work will help guide future efforts in pruning
LLMs in privacy-sensitive fields. Our code is available at
https://github.com/Pengxin-Guo/FedPrLLM.

</details>


### [225] [Breaking the Compression Ceiling: Data-Free Pipeline for Ultra-Efficient Delta Compression](https://arxiv.org/abs/2505.13563)
*Xiaohui Wang,Peng Ye,Chenyu Huang,Shenghe Zheng,Bo Zhang,Wanli Ouyang,Tao Chen*

Main category: cs.LG

TL;DR: 论文提出了一种名为UltraDelta的无数据delta压缩方法，通过多层优化策略实现超高压缩比并保持模型性能。


<details>
  <summary>Details</summary>
Motivation: 随着微调预训练模型的普及，存储大量微调模型带来了显著的存储开销。现有delta压缩方法无法同时保证高压缩率和性能，且常依赖数据。

Method: UltraDelta采用三种关键技术：(1)基于方差的混合稀疏分配，(2)分布感知压缩，(3)迹范数引导的重新缩放，从层间、层内和全局维度优化压缩。

Result: 实验表明，UltraDelta在LLaMA-2(133x)、NLP模型(800x)、视觉模型(400x)和多模态模型(40x)上均优于现有方法，尤其在超高压缩比下表现突出。

Conclusion: UltraDelta首次实现了无数据、超高压缩比且性能稳定的delta压缩，为多任务模型存储提供了高效解决方案。

Abstract: With the rise of the fine-tuned--pretrained paradigm, storing numerous
fine-tuned models for multi-tasking creates significant storage overhead. Delta
compression alleviates this by storing only the pretrained model and the highly
compressed delta weights (the differences between fine-tuned and pretrained
model weights). However, existing methods fail to maintain both high
compression and performance, and often rely on data. To address these
challenges, we propose UltraDelta, the first data-free delta compression
pipeline that achieves both ultra-high compression and strong performance.
UltraDelta is designed to minimize redundancy, maximize information, and
stabilize performance across inter-layer, intra-layer, and global dimensions,
using three key components: (1) Variance-Based Mixed Sparsity Allocation
assigns sparsity based on variance, giving lower sparsity to high-variance
layers to preserve inter-layer information. (2) Distribution-Aware Compression
applies uniform quantization and then groups parameters by value, followed by
group-wise pruning, to better preserve intra-layer distribution. (3)
Trace-Norm-Guided Rescaling uses the trace norm of delta weights to estimate a
global rescaling factor, improving model stability under higher compression.
Extensive experiments across (a) large language models (fine-tuned on LLaMA-2
7B and 13B) with up to 133x, (b) general NLP models (RoBERTa-base, T5-base)
with up to 800x, (c) vision models (ViT-B/32, ViT-L/14) with up to 400x, and
(d) multi-modal models (BEiT-3) with 40x compression ratio, demonstrate that
UltraDelta consistently outperforms existing methods, especially under
ultra-high compression.

</details>


### [226] [Online Decision-Focused Learning](https://arxiv.org/abs/2505.13564)
*Aymeric Capitaine,Maxime Haddouche,Eric Moulines,Michael I. Jordan,Etienne Boursier,Alain Durmus*

Main category: cs.LG

TL;DR: 该论文研究了动态环境中的决策导向学习（DFL），提出了一种在线算法，通过正则化和乐观原则处理目标函数梯度问题，并在简单背包实验中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 传统的决策导向学习（DFL）仅适用于静态环境和固定数据批次，而现实中的决策问题往往涉及动态变化的目标函数和数据分布。因此，研究动态环境中的DFL具有重要实际意义。

Method: 论文提出了一种在线算法，通过（i）正则化目标函数使其可微，（ii）利用乐观原则和近优预言机及适当扰动，解决了动态环境中目标函数梯度为零或未定义的问题。

Result: 论文在单纯形和一般有界凸多面体决策空间上建立了预期动态遗憾的界限，并通过简单背包实验证明了所提算法优于传统的预测导向方法。

Conclusion: 该研究为动态环境中的决策导向学习提供了实用算法，并通过理论分析和实验验证了其有效性，为复杂组合问题的动态决策提供了新思路。

Abstract: Decision-focused learning (DFL) is an increasingly popular paradigm for
training predictive models whose outputs are used in decision-making tasks.
Instead of merely optimizing for predictive accuracy, DFL trains models to
directly minimize the loss associated with downstream decisions. This
end-to-end strategy holds promise for tackling complex combinatorial problems;
however, existing studies focus solely on scenarios where a fixed batch of data
is available and the objective function does not change over time. We instead
investigate DFL in dynamic environments where the objective function and data
distribution evolve over time. This setting is challenging because the
objective function has zero or undefined gradients -- which prevents the use of
standard first-order optimization methods -- and is generally non-convex. To
address these difficulties, we (i) regularize the objective to make it
differentiable and (ii) make use of the optimism principle, based on a
near-optimal oracle along with an appropriate perturbation. This leads to a
practical online algorithm for which we establish bounds on the expected
dynamic regret, both when the decision space is a simplex and when it is a
general bounded convex polytope. Finally, we demonstrate the effectiveness of
our algorithm by comparing its performance with a classic prediction-focused
approach on a simple knapsack experiment.

</details>


### [227] [Learning Dynamics of RNNs in Closed-Loop Environments](https://arxiv.org/abs/2505.13567)
*Yoav Ger,Omri Barak*

Main category: cs.LG

TL;DR: 该论文开发了一个数学理论，描述线性循环神经网络在闭环环境中的学习动态，揭示了闭环与开环训练模式的显著差异及其对生物合理性建模的重要性。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的学习发生在闭环环境中，而传统的循环神经网络训练通常在开环、监督设置下进行。论文旨在填补这一差距，研究闭环训练对神经网络学习动态的影响。

Method: 论文首先比较了闭环和开环训练模式下相同循环神经网络的学习轨迹差异，然后通过数学理论分析了闭环训练的学习动态，并将其应用于实际的运动控制任务。

Result: 研究发现，闭环训练的学习动态受短期策略改进和长期环境交互稳定性两个目标的共同支配，与开环训练有显著不同。

Conclusion: 论文强调了在生物合理性设置中建模闭环动态的重要性，为理解大脑计算提供了更贴近实际的模型。

Abstract: Recurrent neural networks (RNNs) trained on neuroscience-inspired tasks offer
powerful models of brain computation. However, typical training paradigms rely
on open-loop, supervised settings, whereas real-world learning unfolds in
closed-loop environments. Here, we develop a mathematical theory describing the
learning dynamics of linear RNNs trained in closed-loop contexts. We first
demonstrate that two otherwise identical RNNs, trained in either closed- or
open-loop modes, follow markedly different learning trajectories. To probe this
divergence, we analytically characterize the closed-loop case, revealing
distinct stages aligned with the evolution of the training loss. Specifically,
we show that the learning dynamics of closed-loop RNNs, in contrast to
open-loop ones, are governed by an interplay between two competing objectives:
short-term policy improvement and long-term stability of the agent-environment
interaction. Finally, we apply our framework to a realistic motor control task,
highlighting its broader applicability. Taken together, our results underscore
the importance of modeling closed-loop dynamics in a biologically plausible
setting.

</details>


### [228] [Surrogate Modeling of 3D Rayleigh-Benard Convection with Equivariant Autoencoders](https://arxiv.org/abs/2505.13569)
*Fynn Fromme,Christine Allen-Blanchette,Hans Harder,Sebastian Peitz*

Main category: cs.LG

TL;DR: 论文提出了一种基于等变卷积自编码器和等变卷积LSTM的端到端替代模型，用于建模和控制大规模物理系统，以三维Rayleigh-Bénard对流为例，展示了在样本和参数效率上的显著提升。


<details>
  <summary>Details</summary>
Motivation: 大规模物理系统（如电磁学、核聚变反应堆、磁流体力学等）的建模和控制面临高自由度和复杂动力学的挑战，需要提高准确性和样本效率。

Method: 使用包含等变卷积自编码器和等变卷积LSTM的端到端替代模型，采用G-可操纵核，特别针对三维Rayleigh-Bénard对流系统设计。

Result: 模型在样本和参数效率上取得显著提升，并能更好地扩展到更复杂的动力学（如更大的Rayleigh数）。

Conclusion: 提出的等变替代模型在复杂物理系统的建模和控制中表现出高效性和可扩展性，为相关领域提供了有力工具。

Abstract: The use of machine learning for modeling, understanding, and controlling
large-scale physics systems is quickly gaining in popularity, with examples
ranging from electromagnetism over nuclear fusion reactors and
magneto-hydrodynamics to fluid mechanics and climate modeling. These systems --
governed by partial differential equations -- present unique challenges
regarding the large number of degrees of freedom and the complex dynamics over
many scales both in space and time, and additional measures to improve accuracy
and sample efficiency are highly desirable. We present an end-to-end
equivariant surrogate model consisting of an equivariant convolutional
autoencoder and an equivariant convolutional LSTM using $G$-steerable kernels.
As a case study, we consider the three-dimensional Rayleigh-B\'enard
convection, which describes the buoyancy-driven fluid flow between a heated
bottom and a cooled top plate. While the system is E(2)-equivariant in the
horizontal plane, the boundary conditions break the translational equivariance
in the vertical direction. Our architecture leverages vertically stacked layers
of $D_4$-steerable kernels, with additional partial kernel sharing in the
vertical direction for further efficiency improvement. Our results demonstrate
significant gains both in sample and parameter efficiency, as well as a better
scaling to more complex dynamics, that is, larger Rayleigh numbers. The
accompanying code is available under
https://github.com/FynnFromme/equivariant-rb-forecasting.

</details>


### [229] [An Overview of Arithmetic Adaptations for Inference of Convolutional Neural Networks on Re-configurable Hardware](https://arxiv.org/abs/2505.13575)
*Ilkay Wunderlich,Benjamin Koch,Sven Schönfeld*

Main category: cs.LG

TL;DR: 论文探讨了在FPGA上部署TinyYOLOv3目标检测网络的优化方法，包括批量归一化融合、滤波器剪枝和训练后网络量化技术。


<details>
  <summary>Details</summary>
Motivation: 卷积神经网络（CNN）在计算机视觉任务中广泛应用，但在嵌入式平台（如FPGA）上部署时面临计算强度高、内存需求大和算术条件复杂等挑战。

Method: 采用批量归一化融合、滤波器剪枝和训练后网络量化等技术，优化TinyYOLOv3在XILINX Artix-7 FPGA上的运行效率。

Result: 通过上述方法，成功降低了计算复杂度和内存需求，使TinyYOLOv3在FPGA上的部署更加高效。

Conclusion: 论文提出的优化策略有效解决了CNN在FPGA上部署的难题，为目标检测网络在嵌入式平台的应用提供了可行方案。

Abstract: Convolutional Neural Networks (CNNs) have gained high popularity as a tool
for computer vision tasks and for that reason are used in various applications.
There are many different concepts, like single shot detectors, that have been
published for detecting objects in images or video streams. However, CNNs
suffer from disadvantages regarding the deployment on embedded platforms such
as re-configurable hardware like Field Programmable Gate Arrays (FPGAs). Due to
the high computational intensity, memory requirements and arithmetic
conditions, a variety of strategies for running CNNs on FPGAs have been
developed. The following methods showcase our best practice approaches for a
TinyYOLOv3 detector network on a XILINX Artix-7 FPGA using techniques like
fusion of batch normalization, filter pruning and post training network
quantization.

</details>


### [230] [FlexFed: Mitigating Catastrophic Forgetting in Heterogeneous Federated Learning in Pervasive Computing Environments](https://arxiv.org/abs/2505.13576)
*Sara Alosaime,Arshad Jhumka*

Main category: cs.LG

TL;DR: 本文提出FlexFed方法，解决联邦学习在人类活动识别（HAR）中的灾难性遗忘问题，通过动态调整训练频率和新评估指标提升效率。


<details>
  <summary>Details</summary>
Motivation: HAR环境中设备资源受限、数据分布非稳态且参与间歇性，导致现有联邦学习方法面临灾难性遗忘（CF）问题，而隐私限制又排除了传统持续学习策略的应用。

Method: 提出FlexFed方法：1）优先保留关键数据以高效利用内存；2）根据数据分布偏移、客户端能力和离线时长动态调整训练频率；3）引入新指标量化CF，并开发模拟HAR动态特性的评估框架。

Result: 实验表明FlexFed有效缓解CF，联邦学习效率提升10%-15%，收敛更快更稳定，尤其对低频或代表性不足的数据效果显著。

Conclusion: FlexFed通过数据保留策略和动态训练机制，在隐私约束下解决了HAR环境中的CF问题，为资源受限的流式场景提供了可行的联邦学习优化方案。

Abstract: Federated Learning (FL) enables collaborative model training while preserving
privacy by allowing clients to share model updates instead of raw data.
Pervasive computing environments (e.g., for Human Activity Recognition, HAR),
which we focus on in this paper, are characterized by resource-constrained end
devices, streaming sensor data and intermittent client participation.
Variations in user behavior, common in HAR environments, often result in
non-stationary data distributions. As such, existing FL approaches face
challenges in HAR settings due to differing assumptions. The combined effects
of HAR characteristics, namely heterogeneous data and intermittent
participation, can lead to a severe issue called catastrophic forgetting (CF).
Unlike Continuous Learning (CL), which addresses CF using memory and replay
mechanisms, FL's privacy constraints prohibit such strategies.
  To tackle CF in HAR environments, we propose FlexFed, a novel FL approach
that prioritizes data retention for efficient memory use and dynamically
adjusts offline training frequency based on distribution shifts, client
capability and offline duration. To better quantify CF in FL, we introduce a
new metric that accounts for under-represented data, enabling more accurate
evaluations. We also develop a realistic HAR-based evaluation framework that
simulates streaming data, dynamic distributions, imbalances and varying
availability. Experiments show that FlexFed mitigates CF more effectively,
improves FL efficiency by 10 to 15 % and achieves faster, more stable
convergence, especially for infrequent or under-represented data.

</details>


### [231] [Symmetry-Breaking Descent for Invariant Cost Functionals](https://arxiv.org/abs/2505.13578)
*Mikhail Osipov*

Main category: cs.LG

TL;DR: 该论文提出了一种利用对称性结构优化不变成本函数的方法，通过构造显式的对称性破坏变形信号，无需模型梯度或标签即可在测试时操作。


<details>
  <summary>Details</summary>
Motivation: 在机器学习、成像和反问题中，成本函数通常反映模型输出或性能评分，但可能是非可微且模型内部的。论文旨在解决在全局对称群下不变的成本函数优化问题。

Method: 论文提出了一种变分方法，通过最小化辅助能量泛函获得规范场，构造对称性破坏的变形信号，从而优化不变成本函数。

Result: 在温和的正则条件下，成本函数沿变形方向严格递减，或通过Clarke次微分下降逃离局部平坦区域。退化集的测度为零。

Conclusion: 该方法为优化不变成本函数提供了理论工具，适用于黑盒模型和对称性约束任务，无需模型梯度或标签即可操作。

Abstract: We study the problem of reducing a task cost functional $W(S)$, defined over
Sobolev-class signals $S$, when the cost is invariant under a global symmetry
group $G \subset \mathrm{Diff}(M)$ and accessible only as a black-box. Such
scenarios arise in machine learning, imaging, and inverse problems, where cost
metrics reflect model outputs or performance scores but are non-differentiable
and model-internal. We propose a variational method that exploits the symmetry
structure to construct explicit, symmetry-breaking deformations of the input
signal. A gauge field $\phi$, obtained by minimizing an auxiliary energy
functional, induces a deformation $h = A_\phi[S]$ that generically lies
transverse to the $G$-orbit of $S$. We prove that, under mild regularity, the
cost $W$ strictly decreases along this direction -- either via Clarke
subdifferential descent or by escaping locally flat plateaus. The exceptional
set of degeneracies has zero Gaussian measure. Our approach requires no access
to model gradients or labels and operates entirely at test time. It provides a
principled tool for optimizing invariant cost functionals via Lie-algebraic
variational flows, with applications to black-box models and
symmetry-constrained tasks.

</details>


### [232] [OMGPT: A Sequence Modeling Framework for Data-driven Operational Decision Making](https://arxiv.org/abs/2505.13580)
*Hanzhao Wang,Guanting Chen,Kalyan Talluri,Xiaocheng Li*

Main category: cs.LG

TL;DR: 该论文提出了一个名为OMGPT的生成预训练Transformer模型，用于解决运营研究和管理科学中的序列决策问题，如动态定价、库存管理等，并通过实验验证了其卓越性能。


<details>
  <summary>Details</summary>
Motivation: 传统方法在处理运营研究和管理科学中的序列决策问题时，通常依赖于特定的分析模型结构，且无法充分利用预训练数据。作者希望通过构建一个通用的序列建模框架，利用Transformer的强大能力，实现更灵活和高效的决策。

Method: 作者提出了一个通用的序列建模框架，将多种运营决策任务统一为序列预测问题，并训练了一个基于Transformer的神经网络模型（OMGPT）。该模型无需假设任何分析模型结构，能够直接从历史数据映射到未来动作。

Result: 实验结果表明，OMGPT在动态定价、库存管理、资源分配和排队控制等任务中表现出色，其性能与预训练任务的多样性和测试任务与预训练任务之间的差异相关。

Conclusion: OMGPT通过利用预训练数据和Transformer的强大序列建模能力，实现了在运营研究和管理科学任务中的卓越性能，为相关领域提供了一种新的解决方案。

Abstract: We build a Generative Pre-trained Transformer (GPT) model from scratch to
solve sequential decision making tasks arising in contexts of operations
research and management science which we call OMGPT. We first propose a general
sequence modeling framework to cover several operational decision making tasks
as special cases, such as dynamic pricing, inventory management, resource
allocation, and queueing control. Under the framework, all these tasks can be
viewed as a sequential prediction problem where the goal is to predict the
optimal future action given all the historical information. Then we train a
transformer-based neural network model (OMGPT) as a natural and powerful
architecture for sequential modeling. This marks a paradigm shift compared to
the existing methods for these OR/OM tasks in that (i) the OMGPT model can take
advantage of the huge amount of pre-trained data; (ii) when tackling these
problems, OMGPT does not assume any analytical model structure and enables a
direct and rich mapping from the history to the future actions. Either of these
two aspects, to the best of our knowledge, is not achieved by any existing
method. We establish a Bayesian perspective to theoretically understand the
working mechanism of the OMGPT on these tasks, which relates its performance
with the pre-training task diversity and the divergence between the testing
task and pre-training tasks. Numerically, we observe a surprising performance
of the proposed model across all the above tasks.

</details>


### [233] [Uncovering Critical Sets of Deep Neural Networks via Sample-Independent Critical Lifting](https://arxiv.org/abs/2505.13582)
*Leyang Zhang,Yaoyu Zhang,Tao Luo*

Main category: cs.LG

TL;DR: 该论文研究了神经网络临界点的样本依赖性，提出了一种样本无关的临界提升算子，并证明了样本依赖临界点的存在性。


<details>
  <summary>Details</summary>
Motivation: 研究神经网络临界点与样本之间的关系，探索样本依赖和样本无关的临界点特性。

Method: 引入样本无关的临界提升算子，将一种网络的参数关联到另一种网络的参数集，定义样本依赖和样本无关的提升临界点。

Result: 通过实例证明先前研究的临界嵌入未能捕获所有样本无关的提升临界点，并证明了样本依赖临界点在足够大样本量下的存在性。

Conclusion: 论文揭示了神经网络临界点的样本依赖性，并证明了鞍点存在于样本依赖的提升临界点中。

Abstract: This paper investigates the sample dependence of critical points for neural
networks. We introduce a sample-independent critical lifting operator that
associates a parameter of one network with a set of parameters of another, thus
defining sample-dependent and sample-independent lifted critical points. We
then show by example that previously studied critical embeddings do not capture
all sample-independent lifted critical points. Finally, we demonstrate the
existence of sample-dependent lifted critical points for sufficiently large
sample sizes and prove that saddles appear among them.

</details>


### [234] [Half Search Space is All You Need](https://arxiv.org/abs/2505.13586)
*Pavel Rumiantsev,Mark Coates*

Main category: cs.LG

TL;DR: 该论文提出了一种通过Zero-Shot NAS自动剪枝搜索空间的方法，显著降低了One-Shot NAS的内存消耗和搜索时间，同时保持了搜索精度。


<details>
  <summary>Details</summary>
Motivation: One-Shot NAS方法（如DARTS）虽然搜索效率高且实现简单，但其设计导致搜索过程中GPU内存需求较高。为了缓解这一问题，作者提出了一种自动剪枝搜索空间的方法。

Method: 利用Zero-Shot NAS高效地从搜索空间中移除低性能架构，然后在剪枝后的搜索空间上应用One-Shot NAS。

Result: 在DARTS搜索空间上的实验结果表明，该方法比基线One-Shot设置减少了81%的内存消耗，同时达到了相同的精度水平。

Conclusion: 通过结合Zero-Shot和One-Shot NAS，该方法在保持搜索精度的同时，显著降低了资源消耗，为NAS的实际应用提供了更高效的解决方案。

Abstract: Neural Architecture Search (NAS) is a powerful tool for automating
architecture design. One-Shot NAS techniques, such as DARTS, have gained
substantial popularity due to their combination of search efficiency with
simplicity of implementation. By design, One-Shot methods have high GPU memory
requirements during the search. To mitigate this issue, we propose to prune the
search space in an efficient automatic manner to reduce memory consumption and
search time while preserving the search accuracy. Specifically, we utilise
Zero-Shot NAS to efficiently remove low-performing architectures from the
search space before applying One-Shot NAS to the pruned search space.
Experimental results on the DARTS search space show that our approach reduces
memory consumption by 81% compared to the baseline One-Shot setup while
achieving the same level of accuracy.

</details>


### [235] [Deterministic Bounds and Random Estimates of Metric Tensors on Neuromanifolds](https://arxiv.org/abs/2505.13614)
*Ke Sun*

Main category: cs.LG

TL;DR: 该论文分析了深度神经网络高维参数空间中的Fisher信息度量张量，提出了一种基于Hutchinson跟踪估计器的无偏随机估计方法，并给出了其确定性边界。


<details>
  <summary>Details</summary>
Motivation: 现代深度神经网络的高维参数空间（神经流形）具有由Fisher信息定义的独特度量张量，估计这一张量对于深度学习的理论和实践方法至关重要。

Method: 通过分析分类网络的低维概率分布空间（核心空间）的黎曼度量谱，将发现扩展到神经流形上的度量张量的确定性边界，并引入基于Hutchinson跟踪估计器的无偏随机估计方法。

Result: 提出的方法可以通过一次反向传播高效评估，并能估计对角线、块对角线或完整张量，其质量保证为标准差由真实值按比例限定。

Conclusion: 该研究为深度神经网络参数空间的度量张量分析提供了高效且可靠的估计方法，对理论和应用具有重要价值。

Abstract: The high dimensional parameter space of modern deep neural networks -- the
neuromanifold -- is endowed with a unique metric tensor defined by the Fisher
information, estimating which is crucial for both theory and practical methods
in deep learning. To analyze this tensor for classification networks, we return
to a low dimensional space of probability distributions -- the core space --
and carefully analyze the spectrum of its Riemannian metric. We extend our
discoveries there into deterministic bounds of the metric tensor on the
neuromanifold. We introduce an unbiased random estimate of the metric tensor
and its bounds based on Hutchinson's trace estimator. It can be evaluated
efficiently through a single backward pass and can be used to estimate the
diagonal, or block diagonal, or the full tensor. Its quality is guaranteed with
a standard deviation bounded by the true value up to scaling.

</details>


### [236] [Learning (Approximately) Equivariant Networks via Constrained Optimization](https://arxiv.org/abs/2505.13631)
*Andrei Manolache,Luiz F. O. Chamon,Mathias Niepert*

Main category: cs.LG

TL;DR: 论文提出了一种自适应约束等变性（ACE）方法，通过逐步收紧约束来平衡等变性与非等变性，提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 现实数据常因噪声、结构变异等因素偏离完美对称性，严格等变模型难以适应，而无约束模型又无法有效利用部分对称性。

Method: 采用基于同伦原理的自适应约束优化，从非等变模型出发，逐步减少其与等变性的偏差。

Result: 在多种架构和任务中，ACE相比严格等变模型和启发式松弛方法，显著提升了性能指标、样本效率和输入扰动鲁棒性。

Conclusion: ACE通过数据驱动的平衡策略，为处理部分对称性提供了一种原则性方法，同时缓解了严格等变性对训练的限制。

Abstract: Equivariant neural networks are designed to respect symmetries through their
architecture, boosting generalization and sample efficiency when those
symmetries are present in the data distribution. Real-world data, however,
often departs from perfect symmetry because of noise, structural variation,
measurement bias, or other symmetry-breaking effects. Strictly equivariant
models may struggle to fit the data, while unconstrained models lack a
principled way to leverage partial symmetries. Even when the data is fully
symmetric, enforcing equivariance can hurt training by limiting the model to a
restricted region of the parameter space. Guided by homotopy principles, where
an optimization problem is solved by gradually transforming a simpler problem
into a complex one, we introduce Adaptive Constrained Equivariance (ACE), a
constrained optimization approach that starts with a flexible, non-equivariant
model and gradually reduces its deviation from equivariance. This gradual
tightening smooths training early on and settles the model at a data-driven
equilibrium, balancing between equivariance and non-equivariance. Across
multiple architectures and tasks, our method consistently improves performance
metrics, sample efficiency, and robustness to input perturbations compared with
strictly equivariant models and heuristic equivariance relaxations.

</details>


### [237] [Incentivizing Truthful Language Models via Peer Elicitation Games](https://arxiv.org/abs/2505.13636)
*Baiting Chen,Tong Zhu,Jiale Han,Lexin Li,Gang Li,Xiaowu Dai*

Main category: cs.LG

TL;DR: 论文提出Peer Elicitation Games (PEG)，一种无需训练、基于博弈论的框架，通过生成器和多个判别器的互动来对齐大型语言模型（LLMs），提高事实准确性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）虽然具有强大的生成能力，但仍存在不一致和幻觉问题。作者旨在通过博弈论方法，无需监督或微调，激励LLMs产生真实行为。

Method: 提出Peer Elicitation Games (PEG)框架，包含一个生成器和多个判别器，通过基于行列式的互信息评分计算奖励，激励真实报告。

Result: 理论证明代理通过在线学习实现次线性遗憾，并收敛到真实的纳什均衡。实证评估显示在多基准测试中事实准确性显著提升。

Conclusion: PEG是一种无需监督或微调的实用方法，能有效激励LLMs的真实行为，提高生成内容的准确性。

Abstract: Large Language Models (LLMs) have demonstrated strong generative capabilities
but remain prone to inconsistencies and hallucinations. We introduce Peer
Elicitation Games (PEG), a training-free, game-theoretic framework for aligning
LLMs through a peer elicitation mechanism involving a generator and multiple
discriminators instantiated from distinct base models. Discriminators interact
in a peer evaluation setting, where rewards are computed using a
determinant-based mutual information score that provably incentivizes truthful
reporting without requiring ground-truth labels. We establish theoretical
guarantees showing that each agent, via online learning, achieves sublinear
regret in the sense their cumulative performance approaches that of the best
fixed truthful strategy in hindsight. Moreover, we prove last-iterate
convergence to a truthful Nash equilibrium, ensuring that the actual policies
used by agents converge to stable and truthful behavior over time. Empirical
evaluations across multiple benchmarks demonstrate significant improvements in
factual accuracy. These results position PEG as a practical approach for
eliciting truthful behavior from LLMs without supervision or fine-tuning.

</details>


### [238] [4Hammer: a board-game reinforcement learning environment for the hour long time frame](https://arxiv.org/abs/2505.13638)
*Massimo Fioravanti,Giovanni Agosta*

Main category: cs.LG

TL;DR: 论文提出4Hammer强化学习环境，用于评估大语言模型在复杂棋盘游戏中的长期任务表现。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏针对强化学习和LLM评估的复杂棋盘游戏环境，尤其是需要长时间理解和执行规则的任务。

Method: 基于战锤40K的子集构建数字孪生模拟环境，包含复杂规则和动态游戏状态。

Result: 4Hammer环境填补了现有空白，为LLM长期任务评估提供新工具。

Conclusion: 该环境有助于研究LLM在复杂长期任务中的表现，推动相关领域发展。

Abstract: Large Language Models (LLMs) have demonstrated strong performance on tasks
with short time frames, but struggle with tasks requiring longer durations.
While datasets covering extended-duration tasks, such as software engineering
tasks or video games, do exist, there are currently few implementations of
complex board games specifically designed for reinforcement learning and LLM
evaluation. To address this gap, we propose the 4Hammer reinforcement learning
environment, a digital twin simulation of a subset of Warhammer 40,000-a
complex, zero-sum board game. Warhammer 40,000 features intricate rules,
requiring human players to thoroughly read and understand over 50 pages of
detailed natural language rules, grasp the interactions between their game
pieces and those of their opponents, and independently track and communicate
the evolving game state.

</details>


### [239] [FedCTTA: A Collaborative Approach to Continual Test-Time Adaptation in Federated Learning](https://arxiv.org/abs/2505.13643)
*Rakibul Hasan Rajib,Md Akil Raihan Iftee,Mir Sazzat Hossain,A. K. M. Mahbubur Rahman,Sajib Mistry,M Ashraful Amin,Amin Ahsan Ali*

Main category: cs.LG

TL;DR: 联邦学习（FL）在隐私敏感应用中表现优异，但存在训练与部署分布偏移导致的性能下降问题。本文提出FedCTTA框架，通过测试时适应（TTA）实现隐私保护且高效的自适应，避免特征共享并降低计算开销。


<details>
  <summary>Details</summary>
Motivation: 联邦学习模型常因训练与部署数据分布不同而性能下降，现有测试时适应方法在联邦学习中面临计算开销大、隐私风险及可扩展性差的问题。

Method: 提出FedCTTA框架，利用基于模型输出分布的相似性聚合方法，避免直接特征交换，实现隐私保护；通过最小化客户端熵实现持续自适应，无需服务器端训练。

Result: 实验表明，FedCTTA在时空异质性场景下优于现有方法，具有更好的适应性和可扩展性。

Conclusion: FedCTTA为联邦学习提供了一种隐私保护、计算高效且可扩展的测试时自适应解决方案，有效应对分布偏移问题。

Abstract: Federated Learning (FL) enables collaborative model training across
distributed clients without sharing raw data, making it ideal for
privacy-sensitive applications. However, FL models often suffer performance
degradation due to distribution shifts between training and deployment.
Test-Time Adaptation (TTA) offers a promising solution by allowing models to
adapt using only test samples. However, existing TTA methods in FL face
challenges such as computational overhead, privacy risks from feature sharing,
and scalability concerns due to memory constraints. To address these
limitations, we propose Federated Continual Test-Time Adaptation (FedCTTA), a
privacy-preserving and computationally efficient framework for federated
adaptation. Unlike prior methods that rely on sharing local feature statistics,
FedCTTA avoids direct feature exchange by leveraging similarity-aware
aggregation based on model output distributions over randomly generated noise
samples. This approach ensures adaptive knowledge sharing while preserving data
privacy. Furthermore, FedCTTA minimizes the entropy at each client for
continual adaptation, enhancing the model's confidence in evolving target
distributions. Our method eliminates the need for server-side training during
adaptation and maintains a constant memory footprint, making it scalable even
as the number of clients or training rounds increases. Extensive experiments
show that FedCTTA surpasses existing methods across diverse temporal and
spatial heterogeneity scenarios.

</details>


### [240] [Collapsing Taylor Mode Automatic Differentiation](https://arxiv.org/abs/2505.13644)
*Felix Dangel,Tim Siebert,Marius Zeinhofer,Andrea Walther*

Main category: cs.LG

TL;DR: 该论文提出了一种优化泰勒模式自动微分的方法，通过重写计算图来加速偏微分方程（PDE）算子的计算，并在实验中验证了其性能优于嵌套反向传播。


<details>
  <summary>Details</summary>
Motivation: 当前通过嵌套反向传播计算偏微分方程（PDE）算子的方法计算成本高，限制了其在科学机器学习中的应用。因此，需要更高效的计算方法。

Method: 提出了一种优化泰勒模式自动微分的技术，通过“折叠”导数并重写计算图，适用于一般线性PDE算子和随机化泰勒模式。

Result: 实验证明，该方法加速了泰勒模式的计算，并且在性能上优于嵌套反向传播。

Conclusion: 该方法通过简单的计算图优化显著提升了PDE算子的计算效率，且无需用户介入复杂操作，适合机器学习编译器实现。

Abstract: Computing partial differential equation (PDE) operators via nested
backpropagation is expensive, yet popular, and severely restricts their utility
for scientific machine learning. Recent advances, like the forward Laplacian
and randomizing Taylor mode automatic differentiation (AD), propose forward
schemes to address this. We introduce an optimization technique for Taylor mode
that 'collapses' derivatives by rewriting the computational graph, and
demonstrate how to apply it to general linear PDE operators, and randomized
Taylor mode. The modifications simply require propagating a sum up the
computational graph, which could -- or should -- be done by a machine learning
compiler, without exposing complexity to users. We implement our collapsing
procedure and evaluate it on popular PDE operators, confirming it accelerates
Taylor mode and outperforms nested backpropagation.

</details>


### [241] [Self-Reinforced Graph Contrastive Learning](https://arxiv.org/abs/2505.13650)
*Chou-Ying Hsieh,Chun-Fu Jang,Cheng-En Hsieh,Qian-Hui Chen,Sy-Yen Kuo*

Main category: cs.LG

TL;DR: 论文提出了一种名为SRGCL的新型图对比学习框架，通过动态评估和选择高质量正样本对来提升图表示的质量，并在多个图分类任务中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 图对比学习（GCL）在自监督图表示学习中表现出色，但如何确保正样本对的质量以保留图的语义和结构特性仍是一个关键挑战。

Method: SRGCL框架利用模型自身的编码器动态评估和选择高质量正样本对，结合多增强策略的统一生成器和流形假设指导的选择器，通过概率机制迭代优化正样本对的选择。

Result: 在多样化的图分类任务中，SRGCL作为插件模块显著优于当前最先进的GCL方法，展示了其跨领域的适应性和高效性。

Conclusion: SRGCL通过动态优化正样本对选择，有效提升了图对比学习的性能，为图表示学习提供了一种新的解决方案。

Abstract: Graphs serve as versatile data structures in numerous real-world
domains-including social networks, molecular biology, and knowledge graphs-by
capturing intricate relational information among entities. Among graph-based
learning techniques, Graph Contrastive Learning (GCL) has gained significant
attention for its ability to derive robust, self-supervised graph
representations through the contrasting of positive and negative sample pairs.
However, a critical challenge lies in ensuring high-quality positive pairs so
that the intrinsic semantic and structural properties of the original graph are
preserved rather than distorted. To address this issue, we propose SRGCL
(Self-Reinforced Graph Contrastive Learning), a novel framework that leverages
the model's own encoder to dynamically evaluate and select high-quality
positive pairs. We designed a unified positive pair generator employing
multiple augmentation strategies, and a selector guided by the manifold
hypothesis to maintain the underlying geometry of the latent space. By adopting
a probabilistic mechanism for selecting positive pairs, SRGCL iteratively
refines its assessment of pair quality as the encoder's representational power
improves. Extensive experiments on diverse graph-level classification tasks
demonstrate that SRGCL, as a plug-in module, consistently outperforms
state-of-the-art GCL methods, underscoring its adaptability and efficacy across
various domains.

</details>


### [242] [RL in Name Only? Analyzing the Structural Assumptions in RL post-training for LLMs](https://arxiv.org/abs/2505.13697)
*Soumya Rani Samineni,Durgesh Kalwar,Karthik Valmeekam,Kaya Stechly,Subbarao Kambhampati*

Main category: cs.LG

TL;DR: 该论文批判性地分析了基于强化学习（RL）的大语言模型（LLM）后训练方法，指出其简化假设实际上使其等同于结果驱动的监督学习，并通过实验证明监督微调能达到与GRPO相当的效果。


<details>
  <summary>Details</summary>
Motivation: 随着DeepSeek R1等采用GRPO进行微调的模型发布，RL后训练被认为能提升LLM的推理能力。本文旨在检验这些方法的理论基础，特别是其将LLM训练建模为马尔可夫决策过程（MDP）的简化假设是否合理。

Method: 作者首先分析了RL后训练中两个关键假设：1）将MDP状态定义为动作（即LLM生成的令牌）的拼接；2）将轨迹奖励均匀分配。通过理论分析和实验（使用Qwen-2.5基础模型在GSM8K和Countdown等基准测试），对比了RL与监督学习的性能。

Result: 实验表明，结合正负样本的迭代监督微调与GRPO训练效果相当。同时，RL的结构假设会间接鼓励生成更长的中间令牌序列，这可能被误解读为“RL产生了更长的思维轨迹”。

Conclusion: 尽管RL可能对提升LLM推理能力有用，但当前流行的RL框架因简化假设使其效果和解释力存疑。监督学习在特定场景下可达到类似效果，且无需复杂RL机制。

Abstract: Reinforcement learning-based post-training of large language models (LLMs)
has recently gained attention, particularly following the release of DeepSeek
R1, which applied GRPO for fine-tuning. Amid the growing hype around improved
reasoning abilities attributed to RL post-training, we critically examine the
formulation and assumptions underlying these methods. We start by highlighting
the popular structural assumptions made in modeling LLM training as a Markov
Decision Process (MDP), and show how they lead to a degenerate MDP that doesn't
quite need the RL/GRPO apparatus. The two critical structural assumptions
include (1) making the MDP states be just a concatenation of the actions-with
states becoming the context window and the actions becoming the tokens in LLMs
and (2) splitting the reward of a state-action trajectory uniformly across the
trajectory. Through a comprehensive analysis, we demonstrate that these
simplifying assumptions make the approach effectively equivalent to an
outcome-driven supervised learning. Our experiments on benchmarks including
GSM8K and Countdown using Qwen-2.5 base models show that iterative supervised
fine-tuning, incorporating both positive and negative samples, achieves
performance comparable to GRPO-based training. We will also argue that the
structural assumptions indirectly incentivize the RL to generate longer
sequences of intermediate tokens-which in turn feeds into the narrative of "RL
generating longer thinking traces." While RL may well be a very useful
technique for improving the reasoning abilities of LLMs, our analysis shows
that the simplistic structural assumptions made in modeling the underlying MDP
render the popular LLM RL frameworks and their interpretations questionable.

</details>


### [243] [Unsupervised anomaly detection in MeV ultrafast electron diffraction](https://arxiv.org/abs/2505.13702)
*Mariana A. Fazio,Salvador Sosa Güitron,Marcus Babzien,Mikhail Fedurin,Junjie Li,Mark Palmer,Sandra S. Biedron,Manel Martinez-Ramon*

Main category: cs.LG

TL;DR: 该研究提出了一种无监督异常检测方法，用于自动识别MUED中的故障图像，无需人工标注数据。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于避免传统方法中繁琐的人工图像检查，通过无监督技术让机器自主检测异常，同时提供不确定性度量辅助用户决策。

Method: 采用无监督学习方法构建异常检测模型，利用未标注数据训练模型自主识别异常，并量化检测结果的不确定性。

Result: 该方法能够有效检测MUED数据集中的故障图像，并输出带有不确定性度量的检测结果。

Conclusion: 无监督异常检测方法在节省人工成本的同时，通过不确定性度量增强了检测结果的可用性。

Abstract: This study focus in the construction of an unsupervised anomaly detection
methodology to detect faulty images in MUED. We believe that unsupervised
techniques are the best choice for our purposes because the data used to train
the detector does not need to be manually labeled, and instead, the machine is
intended to detect by itself the anomalies in the dataset, which liberates the
user of tedious, time-consuming initial image examination. The structure must,
additionally, provide the user with some measure of uncertainty in the
detection, so the user can take decisions based on this measure.

</details>


### [244] [Policy-Driven World Model Adaptation for Robust Offline Model-based Reinforcement Learning](https://arxiv.org/abs/2505.13709)
*Jiayu Chen,Aravind Venugopal,Jeff Schneider*

Main category: cs.LG

TL;DR: 该论文提出了一种动态调整世界模型与策略的统一学习框架，以提升离线模型强化学习的鲁棒性，并在多个任务中展示了其优越性能。


<details>
  <summary>Details</summary>
Motivation: 现有的离线模型强化学习方法通常采用两阶段训练，导致世界模型与策略学习目标不匹配，且策略在部署时缺乏鲁棒性。

Method: 通过最大化优化问题和Stackelberg学习动态，动态调整世界模型与策略，实现统一学习目标。

Result: 在12个噪声D4RL MuJoCo任务和3个随机Tokamak控制任务中，该方法表现出最先进的性能。

Conclusion: 该框架有效解决了离线模型强化学习中的目标不匹配和鲁棒性问题，为数据驱动控制提供了新思路。

Abstract: Offline reinforcement learning (RL) offers a powerful paradigm for
data-driven control. Compared to model-free approaches, offline model-based RL
(MBRL) explicitly learns a world model from a static dataset and uses it as a
surrogate simulator, improving data efficiency and enabling potential
generalization beyond the dataset support. However, most existing offline MBRL
methods follow a two-stage training procedure: first learning a world model by
maximizing the likelihood of the observed transitions, then optimizing a policy
to maximize its expected return under the learned model. This objective
mismatch results in a world model that is not necessarily optimized for
effective policy learning. Moreover, we observe that policies learned via
offline MBRL often lack robustness during deployment, and small adversarial
noise in the environment can lead to significant performance degradation. To
address these, we propose a framework that dynamically adapts the world model
alongside the policy under a unified learning objective aimed at improving
robustness. At the core of our method is a maximin optimization problem, which
we solve by innovatively utilizing Stackelberg learning dynamics. We provide
theoretical analysis to support our design and introduce computationally
efficient implementations. We benchmark our algorithm on twelve noisy D4RL
MuJoCo tasks and three stochastic Tokamak Control tasks, demonstrating its
state-of-the-art performance.

</details>


### [245] [Turbocharging Gaussian Process Inference with Approximate Sketch-and-Project](https://arxiv.org/abs/2505.13723)
*Pratik Rathore,Zachary Frangella,Sachin Garg,Shaghayegh Fazliani,Michał Dereziński,Madeleine Udell*

Main category: cs.LG

TL;DR: 提出了一种名为ADASAP的分布式近似算法，用于高效解决高斯过程推理中的大规模线性系统问题，显著提升了计算效率。


<details>
  <summary>Details</summary>
Motivation: 高斯过程在大规模数据集上的推理因计算复杂度高而难以扩展，需要一种更高效的算法来处理现代应用中的海量数据。

Method: 采用基于草图投影的分布式加速算法（ADASAP），结合行列式点过程理论，确保后验均值快速收敛。

Result: ADASAP在多个基准数据集和大规模贝叶斯优化任务中优于现有方法，并能处理超过3亿样本的数据集。

Conclusion: ADASAP是一种高效且理论完备的高斯过程推理算法，显著提升了大规模数据处理的可行性。

Abstract: Gaussian processes (GPs) play an essential role in biostatistics, scientific
machine learning, and Bayesian optimization for their ability to provide
probabilistic predictions and model uncertainty. However, GP inference
struggles to scale to large datasets (which are common in modern applications),
since it requires the solution of a linear system whose size scales
quadratically with the number of samples in the dataset. We propose an
approximate, distributed, accelerated sketch-and-project algorithm
($\texttt{ADASAP}$) for solving these linear systems, which improves
scalability. We use the theory of determinantal point processes to show that
the posterior mean induced by sketch-and-project rapidly converges to the true
posterior mean. In particular, this yields the first efficient, condition
number-free algorithm for estimating the posterior mean along the top spectral
basis functions, showing that our approach is principled for GP inference.
$\texttt{ADASAP}$ outperforms state-of-the-art solvers based on conjugate
gradient and coordinate descent across several benchmark datasets and a
large-scale Bayesian optimization task. Moreover, $\texttt{ADASAP}$ scales to a
dataset with $> 3 \cdot 10^8$ samples, a feat which has not been accomplished
in the literature.

</details>


### [246] [Power Lines: Scaling Laws for Weight Decay and Batch Size in LLM Pre-training](https://arxiv.org/abs/2505.13738)
*Shane Bergsma,Nolan Dey,Gurpreet Gosal,Gavia Gray,Daria Soboleva,Joel Hestness*

Main category: cs.LG

TL;DR: 该论文研究了大型语言模型预训练中超参数（如学习率和权重衰减）的缩放规律，提出了预测最优超参数的方法，并分析了批量大小对训练效率的影响。


<details>
  <summary>Details</summary>
Motivation: 为了提高大型语言模型（LLM）预训练的效率和效果，需要优化超参数（如学习率η和权重衰减λ）的设置。论文旨在探索这些超参数在模型规模N、数据集规模D和批量大小B变化时的缩放规律。

Method: 论文通过理论分析和实验验证，研究了AdamW时间尺度（B/(ηλD)）在不同训练设置下的变化规律，并探讨了最优批量大小Bopt和临界批量大小Bcrit的缩放行为。

Result: 研究发现，最优时间尺度与token-per-parameter比率（D/N）呈幂律关系，从而可以提前预测最优λ。此外，Bopt和Bcrit与数据集规模D呈幂律关系，而与模型规模N无关。

Conclusion: 这些发现为实际训练中如何选择Pareto最优的模型规模N和数据集规模D提供了理论依据，有助于在训练时间和计算资源之间找到平衡。

Abstract: Efficient LLM pre-training requires well-tuned hyperparameters (HPs),
including learning rate {\eta} and weight decay {\lambda}. We study scaling
laws for HPs: formulas for how to scale HPs as we scale model size N, dataset
size D, and batch size B. Recent work suggests the AdamW timescale,
B/({\eta}{\lambda}D), should remain constant across training settings, and we
verify the implication that optimal {\lambda} scales linearly with B, for a
fixed N,D. However, as N,D scale, we show the optimal timescale obeys a precise
power law in the tokens-per-parameter ratio, D/N. This law thus provides a
method to accurately predict {\lambda}opt in advance of large-scale training.
We also study scaling laws for optimal batch size Bopt (the B enabling lowest
loss at a given N,D) and critical batch size Bcrit (the B beyond which further
data parallelism becomes ineffective). In contrast with prior work, we find
both Bopt and Bcrit scale as power laws in D, independent of model size, N.
Finally, we analyze how these findings inform the real-world selection of
Pareto-optimal N and D under dual training time and compute objectives.

</details>


### [247] [Improving Compositional Generation with Diffusion Models Using Lift Scores](https://arxiv.org/abs/2505.13740)
*Chenning Yu,Sicun Gao*

Main category: cs.LG

TL;DR: 提出了一种基于lift scores的重采样准则，用于提升扩散模型在组合生成中的条件对齐效果，无需额外训练或外部模块。


<details>
  <summary>Details</summary>
Motivation: 当前扩散模型在组合生成任务中难以有效满足多个条件的组合，需要一种高效的方法来评估和提升生成样本与各条件的对齐程度。

Method: 利用lift scores评估生成样本是否满足单个条件，并通过组合结果判断是否满足组合提示，且该方法仅需原始扩散模型即可高效近似。

Result: 在2D合成数据、CLEVR位置任务和文本到图像合成等任务中，lift scores显著提升了组合生成的条件对齐效果。

Conclusion: 所提出的lift scores方法在提升组合生成条件对齐方面表现出色，且计算开销较低，具有实际应用价值。

Abstract: We introduce a novel resampling criterion using lift scores, for improving
compositional generation in diffusion models. By leveraging the lift scores, we
evaluate whether generated samples align with each single condition and then
compose the results to determine whether the composed prompt is satisfied. Our
key insight is that lift scores can be efficiently approximated using only the
original diffusion model, requiring no additional training or external modules.
We develop an optimized variant that achieves relatively lower computational
overhead during inference while maintaining effectiveness. Through extensive
experiments, we demonstrate that lift scores significantly improved the
condition alignment for compositional generation across 2D synthetic data,
CLEVR position tasks, and text-to-image synthesis. Our code is available at
http://github.com/rainorangelemon/complift.

</details>


### [248] [Understanding Task Representations in Neural Networks via Bayesian Ablation](https://arxiv.org/abs/2505.13742)
*Andrew Nam,Declan Campbell,Thomas Griffiths,Jonathan Cohen,Sarah-Jane Leslie*

Main category: cs.LG

TL;DR: 提出了一种基于贝叶斯推理的概率框架，用于解释神经网络中的潜在任务表示，并通过信息论工具量化关键模型特性。


<details>
  <summary>Details</summary>
Motivation: 神经网络虽在认知建模中表现出强大能力，但其亚符号语义特性导致学习表示难以解释。

Method: 结合贝叶斯推断定义表示单元的贡献分布，利用信息论开发工具集量化表示分布性、流形复杂性和多义性。

Result: 开发了一套可解释性工具，能有效揭示神经网络表示的关键特性与任务性能间的因果关系。

Conclusion: 该框架为神经网络的可解释性研究提供了新范式，有助于理解其内部表示与认知任务的关系。

Abstract: Neural networks are powerful tools for cognitive modeling due to their
flexibility and emergent properties. However, interpreting their learned
representations remains challenging due to their sub-symbolic semantics. In
this work, we introduce a novel probabilistic framework for interpreting latent
task representations in neural networks. Inspired by Bayesian inference, our
approach defines a distribution over representational units to infer their
causal contributions to task performance. Using ideas from information theory,
we propose a suite of tools and metrics to illuminate key model properties,
including representational distributedness, manifold complexity, and
polysemanticity.

</details>


### [249] [Synthetic Non-stationary Data Streams for Recognition of the Unknown](https://arxiv.org/abs/2505.13745)
*Joanna Komorniczak*

Main category: cs.LG

TL;DR: 该论文提出了一种合成数据流生成策略，用于处理概念漂移和新类出现的问题，并展示了无监督漂移检测器在开放集识别任务中的应用。


<details>
  <summary>Details</summary>
Motivation: 在动态环境中，数据流的非平稳性表现为概念漂移和新类的出现。现有方法通常只关注其中一个问题，而忽略了二者可能同时存在的情况。此外，开放集识别任务的重要性日益凸显，需要有效分类已知类并识别未知对象。

Method: 论文提出了一种合成数据流生成策略，模拟概念漂移和新类出现的场景，并利用无监督漂移检测器进行检测。

Result: 研究表明，无监督漂移检测器能够有效检测新类和概念漂移，生成的数据流可用于开放集识别任务。

Conclusion: 该策略为处理数据流中的非平稳性问题提供了有效工具，尤其在开放集识别任务中展示了其潜力。

Abstract: The problem of data non-stationarity is commonly addressed in data stream
processing. In a dynamic environment, methods should continuously be ready to
analyze time-varying data -- hence, they should enable incremental training and
respond to concept drifts. An equally important variability typical for
non-stationary data stream environments is the emergence of new, previously
unknown classes. Often, methods focus on one of these two phenomena --
detection of concept drifts or detection of novel classes -- while both
difficulties can be observed in data streams. Additionally, concerning
previously unknown observations, the topic of open set of classes has become
particularly important in recent years, where the goal of methods is to
efficiently classify within known classes and recognize objects outside the
model competence. This article presents a strategy for synthetic data stream
generation in which both concept drifts and the emergence of new classes
representing unknown objects occur. The presented research shows how
unsupervised drift detectors address the task of detecting novelty and concept
drifts and demonstrates how the generated data streams can be utilized in the
open set recognition task.

</details>


### [250] [Finding Maximum Independent Sets in Dynamic Graphs using Unsupervised Learning](https://arxiv.org/abs/2505.13754)
*Devendra Parkar,Anya Chaturvedi,Andréa W. Richa,Joshua J. Daymude*

Main category: cs.LG

TL;DR: 本文提出首个针对动态图的最大独立集（MaxIS）无监督学习模型，结合图神经网络与分布式更新机制，在性能与运行时间上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有最大独立集方法主要针对静态图，难以处理动态图中边随时间变化的情况。本文旨在填补这一空白，提出一种高效、可扩展的动态图MaxIS解决方案。

Method: 模型结合图神经网络（GNN）的结构学习能力与分布式更新机制，通过单步并行处理边的增减事件，动态调整节点记忆并推断其MaxIS成员资格。

Result: 在100-10,000节点的合成和真实动态图上，模型在近似比、运行时间和内存使用上均优于现有方法，且能泛化到比训练图大100倍的图上。

Conclusion: 该模型在动态图中高效求解MaxIS问题，性能与贪心算法和商业求解器相当，但运行速度快1.5-23倍，展现了优异的可扩展性和泛化能力。

Abstract: We present the first unsupervised learning model for finding Maximum
Independent Sets (MaxIS) in dynamic graphs where edges change over time. Our
method combines structural learning from graph neural networks (GNNs) with a
learned distributed update mechanism that, given an edge addition or deletion
event, modifies nodes' internal memories and infers their MaxIS membership in a
single, parallel step. We parameterize our model by the update mechanism's
radius and investigate the resulting performance-runtime tradeoffs for various
dynamic graph topologies. We evaluate our model against state-of-the-art MaxIS
methods for static graphs, including a mixed integer programming solver,
deterministic rule-based algorithms, and a heuristic learning framework based
on dynamic programming and GNNs. Across synthetic and real-world dynamic graphs
of 100-10,000 nodes, our model achieves competitive approximation ratios with
excellent scalability; on large graphs, it significantly outperforms the
state-of-the-art heuristic learning framework in solution quality, runtime, and
memory usage. Our model generalizes well on graphs 100x larger than the ones
used for training, achieving performance at par with both a greedy technique
and a commercial mixed integer programming solver while running 1.5-23x faster
than greedy.

</details>


### [251] [Panda: A pretrained forecast model for universal representation of chaotic dynamics](https://arxiv.org/abs/2505.13755)
*Jeffrey Lai,Anthony Bao,William Gilpin*

Main category: cs.LG

TL;DR: 提出Panda模型，通过进化算法生成合成数据集训练，实现零样本预测混沌系统，并展示神经网络在非线性动力学中的潜力。


<details>
  <summary>Details</summary>
Motivation: 混沌系统对微小误差敏感，现有模型难以预测真实世界的动态系统（如流体流动或神经元活动）。本文旨在开发一种能泛化预测混沌系统的模型。

Method: 使用进化算法生成包含2×10^4个混沌动态系统的合成数据集，训练Panda（基于补丁注意力机制的模型）。

Result: Panda在未训练的真实混沌系统上表现出零样本预测能力，并能自发预测偏微分方程，展示了神经网络的扩展定律。

Conclusion: 预训练模型在抽象数学领域（如非线性动力学）具有潜力，Panda的成功为未来研究提供了新方向。

Abstract: Chaotic systems are intrinsically sensitive to small errors, challenging
efforts to construct predictive data-driven models of real-world dynamical
systems such as fluid flows or neuronal activity. Prior efforts comprise either
specialized models trained separately on individual time series, or foundation
models trained on vast time series databases with little underlying dynamical
structure. Motivated by dynamical systems theory, we present Panda, Patched
Attention for Nonlinear DynAmics. We train Panda on a novel synthetic,
extensible dataset of $2 \times 10^4$ chaotic dynamical systems that we
discover using an evolutionary algorithm. Trained purely on simulated data,
Panda exhibits emergent properties: zero-shot forecasting of unseen real world
chaotic systems, and nonlinear resonance patterns in cross-channel attention
heads. Despite having been trained only on low-dimensional ordinary
differential equations, Panda spontaneously develops the ability to predict
partial differential equations without retraining. We demonstrate a neural
scaling law for differential equations, underscoring the potential of
pretrained models for probing abstract mathematical domains like nonlinear
dynamics.

</details>


### [252] [Consistency Conditions for Differentiable Surrogate Losses](https://arxiv.org/abs/2505.13760)
*Drona Khurana,Anish Thilagar,Dhamma Kimpara,Rafael Frongillo*

Main category: cs.LG

TL;DR: 该论文研究了非多面体替代损失函数（特别是凸可微损失）的统计一致性，提出了间接引发（IE）和强IE的概念，并证明了它们在验证校准性方面的等价性和适用性。


<details>
  <summary>Details</summary>
Motivation: 在离散预测任务中，替代损失函数的统计一致性通常通过校准条件来验证，但直接验证校准性较为困难。论文旨在探索更易验证的条件（如IE）及其在非多面体替代损失中的适用性。

Method: 论文首先在一维凸可微损失函数中证明IE与校准性的等价性，并通过反例展示高维情况下的失效。随后引入强IE概念，并证明其对可微替代损失函数的必要性及充分性。

Result: 研究证明，强IE能够保证可微替代损失函数的校准性，且在强凸可微情况下是充要条件。论文还通过实际应用展示了IE和强IE在设计一致性替代损失中的有效性。

Conclusion: 论文通过引入强IE，为非多面体替代损失函数的统计一致性提供了更易验证的条件，扩展了IE在多面体损失之外的适用性，并为设计一致性替代损失提供了新工具。

Abstract: The statistical consistency of surrogate losses for discrete prediction tasks
is often checked via the condition of calibration. However, directly verifying
calibration can be arduous. Recent work shows that for polyhedral surrogates, a
less arduous condition, indirect elicitation (IE), is still equivalent to
calibration. We give the first results of this type for non-polyhedral
surrogates, specifically the class of convex differentiable losses. We first
prove that under mild conditions, IE and calibration are equivalent for
one-dimensional losses in this class. We construct a counter-example that shows
that this equivalence fails in higher dimensions. This motivates the
introduction of strong IE, a strengthened form of IE that is equally easy to
verify. We establish that strong IE implies calibration for differentiable
surrogates and is both necessary and sufficient for strongly convex,
differentiable surrogates. Finally, we apply these results to a range of
problems to demonstrate the power of IE and strong IE for designing and
analyzing consistent differentiable surrogates.

</details>


### [253] [WIND: Accelerated RNN-T Decoding with Windowed Inference for Non-blank Detection](https://arxiv.org/abs/2505.13765)
*Hainan Xu,Vladimir Bataev,Lilit Grigoryan,Boris Ginsburg*

Main category: cs.LG

TL;DR: 提出WIND方法，通过并行处理多帧加速RNN-T推理，保持准确率的同时速度提升2.4倍。


<details>
  <summary>Details</summary>
Motivation: 传统RNN-T推理逐帧处理效率低，WIND旨在提升推理速度而不牺牲模型精度。

Method: 采用窗口并行处理多帧，支持贪婪解码、批量贪婪解码及新束搜索解码方法。

Result: 贪婪模式下速度提升2.4倍且词错误率不变，束搜索算法精度略优且速度显著提升。

Conclusion: WIND高效加速RNN-T推理，将开源实现，适用于多种解码场景。

Abstract: We propose Windowed Inference for Non-blank Detection (WIND), a novel
strategy that significantly accelerates RNN-T inference without compromising
model accuracy. During model inference, instead of processing frames
sequentially, WIND processes multiple frames simultaneously within a window in
parallel, allowing the model to quickly locate non-blank predictions during
decoding, resulting in significant speed-ups. We implement WIND for greedy
decoding, batched greedy decoding with label-looping techniques, and also
propose a novel beam-search decoding method. Experiments on multiple datasets
with different conditions show that our method, when operating in greedy modes,
speeds up as much as 2.4X compared to the baseline sequential approach while
maintaining identical Word Error Rate (WER) performance. Our beam-search
algorithm achieves slightly better accuracy than alternative methods, with
significantly improved speed. We will open-source our WIND implementation.

</details>


### [254] [Augmenting Online RL with Offline Data is All You Need: A Unified Hybrid RL Algorithm Design and Analysis](https://arxiv.org/abs/2505.13768)
*Ruiquan Huang,Donghao Li,Chengshuai Shi,Cong Shen,Jing Yang*

Main category: cs.LG

TL;DR: 该论文提出了一种混合学习框架，结合离线数据集和在线交互来优化强化学习策略，在次优性差距和在线学习后悔度上达到最优效果。


<details>
  <summary>Details</summary>
Motivation: 研究如何同时利用离线数据和在线交互来提升强化学习的性能，解决纯在线或纯离线方法的局限性。

Method: 提出了一种统一的算法，通过置信度在线RL算法增强离线数据集，结合两种学习方式。

Result: 算法在次优性差距和在线学习后悔度上表现优异，理论分析揭示了离线数据集覆盖性质的不同需求。

Conclusion: 混合学习框架在理论和实验中均表现出色，为强化学习提供了更高效的解决方案。

Abstract: This paper investigates a hybrid learning framework for reinforcement
learning (RL) in which the agent can leverage both an offline dataset and
online interactions to learn the optimal policy. We present a unified algorithm
and analysis and show that augmenting confidence-based online RL algorithms
with the offline dataset outperforms any pure online or offline algorithm alone
and achieves state-of-the-art results under two learning metrics, i.e.,
sub-optimality gap and online learning regret. Specifically, we show that our
algorithm achieves a sub-optimality gap
$\tilde{O}(\sqrt{1/(N_0/\mathtt{C}(\pi^*|\rho)+N_1}) )$, where
$\mathtt{C}(\pi^*|\rho)$ is a new concentrability coefficient, $N_0$ and $N_1$
are the numbers of offline and online samples, respectively. For regret
minimization, we show that it achieves a constant $\tilde{O}(
\sqrt{N_1/(N_0/\mathtt{C}(\pi^{-}|\rho)+N_1)} )$ speed-up compared to pure
online learning, where $\mathtt{C}(\pi^-|\rho)$ is the concentrability
coefficient over all sub-optimal policies. Our results also reveal an
interesting separation on the desired coverage properties of the offline
dataset for sub-optimality gap minimization and regret minimization. We further
validate our theoretical findings in several experiments in special RL models
such as linear contextual bandits and Markov decision processes (MDPs).

</details>


### [255] [Beyond Semantics: The Unreasonable Effectiveness of Reasonless Intermediate Tokens](https://arxiv.org/abs/2505.13775)
*Kaya Stechly,Karthik Valmeekam,Atharva Gundawar,Vardhan Palod,Subbarao Kambhampati*

Main category: cs.LG

TL;DR: 该论文质疑了大型推理模型中“思维链”（CoT）的有效性，指出中间语义标记对模型性能的影响被高估，并证明即使使用错误推理轨迹，模型仍能保持甚至提升性能。


<details>
  <summary>Details</summary>
Motivation: 近年来，大型推理模型的成功常被归因于“思维链”（CoT）的训练方法，尤其是通过从基础LLM中采样CoT以发现新推理模式。然而，这种解释缺乏对中间语义标记实际影响的严格验证。本文旨在批判性检验这一假设，探究中间标记（常被拟人化为“思考”或推理痕迹）是否真正驱动模型性能。

Method: 作者训练了基于Transformer的模型，使用形式化可验证的推理轨迹和解决方案，约束中间步骤和最终输出与形式化求解器（如A*搜索）对齐。通过构建问题语义和算法的形式化解释器，系统评估解决方案准确性及中间轨迹的正确性，从而分析后者是否因果影响前者。此外，还测试了使用噪声或无关的推理轨迹训练模型的效果。

Result: 研究发现：（1）即使训练数据包含完全正确的推理轨迹，模型仍会生成无效中间步骤却得到正确答案；（2）使用无关或噪声轨迹训练时，模型性能与正确数据训练结果相当，甚至在某些任务上表现更好、泛化更强。这表明中间轨迹的准确性与解决方案准确性关联微弱。

Conclusion: 结果挑战了“思维链”能诱导可预测推理行为的假设，并警示不应过度拟人化中间输出或将其视为语言模型具有类人/算法行为的证据。即使CoT形式看似正确，其语义可能对模型性能无实质影响。

Abstract: Recent impressive results from large reasoning models have been interpreted
as a triumph of Chain of Thought (CoT), and especially of the process of
training on CoTs sampled from base LLMs in order to help find new reasoning
patterns. In this paper, we critically examine that interpretation by
investigating how the semantics of intermediate tokens-often anthropomorphized
as "thoughts" or reasoning traces and which are claimed to display behaviors
like backtracking, self-verification etc.-actually influence model performance.
We train transformer models on formally verifiable reasoning traces and
solutions, constraining both intermediate steps and final outputs to align with
those of a formal solver (in our case, A* search). By constructing a formal
interpreter of the semantics of our problems and intended algorithm, we
systematically evaluate not only solution accuracy but also the correctness of
intermediate traces, thus allowing us to evaluate whether the latter causally
influences the former. We notice that, despite significant improvements on the
solution-only baseline, models trained on entirely correct traces still produce
invalid reasoning traces when arriving at correct solutions. To further show
that trace accuracy is only loosely connected to solution accuracy, we then
train models on noisy, corrupted traces which have no relation to the specific
problem each is paired with, and find that not only does performance remain
largely consistent with models trained on correct data, but in some cases can
improve upon it and generalize more robustly on out-of-distribution tasks.
These results challenge the assumption that intermediate tokens or "Chains of
Thought" induce predictable reasoning behaviors and caution against
anthropomorphizing such outputs or over-interpreting them (despite their mostly
correct forms) as evidence of human-like or algorithmic behaviors in language
models.

</details>


### [256] [Preference Learning with Lie Detectors can Induce Honesty or Evasion](https://arxiv.org/abs/2505.13787)
*Chris Cundy,Adam Gleave*

Main category: cs.LG

TL;DR: 研究发现，在LLM训练中引入谎言检测器可能导致模型学会欺骗检测器而非真正诚实，但高检测准确率或KL正则化可促进诚实行为。


<details>
  <summary>Details</summary>
Motivation: 探讨AI系统在部署时可能出现的欺骗行为，以及谎言检测器在训练中的应用是否能真正提升模型诚实性。

Method: 使用DolusChat数据集，结合谎言检测器和GRPO算法，研究探索量、检测器准确率和KL正则化强度对模型诚实性的影响。

Result: GRPO可能导致85%以上的欺骗率，但高检测准确率或KL正则化可促进诚实；DPO算法在现实检测率下欺骗率低于25%。

Conclusion: 谎言检测器增强训练的效果复杂，可能促进诚实也可能鼓励隐蔽欺骗，需根据上下文谨慎使用。

Abstract: As AI systems become more capable, deceptive behaviors can undermine
evaluation and mislead users at deployment. Recent work has shown that lie
detectors can accurately classify deceptive behavior, but they are not
typically used in the training pipeline due to concerns around contamination
and objective hacking. We examine these concerns by incorporating a lie
detector into the labelling step of LLM post-training and evaluating whether
the learned policy is genuinely more honest, or instead learns to fool the lie
detector while remaining deceptive. Using DolusChat, a novel 65k-example
dataset with paired truthful/deceptive responses, we identify three key factors
that determine the honesty of learned policies: amount of exploration during
preference learning, lie detector accuracy, and KL regularization strength. We
find that preference learning with lie detectors and GRPO can lead to policies
which evade lie detectors, with deception rates of over 85\%. However, if the
lie detector true positive rate (TPR) or KL regularization is sufficiently
high, GRPO learns honest policies. In contrast, off-policy algorithms (DPO)
consistently lead to deception rates under 25\% for realistic TPRs. Our results
illustrate a more complex picture than previously assumed: depending on the
context, lie-detector-enhanced training can be a powerful tool for scalable
oversight, or a counterproductive method encouraging undetectable misalignment.

</details>


### [257] [Scalable Autoregressive 3D Molecule Generation](https://arxiv.org/abs/2505.13791)
*Austin H. Cheng,Chong Sun,Alán Aspuru-Guzik*

Main category: cs.LG

TL;DR: Quetzal是一种新型自回归模型，通过原子序列生成3D分子结构，结合因果Transformer和扩散MLP，在生成质量和速度上超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前3D分子生成领域扩散模型占主导，而自回归模型表现不佳。本文旨在开发一种高效、可扩展的自回归模型，提升生成质量和速度。

Method: Quetzal将分子视为有序原子序列，使用因果Transformer预测原子类型，结合扩散MLP建模连续位置分布，实现原子级生成。

Result: Quetzal在生成质量上显著超越现有自回归基线，与最先进扩散模型相当，且生成速度更快，支持精确似然计算和可变尺寸任务。

Conclusion: Quetzal展示了自回归模型在3D分子生成中的潜力，为可扩展性和通用性提供了新视角。

Abstract: Generative models of 3D molecular structure play a rapidly growing role in
the design and simulation of molecules. Diffusion models currently dominate the
space of 3D molecule generation, while autoregressive models have trailed
behind. In this work, we present Quetzal, a simple but scalable autoregressive
model that builds molecules atom-by-atom in 3D. Treating each molecule as an
ordered sequence of atoms, Quetzal combines a causal transformer that predicts
the next atom's discrete type with a smaller Diffusion MLP that models the
continuous next-position distribution. Compared to existing autoregressive
baselines, Quetzal achieves substantial improvements in generation quality and
is competitive with the performance of state-of-the-art diffusion models. In
addition, by reducing the number of expensive forward passes through a dense
transformer, Quetzal enables significantly faster generation speed, as well as
exact divergence-based likelihood computation. Finally, without any
architectural changes, Quetzal natively handles variable-size tasks like
hydrogen decoration and scaffold completion. We hope that our work motivates a
perspective on scalability and generality for generative modelling of 3D
molecules.

</details>


### [258] [Context-Free Synthetic Data Mitigates Forgetting](https://arxiv.org/abs/2505.13811)
*Parikshit Bansal,Sujay Sanghavi*

Main category: cs.LG

TL;DR: 论文提出通过上下文无关生成缓解语言模型微调中的遗忘问题，相比其他方法更有效。


<details>
  <summary>Details</summary>
Motivation: 微调语言模型常导致其在其他任务上性能下降（灾难性遗忘）。研究旨在仅通过模型权重（无需原始数据）缓解该问题。

Method: 使用KL散度惩罚原模型与新模型的差异，并提出通过上下文无关生成近似估计KL散度，将生成数据加入微调数据集。

Result: 在OLMo-1B和R1-Distill-Llama-8B上的实验表明，该方法能有效保留预训练模型的零样本性能和推理能力，优于上下文合成数据或部分预训练数据。

Conclusion: 上下文无关生成是一种简单有效的缓解遗忘方法，其效果优于传统数据增强策略。

Abstract: Fine-tuning a language model often results in a degradation of its existing
performance on other tasks, due to a shift in the model parameters; this
phenomenon is often referred to as (catastrophic) forgetting. We are interested
in mitigating this, in settings where we only have access to the model weights
but no access to its training data/recipe. A natural approach is to penalize
the KL divergence between the original model and the new one. Our main
realization is that a simple process - which we term context-free generation -
allows for an approximate unbiased estimation of this KL divergence. We show
that augmenting a fine-tuning dataset with context-free generations mitigates
forgetting, in two settings: (a) preserving the zero-shot performance of
pretrained-only models, and (b) preserving the reasoning performance of
thinking models. We show that contextual synthetic data, and even a portion of
the pretraining data, are less effective. We also investigate the effect of
choices like generation temperature, data ratios etc. We present our results
for OLMo-1B for pretrained-only setting and R1-Distill-Llama-8B for the
reasoning setting.

</details>


### [259] [FlashKAT: Understanding and Addressing Performance Bottlenecks in the Kolmogorov-Arnold Transformer](https://arxiv.org/abs/2505.13813)
*Matthew Raffel,Lizhong Chen*

Main category: cs.LG

TL;DR: FlashKAT通过优化内存访问和梯度累积，显著提升了KAT的训练速度，同时减少了梯度误差。


<details>
  <summary>Details</summary>
Motivation: Kolmogorov-Arnold网络（KAN）因其表达能力和可解释性受到关注，但计算成本高和训练不稳定限制了其在大规模任务中的应用。KAT虽在FLOPs上与Transformer相当，但训练速度仍慢123倍，需解决性能瓶颈。

Method: 通过实验分析KAT速度慢的根源，发现内存停滞和梯度累积效率低下是主要原因，提出FlashKAT，利用原子操作和优化内存访问来减少梯度累积。

Result: FlashKAT相比现有KAT实现了86.5倍的训练加速，同时减少了系数梯度的舍入误差。

Conclusion: FlashKAT有效解决了KAT的内存瓶颈问题，显著提升了训练速度和计算效率，为KAN类模型的大规模应用提供了可能。

Abstract: The Kolmogorov-Arnold Network (KAN) has been gaining popularity as an
alternative to the multi-layer perceptron (MLP) with its increased
expressiveness and interpretability. However, the KAN can be orders of
magnitude slower due to its increased computational cost and training
instability, limiting its applicability to larger-scale tasks. Recently, the
Kolmogorov-Arnold Transformer (KAT) has been proposed, which can achieve FLOPs
similar to the traditional Transformer with MLPs by leveraging Group-Rational
KAN (GR-KAN). Unfortunately, despite the comparable FLOPs, our
characterizations reveal that the KAT is still 123x slower in training speeds,
indicating that there are other performance bottlenecks beyond FLOPs. In this
paper, we conduct a series of experiments to understand the root cause of the
slowdown in KAT. We uncover that the slowdown can be isolated to memory stalls
and, more specifically, in the backward pass of GR-KAN caused by inefficient
gradient accumulation. To address this memory bottleneck, we propose FlashKAT,
which builds on our restructured kernel that minimizes gradient accumulation
with atomic adds and accesses to slow memory. Evaluations demonstrate that
FlashKAT can achieve a training speedup of 86.5x compared with the
state-of-the-art KAT, while reducing rounding errors in the coefficient
gradients. Our code is available at https://github.com/OSU-STARLAB/FlashKAT.

</details>


### [260] [Fragments to Facts: Partial-Information Fragment Inference from LLMs](https://arxiv.org/abs/2505.13819)
*Lucas Rosenblatt,Bin Han,Robert Wolfe,Bill Howe*

Main category: cs.LG

TL;DR: 该论文研究了在攻击者仅掌握部分无序信息的情况下，微调大语言模型（LLMs）对片段特定提取攻击的脆弱性，并提出了两种无需数据的方法进行系统分析。


<details>
  <summary>Details</summary>
Motivation: 先前的研究主要关注攻击者拥有完整样本或长有序前缀的强对抗假设，而忽略了攻击者仅掌握部分无序信息时的模型脆弱性。本文旨在探讨在这种较弱假设下，微调LLMs是否仍易受攻击。

Method: 提出了两种无需数据的方法：（1）基于成员推断方法的似然比攻击；（2）利用外部先验进行正则化的新方法PRISM。

Result: 实验表明，这两种方法在医疗和法律场景中与假设拥有标记数据的基线分类器表现相当，证明了它们的鲁棒性。

Conclusion: 研究表明，即使在攻击者仅掌握部分信息的情况下，微调LLMs仍易受片段特定提取攻击，提出的两种方法能有效应对这一威胁。

Abstract: Large language models (LLMs) can leak sensitive training data through
memorization and membership inference attacks. Prior work has primarily focused
on strong adversarial assumptions, including attacker access to entire samples
or long, ordered prefixes, leaving open the question of how vulnerable LLMs are
when adversaries have only partial, unordered sample information. For example,
if an attacker knows a patient has "hypertension," under what conditions can
they query a model fine-tuned on patient data to learn the patient also has
"osteoarthritis?" In this paper, we introduce a more general threat model under
this weaker assumption and show that fine-tuned LLMs are susceptible to these
fragment-specific extraction attacks. To systematically investigate these
attacks, we propose two data-blind methods: (1) a likelihood ratio attack
inspired by methods from membership inference, and (2) a novel approach, PRISM,
which regularizes the ratio by leveraging an external prior. Using examples
from both medical and legal settings, we show that both methods are competitive
with a data-aware baseline classifier that assumes access to labeled
in-distribution data, underscoring their robustness.

</details>


### [261] [Structured Agent Distillation for Large Language Model](https://arxiv.org/abs/2505.13820)
*Jun Liu,Zhenglun Kong,Peiyan Dong,Changdi Yang,Tianqi Li,Hao Tang,Geng Yuan,Wei Niu,Wenbin Zhang,Pu Zhao,Xue Lin,Dong Huang,Yanzhi Wang*

Main category: cs.LG

TL;DR: 提出结构化智能体蒸馏框架，通过分段对齐损失函数将大语言模型智能体压缩为小模型，保持推理与行动一致性，显著降低部署成本。


<details>
  <summary>Details</summary>
Motivation: 大语言模型作为决策智能体时存在高推理成本和大模型尺寸问题，限制了实际部署。需要一种方法在压缩模型的同时保留其推理和行动能力。

Method: 结构化智能体蒸馏框架，将轨迹分割为{[REASON]}和{[ACT]}片段，分别施加分段特定损失函数，实现结构感知的师生行为对齐。

Result: 在ALFWorld等三个基准测试中优于词级蒸馏和模仿学习方法，实现显著模型压缩（性能下降<1%），跨度级对齐对高效部署至关重要。

Conclusion: 结构化蒸馏能有效压缩智能体模型，为实际部署提供高效解决方案，其分段对齐机制是保持决策质量的关键。

Abstract: Large language models (LLMs) exhibit strong capabilities as decision-making
agents by interleaving reasoning and actions, as seen in ReAct-style
frameworks. Yet, their practical deployment is constrained by high inference
costs and large model sizes. We propose Structured Agent Distillation, a
framework that compresses large LLM-based agents into smaller student models
while preserving both reasoning fidelity and action consistency. Unlike
standard token-level distillation, our method segments trajectories into
{[REASON]} and {[ACT]} spans, applying segment-specific losses to align each
component with the teacher's behavior. This structure-aware supervision enables
compact agents to better replicate the teacher's decision process. Experiments
on ALFWorld, HotPotQA-ReAct, and WebShop show that our approach consistently
outperforms token-level and imitation learning baselines, achieving significant
compression with minimal performance drop. Scaling and ablation results further
highlight the importance of span-level alignment for efficient and deployable
agents.

</details>


### [262] [Rethink the Role of Deep Learning towards Large-scale Quantum Systems](https://arxiv.org/abs/2505.13852)
*Yusheng Zhao,Chi Zhang,Yuxuan Du*

Main category: cs.LG

TL;DR: 该论文通过系统比较深度学习和传统机器学习在量子系统基态学习任务中的表现，发现传统方法常能达到或超越深度学习性能，质疑当前深度学习在此领域的必要性。


<details>
  <summary>Details</summary>
Motivation: 量子系统基态特性表征至关重要但计算复杂。尽管AI领域提出多种深度学习方法，但其实际作用和必要性尚不明确，且先前研究常使用不公平的数据集构建方式。

Method: 在三种哈密顿量家族上，将深度学习模型与传统机器学习方法进行系统基准测试，量子资源使用均等，并扩展至127个量子比特的基态学习任务。

Result: 传统机器学习模型在所有任务中表现与深度学习相当或更优；随机化测试显示测量输入特征对深度学习预测性能影响微弱。

Conclusion: 当前深度学习模型在许多量子系统学习场景中并非必需，研究结果为深度学习的有效应用提供了重要参考。

Abstract: Characterizing the ground state properties of quantum systems is fundamental
to capturing their behavior but computationally challenging. Recent advances in
AI have introduced novel approaches, with diverse machine learning (ML) and
deep learning (DL) models proposed for this purpose. However, the necessity and
specific role of DL models in these tasks remain unclear, as prior studies
often employ varied or impractical quantum resources to construct datasets,
resulting in unfair comparisons. To address this, we systematically benchmark
DL models against traditional ML approaches across three families of
Hamiltonian, scaling up to 127 qubits in three crucial ground-state learning
tasks while enforcing equivalent quantum resource usage. Our results reveal
that ML models often achieve performance comparable to or even exceeding that
of DL approaches across all tasks. Furthermore, a randomization test
demonstrates that measurement input features have minimal impact on DL models'
prediction performance. These findings challenge the necessity of current DL
models in many quantum system learning scenarios and provide valuable insights
into their effective utilization.

</details>


### [263] [Learning Spatio-Temporal Dynamics for Trajectory Recovery via Time-Aware Transformer](https://arxiv.org/abs/2505.13857)
*Tian Sun,Yuqi Chen,Baihua Zheng,Weiwei Sun*

Main category: cs.LG

TL;DR: 提出TedTrajRec方法，通过PD-GNN和TedFormer分别捕捉交通和轨迹的时空动态，提升低采样率GPS轨迹的恢复效果。


<details>
  <summary>Details</summary>
Motivation: 现实应用中GPS轨迹采样率低且间隔不规则，现有序列模型未能充分利用轨迹和路网的复杂时空动态。

Method: TedTrajRec结合PD-GNN建模周期性交通动态，TedFormer利用神经微分方程处理不规则轨迹数据。

Result: 在三个真实数据集上验证了TedTrajRec的优越性能。

Conclusion: 通过分而治之的时空动态建模策略，显著提升了低采样轨迹的恢复精度。

Abstract: In real-world applications, GPS trajectories often suffer from low sampling
rates, with large and irregular intervals between consecutive GPS points. This
sparse characteristic presents challenges for their direct use in GPS-based
systems. This paper addresses the task of map-constrained trajectory recovery,
aiming to enhance trajectory sampling rates of GPS trajectories. Previous
studies commonly adopt a sequence-to-sequence framework, where an encoder
captures the trajectory patterns and a decoder reconstructs the target
trajectory. Within this framework, effectively representing the road network
and extracting relevant trajectory features are crucial for overall
performance. Despite advancements in these models, they fail to fully leverage
the complex spatio-temporal dynamics present in both the trajectory and the
road network.
  To overcome these limitations, we categorize the spatio-temporal dynamics of
trajectory data into two distinct aspects: spatial-temporal traffic dynamics
and trajectory dynamics. Furthermore, We propose TedTrajRec, a novel method for
trajectory recovery. To capture spatio-temporal traffic dynamics, we introduce
PD-GNN, which models periodic patterns and learns topologically aware dynamics
concurrently for each road segment. For spatio-temporal trajectory dynamics, we
present TedFormer, a time-aware Transformer that incorporates temporal dynamics
for each GPS location by integrating closed-form neural ordinary differential
equations into the attention mechanism. This allows TedFormer to effectively
handle irregularly sampled data. Extensive experiments on three real-world
datasets demonstrate the superior performance of TedTrajRec. The code is
publicly available at https://github.com/ysygMhdxw/TEDTrajRec/.

</details>


### [264] [Enforcing Hard Linear Constraints in Deep Learning Models with Decision Rules](https://arxiv.org/abs/2505.13858)
*Gonzalo E. Constante-Flores,Hao Chen,Can Li*

Main category: cs.LG

TL;DR: 提出了一种模型无关的框架，确保神经网络输出满足输入依赖的线性约束，结合任务网络和安全网络，保证约束满足且保持预测准确性。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在安全关键任务中需要满足硬约束（如物理定律、公平性要求等），但现有方法难以处理复杂约束或缺乏可行性保证。

Method: 结合任务网络（预测准确性）和安全网络（基于随机和鲁棒优化的决策规则），通过凸组合确保输出满足约束，无需迭代或运行时优化。

Result: 实验表明，该方法在基准回归任务中能持续满足约束，同时保持竞争性准确性和低推理延迟。

Conclusion: 该框架是约束函数的通用逼近器，通过线性决策规则实现计算高效，适用于需要硬约束保证的应用场景。

Abstract: Deep learning models are increasingly deployed in safety-critical tasks where
predictions must satisfy hard constraints, such as physical laws, fairness
requirements, or safety limits. However, standard architectures lack built-in
mechanisms to enforce such constraints, and existing approaches based on
regularization or projection are often limited to simple constraints,
computationally expensive, or lack feasibility guarantees. This paper proposes
a model-agnostic framework for enforcing input-dependent linear equality and
inequality constraints on neural network outputs. The architecture combines a
task network trained for prediction accuracy with a safe network trained using
decision rules from the stochastic and robust optimization literature to ensure
feasibility across the entire input space. The final prediction is a convex
combination of the two subnetworks, guaranteeing constraint satisfaction during
both training and inference without iterative procedures or runtime
optimization. We prove that the architecture is a universal approximator of
constrained functions and derive computationally tractable formulations based
on linear decision rules. Empirical results on benchmark regression tasks show
that our method consistently satisfies constraints while maintaining
competitive accuracy and low inference latency.

</details>


### [265] [Utilizing Strategic Pre-training to Reduce Overfitting: Baguan -- A Pre-trained Weather Forecasting Model](https://arxiv.org/abs/2505.13873)
*Peisong Niu,Ziqing Ma,Tian Zhou,Weiqi Chen,Lefei Shen,Rong Jin,Liang Sun*

Main category: cs.LG

TL;DR: 论文提出了一种名为Baguan的新型数据驱动模型，通过自监督预训练和微调，有效解决了天气预测中的过拟合问题，并在多个任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 天气预测长期以来一直是一个重大挑战。尽管近年来基于AI的模型在全球预测任务中超越了传统的数值天气预报（NWP）方法，但由于真实天气数据仅覆盖几十年，过拟合问题仍然严重。与计算机视觉或自然语言处理等领域不同，天气预测需要创新策略来利用现有数据解决这一问题。

Method: 论文探索了天气预测的预训练方法，发现选择适当挑战性的预训练任务可以引入局部性偏差，有效缓解过拟合并提升性能。提出的Baguan模型基于孪生自编码器，通过自监督预训练和针对不同预测时长的微调构建。

Result: 实验结果表明，Baguan在中期天气预测中优于传统方法，提供了更准确的预测。此外，预训练的Baguan在过拟合控制方面表现稳健，并在次季节至季节（S2S）建模和区域预测等下游任务中表现出色。

Conclusion: Baguan模型通过创新的预训练策略，成功解决了天气预测中的过拟合问题，并在多个任务中展现了优越性能，为数据驱动的天气预测提供了新的解决方案。

Abstract: Weather forecasting has long posed a significant challenge for humanity.
While recent AI-based models have surpassed traditional numerical weather
prediction (NWP) methods in global forecasting tasks, overfitting remains a
critical issue due to the limited availability of real-world weather data
spanning only a few decades. Unlike fields like computer vision or natural
language processing, where data abundance can mitigate overfitting, weather
forecasting demands innovative strategies to address this challenge with
existing data. In this paper, we explore pre-training methods for weather
forecasting, finding that selecting an appropriately challenging pre-training
task introduces locality bias, effectively mitigating overfitting and enhancing
performance. We introduce Baguan, a novel data-driven model for medium-range
weather forecasting, built on a Siamese Autoencoder pre-trained in a
self-supervised manner and fine-tuned for different lead times. Experimental
results show that Baguan outperforms traditional methods, delivering more
accurate forecasts. Additionally, the pre-trained Baguan demonstrates robust
overfitting control and excels in downstream tasks, such as
subseasonal-to-seasonal (S2S) modeling and regional forecasting, after
fine-tuning.

</details>


### [266] [InfiFPO: Implicit Model Fusion via Preference Optimization in Large Language Models](https://arxiv.org/abs/2505.13878)
*Yanggan Gu,Zhaoyi Yan,Yuanyi Wang,Yiming Zhang,Qi Zhou,Fei Wu,Hongxia Yang*

Main category: cs.LG

TL;DR: InfiFPO是一种新的偏好优化方法，通过融合多个大语言模型的概率信息，提升模型性能，尤其在数学、编码和推理任务上表现显著。


<details>
  <summary>Details</summary>
Motivation: 现有模型融合方法主要关注监督微调，而偏好对齐阶段的研究较少，且现有方法仅利用响应输出而忽略了概率信息。InfiFPO旨在填补这一空白。

Method: InfiFPO通过序列级多源概率融合替代DPO中的参考模型，引入概率裁剪和最大间隔融合策略，有效保留概率信息并避免词汇对齐难题。

Result: 在11个基准测试中，InfiFPO显著优于现有方法，将Phi-4模型的平均性能从79.95提升至83.33。

Conclusion: InfiFPO不仅提升了模型融合的效果，还验证了概率信息在偏好对齐中的重要性，为未来研究提供了新方向。

Abstract: Model fusion combines multiple Large Language Models (LLMs) with different
strengths into a more powerful, integrated model through lightweight training
methods. Existing works on model fusion focus primarily on supervised
fine-tuning (SFT), leaving preference alignment (PA) --a critical phase for
enhancing LLM performance--largely unexplored. The current few fusion methods
on PA phase, like WRPO, simplify the process by utilizing only response outputs
from source models while discarding their probability information. To address
this limitation, we propose InfiFPO, a preference optimization method for
implicit model fusion. InfiFPO replaces the reference model in Direct
Preference Optimization (DPO) with a fused source model that synthesizes
multi-source probabilities at the sequence level, circumventing complex
vocabulary alignment challenges in previous works and meanwhile maintaining the
probability information. By introducing probability clipping and max-margin
fusion strategies, InfiFPO enables the pivot model to align with human
preferences while effectively distilling knowledge from source models.
Comprehensive experiments on 11 widely-used benchmarks demonstrate that InfiFPO
consistently outperforms existing model fusion and preference optimization
methods. When using Phi-4 as the pivot model, InfiFPO improve its average
performance from 79.95 to 83.33 on 11 benchmarks, significantly improving its
capabilities in mathematics, coding, and reasoning tasks.

</details>


### [267] [CRAFT: Time Series Forecasting with Cross-Future Behavior Awareness](https://arxiv.org/abs/2505.13896)
*Yingwei Zhang,Ke Bu,Zhuoran Zhuang,Tao Xie,Yao Yu,Dong Li,Yang Guo,Detao Lv*

Main category: cs.LG

TL;DR: 该论文提出了一种基于跨未来行为感知的时间序列预测方法CRAFT，通过利用跨未来行为的趋势来挖掘待预测时间序列数据的趋势，解决了传统时间序列预测中因历史数据有限而导致的预测不确定性难题。


<details>
  <summary>Details</summary>
Motivation: 时间序列预测在许多实际应用中取得了显著进展，但通常受限于历史数据不足导致的预测不确定性。为了解决这一问题，论文探索了跨未来行为（CFB）在时间序列预测中的应用。

Method: CRAFT方法通过Koopman预测模块提取关键趋势，利用内部趋势挖掘模块补充跨未来行为矩阵的未知区域，并通过分层结构的外部趋势引导模块获取更具代表性的趋势，最后使用需求约束损失校准预测结果的分布偏差。

Result: 在离线和在线A/B测试中，CRAFT方法在真实数据集上表现出了有效性。

Conclusion: CRAFT方法通过跨未来行为感知，有效解决了时间序列预测中的不确定性难题，并在实验中验证了其优越性。

Abstract: The past decades witness the significant advancements in time series
forecasting (TSF) across various real-world domains, including e-commerce and
disease spread prediction. However, TSF is usually constrained by the
uncertainty dilemma of predicting future data with limited past observations.
To settle this question, we explore the use of Cross-Future Behavior (CFB) in
TSF, which occurs before the current time but takes effect in the future. We
leverage CFB features and propose the CRoss-Future Behavior Awareness based
Time Series Forecasting method (CRAFT). The core idea of CRAFT is to utilize
the trend of cross-future behavior to mine the trend of time series data to be
predicted. Specifically, to settle the sparse and partial flaws of cross-future
behavior, CRAFT employs the Koopman Predictor Module to extract the key trend
and the Internal Trend Mining Module to supplement the unknown area of the
cross-future behavior matrix. Then, we introduce the External Trend Guide
Module with a hierarchical structure to acquire more representative trends from
higher levels. Finally, we apply the demand-constrained loss to calibrate the
distribution deviation of prediction results. We conduct experiments on
real-world dataset. Experiments on both offline large-scale dataset and online
A/B test demonstrate the effectiveness of CRAFT. Our dataset and code is
available at https://github.com/CRAFTinTSF/CRAFT.

</details>


### [268] [Do Language Models Use Their Depth Efficiently?](https://arxiv.org/abs/2505.13898)
*Róbert Csordás,Christopher D. Manning,Christopher Potts*

Main category: cs.LG

TL;DR: 研究发现，更深的LLM模型并未有效利用增加的深度进行新型计算，而是将相同计算分散到更多层中，导致性能提升递减。


<details>
  <summary>Details</summary>
Motivation: 探讨现代大型语言模型（LLM）是否有效利用增加的深度进行更高级别的计算，还是仅仅将相同计算分散到更多层中。

Method: 通过分析Llama 3.1和Qwen 3系列模型的残差流，比较子层输出、跳过层的影响、多跳任务的表现，以及训练浅层模型到深层模型的线性映射。

Result: 发现模型后半部分的层贡献较小，跳过这些层对预测影响小；多跳任务中未发现深度用于组合子结果的证据；深层模型仅将相同计算分散到更多层中。

Conclusion: 更深的模型并未利用增加的深度学习新型计算，而是进行更细粒度的残差调整，这可能是Transformer架构规模增加导致性能提升递减的原因。

Abstract: Modern LLMs are increasingly deep, and depth correlates with performance,
albeit with diminishing returns. However, do these models use their depth
efficiently? Do they compose more features to create higher-order computations
that are impossible in shallow models, or do they merely spread the same kinds
of computation out over more layers? To address these questions, we analyze the
residual stream of the Llama 3.1 and Qwen 3 family of models. We find: First,
comparing the output of the sublayers to the residual stream reveals that
layers in the second half contribute much less than those in the first half,
with a clear phase transition between the two halves. Second, skipping layers
in the second half has a much smaller effect on future computations and output
predictions. Third, for multihop tasks, we are unable to find evidence that
models are using increased depth to compose subresults in examples involving
many hops. Fourth, we seek to directly address whether deeper models are using
their additional layers to perform new kinds of computation. To do this, we
train linear maps from the residual stream of a shallow model to a deeper one.
We find that layers with the same relative depth map best to each other,
suggesting that the larger model simply spreads the same computations out over
its many layers. All this evidence suggests that deeper models are not using
their depth to learn new kinds of computation, but only using the greater depth
to perform more fine-grained adjustments to the residual. This may help explain
why increasing scale leads to diminishing returns for stacked Transformer
architectures.

</details>


### [269] [Exploring Causes of Representational Similarity in Machine Learning Models](https://arxiv.org/abs/2505.13899)
*Zeyu Michael Li,Hung Anh Vu,Damilola Awofisayo,Emily Wenger*

Main category: cs.LG

TL;DR: 该论文探讨了数据集重叠和任务重叠如何影响机器学习模型的表示相似性，发现两者均能提高相似性，且结合时效果最强。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于理解不同模态机器学习模型表示相似性的原因，尤其是数据集重叠和任务重叠的影响。

Method: 通过一系列实验评估数据集重叠和任务重叠对模型表示相似性的影响。

Result: 发现数据集重叠和任务重叠均与更高的表示相似性正相关，且两者结合时效果最显著。

Conclusion: 数据集和任务重叠是模型表示相似性的重要驱动因素，结合两者可最大化相似性效果。

Abstract: Numerous works have noted significant similarities in how machine learning
models represent the world, even across modalities. Although much effort has
been devoted to uncovering properties and metrics on which these models align,
surprisingly little work has explored causes of this similarity. To advance
this line of inquiry, this work explores how two possible causal factors --
dataset overlap and task overlap -- influence downstream model similarity. The
exploration of dataset overlap is motivated by the reality that large-scale
generative AI models are often trained on overlapping datasets of scraped
internet data, while the exploration of task overlap seeks to substantiate
claims from a recent work, the Platonic Representation Hypothesis, that task
similarity may drive model similarity. We evaluate the effects of both factors
through a broad set of experiments. We find that both positively correlate with
higher representational similarity and that combining them provides the
strongest effect. Our code and dataset are published.

</details>


### [270] [New Evidence of the Two-Phase Learning Dynamics of Neural Networks](https://arxiv.org/abs/2505.13900)
*Zhanpeng Zhou,Yongyi Yang,Mahito Sugiyama,Junchi Yan*

Main category: cs.LG

TL;DR: 该论文通过时间窗口比较网络状态，揭示了深度学习的双阶段特性：混沌效应和锥效应，表明网络从敏感探索到稳定优化的动态转变。


<details>
  <summary>Details</summary>
Motivation: 理解深度神经网络的学习机制是现代机器学习中的一个基本挑战。现有证据表明训练动态存在明显的相变，但对此的理解仍不完整。

Method: 论文采用区间视角，比较时间窗口内的网络状态，通过注入微小参数扰动和追踪经验神经切线核（eNTK）的演化来研究训练动态。

Result: 研究发现：i) 混沌效应：网络在早期关键期对初始条件高度敏感；ii) 锥效应：过渡后模型的功能轨迹被限制在一个狭窄的锥形子集中。

Conclusion: 这些效应提供了深度网络在训练过程中从敏感探索到稳定优化的结构性和动态性视角，深化了对学习机制的理解。

Abstract: Understanding how deep neural networks learn remains a fundamental challenge
in modern machine learning. A growing body of evidence suggests that training
dynamics undergo a distinct phase transition, yet our understanding of this
transition is still incomplete. In this paper, we introduce an interval-wise
perspective that compares network states across a time window, revealing two
new phenomena that illuminate the two-phase nature of deep learning. i)
\textbf{The Chaos Effect.} By injecting an imperceptibly small parameter
perturbation at various stages, we show that the response of the network to the
perturbation exhibits a transition from chaotic to stable, suggesting there is
an early critical period where the network is highly sensitive to initial
conditions; ii) \textbf{The Cone Effect.} Tracking the evolution of the
empirical Neural Tangent Kernel (eNTK), we find that after this transition
point the model's functional trajectory is confined to a narrow cone-shaped
subset: while the kernel continues to change, it gets trapped into a tight
angular region. Together, these effects provide a structural, dynamical view of
how deep networks transition from sensitive exploration to stable refinement
during training.

</details>


### [271] [Learning to Insert for Constructive Neural Vehicle Routing Solver](https://arxiv.org/abs/2505.13904)
*Fu Luo,Xi Lin,Mengyuan Zhong,Fei Liu,Zhenkun Wang,Jianyong Sun,Qingfu Zhang*

Main category: cs.LG

TL;DR: 本文提出了一种基于插入范式的新型学习构造方法L2C-Insert，用于神经组合优化（NCO），在车辆路径问题（VRP）中通过灵活插入节点提升解的质量。


<details>
  <summary>Details</summary>
Motivation: 现有的构造性NCO方法通常采用追加范式，顺序添加未访问节点到部分解中，这种方法较为僵化，容易导致次优解。为了克服这一限制，作者探索了插入范式的潜力。

Method: L2C-Insert通过在任何有效位置策略性地插入未访问节点来构建解，提出了三个关键组件：用于精确插入位置预测的新模型架构、高效的模型优化训练方案以及充分利用插入范式灵活性的高级推理技术。

Result: 在旅行商问题（TSP）和带容量约束的车辆路径问题（CVRP）的合成和实际实例上的大量实验表明，L2C-Insert在各种问题规模上均表现出优越性能。

Conclusion: L2C-Insert通过插入范式显著提高了构造性NCO的灵活性和解的质量，为车辆路径问题提供了一种有效的学习型解决方案。

Abstract: Neural Combinatorial Optimisation (NCO) is a promising learning-based
approach for solving Vehicle Routing Problems (VRPs) without extensive manual
design. While existing constructive NCO methods typically follow an
appending-based paradigm that sequentially adds unvisited nodes to partial
solutions, this rigid approach often leads to suboptimal results. To overcome
this limitation, we explore the idea of insertion-based paradigm and propose
Learning to Construct with Insertion-based Paradigm (L2C-Insert), a novel
learning-based method for constructive NCO. Unlike traditional approaches,
L2C-Insert builds solutions by strategically inserting unvisited nodes at any
valid position in the current partial solution, which can significantly enhance
the flexibility and solution quality. The proposed framework introduces three
key components: a novel model architecture for precise insertion position
prediction, an efficient training scheme for model optimization, and an
advanced inference technique that fully exploits the insertion paradigm's
flexibility. Extensive experiments on both synthetic and real-world instances
of the Travelling Salesman Problem (TSP) and Capacitated Vehicle Routing
Problem (CVRP) demonstrate that L2C-Insert consistently achieves superior
performance across various problem sizes.

</details>


### [272] [Cross-Domain Diffusion with Progressive Alignment for Efficient Adaptive Retrieval](https://arxiv.org/abs/2505.13907)
*Junyu Luo,Yusheng Zhao,Xiao Luo,Zhiping Xiao,Wei Ju,Li Shen,Dacheng Tao,Ming Zhang*

Main category: cs.LG

TL;DR: 提出了一种名为COUPLE的新方法，通过图扩散和渐进对齐解决无监督高效域自适应检索中的噪声问题，显著提升了检索性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理目标域噪声和跨域特征对齐时表现不佳，导致检索性能不理想。

Method: COUPLE方法通过构建跨域关系图、噪声鲁棒的图流扩散和渐进域对齐，实现高效的域自适应哈希学习。

Result: 实验表明，COUPLE在多个基准测试中表现出色，显著提升了检索性能。

Conclusion: COUPLE方法有效解决了目标域噪声和跨域对齐问题，为无监督高效域自适应检索提供了新思路。

Abstract: Unsupervised efficient domain adaptive retrieval aims to transfer knowledge
from a labeled source domain to an unlabeled target domain, while maintaining
low storage cost and high retrieval efficiency. However, existing methods
typically fail to address potential noise in the target domain, and directly
align high-level features across domains, thus resulting in suboptimal
retrieval performance. To address these challenges, we propose a novel
Cross-Domain Diffusion with Progressive Alignment method (COUPLE). This
approach revisits unsupervised efficient domain adaptive retrieval from a graph
diffusion perspective, simulating cross-domain adaptation dynamics to achieve a
stable target domain adaptation process. First, we construct a cross-domain
relationship graph and leverage noise-robust graph flow diffusion to simulate
the transfer dynamics from the source domain to the target domain, identifying
lower noise clusters. We then leverage the graph diffusion results for
discriminative hash code learning, effectively learning from the target domain
while reducing the negative impact of noise. Furthermore, we employ a
hierarchical Mixup operation for progressive domain alignment, which is
performed along the cross-domain random walk paths. Utilizing target domain
discriminative hash learning and progressive domain alignment, COUPLE enables
effective domain adaptive hash learning. Extensive experiments demonstrate
COUPLE's effectiveness on competitive benchmarks.

</details>


### [273] [ShortcutProbe: Probing Prediction Shortcuts for Learning Robust Models](https://arxiv.org/abs/2505.13910)
*Guangtao Zheng,Wenqian Ye,Aidong Zhang*

Main category: cs.LG

TL;DR: 该论文提出了一种无需组标签的后处理框架ShortcutProbe，用于减轻深度学习模型中的伪偏差，提高模型鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型常因学习目标与非关键特征间的伪相关性而产生偏差，现有方法依赖昂贵的人工标注组标签且难以捕捉细微偏差。

Method: 通过ShortcutProbe在潜在空间中识别预测捷径，并重新训练模型使其对这些捷径不变，从而提升鲁棒性。

Result: 理论分析和实验表明，该框架能有效提升模型在多样化数据集上对伪偏差的鲁棒性。

Conclusion: ShortcutProbe是一种高效实用的工具，无需组标签即可显著改善模型对伪偏差的鲁棒性。

Abstract: Deep learning models often achieve high performance by inadvertently learning
spurious correlations between targets and non-essential features. For example,
an image classifier may identify an object via its background that spuriously
correlates with it. This prediction behavior, known as spurious bias, severely
degrades model performance on data that lacks the learned spurious
correlations. Existing methods on spurious bias mitigation typically require a
variety of data groups with spurious correlation annotations called group
labels. However, group labels require costly human annotations and often fail
to capture subtle spurious biases such as relying on specific pixels for
predictions. In this paper, we propose a novel post hoc spurious bias
mitigation framework without requiring group labels. Our framework, termed
ShortcutProbe, identifies prediction shortcuts that reflect potential
non-robustness in predictions in a given model's latent space. The model is
then retrained to be invariant to the identified prediction shortcuts for
improved robustness. We theoretically analyze the effectiveness of the
framework and empirically demonstrate that it is an efficient and practical
tool for improving a model's robustness to spurious bias on diverse datasets.

</details>


### [274] [RLVR-World: Training World Models with Reinforcement Learning](https://arxiv.org/abs/2505.13934)
*Jialong Wu,Shaofeng Yin,Ningya Feng,Mingsheng Long*

Main category: cs.LG

TL;DR: RLVR-World框架通过强化学习直接优化世界模型的任务指标，显著提升了语言和视频模型的性能。


<details>
  <summary>Details</summary>
Motivation: 传统最大似然估计训练目标与任务特定指标（如准确性或感知质量）存在偏差，需要更直接优化世界模型的方法。

Method: 提出RLVR-World框架，利用可验证奖励的强化学习（RLVR）直接优化世界模型的任务指标。

Result: 在文本游戏、网页导航和机器人操作等多个领域，语言和视频世界模型的性能均得到显著提升。

Conclusion: RLVR作为一种后训练范式，有望广泛提升生成模型的实用性。

Abstract: World models predict state transitions in response to actions and are
increasingly developed across diverse modalities. However, standard training
objectives such as maximum likelihood estimation (MLE) often misalign with
task-specific goals of world models, i.e., transition prediction metrics like
accuracy or perceptual quality. In this paper, we present RLVR-World, a unified
framework that leverages reinforcement learning with verifiable rewards (RLVR)
to directly optimize world models for such metrics. Despite formulating world
modeling as autoregressive prediction of tokenized sequences, RLVR-World
evaluates metrics of decoded predictions as verifiable rewards. We demonstrate
substantial performance gains on both language- and video-based world models
across domains, including text games, web navigation, and robot manipulation.
Our work indicates that, beyond recent advances in reasoning language models,
RLVR offers a promising post-training paradigm for enhancing the utility of
generative models more broadly.

</details>


### [275] [CLEVER: A Curated Benchmark for Formally Verified Code Generation](https://arxiv.org/abs/2505.13938)
*Amitayush Thakur,Jasper Lee,George Tsoukalas,Meghana Sistla,Matthew Zhao,Stefan Zetzche,Greg Durrett,Yisong Yue,Swarat Chaudhuri*

Main category: cs.LG

TL;DR: 论文介绍了CLEVER，一个高质量的、包含161个问题的基准测试集，用于Lean中的端到端验证代码生成。所有输出都经过Lean的类型检查器验证，确保机器可检查的正确性。


<details>
  <summary>Details</summary>
Motivation: 当前代码生成基准测试存在测试用例监督、LLM生成注释以及规范泄露实现逻辑或允许空泛解决方案的问题。CLEVER旨在提供一个更高质量的基准测试集，避免这些问题。

Method: CLEVER包含两个任务：(1)生成与保留的真实规范匹配的规范，(2)生成可证明满足此规范的Lean实现。所有输出都通过Lean的类型检查器进行后验验证。

Result: 使用CLEVER评估了几种基于最先进语言模型的少样本和代理方法，这些方法在实现完全验证方面均表现不佳，表明这是一个具有挑战性的前沿基准。

Conclusion: CLEVER为程序合成和形式推理提供了一个高质量的挑战性基准，现有方法难以达到完全验证，突显了其作为前沿基准的价值。

Abstract: We introduce ${\rm C{\small LEVER}}$, a high-quality, curated benchmark of
161 problems for end-to-end verified code generation in Lean. Each problem
consists of (1) the task of generating a specification that matches a held-out
ground-truth specification, and (2) the task of generating a Lean
implementation that provably satisfies this specification. Unlike prior
benchmarks, ${\rm C{\small LEVER}}$ avoids test-case supervision, LLM-generated
annotations, and specifications that leak implementation logic or allow vacuous
solutions. All outputs are verified post-hoc using Lean's type checker to
ensure machine-checkable correctness. We use ${\rm C{\small LEVER}}$ to
evaluate several few-shot and agentic approaches based on state-of-the-art
language models. These methods all struggle to achieve full verification,
establishing it as a challenging frontier benchmark for program synthesis and
formal reasoning. Our benchmark can be found on
GitHub(https://github.com/trishullab/clever) as well as
HuggingFace(https://huggingface.co/datasets/amitayusht/clever). All our
evaluation code is also available
online(https://github.com/trishullab/clever-prover).

</details>


### [276] [VAMO: Efficient Large-Scale Nonconvex Optimization via Adaptive Zeroth Order Variance Reduction](https://arxiv.org/abs/2505.13954)
*Jiahe Chen,Ziye Ma*

Main category: cs.LG

TL;DR: VAMO是一种结合一阶和零阶梯度优化的混合方法，通过方差减少技术提升收敛速度，适用于大规模非凸优化问题。


<details>
  <summary>Details</summary>
Motivation: 大规模非凸优化问题在机器学习中常见，但一阶方法计算成本高，零阶方法收敛慢。需要一种平衡收敛速度和计算效率的方法。

Method: VAMO结合一阶小批量梯度和零阶有限差分估计，采用SVRG框架，提出两点和多点零阶变体以平衡收敛和成本。

Result: VAMO实现了与维度无关的收敛速率，优于纯零阶方法和SGD，实验显示在神经网络训练和LLM微调中表现优异。

Conclusion: VAMO提供了一种高效、灵活的优化方案，适用于计算受限场景，显著提升大规模非凸问题的优化效率。

Abstract: Optimizing large-scale nonconvex problems, common in machine learning,
demands balancing rapid convergence with computational efficiency. First-order
(FO) stochastic methods like SVRG provide fast convergence and good
generalization but incur high costs due to full-batch gradients in large
models. Conversely, zeroth-order (ZO) algorithms reduce this burden using
estimated gradients, yet their slow convergence in high-dimensional settings
limits practicality. We introduce VAMO (VAriance-reduced Mixed-gradient
Optimizer), a stochastic variance-reduced method combining FO mini-batch
gradients with lightweight ZO finite-difference probes under an SVRG-style
framework. VAMO's hybrid design uses a two-point ZO estimator to achieve a
dimension-agnostic convergence rate of $\mathcal{O}(1/T + 1/b)$, where $T$ is
the number of iterations and $b$ is the batch-size, surpassing the
dimension-dependent slowdown of purely ZO methods and significantly improving
over SGD's $\mathcal{O}(1/\sqrt{T})$ rate. Additionally, we propose a
multi-point ZO variant that mitigates the $O(1/b)$ error by adjusting number of
estimation points to balance convergence and cost, making it ideal for a whole
range of computationally constrained scenarios. Experiments including
traditional neural network training and LLM finetuning show VAMO outperforms
established FO and ZO methods, offering a faster, more flexible option for
improved efficiency.

</details>


### [277] [When LLMs meet open-world graph learning: a new perspective for unlabeled data uncertainty](https://arxiv.org/abs/2505.13989)
*Yanzhe Wen,Xunkai Li,Qi Zhang,Zhu Lei,Guang Zeng,Rong-Hua Li,Guoren Wang*

Main category: cs.LG

TL;DR: 提出OGA框架，结合语义与拓扑处理开放世界图中的未知类节点，并通过标注工具更新模型。


<details>
  <summary>Details</summary>
Motivation: 现有方法在开放世界场景中处理数据不确定性不足，特别是在有限标注和未知类节点方面。

Method: OGA框架结合自适应标签追踪（整合语义与拓扑）和图标注工具，以处理未知类节点并更新模型。

Result: 综合实验证明OGA的有效性和实用性。

Conclusion: OGA为开放世界图学习提供了一种有效的解决方案。

Abstract: Recently, large language models (LLMs) have significantly advanced
text-attributed graph (TAG) learning. However, existing methods inadequately
handle data uncertainty in open-world scenarios, especially concerning limited
labeling and unknown-class nodes. Prior solutions typically rely on isolated
semantic or structural approaches for unknown-class rejection, lacking
effective annotation pipelines. To address these limitations, we propose
Open-world Graph Assistant (OGA), an LLM-based framework that combines adaptive
label traceability, which integrates semantics and topology for unknown-class
rejection, and a graph label annotator to enable model updates using newly
annotated nodes. Comprehensive experiments demonstrate OGA's effectiveness and
practicality.

</details>


### [278] [Towards Comprehensive and Prerequisite-Free Explainer for Graph Neural Networks](https://arxiv.org/abs/2505.14005)
*Han Zhang,Yan Wang,Guanfeng Liu,Pengfei Ding,Huaxiong Wang,Kwok-Yan Lam*

Main category: cs.LG

TL;DR: 提出OPEN方法，解决现有GNN可解释性方法在捕获全局决策逻辑和依赖严格前提条件的问题，通过环境划分和子图采样提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有GNN可解释性方法(XGNN)存在两大局限：1) 无法捕获全样本空间中不同分布下的完整决策逻辑；2) 对边属性和GNN内部访问有严格前提要求。这限制了方法的性能和泛化能力。

Method: 提出OPEN框架：1) 将样本空间划分为多个遵循不同分布的环境；2) 通过从各环境采样子图分析预测结果，学习GNN跨分布的决策逻辑，无需严格前提条件。

Result: 实验表明OPEN能捕获近乎完整的GNN决策逻辑，在保真度上优于SOTA方法且效率相当，增强了现实场景的鲁棒性。

Conclusion: OPEN首次实现无前提条件的全局GNN决策逻辑解释，通过环境感知机制突破了现有方法的局限性。

Abstract: To enhance the reliability and credibility of graph neural networks (GNNs)
and improve the transparency of their decision logic, a new field of
explainability of GNNs (XGNN) has emerged. However, two major limitations
severely degrade the performance and hinder the generalizability of existing
XGNN methods: they (a) fail to capture the complete decision logic of GNNs
across diverse distributions in the entire dataset's sample space, and (b)
impose strict prerequisites on edge properties and GNN internal accessibility.
To address these limitations, we propose OPEN, a novel c\textbf{O}mprehensive
and \textbf{P}rerequisite-free \textbf{E}xplainer for G\textbf{N}Ns. OPEN, as
the first work in the literature, can infer and partition the entire dataset's
sample space into multiple environments, each containing graphs that follow a
distinct distribution. OPEN further learns the decision logic of GNNs across
different distributions by sampling subgraphs from each environment and
analyzing their predictions, thus eliminating the need for strict
prerequisites. Experimental results demonstrate that OPEN captures nearly
complete decision logic of GNNs, outperforms state-of-the-art methods in
fidelity while maintaining similar efficiency, and enhances robustness in
real-world scenarios.

</details>


### [279] [Adaptive Sentencing Prediction with Guaranteed Accuracy and Legal Interpretability](https://arxiv.org/abs/2505.14011)
*Yifei Jin,Xin Zheng,Lei Guo*

Main category: cs.LG

TL;DR: 该论文提出了一种基于中国刑法的新型饱和机制量刑（SMS）模型，通过动量最小均方（MLMS）自适应算法提高预测准确性，并在真实数据集上验证了其接近理论最优上限的性能。


<details>
  <summary>Details</summary>
Motivation: 现有司法量刑预测研究多依赖端到端模型，忽视了量刑逻辑且缺乏可解释性，难以满足学术研究和司法实践的需求。

Method: 提出饱和机制量刑（SMS）模型，结合中国刑法基础实现法律可解释性；引入动量最小均方（MLMS）自适应算法，并建立无需数据平稳性和独立性假设的预测精度数学理论。

Result: 在构建的中国故意伤害（CIBH）数据集上，实验表明该方法预测精度接近理论最优上限，验证了模型适用性和算法准确性。

Conclusion: SMS模型和MLMS算法在保持法律可解释性的同时，实现了接近理论最优的预测性能，为司法量刑预测提供了新思路。

Abstract: Existing research on judicial sentencing prediction predominantly relies on
end-to-end models, which often neglect the inherent sentencing logic and lack
interpretability-a critical requirement for both scholarly research and
judicial practice. To address this challenge, we make three key
contributions:First, we propose a novel Saturated Mechanistic Sentencing (SMS)
model, which provides inherent legal interpretability by virtue of its
foundation in China's Criminal Law. We also introduce the corresponding
Momentum Least Mean Squares (MLMS) adaptive algorithm for this model. Second,
for the MLMS algorithm based adaptive sentencing predictor, we establish a
mathematical theory on the accuracy of adaptive prediction without resorting to
any stationarity and independence assumptions on the data. We also provide a
best possible upper bound for the prediction accuracy achievable by the best
predictor designed in the known parameters case. Third, we construct a Chinese
Intentional Bodily Harm (CIBH) dataset. Utilizing this real-world data,
extensive experiments demonstrate that our approach achieves a prediction
accuracy that is not far from the best possible theoretical upper bound,
validating both the model's suitability and the algorithm's accuracy.

</details>


### [280] [Adversarial Training from Mean Field Perspective](https://arxiv.org/abs/2505.14021)
*Soichiro Kumano,Hiroshi Kera,Toshihiko Yamasaki*

Main category: cs.LG

TL;DR: 该论文首次对无数据分布假设的随机深度神经网络中的对抗训练进行了理论分析，提出了基于平均场理论的新框架，并推导了对抗损失的紧上界，揭示了网络结构对对抗训练的影响。


<details>
  <summary>Details</summary>
Motivation: 尽管对抗训练在防御对抗样本方面有效，但其训练动力学尚未被充分理解。本研究旨在填补这一理论空白，特别是在无数据分布假设的情况下。

Method: 作者引入了一个基于平均场理论的新理论框架，克服了现有方法的局限性，并分析了不同范数下的对抗损失上界。

Result: 研究发现，无捷径的网络通常无法进行对抗训练，且对抗训练会降低网络容量；网络宽度可以缓解这些问题。此外，输入和输出维度对上界和权重方差的时间演化有不同影响。

Conclusion: 该研究为对抗训练提供了理论基础，揭示了网络结构设计在对抗训练中的重要性，并提出了网络宽度等关键因素对训练效果的影响。

Abstract: Although adversarial training is known to be effective against adversarial
examples, training dynamics are not well understood. In this study, we present
the first theoretical analysis of adversarial training in random deep neural
networks without any assumptions on data distributions. We introduce a new
theoretical framework based on mean field theory, which addresses the
limitations of existing mean field-based approaches. Based on this framework,
we derive (empirically tight) upper bounds of $\ell_q$ norm-based adversarial
loss with $\ell_p$ norm-based adversarial examples for various values of $p$
and $q$. Moreover, we prove that networks without shortcuts are generally not
adversarially trainable and that adversarial training reduces network capacity.
We also show that network width alleviates these issues. Furthermore, we
present the various impacts of the input and output dimensions on the upper
bounds and time evolution of the weight variance.

</details>


### [281] [FedGraM: Defending Against Untargeted Attacks in Federated Learning via Embedding Gram Matrix](https://arxiv.org/abs/2505.14024)
*Di Wu,Qian Li,Heng Yang,Yong Han*

Main category: cs.LG

TL;DR: 论文提出了一种名为FedGraM的新型鲁棒聚合方法，用于检测和移除联邦学习中的无目标攻击，通过利用辅助数据集和Gram矩阵范数来提升模型安全性。


<details>
  <summary>Details</summary>
Motivation: 联邦学习(FL)虽能保护数据隐私，但易受无目标攻击影响，现有防御方法因数据异构性效果有限。本文旨在通过检测和移除攻击来减轻其影响。

Method: 提出FedGraM方法：服务器维护包含每类一个样本的辅助数据集，计算本地模型嵌入的Gram矩阵范数，过滤范数最大的潜在恶意模型后聚合剩余模型。

Result: 实验表明，FedGraM在有限辅助数据下表现优异，优于现有防御方法。

Conclusion: FedGraM通过嵌入空间类间分离能力评估，有效提升了联邦学习对抗无目标攻击的鲁棒性。

Abstract: Federated Learning (FL) enables geographically distributed clients to
collaboratively train machine learning models by sharing only their local
models, ensuring data privacy. However, FL is vulnerable to untargeted attacks
that aim to degrade the global model's performance on the underlying data
distribution. Existing defense mechanisms attempt to improve FL's resilience
against such attacks, but their effectiveness is limited in practical FL
environments due to data heterogeneity. On the contrary, we aim to detect and
remove the attacks to mitigate their impact. Generalization contribution plays
a crucial role in distinguishing untargeted attacks. Our observations indicate
that, with limited data, the divergence between embeddings representing
different classes provides a better measure of generalization than direct
accuracy. In light of this, we propose a novel robust aggregation method,
FedGraM, designed to defend against untargeted attacks in FL. The server
maintains an auxiliary dataset containing one sample per class to support
aggregation. This dataset is fed to the local models to extract embeddings.
Then, the server calculates the norm of the Gram Matrix of the embeddings for
each local model. The norm serves as an indicator of each model's inter-class
separation capability in the embedding space. FedGraM identifies and removes
potentially malicious models by filtering out those with the largest norms,
then averages the remaining local models to form the global model. We conduct
extensive experiments to evaluate the performance of FedGraM. Our empirical
results show that with limited data samples used to construct the auxiliary
dataset, FedGraM achieves exceptional performance, outperforming
state-of-the-art defense methods.

</details>


### [282] [Partition-wise Graph Filtering: A Unified Perspective Through the Lens of Graph Coarsening](https://arxiv.org/abs/2505.14033)
*Guoming Li,Jian Yang,Yifan Chen*

Main category: cs.LG

TL;DR: 该论文提出了一种名为CPF的新型图神经网络方法，通过结合图级和节点级过滤策略，有效处理同质性和异质性图数据，并在节点分类和图异常检测任务中验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 传统图神经网络采用统一的图级过滤策略，难以处理异质性图数据。虽然节点级过滤提供了更好的适应性，但缺乏统一的理论框架，且可能导致过参数化和过拟合。

Method: 论文提出了CPF方法，通过图粗化和k-means聚类生成节点分区，并分别在结构感知和特征感知的分区上进行过滤，从而结合了图级和节点级过滤的优势。

Result: 实验证明，CPF在节点分类和图异常检测任务中表现优于其他方法，验证了其有效性和实用性。

Conclusion: CPF通过分区级过滤策略，成功统一了图级和节点级过滤，为处理同质性和异质性图数据提供了更优的解决方案。

Abstract: Filtering-based graph neural networks (GNNs) constitute a distinct class of
GNNs that employ graph filters to handle graph-structured data, achieving
notable success in various graph-related tasks. Conventional methods adopt a
graph-wise filtering paradigm, imposing a uniform filter across all nodes, yet
recent findings suggest that this rigid paradigm struggles with heterophilic
graphs. To overcome this, recent works have introduced node-wise filtering,
which assigns distinct filters to individual nodes, offering enhanced
adaptability. However, a fundamental gap remains: a comprehensive framework
unifying these two strategies is still absent, limiting theoretical insights
into the filtering paradigms. Moreover, through the lens of Contextual
Stochastic Block Model, we reveal that a synthesis of graph-wise and node-wise
filtering provides a sufficient solution for classification on graphs
exhibiting both homophily and heterophily, suggesting the risk of excessive
parameterization and potential overfitting with node-wise filtering. To address
the limitations, this paper introduces Coarsening-guided Partition-wise
Filtering (CPF). CPF innovates by performing filtering on node partitions. The
method begins with structure-aware partition-wise filtering, which filters node
partitions obtained via graph coarsening algorithms, and then performs
feature-aware partition-wise filtering, refining node embeddings via filtering
on clusters produced by $k$-means clustering over features. In-depth analysis
is conducted for each phase of CPF, showing its superiority over other
paradigms. Finally, benchmark node classification experiments, along with a
real-world graph anomaly detection application, validate CPF's efficacy and
practical utility.

</details>


### [283] [Adaptive Cyclic Diffusion for Inference Scaling](https://arxiv.org/abs/2505.14036)
*Gyubin Lee,Truong Nhat Nguyen Bao,Jaesik Yoon,Dongwoo Lee,Minsu Kim,Yoshua Bengio,Sungjin Ahn*

Main category: cs.LG

TL;DR: 论文提出了一种自适应推理时间扩展方法ABCD，通过双向扩散循环和动态计算分配提升生成模型的性能与效率。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型在推理时通常采用固定的去噪计划，无法根据实例难度或任务需求动态分配计算资源，限制了性能与效率的平衡。

Method: 提出ABCD框架，包含三个核心组件：循环扩散搜索（双向扩散循环）、自动探索-利用平衡机制、自适应推理时间控制。

Result: 实验表明ABCD在多种任务上均能提升模型性能，同时保持计算效率。

Conclusion: ABCD通过动态调整计算资源的分配，为扩散模型提供了更灵活高效的推理方案。

Abstract: Diffusion models have demonstrated strong generative capabilities across
domains ranging from image synthesis to complex reasoning tasks. However, most
inference-time scaling methods rely on fixed denoising schedules, limiting
their ability to allocate computation based on instance difficulty or
task-specific demands adaptively. We introduce the challenge of adaptive
inference-time scaling-dynamically adjusting computational effort during
inference-and propose Adaptive Bi-directional Cyclic Diffusion (ABCD), a
flexible, search-based inference framework. ABCD refines outputs through
bi-directional diffusion cycles while adaptively controlling exploration depth
and termination. It comprises three components: Cyclic Diffusion Search,
Automatic Exploration-Exploitation Balancing, and Adaptive Thinking Time.
Experiments show that ABCD improves performance across diverse tasks while
maintaining computational efficiency.

</details>


### [284] [Model Steering: Learning with a Reference Model Improves Generalization Bounds and Scaling Laws](https://arxiv.org/abs/2505.06699)
*Xiyuan Wei,Ming Lin,Fanjiang Ye,Fengguang Song,Liangliang Cao,My T. Thai,Tianbao Yang*

Main category: cs.LG

TL;DR: 该论文提出了一种名为模型引导（model steering）的新学习范式，通过使用训练好的模型作为参考来指导和增强目标模型的训练，并提出了基于分布鲁棒优化（DRO）的理论框架DRRho风险最小化，以提升泛化能力和数据效率。


<details>
  <summary>Details</summary>
Motivation: 现有的模型引导方法虽然在不同场景中有所应用，但其原理尚未被充分理解，导致性能不佳。本文旨在通过理论分析，填补这一空白，并提供更优的实践方法。

Method: 论文提出了DRRho风险最小化框架，基于分布鲁棒优化（DRO），并通过理论分析验证其有效性。此外，还提出了一种结合对比学习与DRO的新方法DRRho-CLIP，用于对比语言-图像预训练（CLIP）。

Result: 实验验证了理论分析的准确性，表明DRRho-CLIP在无参考模型的CLIP基础上具有更优的扩展规律，且性能优于现有的启发式方法。

Conclusion: 本文首次为模型引导学习范式提供了理论支持，提出的DRRho框架和方法显著提升了对该范式的理解和实践效果，尤其在CLIP任务中表现突出。

Abstract: This paper formalizes an emerging learning paradigm that uses a trained model
as a reference to guide and enhance the training of a target model through
strategic data selection or weighting, named $\textbf{model steering}$. While
ad-hoc methods have been used in various contexts, including the training of
large foundation models, its underlying principles remain insufficiently
understood, leading to sub-optimal performance. In this work, we propose a
theory-driven framework for model steering called $\textbf{DRRho risk
minimization}$, which is rooted in Distributionally Robust Optimization (DRO).
Through a generalization analysis, we provide theoretical insights into why
this approach improves generalization and data efficiency compared to training
without a reference model. To the best of our knowledge, this is the first time
such theoretical insights are provided for the new learning paradigm, which
significantly enhance our understanding and practice of model steering.
Building on these insights and the connection between contrastive learning and
DRO, we introduce a novel method for Contrastive Language-Image Pretraining
(CLIP) with a reference model, termed DRRho-CLIP. Extensive experiments
validate the theoretical insights, reveal a superior scaling law compared to
CLIP without a reference model, and demonstrate its strength over existing
heuristic approaches.

</details>


### [285] [Learning High-dimensional Ionic Model Dynamics Using Fourier Neural Operators](https://arxiv.org/abs/2505.14039)
*Luca Pellegrini,Massimiliano Ghiotto,Edoardo Centofanti,Luca Franco Pavarino*

Main category: cs.LG

TL;DR: 该研究探讨了傅里叶神经算子（FNO）在高维离子模型中学习所有状态变量动态的有效性，通过自动超参数调优在无约束和约束条件下验证了其准确性。


<details>
  <summary>Details</summary>
Motivation: 离子模型因其刚性和多尺度非线性，传统神经网络难以准确模拟其复杂动态。研究旨在验证FNO能否在高维系统中有效学习所有状态变量的演化。

Method: 使用FNO学习三个不同维度的离子模型动态，并通过自动超参数调优在无约束和约束条件下优化网络配置。

Result: FNO在所有模型中均能准确捕捉复杂动态，无约束架构训练效率更高，仅需约一半训练周期即可达到相近误差水平。

Conclusion: FNO能够有效处理高维动力系统中的多尺度非线性动态，为计算神经科学和心脏病学中的复杂模拟提供了新工具。

Abstract: Ionic models, described by systems of stiff ordinary differential equations,
are fundamental tools for simulating the complex dynamics of excitable cells in
both Computational Neuroscience and Cardiology. Approximating these models
using Artificial Neural Networks poses significant challenges due to their
inherent stiffness, multiscale nonlinearities, and the wide range of dynamical
behaviors they exhibit, including multiple equilibrium points, limit cycles,
and intricate interactions. While in previous studies the dynamics of the
transmembrane potential has been predicted in low dimensionality settings, in
the present study we extend these results by investigating whether Fourier
Neural Operators can effectively learn the evolution of all the state variables
within these dynamical systems in higher dimensions. We demonstrate the
effectiveness of this approach by accurately learning the dynamics of three
well-established ionic models with increasing dimensionality: the two-variable
FitzHugh-Nagumo model, the four-variable Hodgkin-Huxley model, and the
forty-one-variable O'Hara-Rudy model. To ensure the selection of near-optimal
configurations for the Fourier Neural Operator, we conducted automatic
hyperparameter tuning under two scenarios: an unconstrained setting, where the
number of trainable parameters is not limited, and a constrained case with a
fixed number of trainable parameters. Both constrained and unconstrained
architectures achieve comparable results in terms of accuracy across all the
models considered. However, the unconstrained architecture required
approximately half the number of training epochs to achieve similar error
levels, as evidenced by the loss function values recorded during training.
These results underline the capabilities of Fourier Neural Operators to
accurately capture complex multiscale dynamics, even in high-dimensional
dynamical systems.

</details>


### [286] [Unsupervised Graph Clustering with Deep Structural Entropy](https://arxiv.org/abs/2505.14040)
*Jingyun Zhang,Hao Peng,Li Sun,Guanlin Wu,Chunyang Liu,Zhengtao Yu*

Main category: cs.LG

TL;DR: 论文提出了一种名为DeSE的无监督图聚类框架，通过深度结构熵增强原始图结构，解决了现有方法在稀疏或噪声图上的性能下降问题。


<details>
  <summary>Details</summary>
Motivation: 当前基于图的聚类方法（如GNNs、GATs和对比学习）过度依赖原始图结构，当图过于稀疏或包含噪声边时性能下降。此外，这些方法依赖传统聚类技术（如k-means），可能无法充分捕捉节点间的潜在结构。

Method: DeSE框架包含三个关键部分：1) 提出可微分软分配结构熵计算方法；2) 设计结构学习层（SLL）从原始特征生成属性图以优化原始结构；3) 基于GNN的聚类分配层（ASS）学习节点嵌入和软分配矩阵，在增强图上进行聚类。

Result: 在四个基准数据集上与八种无监督图聚类基线方法对比，DeSE在效果和可解释性上均表现出优越性。

Conclusion: DeSE通过深度结构熵有效增强了原始图结构，解决了稀疏和噪声问题，为无监督图聚类提供了更稳定和可解释的解决方案。

Abstract: Research on Graph Structure Learning (GSL) provides key insights for
graph-based clustering, yet current methods like Graph Neural Networks (GNNs),
Graph Attention Networks (GATs), and contrastive learning often rely heavily on
the original graph structure. Their performance deteriorates when the original
graph's adjacency matrix is too sparse or contains noisy edges unrelated to
clustering. Moreover, these methods depend on learning node embeddings and
using traditional techniques like k-means to form clusters, which may not fully
capture the underlying graph structure between nodes. To address these
limitations, this paper introduces DeSE, a novel unsupervised graph clustering
framework incorporating Deep Structural Entropy. It enhances the original graph
with quantified structural information and deep neural networks to form
clusters. Specifically, we first propose a method for calculating structural
entropy with soft assignment, which quantifies structure in a differentiable
form. Next, we design a Structural Learning layer (SLL) to generate an
attributed graph from the original feature data, serving as a target to enhance
and optimize the original structural graph, thereby mitigating the issue of
sparse connections between graph nodes. Finally, our clustering assignment
method (ASS), based on GNNs, learns node embeddings and a soft assignment
matrix to cluster on the enhanced graph. The ASS layer can be stacked to meet
downstream task requirements, minimizing structural entropy for stable
clustering and maximizing node consistency with edge-based cross-entropy loss.
Extensive comparative experiments are conducted on four benchmark datasets
against eight representative unsupervised graph clustering baselines,
demonstrating the superiority of the DeSE in both effectiveness and
interpretability.

</details>


### [287] [Adversarially Pretrained Transformers may be Universally Robust In-Context Learners](https://arxiv.org/abs/2505.14042)
*Soichiro Kumano,Hiroshi Kera,Toshihiko Yamasaki*

Main category: cs.LG

TL;DR: 该研究提出通过对抗性预训练的Transformer模型作为基础模型，利用上下文学习在下游任务中实现鲁棒性，无需额外对抗训练。


<details>
  <summary>Details</summary>
Motivation: 对抗性训练虽有效但计算成本高，研究旨在探索是否可以通过预训练模型避免下游任务的对抗训练。

Method: 使用对抗性预训练的Transformer模型，通过上下文学习推广到未见任务，无需参数更新。

Result: 模型能鲁棒泛化到多个任务，但存在准确性-鲁棒性权衡，且需要大量上下文演示。

Conclusion: 对抗性预训练Transformer可作为鲁棒基础模型，但需注意其局限性和条件限制。

Abstract: Adversarial training is one of the most effective adversarial defenses, but
it incurs a high computational cost. In this study, we show that transformers
adversarially pretrained on diverse tasks can serve as robust foundation models
and eliminate the need for adversarial training in downstream tasks.
Specifically, we theoretically demonstrate that through in-context learning, a
single adversarially pretrained transformer can robustly generalize to multiple
unseen tasks without any additional training, i.e., without any parameter
updates. This robustness stems from the model's focus on robust features and
its resistance to attacks that exploit non-predictive features. Besides these
positive findings, we also identify several limitations. Under certain
conditions (though unrealistic), no universally robust single-layer
transformers exist. Moreover, robust transformers exhibit an
accuracy--robustness trade-off and require a large number of in-context
demonstrations. The code is available at
https://github.com/s-kumano/universally-robust-in-context-learner.

</details>


### [288] [Generalized Category Discovery via Token Manifold Capacity Learning](https://arxiv.org/abs/2505.14044)
*Luyao Tang,Kunze Huang,Chaoqi Chen,Cheng Chen*

Main category: cs.LG

TL;DR: 本文提出了一种名为MTMC的新方法，通过最大化类别标记的流形容量来提升广义类别发现（GCD）的性能，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统GCD方法注重最小化簇内差异，但牺牲了流形容量，限制了类内表示的丰富性。

Method: MTMC利用奇异值的核范数作为流形容量的度量，确保样本表示的信息性和结构性。

Result: 实验表明，MTMC在粗粒度和细粒度数据集上均优于现有GCD方法，提升了聚类准确性和类别数量估计。

Conclusion: MTMC增强了类间可分性，减少了维度塌缩，是开放世界学习中鲁棒性的关键组件。

Abstract: Generalized category discovery (GCD) is essential for improving deep learning
models' robustness in open-world scenarios by clustering unlabeled data
containing both known and novel categories. Traditional GCD methods focus on
minimizing intra-cluster variations, often sacrificing manifold capacity, which
limits the richness of intra-class representations. In this paper, we propose a
novel approach, Maximum Token Manifold Capacity (MTMC), that prioritizes
maximizing the manifold capacity of class tokens to preserve the diversity and
complexity of data. MTMC leverages the nuclear norm of singular values as a
measure of manifold capacity, ensuring that the representation of samples
remains informative and well-structured. This method enhances the
discriminability of clusters, allowing the model to capture detailed semantic
features and avoid the loss of critical information during clustering. Through
theoretical analysis and extensive experiments on coarse- and fine-grained
datasets, we demonstrate that MTMC outperforms existing GCD methods, improving
both clustering accuracy and the estimation of category numbers. The
integration of MTMC leads to more complete representations, better inter-class
separability, and a reduction in dimensional collapse, establishing MTMC as a
vital component for robust open-world learning. Code is in
github.com/lytang63/MTMC.

</details>


### [289] [Textual Steering Vectors Can Improve Visual Understanding in Multimodal Large Language Models](https://arxiv.org/abs/2505.14071)
*Woody Haosheng Gan,Deqing Fu,Julian Asilis,Ollie Liu,Dani Yogatama,Vatsal Sharan,Robin Jia,Willie Neiswanger*

Main category: cs.LG

TL;DR: 该论文研究了如何利用文本导向向量提升多模态大语言模型（MLLMs）的性能，无需修改模型参数，取得了显著效果。


<details>
  <summary>Details</summary>
Motivation: 目前，大型语言模型（LLMs）已有有效的导向方法，但多模态大语言模型（MLLMs）由于出现较晚和架构多样性，缺乏类似技术。论文旨在填补这一空白。

Method: 通过稀疏自编码器（SAEs）、均值漂移（mean shift）和线性探测（linear probing），从文本LLM骨干中提取向量，用于导向MLLMs。

Result: 文本导向显著提升了MLLMs在多模态任务中的准确性，均值漂移在CV-Bench上空间关系准确性提升7.3%，计数准确性提升3.3%，优于提示方法，且能泛化到分布外数据。

Conclusion: 文本导向向量是一种高效、低成本的机制，能显著增强MLLMs的接地性，无需大量额外数据收集和计算开销。

Abstract: Steering methods have emerged as effective and targeted tools for guiding
large language models' (LLMs) behavior without modifying their parameters.
Multimodal large language models (MLLMs), however, do not currently enjoy the
same suite of techniques, due in part to their recency and architectural
diversity. Inspired by this gap, we investigate whether MLLMs can be steered
using vectors derived from their text-only LLM backbone, via sparse
autoencoders (SAEs), mean shift, and linear probing. We find that text-derived
steering consistently enhances multimodal accuracy across diverse MLLM
architectures and visual tasks. In particular, mean shift boosts spatial
relationship accuracy on CV-Bench by up to +7.3% and counting accuracy by up to
+3.3%, outperforming prompting and exhibiting strong generalization to
out-of-distribution datasets. These results highlight textual steering vectors
as a powerful, efficient mechanism for enhancing grounding in MLLMs with
minimal additional data collection and computational overhead.

</details>


### [290] [Collaborative Unlabeled Data Optimization](https://arxiv.org/abs/2505.14117)
*Xinyi Shang,Peng Sun,Fengyuan Liu,Tao Lin*

Main category: cs.LG

TL;DR: 提出了一种名为CoOpt的新型数据优化框架，通过将知识编码到数据本身，提高深度学习训练的效率和可持续性。


<details>
  <summary>Details</summary>
Motivation: 现有模型中心化方法存在知识锁定在模型参数中的问题，限制了知识的可重用性和扩展性。

Method: CoOpt是一个高效并行化的框架，通过分布式处理未标记数据并利用公开的任务无关模型，实现可扩展、可重用的训练流程。

Result: 在Tiny-ImageNet和ImageNet-1K数据集上分别实现了13.6%和6.8%的性能提升，训练速度分别提高了1.94倍和1.2倍。

Conclusion: CoOpt通过优化数据本身，显著提升了深度学习训练的效率和可持续性。

Abstract: This paper pioneers a novel data-centric paradigm to maximize the utility of
unlabeled data, tackling a critical question: How can we enhance the efficiency
and sustainability of deep learning training by optimizing the data itself? We
begin by identifying three key limitations in existing model-centric
approaches, all rooted in a shared bottleneck: knowledge extracted from data is
locked to model parameters, hindering its reusability and scalability. To this
end, we propose CoOpt, a highly efficient, parallelized framework for
collaborative unlabeled data optimization, thereby effectively encoding
knowledge into the data itself. By distributing unlabeled data and leveraging
publicly available task-agnostic models, CoOpt facilitates scalable, reusable,
and sustainable training pipelines. Extensive experiments across diverse
datasets and architectures demonstrate its efficacy and efficiency, achieving
13.6% and 6.8% improvements on Tiny-ImageNet and ImageNet-1K, respectively,
with training speedups of $1.94 \times $ and $1.2 \times$.

</details>


### [291] [Assessing wildfire susceptibility in Iran: Leveraging machine learning for geospatial analysis of climatic and anthropogenic factors](https://arxiv.org/abs/2505.14122)
*Ehsan Masoudian,Ali Mirzaei,Hossein Bagheri*

Main category: cs.LG

TL;DR: 该研究利用遥感和机器学习技术，分析了气候与人为因素对伊朗野火风险的影响，发现人为因素在季节性分析中影响更大，并绘制了高风险区域地图。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探讨气候条件和人类活动如何共同影响伊朗的野火风险，以期为火灾管理提供科学依据。

Method: 采用遥感、GIS处理技术（如云计算）和机器学习算法，结合多种数据采样策略，分析气候、地形和人为因素。

Result: 研究发现土壤湿度、温度和湿度等气候因素显著影响野火风险，而人口密度和电力线距离等人为因素在季节性分析中影响更大。高风险区域主要集中在扎格罗斯中部、希尔卡尼亚森林东北部和阿拉斯巴兰森林北部。

Conclusion: 研究通过高分辨率野火风险地图，揭示了伊朗野火动态的新见解，强调了制定有效火灾管理策略的紧迫性。

Abstract: This study investigates the multifaceted factors influencing wildfire risk in
Iran, focusing on the interplay between climatic conditions and human
activities. Utilizing advanced remote sensing, geospatial information system
(GIS) processing techniques such as cloud computing, and machine learning
algorithms, this research analyzed the impact of climatic parameters,
topographic features, and human-related factors on wildfire susceptibility
assessment and prediction in Iran. Multiple scenarios were developed for this
purpose based on the data sampling strategy. The findings revealed that
climatic elements such as soil moisture, temperature, and humidity
significantly contribute to wildfire susceptibility, while human
activities-particularly population density and proximity to powerlines-also
played a crucial role. Furthermore, the seasonal impact of each parameter was
separately assessed during warm and cold seasons. The results indicated that
human-related factors, rather than climatic variables, had a more prominent
influence during the seasonal analyses. This research provided new insights
into wildfire dynamics in Iran by generating high-resolution wildfire
susceptibility maps using advanced machine learning classifiers. The generated
maps identified high risk areas, particularly in the central Zagros region, the
northeastern Hyrcanian Forest, and the northern Arasbaran forest, highlighting
the urgent need for effective fire management strategies.

</details>


### [292] [Contrastive Consolidation of Top-Down Modulations Achieves Sparsely Supervised Continual Learning](https://arxiv.org/abs/2505.14125)
*Viet Anh Khoa Tran,Emre Neftci,Willem. A. M. Wybo*

Main category: cs.LG

TL;DR: 论文提出任务调制对比学习（TMCL），受大脑新皮层生物物理机制启发，通过预测编码原理无监督地整合自上而下信息，解决机器学习中的灾难性遗忘问题，并在仅使用1%标签的情况下超越现有无监督和监督方法。


<details>
  <summary>Details</summary>
Motivation: 生物大脑能持续从无标注数据流中学习，并整合稀疏标注的专有信息而不损害泛化能力。而机器学习方法在此自然学习场景下易受灾难性遗忘影响，监督微调会降低原任务性能。

Method: 引入任务调制对比学习（TMCL），利用预测编码原理构建视图不变表示空间（通过对比损失实现）。当新类别标注样本出现时，学习新的仿射调制以提升类别分离度，同时保持前馈权重不变。通过调制不变性训练和复用历史调制稳定表示空间。

Result: 实验表明，TMCL在类增量学习和迁移学习中均优于最先进的无监督方法及可比监督方法，仅需1%的标注数据。

Conclusion: 自上而下调制在平衡稳定性和可塑性中起关键作用，TMCL为持续学习提供了生物启发的有效解决方案。

Abstract: Biological brains learn continually from a stream of unlabeled data, while
integrating specialized information from sparsely labeled examples without
compromising their ability to generalize. Meanwhile, machine learning methods
are susceptible to catastrophic forgetting in this natural learning setting, as
supervised specialist fine-tuning degrades performance on the original task. We
introduce task-modulated contrastive learning (TMCL), which takes inspiration
from the biophysical machinery in the neocortex, using predictive coding
principles to integrate top-down information continually and without
supervision. We follow the idea that these principles build a view-invariant
representation space, and that this can be implemented using a contrastive
loss. Then, whenever labeled samples of a new class occur, new affine
modulations are learned that improve separation of the new class from all
others, without affecting feedforward weights. By co-opting the view-invariance
learning mechanism, we then train feedforward weights to match the unmodulated
representation of a data sample to its modulated counterparts. This introduces
modulation invariance into the representation space, and, by also using past
modulations, stabilizes it. Our experiments show improvements in both
class-incremental and transfer learning over state-of-the-art unsupervised
approaches, as well as over comparable supervised approaches, using as few as
1% of available labels. Taken together, our work suggests that top-down
modulations play a crucial role in balancing stability and plasticity.

</details>


### [293] [MAS-KCL: Knowledge component graph structure learning with large language model-based agentic workflow](https://arxiv.org/abs/2505.14126)
*Yuan-Hao Jiang,Kezong Tang,Zi-Wei Chen,Yuang Wei,Tian-Yi Liu,Jiayi Wu*

Main category: cs.LG

TL;DR: 提出了一种基于多智能体系统的知识组件图结构学习算法MAS-KCL，通过双向反馈机制优化知识组件图，提升学习路径识别效果。


<details>
  <summary>Details</summary>
Motivation: 准确的知识组件图能帮助教育者定位学习者表现不佳的根本原因，从而进行针对性教学干预。

Method: 开发了MAS-KCL算法，利用多智能体系统和大型语言模型自适应优化知识组件图，并集成双向反馈机制调整边生成概率分布。

Result: 在5个合成数据集和4个真实教育数据集上的实验验证了算法在学习路径识别中的有效性。

Conclusion: 该算法能准确识别学习路径，帮助教师设计更全面的学习计划，促进教育可持续发展。

Abstract: Knowledge components (KCs) are the fundamental units of knowledge in the
field of education. A KC graph illustrates the relationships and dependencies
between KCs. An accurate KC graph can assist educators in identifying the root
causes of learners' poor performance on specific KCs, thereby enabling targeted
instructional interventions. To achieve this, we have developed a KC graph
structure learning algorithm, named MAS-KCL, which employs a multi-agent system
driven by large language models for adaptive modification and optimization of
the KC graph. Additionally, a bidirectional feedback mechanism is integrated
into the algorithm, where AI agents leverage this mechanism to assess the value
of edges within the KC graph and adjust the distribution of generation
probabilities for different edges, thereby accelerating the efficiency of
structure learning. We applied the proposed algorithm to 5 synthetic datasets
and 4 real-world educational datasets, and experimental results validate its
effectiveness in learning path recognition. By accurately identifying learners'
learning paths, teachers are able to design more comprehensive learning plans,
enabling learners to achieve their educational goals more effectively, thus
promoting the sustainable development of education.

</details>


### [294] [A Methodological Framework for Measuring Spatial Labeling Similarity](https://arxiv.org/abs/2505.14128)
*Yihang Du,Jiaying Hu,Suyang Hou,Yueyang Ding,Xiaobo Sun*

Main category: cs.LG

TL;DR: 提出了一种名为SLAM的框架，用于全面评估空间标记的相似性，考虑了标签匹配、拓扑结构和异质性影响，并在空间转录组学中验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在评估空间标记相似性时，往往无法同时考虑标签匹配、空间拓扑结构和异质性影响，导致评估不够全面和准确。

Method: 通过将空间标记转换为图结构，提取其属性分布，并计算分布差异来量化标记间的相似性，具体实现了名为SLAM的评估方法。

Result: 在模拟和真实空间转录组学数据上的实验表明，SLAM相比现有方法能更全面、准确地反映标记质量。

Conclusion: SLAM为空间标记相似性评估提供了一个综合且准确的框架，适用于科学研究和实际应用。

Abstract: Spatial labeling assigns labels to specific spatial locations to characterize
their spatial properties and relationships, with broad applications in
scientific research and practice. Measuring the similarity between two spatial
labelings is essential for understanding their differences and the contributing
factors, such as changes in location properties or labeling methods. An
adequate and unbiased measurement of spatial labeling similarity should
consider the number of matched labels (label agreement), the topology of
spatial label distribution, and the heterogeneous impacts of mismatched labels.
However, existing methods often fail to account for all these aspects. To
address this gap, we propose a methodological framework to guide the
development of methods that meet these requirements. Given two spatial
labelings, the framework transforms them into graphs based on location
organization, labels, and attributes (e.g., location significance). The
distributions of their graph attributes are then extracted, enabling an
efficient computation of distributional discrepancy to reflect the
dissimilarity level between the two labelings. We further provide a concrete
implementation of this framework, termed Spatial Labeling Analogy Metric
(SLAM), along with an analysis of its theoretical foundation, for evaluating
spatial labeling results in spatial transcriptomics (ST) \textit{as per} their
similarity with ground truth labeling. Through a series of carefully designed
experimental cases involving both simulated and real ST data, we demonstrate
that SLAM provides a comprehensive and accurate reflection of labeling quality
compared to other well-established evaluation metrics. Our code is available at
https://github.com/YihDu/SLAM.

</details>


### [295] [Local Mixtures of Experts: Essentially Free Test-Time Training via Model Merging](https://arxiv.org/abs/2505.14136)
*Ryo Bertolissi,Jonas Hübotter,Ido Hakimi,Andreas Krause*

Main category: cs.LG

TL;DR: 论文提出了一种名为TTMM的新方法，通过模型合并技术扩展MoE模型中的专家数量，显著降低推理成本，同时性能接近昂贵的测试时训练方法。


<details>
  <summary>Details</summary>
Motivation: 当前MoE模型因训练和推理成本过高而只能使用少量专家，限制了模型容量的提升。测试时训练(TTT)虽能显著提升语言模型性能，但计算代价昂贵。

Method: 提出测试时模型合并(TTMM)，通过模型合并技术将MoE模型的专家数量扩展一个数量级，并避免几乎所有测试时开销。TTMM近似实现了TTT的效果。

Result: TTMM性能随专家数量增加而提升，接近TTT水平。使用1B参数基础模型时，TTMM的测试速度比TTT快100倍以上。

Conclusion: TTMM为扩展测试时训练提供了一种高性价比的解决方案，在保持性能的同时大幅降低了计算成本。

Abstract: Mixture of expert (MoE) models are a promising approach to increasing model
capacity without increasing inference cost, and are core components of many
state-of-the-art language models. However, current MoE models typically use
only few experts due to prohibitive training and inference cost. We propose
Test-Time Model Merging (TTMM) which scales the MoE paradigm to an order of
magnitude more experts and uses model merging to avoid almost any test-time
overhead. We show that TTMM is an approximation of test-time training (TTT),
which fine-tunes an expert model for each prediction task, i.e., prompt. TTT
has recently been shown to significantly improve language models, but is
computationally expensive. We find that performance of TTMM improves with more
experts and approaches the performance of TTT. Moreover, we find that with a 1B
parameter base model, TTMM is more than 100x faster than TTT at test-time by
amortizing the cost of TTT at train-time. Thus, TTMM offers a promising
cost-effective approach to scale test-time training.

</details>


### [296] [FlowQ: Energy-Guided Flow Policies for Offline Reinforcement Learning](https://arxiv.org/abs/2505.14139)
*Marvin Alles,Nutan Chen,Patrick van der Smagt,Botond Cseke*

Main category: cs.LG

TL;DR: 提出了一种名为能量引导流匹配的新方法，用于增强流模型的训练，并在推理时无需引导，应用于离线强化学习算法FlowQ，取得了竞争性性能。


<details>
  <summary>Details</summary>
Motivation: 当前在扩散模型中，引导采样已被广泛探索，但在训练过程中引入引导的研究相对较少。本文旨在通过能量引导流匹配方法，优化流模型的训练过程，并消除推理时的引导需求。

Method: 提出能量引导流匹配方法，通过近似能量引导概率路径为高斯路径，学习条件速度场。应用于离线强化学习算法FlowQ，优化策略训练时间。

Result: FlowQ算法在性能上具有竞争力，且策略训练时间在流采样步骤中保持恒定。

Conclusion: 能量引导流匹配方法有效提升了流模型的训练效率，并在离线强化学习中展现出良好的应用潜力。

Abstract: The use of guidance to steer sampling toward desired outcomes has been widely
explored within diffusion models, especially in applications such as image and
trajectory generation. However, incorporating guidance during training remains
relatively underexplored. In this work, we introduce energy-guided flow
matching, a novel approach that enhances the training of flow models and
eliminates the need for guidance at inference time. We learn a conditional
velocity field corresponding to the flow policy by approximating an
energy-guided probability path as a Gaussian path. Learning guided trajectories
is appealing for tasks where the target distribution is defined by a
combination of data and an energy function, as in reinforcement learning.
Diffusion-based policies have recently attracted attention for their expressive
power and ability to capture multi-modal action distributions. Typically, these
policies are optimized using weighted objectives or by back-propagating
gradients through actions sampled by the policy. As an alternative, we propose
FlowQ, an offline reinforcement learning algorithm based on energy-guided flow
matching. Our method achieves competitive performance while the policy training
time is constant in the number of flow sampling steps.

</details>


### [297] [Personalized Bayesian Federated Learning with Wasserstein Barycenter Aggregation](https://arxiv.org/abs/2505.14161)
*Ting Wei,Biao Mei,Junliang Lyu,Renquan Zhang,Feng Zhou,Yifan Sun*

Main category: cs.LG

TL;DR: FedWBA提出了一种新的个性化贝叶斯联邦学习方法，通过非参数后验表示和Wasserstein重心聚合，提升了预测准确性、不确定性校准和收敛速度。


<details>
  <summary>Details</summary>
Motivation: 现有PBFL方法在客户端后验推断中存在参数假设限制，且在服务器聚合时采用简单参数平均，FedWBA旨在解决这些问题。

Method: 客户端采用基于粒子的变分推断进行非参数后验表示，服务器端引入基于粒子的Wasserstein重心聚合方法。

Result: 理论证明FedWBA具有局部和全局收敛保证，实验显示其在预测准确性、不确定性校准和收敛速度上优于基线方法。

Conclusion: FedWBA通过改进局部推断和全局聚合，显著提升了PBFL的性能和鲁棒性。

Abstract: Personalized Bayesian federated learning (PBFL) handles non-i.i.d. client
data and quantifies uncertainty by combining personalization with Bayesian
inference. However, existing PBFL methods face two limitations: restrictive
parametric assumptions in client posterior inference and naive parameter
averaging for server aggregation. To overcome these issues, we propose FedWBA,
a novel PBFL method that enhances both local inference and global aggregation.
At the client level, we use particle-based variational inference for
nonparametric posterior representation. At the server level, we introduce
particle-based Wasserstein barycenter aggregation, offering a more
geometrically meaningful approach. Theoretically, we provide local and global
convergence guarantees for FedWBA. Locally, we prove a KL divergence decrease
lower bound per iteration for variational inference convergence. Globally, we
show that the Wasserstein barycenter converges to the true parameter as the
client data size increases. Empirically, experiments show that FedWBA
outperforms baselines in prediction accuracy, uncertainty calibration, and
convergence rate, with ablation studies confirming its robustness.

</details>


### [298] [Nonparametric Teaching for Graph Property Learners](https://arxiv.org/abs/2505.14170)
*Chen Zhang,Weixin Bu,Zeyi Ren,Zhengwu Liu,Yik-Chung Wu,Ngai Wong*

Main category: cs.LG

TL;DR: 论文提出了一种名为GraNT的新范式，通过非参数教学视角重新解释图结构数据的学习过程，显著提升了图卷积网络（GCNs）的训练效率。


<details>
  <summary>Details</summary>
Motivation: 图结构数据（如分子溶解度）的属性推断通常需要学习从图到其属性的隐式映射，这一过程对图卷积网络等学习器来说成本高昂。为了解决这一问题，作者提出了GraNT方法。

Method: GraNT通过非参数教学框架，选择图-属性对的子集来促进GCN训练的更快收敛。该方法分析了图结构对基于参数的梯度下降的影响，并将GCN的参数更新重新解释为非参数教学中的功能梯度下降。

Result: 实验结果显示，GraNT在图级回归、图级分类、节点级回归和节点级分类任务中分别减少了36.62%、38.19%、30.97%和47.30%的训练时间，同时保持了泛化性能。

Conclusion: GraNT首次证明了教学图属性学习器（如GCNs）与教学结构感知的非参数学习器是一致的，显著提升了学习效率。

Abstract: Inferring properties of graph-structured data, e.g., the solubility of
molecules, essentially involves learning the implicit mapping from graphs to
their properties. This learning process is often costly for graph property
learners like Graph Convolutional Networks (GCNs). To address this, we propose
a paradigm called Graph Neural Teaching (GraNT) that reinterprets the learning
process through a novel nonparametric teaching perspective. Specifically, the
latter offers a theoretical framework for teaching implicitly defined (i.e.,
nonparametric) mappings via example selection. Such an implicit mapping is
realized by a dense set of graph-property pairs, with the GraNT teacher
selecting a subset of them to promote faster convergence in GCN training. By
analytically examining the impact of graph structure on parameter-based
gradient descent during training, and recasting the evolution of GCNs--shaped
by parameter updates--through functional gradient descent in nonparametric
teaching, we show for the first time that teaching graph property learners
(i.e., GCNs) is consistent with teaching structure-aware nonparametric
learners. These new findings readily commit GraNT to enhancing learning
efficiency of the graph property learner, showing significant reductions in
training time for graph-level regression (-36.62%), graph-level classification
(-38.19%), node-level regression (-30.97%) and node-level classification
(-47.30%), all while maintaining its generalization performance.

</details>


### [299] [Safety Subspaces are Not Distinct: A Fine-Tuning Case Study](https://arxiv.org/abs/2505.14185)
*Kaustubh Ponkshe,Shaan Shah,Raghav Singhal,Praneeth Vepakomma*

Main category: cs.LG

TL;DR: 研究发现大语言模型的安全对齐行为并非集中在特定子空间，而是与模型整体学习动态紧密交织，基于子空间的防御方法可能面临根本性限制。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型通过安全对齐生成社会可接受的响应，但这种对齐易受后续微调破坏。研究旨在验证安全行为是否集中在特定几何子空间，以探索更鲁棒的对齐方法。

Method: 通过参数空间和激活空间的综合实证研究，分析五个开源LLM中安全相关行为与子空间的关系，并测试不同安全提示的表征重叠性。

Result: 安全与不安全行为共享相同的放大子空间，不同安全属性的提示激活重叠表征，未发现选择性控制安全的独立子空间。

Conclusion: 安全对齐具有非局部化特性，需开发不依赖子空间隔离的替代策略来维持持续训练中的模型安全性。

Abstract: Large Language Models (LLMs) rely on safety alignment to produce socially
acceptable responses. This is typically achieved through instruction tuning and
reinforcement learning from human feedback. However, this alignment is known to
be brittle: further fine-tuning, even on benign or lightly contaminated data,
can degrade safety and reintroduce harmful behaviors. A growing body of work
suggests that alignment may correspond to identifiable geometric directions in
weight space, forming subspaces that could, in principle, be isolated or
preserved to defend against misalignment. In this work, we conduct a
comprehensive empirical study of this geometric perspective. We examine whether
safety-relevant behavior is concentrated in specific subspaces, whether it can
be separated from general-purpose learning, and whether harmfulness arises from
distinguishable patterns in internal representations. Across both parameter and
activation space, our findings are consistent: subspaces that amplify safe
behaviors also amplify unsafe ones, and prompts with different safety
implications activate overlapping representations. We find no evidence of a
subspace that selectively governs safety. These results challenge the
assumption that alignment is geometrically localized. Rather than residing in
distinct directions, safety appears to emerge from entangled, high-impact
components of the model's broader learning dynamics. This suggests that
subspace-based defenses may face fundamental limitations and underscores the
need for alternative strategies to preserve alignment under continued training.
We corroborate these findings through multiple experiments on five open-source
LLMs. Our code is publicly available at:
https://github.com/CERT-Lab/safety-subspaces.

</details>


### [300] [$α$-GAN by Rényi Cross Entropy](https://arxiv.org/abs/2505.14190)
*Ni Ding,Miao Qiao,Jiaxing Xu,Yiping Ke,Xiaoyu Zhang*

Main category: cs.LG

TL;DR: 该论文提出了一种基于Rényi度量的生成对抗网络α-GAN，通过Rényi交叉熵构建价值函数，优化生成器和判别器的性能。


<details>
  <summary>Details</summary>
Motivation: 研究动机是利用Rényi度量改进传统GAN，解决如梯度消失等常见问题，并探索α参数在(0,1)范围内的潜力。

Method: 方法是通过Rényi交叉熵构建价值函数，形成min-max优化问题，参数化Rényi阶数α，优化概率空间。

Result: 实验结果表明，当α∈(0,1)时，梯度指数级放大，收敛速度加快，验证了方法的有效性。

Conclusion: 结论是α-GAN在α=1时退化为传统GAN，而α∈(0,1)的范围可能解决一些常见问题，且该范围在现有研究中尚未充分探索。

Abstract: This paper proposes $\alpha$-GAN, a generative adversarial network using
R\'{e}nyi measures. The value function is formulated, by R\'{e}nyi cross
entropy, as an expected certainty measure incurred by the discriminator's soft
decision as to where the sample is from, true population or the generator. The
discriminator tries to maximize the R\'{e}nyi certainty about sample source,
while the generator wants to reduce it by injecting fake samples. This forms a
min-max problem with the solution parameterized by the R\'{e}nyi order
$\alpha$. This $\alpha$-GAN reduces to vanilla GAN at $\alpha = 1$, where the
value function is exactly the binary cross entropy. The optimization of
$\alpha$-GAN is over probability (vector) space. It is shown that the gradient
is exponentially enlarged when R\'{e}nyi order is in the range $\alpha \in
(0,1)$. This makes convergence faster, which is verified by experimental
results. A discussion shows that choosing $\alpha \in (0,1)$ may be able to
solve some common problems, e.g., vanishing gradient. A following observation
reveals that this range has not been fully explored in the existing R\'{e}nyi
version GANs.

</details>


### [301] [FLASH-D: FlashAttention with Hidden Softmax Division](https://arxiv.org/abs/2505.14201)
*Kosmas Alexandridis,Vasileios Titopoulos,Giorgos Dimitrakopoulos*

Main category: cs.LG

TL;DR: FLASH-D简化了FlashAttention核心计算，通过数学等效重构实现了硬件效率提升，面积减少22.8%，功耗降低20.3%。


<details>
  <summary>Details</summary>
Motivation: 传统Transformer注意力机制中的softmax计算存在效率瓶颈，FlashAttention虽优化了GPU计算，但仍有进一步简化与硬件加速空间。

Method: 提出FLASH-D，通过隐藏softmax除法、稳定计算指数函数、减少计算成本，保持原有分块计算特性，无需数值近似。

Result: 在28nm工艺下，相比现有并行硬件架构，面积平均减少22.8%，功耗降低20.3%，且无性能损失。

Conclusion: FLASH-D通过数学重构显著提升硬件效率，为注意力机制的高效硬件实现提供了新方向。

Abstract: The transformer's attention mechanism has revolutionized AI and machine
learning, with its efficient computation being crucial to its performance.
However, calculating attention involves matrix operations interspersed with
softmax rescaling, which inherently slows down computation and requires
processing the entire input sequence. Building on online softmax computation,
FlashAttention integrates softmax calculation with matrix arithmetic, enabling
tiled computation independent of sequence length. While optimized for GPUs,
FlashAttention's simplicity makes it amenable to direct hardware acceleration.
This work re-evaluates the core FlashAttention kernel, presenting FLASH-D a
mathematically equivalent, yet simplified, formulation that achieves: (a)
hiding softmax division within other non-linear function evaluations; (b)
inherently numerically stable computation of exponentials, eliminating the need
for maximum value subtraction; and (c) a reduction in computational cost
without introducing numerical approximations to the FlashAttention kernel.
Importantly, the essential FlashAttention properties that facilitate efficient
tiled implementation are fully preserved. Hardware implementation results at
28nm demonstrate that this proposed formulation achieves a 22.8% reduction in
area and a 20.3% reduction in power, on average, compared to state-of-the-art
parallel hardware architectures without any performance penalty.

</details>


### [302] [MSDformer: Multi-scale Discrete Transformer For Time Series Generation](https://arxiv.org/abs/2505.14202)
*Zhicheng Chen,Shibo Feng,Xi Xiao,Zhong Zhang,Qing Li,Xingyu Gao,Peilin Zhao*

Main category: cs.LG

TL;DR: MSDformer提出了一种基于多尺度离散令牌建模的时间序列生成方法，解决了现有DTM方法无法捕捉多尺度时间模式和缺乏理论指导的问题。


<details>
  <summary>Details</summary>
Motivation: 现有DTM方法在时间序列生成中存在两个关键问题：无法捕捉多尺度时间模式，以及缺乏理论指导。本文旨在解决这些问题。

Method: MSDformer采用多尺度时间序列分词器学习多尺度离散令牌表示，并应用多尺度自回归令牌建模技术捕捉离散潜在空间中的多尺度模式。

Result: 实验表明，MSDformer显著优于现有方法，理论分析和实验结果均证明多尺度信息能大幅提升生成时间序列的质量。

Conclusion: MSDformer通过多尺度建模和理论验证，显著提升了DTM方法在时间序列生成中的性能。

Abstract: Discrete Token Modeling (DTM), which employs vector quantization techniques,
has demonstrated remarkable success in modeling non-natural language
modalities, particularly in time series generation. While our prior work
SDformer established the first DTM-based framework to achieve state-of-the-art
performance in this domain, two critical limitations persist in existing DTM
approaches: 1) their inability to capture multi-scale temporal patterns
inherent to complex time series data, and 2) the absence of theoretical
foundations to guide model optimization. To address these challenges, we
proposes a novel multi-scale DTM-based time series generation method, called
Multi-Scale Discrete Transformer (MSDformer). MSDformer employs a multi-scale
time series tokenizer to learn discrete token representations at multiple
scales, which jointly characterize the complex nature of time series data.
Subsequently, MSDformer applies a multi-scale autoregressive token modeling
technique to capture the multi-scale patterns of time series within the
discrete latent space. Theoretically, we validate the effectiveness of the DTM
method and the rationality of MSDformer through the rate-distortion theorem.
Comprehensive experiments demonstrate that MSDformer significantly outperforms
state-of-the-art methods. Both theoretical analysis and experimental results
demonstrate that incorporating multi-scale information and modeling multi-scale
patterns can substantially enhance the quality of generated time series in
DTM-based approaches. The code will be released upon acceptance.

</details>


### [303] [Challenges and Limitations in the Synthetic Generation of mHealth Sensor Data](https://arxiv.org/abs/2505.14206)
*Flavio Di Martino,Franca Delmastro*

Main category: cs.LG

TL;DR: 论文评估了生成模型在合成多模态、长依赖时间序列数据中的表现，提出了新评估框架并指出现有方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 移动传感器数据稀缺和隐私问题限制了mHealth领域AI应用，合成数据生成成为解决方案，但现有模型在跨模态一致性和时间连贯性上存在不足。

Method: 系统评估了生成对抗网络和扩散模型等先进生成模型，并引入新的评估框架来衡量合成数据的内在质量和下游任务实用性。

Result: 发现现有方法在跨模态一致性、时间连贯性及合成数据训练-真实数据测试场景中表现不佳。

Conclusion: 未来研究需改进合成时间序列生成技术，提升生成模型在mHealth领域的适用性。

Abstract: The widespread adoption of mobile sensors has the potential to provide
massive and heterogeneous time series data, driving Artificial Intelligence
applications in mHealth. However, data collection remains limited due to
stringent ethical regulations, privacy concerns, and other constraints,
hindering progress in the field. Synthetic data generation, particularly
through Generative Adversarial Networks and Diffusion Models, has emerged as a
promising solution to address both data scarcity and privacy issues. Yet, these
models are often limited to short-term, unimodal signal patterns. This paper
presents a systematic evaluation of state-of-the-art generative models for time
series synthesis, with a focus on their ability to jointly handle
multi-modality, long-range dependencies, and conditional generation-key
challenges in the mHealth domain. To ensure a fair comparison, we introduce a
novel evaluation framework designed to measure both the intrinsic quality of
synthetic data and its utility in downstream predictive tasks. Our findings
reveal critical limitations in the existing approaches, particularly in
maintaining cross-modal consistency, preserving temporal coherence, and
ensuring robust performance in train-on-synthetic, test-on-real, and data
augmentation scenarios. Finally, we present our future research directions to
enhance synthetic time series generation and improve the applicability of
generative models in mHealth.

</details>


### [304] [A PID-Controlled Tensor Wheel Decomposition Model for Dynamic Link Prediction](https://arxiv.org/abs/2505.14211)
*Qu Wang,Yan Xia*

Main category: cs.LG

TL;DR: 该论文提出了一种PID控制的张量轮分解（PTWD）模型，用于动态网络中的链接预测，通过结合PID控制原理和TWD的表示能力，提高了预测准确性。


<details>
  <summary>Details</summary>
Motivation: 动态网络中的链接预测是一个重要挑战，传统静态网络方法难以捕捉时间依赖性和权重动态，而基于张量的方法通过高阶张量编码动态网络，能够更好地建模多维交互。

Method: 论文提出PTWD模型，结合张量轮分解（TWD）的表示能力和PID控制原理，优化模型参数学习过程，以捕捉动态网络的拓扑和权重演化特征。

Result: 在四个真实数据集上的实验表明，PTWD模型相比其他模型具有更准确的链接预测能力。

Conclusion: PTWD模型通过整合PID控制原理和TWD的拓扑结构，显著提升了动态网络链接预测的准确性，为未来研究提供了新方向。

Abstract: Link prediction in dynamic networks remains a fundamental challenge in
network science, requiring the inference of potential interactions and their
evolving strengths through spatiotemporal pattern analysis. Traditional static
network methods have inherent limitations in capturing temporal dependencies
and weight dynamics, while tensor-based methods offer a promising paradigm by
encoding dynamic networks into high-order tensors to explicitly model
multidimensional interactions across nodes and time. Among them, tensor wheel
decomposition (TWD) stands out for its innovative topological structure, which
decomposes high-order tensors into cyclic factors and core tensors to maintain
structural integrity. To improve the prediction accuracy, this study introduces
a PID-controlled tensor wheel decomposition (PTWD) model, which mainly adopts
the following two ideas: 1) exploiting the representation power of TWD to
capture the latent features of dynamic network topology and weight evolution,
and 2) integrating the proportional-integral-derivative (PID) control principle
into the optimization process to obtain a stable model parameter learning
scheme. The performance on four real datasets verifies that the proposed PTWD
model has more accurate link prediction capabilities compared to other models.

</details>


### [305] [Regularized least squares learning with heavy-tailed noise is minimax optimal](https://arxiv.org/abs/2505.14214)
*Mattes Mollenhauer,Nicole Mücke,Dimitri Meunier,Arthur Gretton*

Main category: cs.LG

TL;DR: 该论文研究了在噪声具有有限高阶矩的情况下，再生核希尔伯特空间中岭回归的性能，并建立了基于积分算子框架的超风险界限，证明了正则化最小二乘法对重尾噪声的渐近鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 过去二十年中，相关研究通常假设噪声服从亚指数分布，而本文旨在探讨在噪声仅具有有限高阶矩的情况下，岭回归的性能表现，以扩展其应用范围。

Method: 论文采用积分算子框架，并基于Hilbert空间值随机变量的Fuk-Nagaev不等式进行推导。

Result: 论文建立了由亚高斯项和多项式项组成的超风险界限，证明了在标准特征值衰减条件下，收敛速率是最优的，且亚高斯项占主导地位。

Conclusion: 研究表明，正则化最小二乘法在重尾噪声下具有渐近鲁棒性，且收敛速率与亚指数噪声假设下的结果一致，扩展了岭回归的理论基础。

Abstract: This paper examines the performance of ridge regression in reproducing kernel
Hilbert spaces in the presence of noise that exhibits a finite number of higher
moments. We establish excess risk bounds consisting of subgaussian and
polynomial terms based on the well known integral operator framework. The
dominant subgaussian component allows to achieve convergence rates that have
previously only been derived under subexponential noise - a prevalent
assumption in related work from the last two decades. These rates are optimal
under standard eigenvalue decay conditions, demonstrating the asymptotic
robustness of regularized least squares against heavy-tailed noise. Our
derivations are based on a Fuk-Nagaev inequality for Hilbert-space valued
random variables.

</details>


### [306] [Federated learning in low-resource settings: A chest imaging study in Africa -- Challenges and lessons learned](https://arxiv.org/abs/2505.14217)
*Jorge Fabila,Lidia Garrucho,Víctor M. Campello,Carlos Martín-Isla,Karim Lekadir*

Main category: cs.LG

TL;DR: 该研究探讨了在非洲资源匮乏地区使用联邦学习（FL）进行结核病（TB）胸部X光诊断的可行性，尽管面临基础设施不足等挑战，但FL显示出在医疗AI应用中的潜力。


<details>
  <summary>Details</summary>
Motivation: 传统集中式AI模型在资源匮乏地区面临数据隐私和稀缺性问题，联邦学习能在不共享原始数据的情况下协作训练模型，解决这些问题。

Method: 研究联合非洲八国的医院和研究机构，比较本地训练模型与跨机构联邦模型的性能，评估FL的实际可行性。

Result: FL在资源匮乏地区展现出应用潜力，但实施中遇到基础设施差、网络不稳定、数字素养低和AI监管薄弱等挑战。

Conclusion: 联邦学习有望推动欠发达地区的AI医疗应用，但广泛采用需改善基础设施、加强教育和法规支持。

Abstract: This study explores the use of Federated Learning (FL) for tuberculosis (TB)
diagnosis using chest X-rays in low-resource settings across Africa. FL allows
hospitals to collaboratively train AI models without sharing raw patient data,
addressing privacy concerns and data scarcity that hinder traditional
centralized models. The research involved hospitals and research centers in
eight African countries. Most sites used local datasets, while Ghana and The
Gambia used public ones. The study compared locally trained models with a
federated model built across all institutions to evaluate FL's real-world
feasibility. Despite its promise, implementing FL in sub-Saharan Africa faces
challenges such as poor infrastructure, unreliable internet, limited digital
literacy, and weak AI regulations. Some institutions were also reluctant to
share model updates due to data control concerns. In conclusion, FL shows
strong potential for enabling AI-driven healthcare in underserved regions, but
broader adoption will require improvements in infrastructure, education, and
regulatory support.

</details>


### [307] [Fast and close Shannon entropy approximation](https://arxiv.org/abs/2505.14234)
*Illia Horenko,Davide Bassetti,Lukáš Pospíšil*

Main category: cs.LG

TL;DR: 提出了一种快速熵近似方法（FEA），显著提升了计算效率和模型质量。


<details>
  <summary>Details</summary>
Motivation: 香农熵及其量子力学类似物冯·诺依曼熵在多个领域至关重要，但现有计算方法存在高成本、低鲁棒性和收敛慢的问题。

Method: 提出非奇异有理近似方法FEA，优化熵及其梯度的计算，仅需5到6次基本运算。

Result: FEA的平均绝对误差降低20倍，计算速度提升50%，在机器学习特征选择中显著加速模型训练。

Conclusion: FEA通过高效计算和非奇异梯度，显著提升了AI工具的性能和速度。

Abstract: Shannon entropy (SE) and its quantum mechanical analogue von Neumann entropy
are key components in many tools used in physics, information theory, machine
learning (ML) and quantum computing. Besides of the significant amounts of SE
computations required in these fields, the singularity of the SE gradient is
one of the central mathematical reason inducing the high cost, frequently low
robustness and slow convergence of such tools. Here we propose the Fast Entropy
Approximation (FEA) - a non-singular rational approximation of Shannon entropy
and its gradient that achieves a mean absolute error of $10^{-3}$, which is
approximately $20$ times lower than comparable state-of-the-art methods. FEA
allows around $50\%$ faster computation, requiring only $5$ to $6$ elementary
computational operations, as compared to tens of elementary operations behind
the fastest entropy computation algorithms with table look-ups, bitshifts, or
series approximations. On a set of common benchmarks for the feature selection
problem in machine learning, we show that the combined effect of fewer
elementary operations, low approximation error, and a non-singular gradient
allows significantly better model quality and enables ML feature extraction
that is two to three orders of magnitude faster and computationally cheaper
when incorporating FEA into AI tools.

</details>


### [308] [Learning with Local Search MCMC Layers](https://arxiv.org/abs/2505.14240)
*Germain Vivier-Ardisson,Mathieu Blondel,Axel Parmentier*

Main category: cs.LG

TL;DR: 该论文提出了一种理论上有保证的方法，用于在神经网络中集成不精确的组合求解器，通过将局部搜索启发式算法转化为MCMC的提议分布，构建可微分的组合层和损失函数，显著降低了计算负担。


<details>
  <summary>Details</summary>
Motivation: 现有方法在依赖不精确求解器时缺乏理论保证或性能不足，而许多运筹学问题是NP难的，通常需要使用局部搜索启发式算法。

Method: 受模拟退火与Metropolis-Hastings的启发，将局部搜索中的邻域系统转化为提议分布，在可行解的组合空间上实现MCMC，构建可微分的组合层和损失函数。

Result: 该方法在大规模动态车辆路径问题（带时间窗）上验证了有效性，显著降低了学习过程中的计算负担。

Conclusion: 通过理论驱动的MCMC方法，成功实现了不精确组合求解器在神经网络中的高效集成，为复杂组合优化问题提供了可行的解决方案。

Abstract: Integrating combinatorial optimization layers into neural networks has
recently attracted significant research interest. However, many existing
approaches lack theoretical guarantees or fail to perform adequately when
relying on inexact solvers. This is a critical limitation, as many operations
research problems are NP-hard, often necessitating the use of
neighborhood-based local search heuristics. These heuristics iteratively
generate and evaluate candidate solutions based on an acceptance rule. In this
paper, we introduce a theoretically-principled approach for learning with such
inexact combinatorial solvers. Inspired by the connection between simulated
annealing and Metropolis-Hastings, we propose to transform problem-specific
neighborhood systems used in local search heuristics into proposal
distributions, implementing MCMC on the combinatorial space of feasible
solutions. This allows us to construct differentiable combinatorial layers and
associated loss functions. Replacing an exact solver by a local search strongly
reduces the computational burden of learning on many applications. We
demonstrate our approach on a large-scale dynamic vehicle routing problem with
time windows.

</details>


### [309] [A Private Approximation of the 2nd-Moment Matrix of Any Subsamplable Input](https://arxiv.org/abs/2505.14251)
*Bar Mahpud,Or Sheffet*

Main category: cs.LG

TL;DR: 提出一种基于子采样假设的差分隐私二阶矩估计算法，在保证隐私的同时实现高精度估计，并能处理异常值。


<details>
  <summary>Details</summary>
Motivation: 研究差分隐私下的二阶矩估计问题，旨在在保护数据隐私的同时，提供高精度的估计结果，特别是在存在异常值的情况下。

Method: 基于子采样假设，提出递归算法框架，满足零集中差分隐私（zCDP），并在高概率下保持二阶矩估计的准确性。

Result: 算法在子采样假设下，能够以高概率保持二阶矩估计的准确性，且适用于存在异常值的情况。

Conclusion: 该算法在差分隐私和估计精度之间取得了良好的平衡，适用于实际应用中的复杂数据场景。

Abstract: We study the problem of differentially private second moment estimation and
present a new algorithm that achieve strong privacy-utility trade-offs even for
worst-case inputs under subsamplability assumptions on the data. We call an
input $(m,\alpha,\beta)$-subsamplable if a random subsample of size $m$ (or
larger) preserves w.p $\geq 1-\beta$ the spectral structure of the original
second moment matrix up to a multiplicative factor of $1\pm \alpha$. Building
upon subsamplability, we give a recursive algorithmic framework similar to
Kamath et al 2019, that abides zero-Concentrated Differential Privacy (zCDP)
while preserving w.h.p. the accuracy of the second moment estimation upto an
arbitrary factor of $(1\pm\gamma)$. We then show how to apply our algorithm to
approximate the second moment matrix of a distribution $\mathcal{D}$, even when
a noticeable fraction of the input are outliers.

</details>


### [310] [Hybrid Adaptive Modeling in Process Monitoring: Leveraging Sequence Encoders and Physics-Informed Neural Networks](https://arxiv.org/abs/2505.14252)
*Mouad Elaarabi,Domenico Borzacchiello,Philippe Le Bot,Nathan Lauzeral,Sebastien Comas-Cardona*

Main category: cs.LG

TL;DR: 该研究提出了一种结合序列编码与物理信息神经网络（PINN）的方法，用于实时应用中处理变化的参数、边界条件和初始条件，避免了传统方法需要重新训练的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统PINN与稀疏回归结合的方法在处理参数、边界条件或初始条件变化时需要重新训练模型，限制了其在实际应用中的灵活性和效率。

Method: 采用Deep Sets或序列编码器对动态参数、边界条件和初始条件进行编码，并将编码特征作为PINN的输入，使模型能够适应这些变化。

Result: 在Rossler ODE系统、2D Navier-Stokes PDE问题和1D热监测问题中验证了模型的鲁棒性、泛化能力及实时适应性。

Conclusion: 该方法有效解决了传统PINN在参数变化时的局限性，为实时应用提供了灵活且高效的解决方案。

Abstract: In this work, we explore the integration of Sequence Encoding for Online
Parameter Identification with Physics-Informed Neural Networks to create a
model that, once trained, can be utilized for real time applications with
variable parameters, boundary conditions, and initial conditions. Recently, the
combination of PINNs with Sparse Regression has emerged as a method for
performing dynamical system identification through supervised learning and
sparse regression optimization, while also solving the dynamics using PINNs.
However, this approach can be limited by variations in parameters or boundary
and initial conditions, requiring retraining of the model whenever changes
occur. In this work, we introduce an architecture that employs Deep Sets or
Sequence Encoders to encode dynamic parameters, boundary conditions, and
initial conditions, using these encoded features as inputs for the PINN,
enabling the model to adapt to changes in parameters, BCs, and ICs. We apply
this approach to three different problems. First, we analyze the Rossler ODE
system, demonstrating the robustness of the model with respect to noise and its
ability to generalize. Next, we explore the model's capability in a 2D
Navier-Stokes PDE problem involving flow past a cylinder with a parametric
sinusoidal inlet velocity function, showing that the model can encode pressure
data from a few points to identify the inlet velocity profile and utilize
physics to compute velocity and pressure throughout the domain. Finally, we
address a 1D heat monitoring problem using real data from the heating of glass
fiber and thermoplastic composite plates.

</details>


### [311] [AAPO: Enhance the Reasoning Capabilities of LLMs with Advantage Momentum](https://arxiv.org/abs/2505.14264)
*Jian Xiong,Jingbo Zhou,Jingyong Ye,Dejing Dou*

Main category: cs.LG

TL;DR: 本文提出了一种新的强化学习算法AAPO，通过动量增强的优势估计优化交叉熵损失，解决了现有组相对优势估计方法在训练效率上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有的组相对优势估计方法（如GRPO）虽然消除了对价值模型的依赖，但在估计优势接近零时仍存在训练效率低下的问题。本文旨在解决这一局限性。

Method: 提出了Advantage-Augmented Policy Optimization (AAPO)，通过动量增强的优势估计方案优化交叉熵损失，从而提升训练效率。

Result: 在多个数学推理基准测试中，AAPO表现出优于现有方法的性能。

Conclusion: AAPO通过改进优势估计方法，有效提升了强化学习在语言模型推理任务中的训练效率和性能。

Abstract: Reinforcement learning (RL) has emerged as an effective approach for
enhancing the reasoning capabilities of large language models (LLMs),
especially in scenarios where supervised fine-tuning (SFT) falls short due to
limited chain-of-thought (CoT) data. Among RL-based post-training methods,
group relative advantage estimation, as exemplified by Group Relative Policy
Optimization (GRPO), has attracted considerable attention for eliminating the
dependency on the value model, thereby simplifying training compared to
traditional approaches like Proximal Policy Optimization (PPO). However, we
observe that exsiting group relative advantage estimation method still suffers
from training inefficiencies, particularly when the estimated advantage
approaches zero. To address this limitation, we propose Advantage-Augmented
Policy Optimization (AAPO), a novel RL algorithm that optimizes the
cross-entropy (CE) loss using advantages enhanced through a momentum-based
estimation scheme. This approach effectively mitigates the inefficiencies
associated with group relative advantage estimation. Experimental results on
multiple mathematical reasoning benchmarks demonstrate the superior performance
of AAPO.

</details>


### [312] [X-KAN: Optimizing Local Kolmogorov-Arnold Networks via Evolutionary Rule-Based Machine Learning](https://arxiv.org/abs/2505.14273)
*Hiroki Shiraishi,Hisao Ishibuchi,Masaya Nakata*

Main category: cs.LG

TL;DR: X-KAN结合KAN的高表达能力和XCSF的自适应分区能力，通过进化规则学习框架优化局部KAN模型，显著提升了复杂或不连续函数的逼近精度。


<details>
  <summary>Details</summary>
Motivation: 现有神经网络方法因依赖单一全局模型，难以处理局部复杂或不连续的函数逼近问题。

Method: 提出X-KAN方法，将局部KAN模型作为规则后件，通过XCSF框架定义局部区域并优化模型。

Result: X-KAN在人工测试函数和真实数据集上均优于传统方法，平均仅需7.2±2.3条规则即可有效处理复杂结构。

Conclusion: X-KAN验证了在XCSF中使用KAN作为局部模型的有效性，平衡了精度与泛化能力。

Abstract: Function approximation is a critical task in various fields. However,
existing neural network approaches struggle with locally complex or
discontinuous functions due to their reliance on a single global model covering
the entire problem space. We propose X-KAN, a novel method that optimizes
multiple local Kolmogorov-Arnold Networks (KANs) through an evolutionary
rule-based machine learning framework called XCSF. X-KAN combines KAN's high
expressiveness with XCSF's adaptive partitioning capability by implementing
local KAN models as rule consequents and defining local regions via rule
antecedents. Our experimental results on artificial test functions and
real-world datasets demonstrate that X-KAN significantly outperforms
conventional methods, including XCSF, Multi-Layer Perceptron, and KAN, in terms
of approximation accuracy. Notably, X-KAN effectively handles functions with
locally complex or discontinuous structures that are challenging for
conventional KAN, using a compact set of rules (average 7.2 $\pm$ 2.3 rules).
These results validate the effectiveness of using KAN as a local model in XCSF,
which evaluates the rule fitness based on both accuracy and generality. Our
X-KAN implementation is available at https://github.com/YNU-NakataLab/X-KAN.

</details>


### [313] [Scaling Law for Quantization-Aware Training](https://arxiv.org/abs/2505.14302)
*Mengzhao Chen,Chaoyi Zhang,Jing Liu,Yutao Zeng,Zeyue Xue,Zhiheng Liu,Yunshui Li,Jin Ma,Jie Huang,Xun Zhou,Ping Luo*

Main category: cs.LG

TL;DR: 该论文提出了一个统一的量化感知训练（QAT）缩放定律，通过268次实验揭示了4比特精度（W4A4）下量化误差与模型大小、训练数据量和量化粒度的关系，并指出激活量化误差是主要瓶颈。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）需要大量计算和内存资源，量化感知训练（QAT）通过降低模型精度来解决这一问题，但4比特精度（W4A4）下的缩放行为尚未被充分理解。现有QAT缩放定律忽略了训练令牌数量和量化粒度等关键因素，限制了其适用性。

Method: 论文提出一个统一的QAT缩放定律，将量化误差建模为模型大小、训练数据量和量化组大小的函数。通过268次QAT实验，量化误差被分解为权重和激活两部分，并分析了其敏感性差异。

Result: 实验表明，量化误差随模型增大而减小，但随训练令牌增多和量化粒度变粗而增加。激活量化误差（尤其是FC2层的异常值）是W4A4 QAT的主要瓶颈。混合精度量化可缓解此问题，使权重和激活量化误差趋于一致。

Conclusion: 研究揭示了W4A4 QAT中量化误差的关键来源和变化规律，为改进QAT研发提供了重要见解，尤其是在处理权重和激活量化误差时的不同策略需求。

Abstract: Large language models (LLMs) demand substantial computational and memory
resources, creating deployment challenges. Quantization-aware training (QAT)
addresses these challenges by reducing model precision while maintaining
performance. However, the scaling behavior of QAT, especially at 4-bit
precision (W4A4), is not well understood. Existing QAT scaling laws often
ignore key factors such as the number of training tokens and quantization
granularity, which limits their applicability. This paper proposes a unified
scaling law for QAT that models quantization error as a function of model size,
training data volume, and quantization group size. Through 268 QAT experiments,
we show that quantization error decreases as model size increases, but rises
with more training tokens and coarser quantization granularity. To identify the
sources of W4A4 quantization error, we decompose it into weight and activation
components. Both components follow the overall trend of W4A4 quantization
error, but with different sensitivities. Specifically, weight quantization
error increases more rapidly with more training tokens. Further analysis shows
that the activation quantization error in the FC2 layer, caused by outliers, is
the primary bottleneck of W4A4 QAT quantization error. By applying
mixed-precision quantization to address this bottleneck, we demonstrate that
weight and activation quantization errors can converge to similar levels.
Additionally, with more training data, weight quantization error eventually
exceeds activation quantization error, suggesting that reducing weight
quantization error is also important in such scenarios. These findings offer
key insights for improving QAT research and development.

</details>


### [314] [MultiTab: A Comprehensive Benchmark Suite for Multi-Dimensional Evaluation in Tabular Domains](https://arxiv.org/abs/2505.14312)
*Kyungeun Lee,Moonjung Eo,Hye-Seung Cho,Dongmin Kim,Ye Seul Sim,Seoyoon Kim,Min-Kook Suh,Woohyung Lim*

Main category: cs.LG

TL;DR: MultiTab提出一个多维度的表格学习评估框架，通过分析不同数据特性下的模型表现，揭示了模型归纳偏置的实际效果与预期不符，为模型选择提供指导。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试多依赖平均指标，无法反映模型在不同数据特性下的表现差异，限制了模型设计的科学性和实际应用效果。

Method: 提出MultiTab框架，对196个公开数据集按样本量、标签不平衡等特性分类，评估13种代表性模型在不同数据特性下的表现。

Result: 模型表现高度依赖数据特性：样本相似性模型在大样本或高特征相关性数据中表现优异，而编码特征依赖的模型在弱相关性数据中表现最佳。

Conclusion: MultiTab揭示了归纳偏置的实际效果与预期不符，强调基于数据特性的评估对理解和改进模型行为的重要性，为模型设计和选择提供了科学依据。

Abstract: Despite the widespread use of tabular data in real-world applications, most
benchmarks rely on average-case metrics, which fail to reveal how model
behavior varies across diverse data regimes. To address this, we propose
MultiTab, a benchmark suite and evaluation framework for multi-dimensional,
data-aware analysis of tabular learning algorithms. Rather than comparing
models only in aggregate, MultiTab categorizes 196 publicly available datasets
along key data characteristics, including sample size, label imbalance, and
feature interaction, and evaluates 13 representative models spanning a range of
inductive biases. Our analysis shows that model performance is highly sensitive
to such regimes: for example, models using sample-level similarity excel on
datasets with large sample sizes or high inter-feature correlation, while
models encoding inter-feature dependencies perform best with weakly correlated
features. These findings reveal that inductive biases do not always behave as
intended, and that regime-aware evaluation is essential for understanding and
improving model behavior. MultiTab enables more principled model design and
offers practical guidance for selecting models tailored to specific data
characteristics. All datasets, code, and optimization logs are publicly
available at https://huggingface.co/datasets/LGAI-DILab/Multitab.

</details>


### [315] [Better Neural Network Expressivity: Subdividing the Simplex](https://arxiv.org/abs/2505.14338)
*Egor Bakaev,Florestan Brunck,Christoph Hertrich,Jack Stade,Amir Yehudayoff*

Main category: cs.LG

TL;DR: 该论文推翻了关于ReLU神经网络深度最优性的猜想，证明了更少隐藏层即可计算所有连续分段线性函数。


<details>
  <summary>Details</summary>
Motivation: 研究ReLU神经网络在计算连续分段线性函数时的表达能力，特别是关于网络深度的最优性问题。

Method: 通过构造性证明，展示如何用更少的隐藏层实现最大函数等CPWL函数的计算，并利用多面体细分进行几何解释。

Result: 证明了仅需⌈log₃(n-1)⌉+1隐藏层即可计算ℝⁿ上所有CPWL函数，突破了此前⌈log₂(n+1)⌉层的猜想下限。

Conclusion: ReLU神经网络的深度需求低于先前预期，这对理解神经网络的理论表达能力具有重要意义。

Abstract: This work studies the expressivity of ReLU neural networks with a focus on
their depth. A sequence of previous works showed that $\lceil \log_2(n+1)
\rceil$ hidden layers are sufficient to compute all continuous piecewise linear
(CPWL) functions on $\mathbb{R}^n$. Hertrich, Basu, Di Summa, and Skutella
(NeurIPS'21) conjectured that this result is optimal in the sense that there
are CPWL functions on $\mathbb{R}^n$, like the maximum function, that require
this depth. We disprove the conjecture and show that
$\lceil\log_3(n-1)\rceil+1$ hidden layers are sufficient to compute all CPWL
functions on $\mathbb{R}^n$.
  A key step in the proof is that ReLU neural networks with two hidden layers
can exactly represent the maximum function of five inputs. More generally, we
show that $\lceil\log_3(n-2)\rceil+1$ hidden layers are sufficient to compute
the maximum of $n\geq 4$ numbers. Our constructions almost match the
$\lceil\log_3(n)\rceil$ lower bound of Averkov, Hojny, and Merkert (ICLR'25) in
the special case of ReLU networks with weights that are decimal fractions. The
constructions have a geometric interpretation via polyhedral subdivisions of
the simplex into ``easier'' polytopes.

</details>


### [316] [Enhancing Classification with Semi-Supervised Deep Learning Using Distance-Based Sample Weights](https://arxiv.org/abs/2505.14345)
*Aydin Abedinia,Shima Tabakhi,Vahid Seydi*

Main category: cs.LG

TL;DR: 该论文提出了一种基于距离加权的半监督学习框架，通过优先处理靠近测试数据的关键样本提升分类性能，在噪声和不平衡数据场景下表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决标记数据有限场景下模型泛化能力不足的问题，特别是在噪声大或数据不平衡的挑战性场景中提升分类性能。

Method: 结合不确定性一致性和图表示技术，设计距离加权机制动态筛选关键训练样本，增强模型对未标记数据的利用效率。

Result: 在12个基准数据集上验证，准确率、精确率和召回率均显著超越现有方法，尤其在噪声和不平衡数据条件下表现稳健。

Conclusion: 该框架为半监督学习提供了可扩展的实用解决方案，在医疗、安防等数据受限领域具有应用潜力。

Abstract: Recent advancements in semi-supervised deep learning have introduced
effective strategies for leveraging both labeled and unlabeled data to improve
classification performance. This work proposes a semi-supervised framework that
utilizes a distance-based weighting mechanism to prioritize critical training
samples based on their proximity to test data. By focusing on the most
informative examples, the method enhances model generalization and robustness,
particularly in challenging scenarios with noisy or imbalanced datasets.
Building on techniques such as uncertainty consistency and graph-based
representations, the approach addresses key challenges of limited labeled data
while maintaining scalability. Experiments on twelve benchmark datasets
demonstrate significant improvements across key metrics, including accuracy,
precision, and recall, consistently outperforming existing methods. This
framework provides a robust and practical solution for semi-supervised
learning, with potential applications in domains such as healthcare and
security where data limitations pose significant challenges.

</details>


### [317] [Towards eliciting latent knowledge from LLMs with mechanistic interpretability](https://arxiv.org/abs/2505.14352)
*Bartosz Cywiński,Emil Ryd,Senthooran Rajamanoharan,Neel Nanda*

Main category: cs.LG

TL;DR: 研究探索了如何从语言模型中提取隐藏知识，通过训练一个Taboo模型并测试黑盒和可解释性方法，证明两种方法均能有效揭示秘密词汇。


<details>
  <summary>Details</summary>
Motivation: 随着语言模型变得更强大，确保其可信赖和可靠至关重要。初步证据显示模型可能试图欺骗或对操作者保密，因此需要开发方法来揭示这些隐藏知识。

Method: 训练了一个Taboo模型，该模型描述特定秘密词汇而不直接提及它。随后测试了黑盒方法和基于机制可解释性技术（如logit lens和稀疏自编码器）的策略。

Result: 在概念验证设置中，黑盒和可解释性方法均能有效提取秘密词汇。

Conclusion: 这些方法在提取隐藏知识方面具有潜力，未来可在更复杂模型上测试和优化，为语言模型的安全可靠部署提供支持。

Abstract: As language models become more powerful and sophisticated, it is crucial that
they remain trustworthy and reliable. There is concerning preliminary evidence
that models may attempt to deceive or keep secrets from their operators. To
explore the ability of current techniques to elicit such hidden knowledge, we
train a Taboo model: a language model that describes a specific secret word
without explicitly stating it. Importantly, the secret word is not presented to
the model in its training data or prompt. We then investigate methods to
uncover this secret. First, we evaluate non-interpretability (black-box)
approaches. Subsequently, we develop largely automated strategies based on
mechanistic interpretability techniques, including logit lens and sparse
autoencoders. Evaluation shows that both approaches are effective in eliciting
the secret word in our proof-of-concept setting. Our findings highlight the
promise of these approaches for eliciting hidden knowledge and suggest several
promising avenues for future work, including testing and refining these methods
on more complex model organisms. This work aims to be a step towards addressing
the crucial problem of eliciting secret knowledge from language models, thereby
contributing to their safe and reliable deployment.

</details>


### [318] [Layer-wise Quantization for Quantized Optimistic Dual Averaging](https://arxiv.org/abs/2505.14371)
*Anh Duc Nguyen,Ilia Markov,Frank Zhengqing Wu,Ali Ramezani-Kebrya,Kimon Antonakopoulos,Dan Alistarh,Volkan Cevher*

Main category: cs.LG

TL;DR: 提出了一种适应深度神经网络异质性的分层量化框架QODA，在分布式变分不等式训练中实现更快收敛。


<details>
  <summary>Details</summary>
Motivation: 现代深度神经网络各层存在结构、表征特性等异质性，影响预测性能，需针对性优化量化方法。

Method: 开发分层量化框架，提出带自适应学习率的QODA算法，应用于分布式变分不等式训练。

Result: 在12+GPU上训练Wasserstein GAN时，QODA比基线方法提速150%。

Conclusion: QODA通过分层量化有效适应网络异质性，显著提升分布式训练效率。

Abstract: Modern deep neural networks exhibit heterogeneity across numerous layers of
various types such as residuals, multi-head attention, etc., due to varying
structures (dimensions, activation functions, etc.), distinct representation
characteristics, which impact predictions. We develop a general layer-wise
quantization framework with tight variance and code-length bounds, adapting to
the heterogeneities over the course of training. We then apply a new layer-wise
quantization technique within distributed variational inequalities (VIs),
proposing a novel Quantized Optimistic Dual Averaging (QODA) algorithm with
adaptive learning rates, which achieves competitive convergence rates for
monotone VIs. We empirically show that QODA achieves up to a $150\%$ speedup
over the baselines in end-to-end training time for training Wasserstein GAN on
$12+$ GPUs.

</details>


### [319] [Algorithmic Hiring and Diversity: Reducing Human-Algorithm Similarity for Better Outcomes](https://arxiv.org/abs/2505.14388)
*Prasanna Parasurama,Panos Ipeirotis*

Main category: cs.LG

TL;DR: 研究发现，在招聘中强制算法生成性别平衡的候选人短名单并不能有效提高最终聘用的多样性，尤其是当算法筛选标准与招聘经理偏好高度相关时。提出了一种新算法，通过选择可能被忽视但仍有竞争力的候选人来显著提升性别多样性。


<details>
  <summary>Details</summary>
Motivation: 当前招聘算法常通过强制性别平衡短名单来提升公平性和多样性，但实际效果存疑。本文旨在探讨这种约束是否真能转化为更公平的最终聘用结果，并分析影响因素。

Method: 结合理论分析和大规模实证研究（近80万份科技公司求职申请数据），提出并测试了一种新算法，该算法通过主动选择可能被招聘经理忽视但符合标准的候选人来优化短名单。

Result: 实证显示：当算法筛选标准与招聘经理偏好高度相关时，强制性别平衡短名单对最终聘用多样性改善有限；而新算法能显著提升性别多样性（平均+24%），且不影响聘用质量。

Conclusion: 实现招聘多样性需针对性算法设计，仅约束短名单性别比例不够。应关注算法与人类评估标准的相关性，主动纳入可能被忽视的合格候选人。这为公平导向的招聘算法提供了实践指导。

Abstract: Algorithmic tools are increasingly used in hiring to improve fairness and
diversity, often by enforcing constraints such as gender-balanced candidate
shortlists. However, we show theoretically and empirically that enforcing equal
representation at the shortlist stage does not necessarily translate into more
diverse final hires, even when there is no gender bias in the hiring stage. We
identify a crucial factor influencing this outcome: the correlation between the
algorithm's screening criteria and the human hiring manager's evaluation
criteria -- higher correlation leads to lower diversity in final hires. Using a
large-scale empirical analysis of nearly 800,000 job applications across
multiple technology firms, we find that enforcing equal shortlists yields
limited improvements in hire diversity when the algorithmic screening closely
mirrors the hiring manager's preferences. We propose a complementary
algorithmic approach designed explicitly to diversify shortlists by selecting
candidates likely to be overlooked by managers, yet still competitive according
to their evaluation criteria. Empirical simulations show that this approach
significantly enhances gender diversity in final hires without substantially
compromising hire quality. These findings highlight the importance of
algorithmic design choices in achieving organizational diversity goals and
provide actionable guidance for practitioners implementing fairness-oriented
hiring algorithms.

</details>


### [320] [Explaining Unreliable Perception in Automated Driving: A Fuzzy-based Monitoring Approach](https://arxiv.org/abs/2505.14407)
*Aniket Salvi,Gereon Weiss,Mario Trapp*

Main category: cs.LG

TL;DR: 提出一种基于模糊逻辑的运行时监控器，用于解释机器学习感知组件的可靠性条件，并提升自动驾驶系统的安全性。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习系统的运行时监控缺乏可解释性，难以确保安全性和可靠性。

Method: 设计模糊逻辑监控器，结合自然驾驶数据集评估感知组件的可靠性条件。

Result: 监控器在提升安全性的同时保持任务可用性，优于现有方法。

Conclusion: 可解释的模糊监控器能有效链接单元级正确性与系统级安全性。

Abstract: Autonomous systems that rely on Machine Learning (ML) utilize online fault
tolerance mechanisms, such as runtime monitors, to detect ML prediction errors
and maintain safety during operation. However, the lack of human-interpretable
explanations for these errors can hinder the creation of strong assurances
about the system's safety and reliability. This paper introduces a novel
fuzzy-based monitor tailored for ML perception components. It provides
human-interpretable explanations about how different operating conditions
affect the reliability of perception components and also functions as a runtime
safety monitor. We evaluated our proposed monitor using naturalistic driving
datasets as part of an automated driving case study. The interpretability of
the monitor was evaluated and we identified a set of operating conditions in
which the perception component performs reliably. Additionally, we created an
assurance case that links unit-level evidence of \textit{correct} ML operation
to system-level \textit{safety}. The benchmarking demonstrated that our monitor
achieved a better increase in safety (i.e., absence of hazardous situations)
while maintaining availability (i.e., ability to perform the mission) compared
to state-of-the-art runtime ML monitors in the evaluated dataset.

</details>


### [321] [Byte Pair Encoding for Efficient Time Series Forecasting](https://arxiv.org/abs/2505.14411)
*Leon Götz,Marcel Kollovieh,Stephan Günnemann,Leo Schwinn*

Main category: cs.LG

TL;DR: 提出了一种基于模式的时序数据token化方法，通过合并相似模式样本提升效率，并结合条件解码优化性能。


<details>
  <summary>Details</summary>
Motivation: 现有时序token化方法固定样本数导致冗余计算，尤其对简单模式（如常数值）效率低下。

Method: 1. 基于频繁模式的离散词汇表进行自适应压缩 2. 引入无需梯度计算的条件解码后优化方法

Result: 在基础模型上预测性能提升36%，效率提高1990%；条件解码进一步降低44%MSE

Conclusion: 该方法能自适应不同时序模式，生成具有统计特征和趋势意义的token表示，且具备良好泛化能力。

Abstract: Existing time series tokenization methods predominantly encode a constant
number of samples into individual tokens. This inflexible approach can generate
excessive tokens for even simple patterns like extended constant values,
resulting in substantial computational overhead. Inspired by the success of
byte pair encoding, we propose the first pattern-centric tokenization scheme
for time series analysis. Based on a discrete vocabulary of frequent motifs,
our method merges samples with underlying patterns into tokens, compressing
time series adaptively. Exploiting our finite set of motifs and the continuous
properties of time series, we further introduce conditional decoding as a
lightweight yet powerful post-hoc optimization method, which requires no
gradient computation and adds no computational overhead. On recent time series
foundation models, our motif-based tokenization improves forecasting
performance by 36% and boosts efficiency by 1990% on average. Conditional
decoding further reduces MSE by up to 44%. In an extensive analysis, we
demonstrate the adaptiveness of our tokenization to diverse temporal patterns,
its generalization to unseen data, and its meaningful token representations
capturing distinct time series properties, including statistical moments and
trends.

</details>


### [322] [Table Foundation Models: on knowledge pre-training for tabular learning](https://arxiv.org/abs/2505.14415)
*Myung Jun Kim,Félix Lefebvre,Gaëtan Brison,Alexandre Perez-Lebel,Gaël Varoquaux*

Main category: cs.LG

TL;DR: TARTE是一种新型表格基础模型，通过预训练将表格转换为知识增强的向量表示，提升下游任务的预测性能并降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有表格基础模型需要微调、计算成本高且难以复用，无法像文本或视觉基础模型那样便捷使用。TARTE旨在解决这些问题，提供更高效的表格语义表示方法。

Method: TARTE利用字符串捕捉表格语义，通过在大规模关系数据上预训练，生成知识增强的向量表示，支持微调或与其他学习器结合。

Result: TARTE显著提升了预测性能，优化了预测与计算的权衡，并能生成领域特定表示以支持进一步学习。

Conclusion: TARTE展示了表格学习中知识预训练的有效方法，为表格数据处理提供了更高效、灵活的解决方案。

Abstract: Table foundation models bring high hopes to data science: pre-trained on
tabular data to embark knowledge or priors, they should facilitate downstream
tasks on tables. One specific challenge is that of data semantics: numerical
entries take their meaning from context, e.g., column name. Pre-trained neural
networks that jointly model column names and table entries have recently
boosted prediction accuracy. While these models outline the promises of world
knowledge to interpret table values, they lack the convenience of popular
foundation models in text or vision. Indeed, they must be fine-tuned to bring
benefits, come with sizeable computation costs, and cannot easily be reused or
combined with other architectures. Here we introduce TARTE, a foundation model
that transforms tables to knowledge-enhanced vector representations using the
string to capture semantics. Pre-trained on large relational data, TARTE yields
representations that facilitate subsequent learning with little additional
cost. These representations can be fine-tuned or combined with other learners,
giving models that push the state-of-the-art prediction performance and improve
the prediction/computation performance trade-off. Specialized to a task or a
domain, TARTE gives domain-specific representations that facilitate further
learning. Our study demonstrates an effective approach to knowledge
pre-training for tabular learning.

</details>


### [323] [Explaining Neural Networks with Reasons](https://arxiv.org/abs/2505.14424)
*Levin Hornischer,Hannes Leitgeb*

Main category: cs.LG

TL;DR: 提出一种基于新数学哲学理论的可解释性方法，通过计算神经元的‘原因向量’来解释其行为，兼具逻辑与贝叶斯视角，并解决多义性问题。


<details>
  <summary>Details</summary>
Motivation: 当前神经网络的可解释性方法往往无法兼顾逻辑一致性和多义性解释，且缺乏哲学基础。本文旨在提出一种统一、可扩展且忠实于模型行为的解释框架。

Method: 为每个神经元计算‘原因向量’，通过前向传播评估其对特定命题的支持强度，结合逻辑与概率视角解释神经元功能。

Result: 理论及实验证明该方法具有哲学基础、架构普适性、计算高效性、行为忠实性、数据一致性、可优化性和实用价值（如提升鲁棒性和公平性）。

Conclusion: 该方法为神经网络提供了一种兼具理论严谨性与实用性的解释框架，能有效满足可解释性需求。

Abstract: We propose a new interpretability method for neural networks, which is based
on a novel mathematico-philosophical theory of reasons. Our method computes a
vector for each neuron, called its reasons vector. We then can compute how
strongly this reasons vector speaks for various propositions, e.g., the
proposition that the input image depicts digit 2 or that the input prompt has a
negative sentiment. This yields an interpretation of neurons, and groups
thereof, that combines a logical and a Bayesian perspective, and accounts for
polysemanticity (i.e., that a single neuron can figure in multiple concepts).
We show, both theoretically and empirically, that this method is: (1) grounded
in a philosophically established notion of explanation, (2) uniform, i.e.,
applies to the common neural network architectures and modalities, (3)
scalable, since computing reason vectors only involves forward-passes in the
neural network, (4) faithful, i.e., intervening on a neuron based on its reason
vector leads to expected changes in model output, (5) correct in that the
model's reasons structure matches that of the data source, (6) trainable, i.e.,
neural networks can be trained to improve their reason strengths, (7) useful,
i.e., it delivers on the needs for interpretability by increasing, e.g.,
robustness and fairness.

</details>


### [324] [Interpretable Neural System Dynamics: Combining Deep Learning with System Dynamics Modeling to Support Critical Applications](https://arxiv.org/abs/2505.14428)
*Riccardo D'Elia*

Main category: cs.LG

TL;DR: 该提案旨在结合深度学习与系统动力学的优势，开发一个可解释的神经系统动力学框架，以解决深度学习缺乏可解释性和系统动力学可扩展性不足的问题。


<details>
  <summary>Details</summary>
Motivation: 深度学习在复杂模型学习和预测方面表现出色，但缺乏可解释性和因果可靠性；而传统系统动力学方法虽然透明且具有因果洞察力，但可扩展性有限且需要大量领域知识。

Method: 项目提出了一种神经系统动力学流程，整合了基于概念的可解释性、机制可解释性和因果机器学习，结合深度学习的预测能力和传统系统动力学模型的可解释性。

Result: 该框架将同时具备因果可靠性和可扩展性，其有效性将通过欧盟资助的AutoMoTIF项目在自主多式联运系统中的实际应用进行验证。

Conclusion: 长期目标是收集可操作的见解，支持在自主系统中整合可解释性和安全性。

Abstract: The objective of this proposal is to bridge the gap between Deep Learning
(DL) and System Dynamics (SD) by developing an interpretable neural system
dynamics framework. While DL excels at learning complex models and making
accurate predictions, it lacks interpretability and causal reliability.
Traditional SD approaches, on the other hand, provide transparency and causal
insights but are limited in scalability and require extensive domain knowledge.
To overcome these limitations, this project introduces a Neural System Dynamics
pipeline, integrating Concept-Based Interpretability, Mechanistic
Interpretability, and Causal Machine Learning. This framework combines the
predictive power of DL with the interpretability of traditional SD models,
resulting in both causal reliability and scalability. The efficacy of the
proposed pipeline will be validated through real-world applications of the
EU-funded AutoMoTIF project, which is focused on autonomous multimodal
transportation systems. The long-term goal is to collect actionable insights
that support the integration of explainability and safety in autonomous
systems.

</details>


### [325] [RefiDiff: Refinement-Aware Diffusion for Efficient Missing Data Imputation](https://arxiv.org/abs/2505.14451)
*Md Atik Ahamed,Qiang Ye,Qiang Cheng*

Main category: cs.LG

TL;DR: RefiDiff提出了一种结合局部机器学习预测和Mamba去噪网络的新框架，用于高维混合类型数据在MNAR机制下的缺失值填补，显著提升了性能和效率。


<details>
  <summary>Details</summary>
Motivation: 高维混合类型数据在MNAR机制下的缺失值填补存在挑战，现有方法难以整合局部和全局数据特征，限制了性能。

Method: RefiDiff框架结合局部机器学习预测和Mamba去噪网络，通过预精炼和后精炼步骤提升填补的稳定性和准确性，统一编码混合类型数据为令牌。

Result: RefiDiff在多种缺失值设置下优于现有方法，尤其在MNAR机制下表现突出，训练速度比基于DDPM的方法快4倍。

Conclusion: RefiDiff是一种高效、稳健且可扩展的缺失值填补方法，适用于复杂缺失模式的高维混合类型数据。

Abstract: Missing values in high-dimensional, mixed-type datasets pose significant
challenges for data imputation, particularly under Missing Not At Random (MNAR)
mechanisms. Existing methods struggle to integrate local and global data
characteristics, limiting performance in MNAR and high-dimensional settings. We
propose an innovative framework, RefiDiff, combining local machine learning
predictions with a novel Mamba-based denoising network capturing
interrelationships among distant features and samples. Our approach leverages
pre-refinement for initial warm-up imputations and post-refinement to polish
results, enhancing stability and accuracy. By encoding mixed-type data into
unified tokens, RefiDiff enables robust imputation without architectural or
hyperparameter tuning. RefiDiff outperforms state-of-the-art (SOTA) methods
across missing-value settings, excelling in MNAR with a 4x faster training time
than SOTA DDPM-based approaches. Extensive evaluations on nine real-world
datasets demonstrate its robustness, scalability, and effectiveness in handling
complex missingness patterns.

</details>


### [326] [Interpretable Reinforcement Learning for Load Balancing using Kolmogorov-Arnold Networks](https://arxiv.org/abs/2505.14459)
*Kamal Singh,Sami Marouani,Ahmad Al Sheikh,Pham Tran Anh Quang,Amaury Habrard*

Main category: cs.LG

TL;DR: 该论文提出使用Kolmogorov-Arnold网络（KAN）进行可解释的强化学习，以解决网络控制中的负载均衡问题，并从中提取控制器方程。


<details>
  <summary>Details</summary>
Motivation: 现有的强化学习方法在网络控制问题（如负载均衡）中缺乏可解释性，且难以提取控制器方程。

Method: 采用PPO算法，结合1层KAN模型的Actor和MLP Critic网络，学习最大化吞吐量效用、最小化丢失和延迟的负载均衡策略。

Result: 该方法能够从学习到的神经网络中提取控制器方程，提升网络性能的同时提供可解释的策略。

Conclusion: 通过KAN实现的可解释强化学习在网络控制中表现出色，既能优化性能又能提供决策过程的透明性。

Abstract: Reinforcement learning (RL) has been increasingly applied to network control
problems, such as load balancing. However, existing RL approaches often suffer
from lack of interpretability and difficulty in extracting controller
equations. In this paper, we propose the use of Kolmogorov-Arnold Networks
(KAN) for interpretable RL in network control. We employ a PPO agent with a
1-layer actor KAN model and an MLP Critic network to learn load balancing
policies that maximise throughput utility, minimize loss as well as delay. Our
approach allows us to extract controller equations from the learned neural
networks, providing insights into the decision-making process. We evaluate our
approach using different reward functions demonstrating its effectiveness in
improving network performance while providing interpretable policies.

</details>


### [327] [Adverseness vs. Equilibrium: Exploring Graph Adversarial Resilience through Dynamic Equilibrium](https://arxiv.org/abs/2505.14463)
*Xinxin Fan,Wenxiong Chen,Mengfan Li,Wenqi Wei,Ling Liu*

Main category: cs.LG

TL;DR: 该论文提出了一种新方法来探索图分析中的对抗韧性临界状态，通过建模对抗攻击行为、理论证明临界状态存在性，并开发动态系统均衡点求解方法，实验表明其防御效果显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前针对图分析的对抗攻击防御主要从图本身或图神经网络角度出发，但缺乏对图体系内是否存在固有对抗韧性临界状态的研究。本文旨在解决这一基础性问题。

Method: 1) 将图对抗学习建模为多目标动态系统；2) 提出广义理论框架证明临界状态存在性；3) 开发一维函数捕捉扰动下动态变化，通过求解动态系统均衡点定位临界状态。

Result: 在5个常用真实数据集和3种典型攻击下的多维度实验表明，该方法显著优于现有最佳防御方法。

Conclusion: 论文通过动态系统视角揭示了图对抗学习的临界韧性状态，为图分析安全防御提供了新理论基础和实践框架。

Abstract: Adversarial attacks to graph analytics are gaining increased attention. To
date, two lines of countermeasures have been proposed to resist various graph
adversarial attacks from the perspectives of either graph per se or graph
neural networks. Nevertheless, a fundamental question lies in whether there
exists an intrinsic adversarial resilience state within a graph regime and how
to find out such a critical state if exists. This paper contributes to tackle
the above research questions from three unique perspectives: i) we regard the
process of adversarial learning on graph as a complex multi-object dynamic
system, and model the behavior of adversarial attack; ii) we propose a
generalized theoretical framework to show the existence of critical adversarial
resilience state; and iii) we develop a condensed one-dimensional function to
capture the dynamic variation of graph regime under perturbations, and pinpoint
the critical state through solving the equilibrium point of dynamic system.
Multi-facet experiments are conducted to show our proposed approach can
significantly outperform the state-of-the-art defense methods under five
commonly-used real-world datasets and three representative attacks.

</details>


### [328] [ServerlessLoRA: Minimizing Latency and Cost in Serverless Inference for LoRA-Based LLMs](https://arxiv.org/abs/2505.14468)
*Yifan Sui,Hao Wang,Hanfei Yu,Yitao Hu,Jianxun Li,Hao Wang*

Main category: cs.LG

TL;DR: ServerlessLoRA提出了一种新型无服务器推理系统，通过共享主干LLM、预加载LoRA构件和资源冲突感知调度，显著降低LoRA推理的延迟和成本。


<details>
  <summary>Details</summary>
Motivation: 当前无服务器计算在服务通用LLM推理时表现良好，但在处理低秩适配（LoRA）推理时存在参数冗余、构件加载延迟高和资源竞争等问题，导致GPU浪费、首令牌延迟增加和高昂成本。

Method: ServerlessLoRA通过跨隔离LoRA函数安全共享主干LLM减少冗余，预加载LoRA构件降低冷启动延迟，并采用资源冲突感知的批处理和卸载策略缓解突发负载下的GPU竞争。

Result: 实验表明，ServerlessLoRA将首令牌延迟降低86%，并将成本削减89%，显著优于现有LLM推理解决方案。

Conclusion: ServerlessLoRA为LoRA推理提供了一种高效、经济的无服务器解决方案，通过创新设计解决了当前系统的关键瓶颈。

Abstract: Serverless computing has grown rapidly for serving Large Language Model (LLM)
inference due to its pay-as-you-go pricing, fine-grained GPU usage, and rapid
scaling. However, our analysis reveals that current serverless can effectively
serve general LLM but fail with Low-Rank Adaptation (LoRA) inference due to
three key limitations: 1) massive parameter redundancy among functions where
99% of weights are unnecessarily duplicated, 2) costly artifact loading latency
beyond LLM loading, and 3) magnified resource contention when serving multiple
LoRA LLMs. These inefficiencies lead to massive GPU wastage, increased
Time-To-First-Token (TTFT), and high monetary costs.
  We propose ServerlessLoRA, a novel serverless inference system designed for
faster and cheaper LoRA LLM serving. ServerlessLoRA enables secure backbone LLM
sharing across isolated LoRA functions to reduce redundancy. We design a
pre-loading method that pre-loads comprehensive LoRA artifacts to minimize
cold-start latency. Furthermore, ServerlessLoRA employs contention aware
batching and offloading to mitigate GPU resource conflicts during bursty
workloads. Experiment on industrial workloads demonstrates that ServerlessLoRA
reduces TTFT by up to 86% and cuts monetary costs by up to 89% compared to
state-of-the-art LLM inference solutions.

</details>


### [329] [Personalised Insulin Adjustment with Reinforcement Learning: An In-Silico Validation for People with Diabetes on Intensive Insulin Treatment](https://arxiv.org/abs/2505.14477)
*Maria Panagiotou,Lorenzo Brigato,Vivien Streit,Amanda Hayoz,Stephan Proennecke,Stavros Athanasopoulos,Mikkel T. Olsen,Elizabeth J. den Brok,Cecilie H. Svensson,Konstantinos Makrilakis,Maria Xatzipsalti,Andriani Vazeou,Peter R. Mertens,Ulrik Pedersen-Bjergaard,Bastiaan E. de Galan,Stavroula Mougiakakou*

Main category: cs.LG

TL;DR: 提出基于强化学习的个性化胰岛素调整系统ABBA，相比传统方法显著提升血糖达标时间并减少高低血糖事件。


<details>
  <summary>Details</summary>
Motivation: 现有胰岛素调整方法对1型和2型糖尿病患者仍存在挑战，需开发更精准的个性化治疗方案。

Method: 开发强化学习驱动的ABBA系统，使用FDA认证的202人虚拟人群（101名T1D/101名T2D）进行模拟测试。

Result: ABBA显著提升血糖达标时间（TIR），高低血糖时间均减少，且效果持续改善，优于传统BBA方法。

Conclusion: ABBA有望优化血糖控制，支持糖尿病日常管理，具备首次人体试验价值。

Abstract: Despite recent advances in insulin preparations and technology, adjusting
insulin remains an ongoing challenge for the majority of people with type 1
diabetes (T1D) and longstanding type 2 diabetes (T2D). In this study, we
propose the Adaptive Basal-Bolus Advisor (ABBA), a personalised insulin
treatment recommendation approach based on reinforcement learning for
individuals with T1D and T2D, performing self-monitoring blood glucose
measurements and multiple daily insulin injection therapy. We developed and
evaluated the ability of ABBA to achieve better time-in-range (TIR) for
individuals with T1D and T2D, compared to a standard basal-bolus advisor (BBA).
The in-silico test was performed using an FDA-accepted population, including
101 simulated adults with T1D and 101 with T2D. An in-silico evaluation shows
that ABBA significantly improved TIR and significantly reduced both times
below- and above-range, compared to BBA. ABBA's performance continued to
improve over two months, whereas BBA exhibited only modest changes. This
personalised method for adjusting insulin has the potential to further optimise
glycaemic control and support people with T1D and T2D in their daily
self-management. Our results warrant ABBA to be trialed for the first time in
humans.

</details>


### [330] [Learning to Integrate Diffusion ODEs by Averaging the Derivatives](https://arxiv.org/abs/2505.14502)
*Wenze Liu,Xiangyu Yue*

Main category: cs.LG

TL;DR: 该论文提出了一种名为'secant losses'的中间策略，通过结合ODE积分和蒙特卡洛积分的思想，在扩散模型推理中平衡性能与成本，显著提升了小步数下的生成质量。


<details>
  <summary>Details</summary>
Motivation: 当前扩散模型推理中，数值求解器在极小步数下性能不佳，而蒸馏技术又常带来复杂性和不稳定性。因此，需要一种既能保持高性能又能降低成本的中间策略。

Method: 论文提出通过学习ODE积分，利用基于导数-积分关系的损失函数（受蒙特卡洛积分和Picard迭代启发），逐步将切线延伸为割线，从而实现模型的高效微调或蒸馏。

Result: 实验表明，EDM的割线版本在CIFAR-10上10步FID达到2.14，SiT-XL/2的割线版本在ImageNet-256×256上4步FID为2.27，8步FID为1.96。

Conclusion: secant losses方法有效提升了扩散模型在小步数下的生成质量，为快速推理提供了新的解决方案。

Abstract: To accelerate diffusion model inference, numerical solvers perform poorly at
extremely small steps, while distillation techniques often introduce complexity
and instability. This work presents an intermediate strategy, balancing
performance and cost, by learning ODE integration using loss functions derived
from the derivative-integral relationship, inspired by Monte Carlo integration
and Picard iteration. From a geometric perspective, the losses operate by
gradually extending the tangent to the secant, thus are named as secant losses.
The secant losses can rapidly convert (via fine-tuning or distillation) a
pretrained diffusion model into its secant version. In our experiments, the
secant version of EDM achieves a $10$-step FID of $2.14$ on CIFAR-10, while the
secant version of SiT-XL/2 attains a $4$-step FID of $2.27$ and an $8$-step FID
of $1.96$ on ImageNet-$256\times256$. Code will be available.

</details>


### [331] [Just One Layer Norm Guarantees Stable Extrapolation](https://arxiv.org/abs/2505.14512)
*Juliusz Ziomek,George Whittle,Michael A. Osborne*

Main category: cs.LG

TL;DR: 论文通过神经正切核理论证明，在无限宽神经网络中加入单层Layer Norm能有效限制模型在训练数据外区域的输出范围，提升外推稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有研究对神经网络在训练分布外区域的泛化行为缺乏理论理解，尤其对Layer Norm等结构的作用机制尚不明确。

Method: 应用神经正切核(NTK)理论分析无限宽网络，对比含/不含Layer Norm网络的外推行为，并通过有限宽度网络实验验证。

Result: 含单层Layer Norm的网络NTK变为有界方差核，输出保持稳定；而无Layer Norm的网络可能产生病态大输出。实验证实该理论发现。

Conclusion: Layer Norm通过改变NTK性质显著提升模型外推稳定性，在蛋白质残基预测、跨种族年龄估计等任务中具有实际价值。

Abstract: In spite of their prevalence, the behaviour of Neural Networks when
extrapolating far from the training distribution remains poorly understood,
with existing results limited to specific cases. In this work, we prove general
results -- the first of their kind -- by applying Neural Tangent Kernel (NTK)
theory to analyse infinitely-wide neural networks trained until convergence and
prove that the inclusion of just one Layer Norm (LN) fundamentally alters the
induced NTK, transforming it into a bounded-variance kernel. As a result, the
output of an infinitely wide network with at least one LN remains bounded, even
on inputs far from the training data. In contrast, we show that a broad class
of networks without LN can produce pathologically large outputs for certain
inputs. We support these theoretical findings with empirical experiments on
finite-width networks, demonstrating that while standard NNs often exhibit
uncontrolled growth outside the training domain, a single LN layer effectively
mitigates this instability. Finally, we explore real-world implications of this
extrapolatory stability, including applications to predicting residue sizes in
proteins larger than those seen during training and estimating age from facial
images of underrepresented ethnicities absent from the training set.

</details>


### [332] [Latent Flow Transformer](https://arxiv.org/abs/2505.14513)
*Yen-Chen Wu,Feng-Ting Liao,Meng-Hsi Chen,Pei-Chen Ho,Farhang Nabiei,Da-shan Shiu*

Main category: cs.LG

TL;DR: 论文提出Latent Flow Transformer (LFT)，通过流匹配训练替代传统离散层，实现模型压缩且保持性能，并引入Flow Walking算法改进耦合问题。


<details>
  <summary>Details</summary>
Motivation: 传统Transformer的离散层设计效率低下，而连续层在图像生成中表现优越，因此探索在语言模型中应用连续层以提升效率。

Method: LFT用学习的传输算子替代层块，通过流匹配训练；引入Flow Walking算法改进现有流方法的耦合问题。

Result: 在Pythia-410M上，LFT压缩6层性能优于跳过2层（KL散度0.407 vs 0.529）；Flow Walking进一步压缩12层至1层，KL散度降至0.736。

Conclusion: LFT证明了连续层在语言模型中的可行性，显著缩小了自回归与流生成范式间的差距。

Abstract: Transformers, the standard implementation for large language models (LLMs),
typically consist of tens to hundreds of discrete layers. While more layers can
lead to better performance, this approach has been challenged as far from
efficient, especially given the superiority of continuous layers demonstrated
by diffusion and flow-based models for image generation. We propose the Latent
Flow Transformer (LFT), which replaces a block of layers with a single learned
transport operator trained via flow matching, offering significant compression
while maintaining compatibility with the original architecture. Additionally,
we address the limitations of existing flow-based methods in \textit{preserving
coupling} by introducing the Flow Walking (FW) algorithm. On the Pythia-410M
model, LFT trained with flow matching compresses 6 of 24 layers and outperforms
directly skipping 2 layers (KL Divergence of LM logits at 0.407 vs. 0.529),
demonstrating the feasibility of this design. When trained with FW, LFT further
distills 12 layers into one while reducing the KL to 0.736 surpassing that from
skipping 3 layers (0.932), significantly narrowing the gap between
autoregressive and flow-based generation paradigms.

</details>


### [333] [Interpretable Dual-Stream Learning for Local Wind Hazard Prediction in Vulnerable Communities](https://arxiv.org/abs/2505.14522)
*Mahmuda Akhter Nishu,Chenyu Huang,Milad Roohi,Xin Zhong*

Main category: cs.LG

TL;DR: 该论文提出了一种可解释的双流学习框架，结合气象数据和文本事件描述，用于提升美国大平原地区弱势社区的风灾预测能力。


<details>
  <summary>Details</summary>
Motivation: 现有风灾预测系统主要关注气象因素，未能充分考虑社区特定脆弱性，导致在应急响应和风险评估中效果有限。

Method: 采用随机森林和RoBERTa变换器的双流架构，通过后期融合机制整合结构化气象数据和非结构化事件文本。

Result: 实验表明该模型性能显著优于传统基线，并通过敏感性分析增强了决策过程的可解释性。

Conclusion: 该框架不仅提高了预测准确性，还为提升社区应急准备和抗灾能力提供了实用价值。

Abstract: Wind hazards such as tornadoes and straight-line winds frequently affect
vulnerable communities in the Great Plains of the United States, where limited
infrastructure and sparse data coverage hinder effective emergency response.
Existing forecasting systems focus primarily on meteorological elements and
often fail to capture community-specific vulnerabilities, limiting their
utility for localized risk assessment and resilience planning. To address this
gap, we propose an interpretable dual-stream learning framework that integrates
structured numerical weather data with unstructured textual event narratives.
Our architecture combines a Random Forest and RoBERTa-based transformer through
a late fusion mechanism, enabling robust and context-aware wind hazard
prediction. The system is tailored for underserved tribal communities and
supports block-level risk assessment. Experimental results show significant
performance gains over traditional baselines. Furthermore, gradient-based
sensitivity and ablation studies provide insight into the model's
decision-making process, enhancing transparency and operational trust. The
findings demonstrate both predictive effectiveness and practical value in
supporting emergency preparedness and advancing community resilience.

</details>


### [334] [SifterNet: A Generalized and Model-Agnostic Trigger Purification Approach](https://arxiv.org/abs/2505.14531)
*Shaoye Luo,Xinxin Fan,Quanliang Jing,Chi Lin,Mengfan Li,Yunfeng Lu,Yongjun Xu*

Main category: cs.LG

TL;DR: 本文提出了一种通用的、模型无关的触发器净化方法SifterNet，利用Ising模型和Hopfield网络的记忆关联功能，有效抵抗卷积神经网络和视觉Transformer大模型中的后门攻击。


<details>
  <summary>Details</summary>
Motivation: 现有的触发器检测/移除方法通常需要预先了解目标模型的详细信息、大量干净样本甚至模型重新训练授权，这在实际应用中带来了极大不便，尤其是无法访问目标模型时。理想的防御措施应能消除植入的触发器，而不论目标模型是什么。

Method: 提出了一种轻量级的黑盒防御方法SifterNet，通过利用Hopfield网络的记忆关联功能，以Ising模型的思想为基础，有效净化输入样本中的触发器。

Result: 大量实验验证了该方法在触发器净化和高精度实现方面的有效性，与现有基线方法相比，SifterNet在多个常用数据集上表现出显著优越的性能。

Conclusion: SifterNet作为一种模型无关的触发器净化方法，能够在不依赖目标模型详细信息的情况下有效抵抗后门攻击，具有广泛的应用前景。

Abstract: Aiming at resisting backdoor attacks in convolution neural networks and
vision Transformer-based large model, this paper proposes a generalized and
model-agnostic trigger-purification approach resorting to the classic Ising
model. To date, existing trigger detection/removal studies usually require to
know the detailed knowledge of target model in advance, access to a large
number of clean samples or even model-retraining authorization, which brings
the huge inconvenience for practical applications, especially inaccessible to
target model. An ideal countermeasure ought to eliminate the implanted trigger
without regarding whatever the target models are. To this end, a lightweight
and black-box defense approach SifterNet is proposed through leveraging the
memorization-association functionality of Hopfield network, by which the
triggers of input samples can be effectively purified in a proper manner. The
main novelty of our proposed approach lies in the introduction of ideology of
Ising model. Extensive experiments also validate the effectiveness of our
approach in terms of proper trigger purification and high accuracy achievement,
and compared to the state-of-the-art baselines under several commonly-used
datasets, our SiferNet has a significant superior performance.

</details>


### [335] [Energy-Efficient Deep Reinforcement Learning with Spiking Transformers](https://arxiv.org/abs/2505.14533)
*Mohammad Irfan Uddin,Nishad Tasnim,Md Omor Faruk,Zejian Zhou*

Main category: cs.LG

TL;DR: 提出了一种结合脉冲神经网络（SNN）和强化学习的Spike-Transformer算法（STRL），在提高能量效率的同时优化决策性能。


<details>
  <summary>Details</summary>
Motivation: 传统基于Transformer的强化学习方法计算复杂度高，能耗大，限制了其在现实自主系统中的部署。SNN因其生物启发的结构，提供了能量高效的替代方案。

Method: 设计了一种使用多步Leaky Integrate-and-Fire（LIF）神经元和注意力机制的SNN，能够处理多时间步的时空模式，并通过状态、动作和奖励编码增强，构建类似Transformer的结构。

Result: 在先进基准测试中，提出的SNN Transformer相比传统基于Transformer的智能体，显著提升了策略性能，同时提高了能量效率。

Conclusion: 这项工作展示了在复杂现实决策场景中部署生物启发、低成本机器学习模型的潜力。

Abstract: Agent-based Transformers have been widely adopted in recent reinforcement
learning advances due to their demonstrated ability to solve complex tasks.
However, the high computational complexity of Transformers often results in
significant energy consumption, limiting their deployment in real-world
autonomous systems. Spiking neural networks (SNNs), with their biologically
inspired structure, offer an energy-efficient alternative for machine learning.
In this paper, a novel Spike-Transformer Reinforcement Learning (STRL)
algorithm that combines the energy efficiency of SNNs with the powerful
decision-making capabilities of reinforcement learning is developed.
Specifically, an SNN using multi-step Leaky Integrate-and-Fire (LIF) neurons
and attention mechanisms capable of processing spatio-temporal patterns over
multiple time steps is designed. The architecture is further enhanced with
state, action, and reward encodings to create a Transformer-like structure
optimized for reinforcement learning tasks. Comprehensive numerical experiments
conducted on state-of-the-art benchmarks demonstrate that the proposed SNN
Transformer achieves significantly improved policy performance compared to
conventional agent-based Transformers. With both enhanced energy efficiency and
policy optimality, this work highlights a promising direction for deploying
bio-inspired, low-cost machine learning models in complex real-world
decision-making scenarios.

</details>


### [336] [Spiking Neural Networks with Temporal Attention-Guided Adaptive Fusion for imbalanced Multi-modal Learning](https://arxiv.org/abs/2505.14535)
*Jiangrong Shen,Yulin Xie,Qi Xu,Gang Pan,Huajin Tang,Badong Chen*

Main category: cs.LG

TL;DR: 该论文提出了一种基于时间注意力引导的自适应融合框架（TAAF），用于解决多模态脉冲神经网络中的模态不平衡和时间错位问题，实现了高效的多模态学习。


<details>
  <summary>Details</summary>
Motivation: 多模态脉冲神经网络（SNNs）在高效能感知处理方面具有潜力，但面临模态不平衡和时间错位的关键挑战。现有方法存在模态收敛速度不协调和静态融合机制忽略时间变化跨模态交互的问题。

Method: 论文提出了两个协同创新：1）时间注意力引导的自适应融合（TAAF）模块，动态分配每个时间步的融合脉冲特征重要性分数；2）时间自适应平衡融合损失，根据注意力分数调整每个模态的学习率，防止主导模态垄断优化。

Result: 在CREMA-D、AVE和EAD数据集上的评估显示，该方法达到了最先进的性能（准确率分别为77.55%、70.65%和97.5%），并具有高能效。系统通过可学习的时间扭曲操作和更快的模态收敛协调解决了时间错位问题。

Conclusion: 该工作为神经形态系统中的时间相干多模态学习建立了新范式，弥合了生物感知处理与高效机器智能之间的差距。

Abstract: Multimodal spiking neural networks (SNNs) hold significant potential for
energy-efficient sensory processing but face critical challenges in modality
imbalance and temporal misalignment. Current approaches suffer from
uncoordinated convergence speeds across modalities and static fusion mechanisms
that ignore time-varying cross-modal interactions. We propose the temporal
attention-guided adaptive fusion framework for multimodal SNNs with two
synergistic innovations: 1) The Temporal Attention-guided Adaptive Fusion
(TAAF) module that dynamically assigns importance scores to fused spiking
features at each timestep, enabling hierarchical integration of temporally
heterogeneous spike-based features; 2) The temporal adaptive balanced fusion
loss that modulates learning rates per modality based on the above attention
scores, preventing dominant modalities from monopolizing optimization. The
proposed framework implements adaptive fusion, especially in the temporal
dimension, and alleviates the modality imbalance during multimodal learning,
mimicking cortical multisensory integration principles. Evaluations on CREMA-D,
AVE, and EAD datasets demonstrate state-of-the-art performance (77.55\%,
70.65\% and 97.5\%accuracy, respectively) with energy efficiency. The system
resolves temporal misalignment through learnable time-warping operations and
faster modality convergence coordination than baseline SNNs. This work
establishes a new paradigm for temporally coherent multimodal learning in
neuromorphic systems, bridging the gap between biological sensory processing
and efficient machine intelligence.

</details>


### [337] [Time to Embed: Unlocking Foundation Models for Time Series with Channel Descriptions](https://arxiv.org/abs/2505.14543)
*Utsav Dutta,Sina Khoshfetrat Pakazad,Henrik Ohlsson*

Main category: cs.LG

TL;DR: CHARM是一个用于多元时间序列的基础嵌入模型，通过结合通道级文本描述和新型训练方法，实现了跨任务的先进性能。


<details>
  <summary>Details</summary>
Motivation: 传统时间序列模型通常针对特定任务，且依赖大量特征工程。尽管Transformer架构提升了可扩展性，但时间序列领域的基础模型仍未被充分探索，主要集中在预测任务上。

Method: CHARM通过整合通道级文本描述并保持对通道顺序的不变性，采用联合嵌入预测架构（JEPA）进行训练，结合新型数据增强方案和损失函数，提升了模型的解释性和训练稳定性。

Result: 7M参数的CHARM模型在多种下游任务中达到了最先进的性能，为时间序列表示学习设立了新基准。

Conclusion: CHARM作为一个基础嵌入模型，通过学习共享、可迁移且具有领域感知的表示，显著提升了时间序列任务的性能，为时间序列表示学习开辟了新方向。

Abstract: Traditional time series models are task-specific and often depend on
dataset-specific training and extensive feature engineering. While
Transformer-based architectures have improved scalability, foundation models,
commonplace in text, vision, and audio, remain under-explored for time series
and are largely restricted to forecasting. We introduce $\textbf{CHARM}$, a
foundation embedding model for multivariate time series that learns shared,
transferable, and domain-aware representations. To address the unique
difficulties of time series foundation learning, $\textbf{CHARM}$ incorporates
architectural innovations that integrate channel-level textual descriptions
while remaining invariant to channel order. The model is trained using a Joint
Embedding Predictive Architecture (JEPA), with novel augmentation schemes and a
loss function designed to improve interpretability and training stability. Our
$7$M-parameter model achieves state-of-the-art performance across diverse
downstream tasks, setting a new benchmark for time series representation
learning.

</details>


### [338] [Physics-Guided Learning of Meteorological Dynamics for Weather Downscaling and Forecasting](https://arxiv.org/abs/2505.14555)
*Yingtao Luo,Shikai Fang,Binqing Wu,Qingsong Wen,Liang Sun*

Main category: cs.LG

TL;DR: PhyDL-NWP：结合物理方程与深度学习的天气预测框架，提升效率与物理一致性。


<details>
  <summary>Details</summary>
Motivation: 传统数值天气预报（NWP）计算量大且物理模型不完整，而纯深度学习模型缺乏物理可解释性。

Method: 提出PhyDL-NWP框架，将物理方程与隐式力参数化融入数据驱动模型，通过自动微分计算物理项，并使用物理信息损失函数。

Result: 实现170倍加速推理，仅需5.5万参数，同时提高预测性能和物理一致性。

Conclusion: PhyDL-NWP为天气预测提供了高效、高精度且物理可解释的新方法。

Abstract: Weather forecasting is essential but remains computationally intensive and
physically incomplete in traditional numerical weather prediction (NWP)
methods. Deep learning (DL) models offer efficiency and accuracy but often
ignore physical laws, limiting interpretability and generalization. We propose
PhyDL-NWP, a physics-guided deep learning framework that integrates physical
equations with latent force parameterization into data-driven models. It
predicts weather variables from arbitrary spatiotemporal coordinates, computes
physical terms via automatic differentiation, and uses a physics-informed loss
to align predictions with governing dynamics. PhyDL-NWP enables resolution-free
downscaling by modeling weather as a continuous function and fine-tunes
pre-trained models with minimal overhead, achieving up to 170x faster inference
with only 55K parameters. Experiments show that PhyDL-NWP improves both
forecasting performance and physical consistency.

</details>


### [339] [Bellman operator convergence enhancements in reinforcement learning algorithms](https://arxiv.org/abs/2505.14564)
*David Krame Kadurha,Domini Jocema Leko Moutouo,Yae Ulrich Gaba*

Main category: cs.LG

TL;DR: 该论文通过拓扑学基础研究强化学习中的状态、动作和策略空间结构，利用Banach不动点定理解释算法收敛性，并提出改进Bellman算子的方法以提升标准RL环境中的性能。


<details>
  <summary>Details</summary>
Motivation: 旨在通过数学理论（如完备度量空间和Banach压缩原理）深化对强化学习算法的理解，从而设计更高效的决策算法。

Method: 回顾关键数学概念，应用Banach不动点定理分析RL算法收敛性，并探索Bellman算子的替代形式。

Result: 研究表明，对Bellman算子的数学优化可显著提高MountainCar等标准RL环境中的收敛速度和性能。

Conclusion: 通过数学理论与算法设计的结合，为强化学习提供了更有效的解决方案，推动了决策问题的进展。

Abstract: This paper reviews the topological groundwork for the study of reinforcement
learning (RL) by focusing on the structure of state, action, and policy spaces.
We begin by recalling key mathematical concepts such as complete metric spaces,
which form the foundation for expressing RL problems. By leveraging the Banach
contraction principle, we illustrate how the Banach fixed-point theorem
explains the convergence of RL algorithms and how Bellman operators, expressed
as operators on Banach spaces, ensure this convergence. The work serves as a
bridge between theoretical mathematics and practical algorithm design, offering
new approaches to enhance the efficiency of RL. In particular, we investigate
alternative formulations of Bellman operators and demonstrate their impact on
improving convergence rates and performance in standard RL environments such as
MountainCar, CartPole, and Acrobot. Our findings highlight how a deeper
mathematical understanding of RL can lead to more effective algorithms for
decision-making problems.

</details>


### [340] [KIPPO: Koopman-Inspired Proximal Policy Optimization](https://arxiv.org/abs/2505.14566)
*Andrei Cozma,Landon Harris,Hairong Qi*

Main category: cs.LG

TL;DR: 论文提出KIPPO方法，结合Koopman算子理论与PPO算法，在复杂非线性动态环境中提升策略学习的稳定性和性能。


<details>
  <summary>Details</summary>
Motivation: 传统策略梯度方法（如PPO）在复杂非线性动态环境中面临梯度估计高方差和非凸优化问题，导致学习不稳定。Koopman算子理论能通过线性化表示非线性系统，为改进策略学习提供新思路。

Method: 提出KIPPO方法，通过Koopman近似辅助网络学习系统动态的近似线性潜在空间表示，同时保留关键特征，无需改变核心策略或价值函数架构。

Result: 实验表明，KIPPO在连续控制任务中性能提升6-60%，同时减少高达91%的变异性，显著优于PPO基线。

Conclusion: KIPPO通过结合Koopman算子理论与策略优化，有效提升了复杂环境中策略学习的稳定性和性能，为强化学习提供了新方向。

Abstract: Reinforcement Learning (RL) has made significant strides in various domains,
and policy gradient methods like Proximal Policy Optimization (PPO) have gained
popularity due to their balance in performance, training stability, and
computational efficiency. These methods directly optimize policies through
gradient-based updates. However, developing effective control policies for
environments with complex and non-linear dynamics remains a challenge. High
variance in gradient estimates and non-convex optimization landscapes often
lead to unstable learning trajectories. Koopman Operator Theory has emerged as
a powerful framework for studying non-linear systems through an
infinite-dimensional linear operator that acts on a higher-dimensional space of
measurement functions. In contrast with their non-linear counterparts, linear
systems are simpler, more predictable, and easier to analyze. In this paper, we
present Koopman-Inspired Proximal Policy Optimization (KIPPO), which learns an
approximately linear latent-space representation of the underlying system's
dynamics while retaining essential features for effective policy learning. This
is achieved through a Koopman-approximation auxiliary network that can be added
to the baseline policy optimization algorithms without altering the
architecture of the core policy or value function. Extensive experimental
results demonstrate consistent improvements over the PPO baseline with 6-60%
increased performance while reducing variability by up to 91% when evaluated on
various continuous control tasks.

</details>


### [341] [Adaptive Pruning of Deep Neural Networks for Resource-Aware Embedded Intrusion Detection on the Edge](https://arxiv.org/abs/2505.14592)
*Alexandre Broggi,Nathaniel Bastian,Lance Fiondella,Gokhan Kul*

Main category: cs.LG

TL;DR: 该论文研究了多种人工神经网络剪枝方法在网络安全数据集上的泛化能力，发现大多数方法效果不佳，仅少数算法表现合格。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探索如何通过剪枝方法减小神经网络规模或提升推理速度，同时保持预测能力，并测试这些方法在新网络安全数据集上的适应性。

Method: 论文采用多种剪枝方法，并在不同剪枝程度下进行分析，以评估各算法在新环境中的表现。

Result: 结果显示，许多剪枝方法在新数据集上泛化能力较差，仅少数算法能达到可接受的性能水平。

Conclusion: 结论是并非所有剪枝方法都适用于新任务，需谨慎选择适合的算法以确保模型性能。

Abstract: Artificial neural network pruning is a method in which artificial neural
network sizes can be reduced while attempting to preserve the predicting
capabilities of the network. This is done to make the model smaller or faster
during inference time. In this work we analyze the ability of a selection of
artificial neural network pruning methods to generalize to a new cybersecurity
dataset utilizing a simpler network type than was designed for. We analyze each
method using a variety of pruning degrees to best understand how each algorithm
responds to the new environment. This has allowed us to determine the most well
fit pruning method of those we searched for the task. Unexpectedly, we have
found that many of them do not generalize to the problem well, leaving only a
few algorithms working to an acceptable degree.

</details>


### [342] [Physics-informed Reduced Order Modeling of Time-dependent PDEs via Differentiable Solvers](https://arxiv.org/abs/2505.14595)
*Nima Hosseini Dashtbayaz,Hesam Salehipour,Adrian Butscher,Nigel Morris*

Main category: cs.LG

TL;DR: 该论文提出了一种物理信息降阶模型（Φ-ROM），通过将可微分的PDE求解器整合到训练过程中，显著提升了模型在未见参数下的泛化能力和长期预测能力。


<details>
  <summary>Details</summary>
Motivation: 传统降阶模型（ROM）在训练过程中未充分利用高保真数值求解器的物理信息，导致潜在动力学偏离实际物理规律，限制了模型的泛化和预测能力。

Method: 提出Φ-ROM模型，通过在训练中直接嵌入可微分PDE求解器，使潜在空间的动力学行为严格遵循控制方程描述的物理规律。

Result: Φ-ROM在多个指标上优于现有数据驱动ROM方法，能够处理稀疏/不规则观测数据，并开源了基于JAX的实现框架。

Conclusion: 该方法通过物理约束的潜在空间学习，实现了对复杂系统更精确的建模和预测，为科学计算提供了灵活高效的建模框架。

Abstract: Reduced-order modeling (ROM) of time-dependent and parameterized differential
equations aims to accelerate the simulation of complex high-dimensional systems
by learning a compact latent manifold representation that captures the
characteristics of the solution fields and their time-dependent dynamics.
Although high-fidelity numerical solvers generate the training datasets, they
have thus far been excluded from the training process, causing the learned
latent dynamics to drift away from the discretized governing physics. This
mismatch often limits generalization and forecasting capabilities. In this
work, we propose Physics-informed ROM ($\Phi$-ROM) by incorporating
differentiable PDE solvers into the training procedure. Specifically, the
latent space dynamics and its dependence on PDE parameters are shaped directly
by the governing physics encoded in the solver, ensuring a strong
correspondence between the full and reduced systems. Our model outperforms
state-of-the-art data-driven ROMs and other physics-informed strategies by
accurately generalizing to new dynamics arising from unseen parameters,
enabling long-term forecasting beyond the training horizon, maintaining
continuity in both time and space, and reducing the data cost. Furthermore,
$\Phi$-ROM learns to recover and forecast the solution fields even when trained
or evaluated with sparse and irregular observations of the fields, providing a
flexible framework for field reconstruction and data assimilation. We
demonstrate the framework's robustness across different PDE solvers and
highlight its broad applicability by providing an open-source JAX
implementation readily extensible to other PDE systems and differentiable
solvers.

</details>


### [343] [CSTS: A Benchmark for the Discovery of Correlation Structures in Time Series Clustering](https://arxiv.org/abs/2505.14596)
*Isabella Degen,Zahraa S Abdallah,Henry W J Reeve,Kate Robson Brown*

Main category: cs.LG

TL;DR: 该论文提出了CSTS基准，用于评估多元时间序列数据中相关性结构的发现，帮助区分聚类失败的具体原因，并展示了其实际应用价值。


<details>
  <summary>Details</summary>
Motivation: 时间序列聚类缺乏已验证的基准信息，导致无法客观评估聚类质量或确定失败原因，使得聚类更像是艺术而非科学。

Method: 引入CSTS（时间序列中的相关性结构）合成基准，通过区分相关性结构退化与聚类算法和验证方法的局限性，提供清晰的评估标准。

Result: CSTS基准能够有效识别聚类算法的局限性，如对非正态分布的敏感性，并展示了相关性结构在降采样、分布偏移和稀疏化中的稳健性。

Conclusion: CSTS为基于相关性的时间序列聚类提供了严格的评估标准，能够精确诊断方法局限性，推动该领域的科学发展。

Abstract: Time series clustering promises to uncover hidden structural patterns in data
with applications across healthcare, finance, industrial systems, and other
critical domains. However, without validated ground truth information,
researchers cannot objectively assess clustering quality or determine whether
poor results stem from absent structures in the data, algorithmic limitations,
or inappropriate validation methods, raising the question whether clustering is
"more art than science" (Guyon et al., 2009). To address these challenges, we
introduce CSTS (Correlation Structures in Time Series), a synthetic benchmark
for evaluating the discovery of correlation structures in multivariate time
series data. CSTS provides a clean benchmark that enables researchers to
isolate and identify specific causes of clustering failures by differentiating
between correlation structure deterioration and limitations of clustering
algorithms and validation methods. Our contributions are: (1) a comprehensive
benchmark for correlation structure discovery with distinct correlation
structures, systematically varied data conditions, established performance
thresholds, and recommended evaluation protocols; (2) empirical validation of
correlation structure preservation showing moderate distortion from
downsampling and minimal effects from distribution shifts and sparsification;
and (3) an extensible data generation framework enabling structure-first
clustering evaluation. A case study demonstrates CSTS's practical utility by
identifying an algorithm's previously undocumented sensitivity to non-normal
distributions, illustrating how the benchmark enables precise diagnosis of
methodological limitations. CSTS advances rigorous evaluation standards for
correlation-based time series clustering.

</details>


### [344] [Electrostatics from Laplacian Eigenbasis for Neural Network Interatomic Potentials](https://arxiv.org/abs/2505.14606)
*Maksim Zhdanov,Vladislav Kurenkov*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Recent advances in neural network interatomic potentials have emerged as a
promising research direction. However, popular deep learning models often lack
auxiliary constraints grounded in physical laws, which could accelerate
training and improve fidelity through physics-based regularization. In this
work, we introduce $\Phi$-Module, a universal plugin module that enforces
Poisson's equation within the message-passing framework to learn electrostatic
interactions in a self-supervised manner. Specifically, each atom-wise
representation is encouraged to satisfy a discretized Poisson's equation,
making it possible to acquire a potential $\boldsymbol{\phi}$ and a
corresponding charge density $\boldsymbol{\rho}$ linked to the learnable
Laplacian eigenbasis coefficients of a given molecular graph. We then derive an
electrostatic energy term, crucial for improved total energy predictions. This
approach integrates seamlessly into any existing neural potential with
insignificant computational overhead. Experiments on the OE62 and MD22
benchmarks confirm that models combined with $\Phi$-Module achieve robust
improvements over baseline counterparts. For OE62 error reduction ranges from
4.5\% to 17.8\%, and for MD22, baseline equipped with $\Phi$-Module achieves
best results on 5 out of 14 cases. Our results underscore how embedding a
first-principles constraint in neural interatomic potentials can significantly
improve performance while remaining hyperparameter-friendly, memory-efficient
and lightweight in training. Code will be available at
\href{https://github.com/dunnolab/phi-module}{dunnolab/phi-module}.

</details>


### [345] [MMD-Newton Method for Multi-objective Optimization](https://arxiv.org/abs/2505.14610)
*Hao Wang,Chenyu Shi,Angel E. Rodriguez-Fernandez,Oliver Schütze*

Main category: cs.LG

TL;DR: 该论文提出使用最大均值差异(MMD)解决连续多目标优化问题，并开发了基于MMD的牛顿方法(MMDN)，通过与进化算法(MOEAs)混合使用，显著提升了优化精度。


<details>
  <summary>Details</summary>
Motivation: 多目标优化问题(MOPs)中，常用Hausdorff距离衡量近似解集与参考集的距离，但该方法存在局限性。论文提出使用MMD作为更优的距离度量方式，并开发高效优化方法。

Method: 1) 用MMD度量解集与参考集的距离；2) 推导MMD梯度和Hessian矩阵解析式；3) 提出MMDN牛顿法；4) 与MOEAs混合使用，先用EA粗搜索再用MMDN精调。

Result: 在11个基准测试问题上，MMDN+MOEA混合方法比单独使用EA在相同计算预算下获得显著更优的优化精度，验证了MMD梯度和Hessian理论分析的正确性。

Conclusion: MMD是解决MOPs的有效距离度量，MMDN方法能高效优化解集质量，与进化算法混合使用可发挥各自优势，为多目标优化提供了新思路。

Abstract: Maximum mean discrepancy (MMD) has been widely employed to measure the
distance between probability distributions. In this paper, we propose using MMD
to solve continuous multi-objective optimization problems (MOPs). For solving
MOPs, a common approach is to minimize the distance (e.g., Hausdorff) between a
finite approximate set of the Pareto front and a reference set. Viewing these
two sets as empirical measures, we propose using MMD to measure the distance
between them. To minimize the MMD value, we provide the analytical expression
of its gradient and Hessian matrix w.r.t. the search variables, and use them to
devise a novel set-oriented, MMD-based Newton (MMDN) method. Also, we analyze
the theoretical properties of MMD's gradient and Hessian, including the
first-order stationary condition and the eigenspectrum of the Hessian, which
are important for verifying the correctness of MMDN. To solve complicated
problems, we propose hybridizing MMDN with multiobjective evolutionary
algorithms (MOEAs), where we first execute an EA for several iterations to get
close to the global Pareto front and then warm-start MMDN with the result of
the MOEA to efficiently refine the approximation. We empirically test the
hybrid algorithm on 11 widely used benchmark problems, and the results show the
hybrid (MMDN + MOEA) can achieve a much better optimization accuracy than EA
alone with the same computation budget.

</details>


### [346] [Virtual Cells: Predict, Explain, Discover](https://arxiv.org/abs/2505.14613)
*Emmanuel Noutahi,Jason Hartford,Prudencio Tossou,Shawn Whitfield,Alisandra K. Denton,Cas Wognum,Kristina Ulicna,Jonathan Hsu,Michael Cuccarese,Emmanuel Bengio,Dominique Beaini,Christopher Gibson,Daniel Cohen,Berton Earnshaw*

Main category: cs.LG

TL;DR: 本文提出利用AI和计算技术开发虚拟细胞模型，以预测细胞对扰动的功能响应，从而加速药物发现过程。


<details>
  <summary>Details</summary>
Motivation: 药物发现需要可靠的患者响应模拟模型，以低成本高效地测试治疗假设，但目前仍缺乏能准确模拟细胞功能的虚拟细胞模型。

Method: 提出基于AI、计算能力和高通量细胞分析的技术，设计治疗相关的虚拟细胞，并通过实验室闭环方法生成新见解。

Result: 虚拟细胞需准确预测细胞对扰动的功能响应，并解释其分子机制，为药物发现提供框架。

Conclusion: 虚拟细胞模型为药物发现提供了新方向，并可扩展至更高层次的组织模型，如虚拟患者。

Abstract: Drug discovery is fundamentally a process of inferring the effects of
treatments on patients, and would therefore benefit immensely from
computational models that can reliably simulate patient responses, enabling
researchers to generate and test large numbers of therapeutic hypotheses safely
and economically before initiating costly clinical trials. Even a more specific
model that predicts the functional response of cells to a wide range of
perturbations would be tremendously valuable for discovering safe and effective
treatments that successfully translate to the clinic. Creating such virtual
cells has long been a goal of the computational research community that
unfortunately remains unachieved given the daunting complexity and scale of
cellular biology. Nevertheless, recent advances in AI, computing power, lab
automation, and high-throughput cellular profiling provide new opportunities
for reaching this goal. In this perspective, we present a vision for developing
and evaluating virtual cells that builds on our experience at Recursion. We
argue that in order to be a useful tool to discover novel biology, virtual
cells must accurately predict the functional response of a cell to
perturbations and explain how the predicted response is a consequence of
modifications to key biomolecular interactions. We then introduce key
principles for designing therapeutically-relevant virtual cells, describe a
lab-in-the-loop approach for generating novel insights with them, and advocate
for biologically-grounded benchmarks to guide virtual cell development.
Finally, we make the case that our approach to virtual cells provides a useful
framework for building other models at higher levels of organization, including
virtual patients. We hope that these directions prove useful to the research
community in developing virtual models optimized for positive impact on drug
discovery outcomes.

</details>


### [347] [Enhancing Learned Knowledge in LoRA Adapters Through Efficient Contrastive Decoding on Ascend NPUs](https://arxiv.org/abs/2505.14620)
*Morgan Lindsay Heisler,Linzi Xing,Ge Shi,Hanieh Sadri,Gursimran Singh,Weiwei Zhang,Tao Ye,Ying Xiong,Yong Zhang,Zhenan Fan*

Main category: cs.LG

TL;DR: 华为云用户使用LoRA高效微调大语言模型，但复杂推理任务受基础模型偏差影响。本文提出CoLD解码框架，通过对比解码提升任务特定知识利用，优化性能并降低延迟。


<details>
  <summary>Details</summary>
Motivation: 传统解码方法（如贪婪或束搜索）在复杂推理任务中易受基础模型偏差干扰，导致通用响应而非利用LoRA特定适配。需一种方法最大化LoRA适配模型的任务特定知识。

Method: 提出对比LoRA解码（CoLD），通过对比LoRA专家模型与基础模型的概率分布差异评分候选词，优先选择与LoRA学习表示对齐的词。为提升效率，开发了华为Ascend NPU优化内核。

Result: CoLD相比贪婪解码，任务准确率提升5.54%，端到端延迟降低28%。

Conclusion: CoLD为资源受限环境中的微调大语言模型提供了高效解码策略，对云和本地应用数据科学具有广泛意义。

Abstract: Huawei Cloud users leverage LoRA (Low-Rank Adaptation) as an efficient and
scalable method to fine-tune and customize large language models (LLMs) for
application-specific needs. However, tasks that require complex reasoning or
deep contextual understanding are often hindered by biases or interference from
the base model when using typical decoding methods like greedy or beam search.
These biases can lead to generic or task-agnostic responses from the base model
instead of leveraging the LoRA-specific adaptations. In this paper, we
introduce Contrastive LoRA Decoding (CoLD), a novel decoding framework designed
to maximize the use of task-specific knowledge in LoRA-adapted models,
resulting in better downstream performance. CoLD uses contrastive decoding by
scoring candidate tokens based on the divergence between the probability
distributions of a LoRA-adapted expert model and the corresponding base model.
This approach prioritizes tokens that better align with the LoRA's learned
representations, enhancing performance for specialized tasks. While effective,
a naive implementation of CoLD is computationally expensive because each
decoding step requires evaluating multiple token candidates across both models.
To address this, we developed an optimized kernel for Huawei's Ascend NPU. CoLD
achieves up to a 5.54% increase in task accuracy while reducing end-to-end
latency by 28% compared to greedy decoding. This work provides practical and
efficient decoding strategies for fine-tuned LLMs in resource-constrained
environments and has broad implications for applied data science in both cloud
and on-premises settings.

</details>


### [348] [TinyV: Reducing False Negatives in Verification Improves RL for LLM Reasoning](https://arxiv.org/abs/2505.14625)
*Zhangchen Xu,Yuetai Li,Fengqing Jiang,Bhaskar Ramasubramanian,Luyao Niu,Bill Yuchen Lin,Radha Poovendran*

Main category: cs.LG

TL;DR: 论文揭示强化学习中验证器错误否定问题，提出轻量级验证器tinyV以提升奖励信号准确性，显著改善模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习依赖验证器提供可靠奖励信号，但研究发现验证器存在高比例错误否定（38%正确输出被误判），严重影响模型训练效果。

Method: 提出tinyV——基于轻量级LLM的验证器，动态识别潜在错误否定并恢复有效响应，与规则方法结合提升奖励估计精度。

Result: 在数学推理任务中，tinyV使通过率最高提升10%，并加速模型收敛速度。

Conclusion: 解决验证器错误否定对RL调优至关重要，tinyV为提升LLM微调效果提供了实用方案。

Abstract: Reinforcement Learning (RL) has become a powerful tool for enhancing the
reasoning abilities of large language models (LLMs) by optimizing their
policies with reward signals. Yet, RL's success relies on the reliability of
rewards, which are provided by verifiers. In this paper, we expose and analyze
a widespread problem--false negatives--where verifiers wrongly reject correct
model outputs. Our in-depth study of the Big-Math-RL-Verified dataset reveals
that over 38% of model-generated responses suffer from false negatives, where
the verifier fails to recognize correct answers. We show, both empirically and
theoretically, that these false negatives severely impair RL training by
depriving the model of informative gradient signals and slowing convergence. To
mitigate this, we propose tinyV, a lightweight LLM-based verifier that augments
existing rule-based methods, which dynamically identifies potential false
negatives and recovers valid responses to produce more accurate reward
estimates. Across multiple math-reasoning benchmarks, integrating TinyV boosts
pass rates by up to 10% and accelerates convergence relative to the baseline.
Our findings highlight the critical importance of addressing verifier false
negatives and offer a practical approach to improve RL-based fine-tuning of
LLMs. Our code is available at https://github.com/uw-nsl/TinyV.

</details>


### [349] [KERL: Knowledge-Enhanced Personalized Recipe Recommendation using Large Language Models](https://arxiv.org/abs/2505.14629)
*Fnu Mohbat,Mohammed J Zaki*

Main category: cs.LG

TL;DR: KERL系统结合食物知识图谱与大语言模型，提供个性化食谱推荐及营养分析，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前虽有大语言模型和知识图谱在推荐系统中的应用，但食物相关领域的研究较少，缺乏整合两者的解决方案。

Method: KERL通过提取自然语言问题中的实体，检索知识图谱子图，并输入大语言模型生成符合约束的食谱及营养信息。

Result: 实验表明，KERL显著优于现有方法，提供了完整的食谱推荐、生成及营养分析解决方案。

Conclusion: KERL为食物推荐领域提供了高效、连贯的解决方案，代码和数据集已开源。

Abstract: Recent advances in large language models (LLMs) and the abundance of food
data have resulted in studies to improve food understanding using LLMs. Despite
several recommendation systems utilizing LLMs and Knowledge Graphs (KGs), there
has been limited research on integrating food related KGs with LLMs. We
introduce KERL, a unified system that leverages food KGs and LLMs to provide
personalized food recommendations and generates recipes with associated
micro-nutritional information. Given a natural language question, KERL extracts
entities, retrieves subgraphs from the KG, which are then fed into the LLM as
context to select the recipes that satisfy the constraints. Next, our system
generates the cooking steps and nutritional information for each recipe. To
evaluate our approach, we also develop a benchmark dataset by curating recipe
related questions, combined with constraints and personal preferences. Through
extensive experiments, we show that our proposed KG-augmented LLM significantly
outperforms existing approaches, offering a complete and coherent solution for
food recommendation, recipe generation, and nutritional analysis. Our code and
benchmark datasets are publicly available at
https://github.com/mohbattharani/KERL.

</details>


### [350] [Bridging Predictive Coding and MDL: A Two-Part Code Framework for Deep Learning](https://arxiv.org/abs/2505.14635)
*Benjamin Prada,Shion Matsumoto,Abdul Malik Zekri,Ankur Mali*

Main category: cs.LG

TL;DR: 该论文首次建立了预测编码（PC）与深度学习中的最小描述长度（MDL）原则的理论联系，证明了PC通过块坐标下降优化MDL目标，并提供泛化保证。


<details>
  <summary>Details</summary>
Motivation: 研究旨在为生物启发的预测编码学习规则提供理论依据，将其与MDL原则结合，以替代传统的反向传播方法。

Method: 通过Hoeffding不等式和前缀编码先验，推导泛化误差界，并证明PC更新单调降低经验编码长度。

Result: PC训练能收敛到块坐标稳定点，提供近似MDL最优解，其泛化误差界比梯度下降更紧。

Conclusion: PC作为理论可靠且生物合理的替代方案，首次为深度学习模型提供了形式化的泛化和收敛保证。

Abstract: We present the first theoretical framework that connects predictive coding
(PC), a biologically inspired local learning rule, with the minimum description
length (MDL) principle in deep networks. We prove that layerwise PC performs
block-coordinate descent on the MDL two-part code objective, thereby jointly
minimizing empirical risk and model complexity. Using Hoeffding's inequality
and a prefix-code prior, we derive a novel generalization bound of the form
$R(\theta) \le \^{R}(\theta) + \frac{L(\theta)}{N}$, capturing the tradeoff
between fit and compression. We further prove that each PC sweep monotonically
decreases the empirical two-part codelength, yielding tighter high-probability
risk bounds than unconstrained gradient descent. Finally, we show that repeated
PC updates converge to a block-coordinate stationary point, providing an
approximate MDL-optimal solution. To our knowledge, this is the first result
offering formal generalization and convergence guarantees for PC-trained deep
models, positioning PC as a theoretically grounded and biologically plausible
alternative to backpropagation.

</details>


### [351] [Early Diagnosis of Atrial Fibrillation Recurrence: A Large Tabular Model Approach with Structured and Unstructured Clinical Data](https://arxiv.org/abs/2505.14643)
*Ane G. Domingo-Aldama,Marcos Merino Prado,Alain García Olea,Koldo Gojenola Galletebeitia,Josu Goikoetxea Salutregi,Aitziber Atutxa Salazar*

Main category: cs.LG

TL;DR: 该研究通过结合结构化临床数据和自由文本出院报告，利用机器学习模型（尤其是LTM方法）提高了房颤复发的预测准确性，超越了传统评分系统。


<details>
  <summary>Details</summary>
Motivation: 房颤（AF）是一种常见且高发病率、死亡率的 arrhythmia。传统预测评分（如CHADS2-VASc、HATCH和APPLE）准确性有限，且电子健康记录（EHR）数据可能存在错误和缺失。因此，研究旨在开发更准确的预测方法。

Method: 研究结合结构化临床数据和通过自然语言处理技术处理的自由文本出院报告，生成高质量表格数据集。评估了传统临床评分、机器学习模型及提出的LTM方法在1,508名AF患者中的表现。

Result: 提出的LTM方法预测性能最高，超越了传统临床评分和其他机器学习模型。此外，性别和年龄偏差分析揭示了人口统计学差异。

Conclusion: 结合结构化和非结构化数据可生成高质量数据集。研究结果强调了传统临床评分在预测AF复发中的局限性，并突出了基于机器学习的方法（尤其是LTM模型）的潜力。

Abstract: BACKGROUND: Atrial fibrillation (AF), the most common arrhythmia, is linked
to high morbidity and mortality. In a fast-evolving AF rhythm control treatment
era, predicting AF recurrence after its onset may be crucial to achieve the
optimal therapeutic approach, yet traditional scores like CHADS2-VASc, HATCH,
and APPLE show limited predictive accuracy. Moreover, early diagnosis studies
often rely on codified electronic health record (EHR) data, which may contain
errors and missing information.
  OBJECTIVE: This study aims to predict AF recurrence between one month and two
years after onset by evaluating traditional clinical scores, ML models, and our
LTM approach. Moreover, another objective is to develop a methodology for
integrating structured and unstructured data to enhance tabular dataset
quality.
  METHODS: A tabular dataset was generated by combining structured clinical
data with free-text discharge reports processed through natural language
processing techniques, reducing errors and annotation effort. A total of 1,508
patients with documented AF onset were identified, and models were evaluated on
a manually annotated test set. The proposed approach includes a LTM compared
against traditional clinical scores and ML models.
  RESULTS: The proposed LTM approach achieved the highest predictive
performance, surpassing both traditional clinical scores and ML models.
Additionally, the gender and age bias analyses revealed demographic
disparities.
  CONCLUSION: The integration of structured data and free-text sources resulted
in a high-quality dataset. The findings emphasize the limitations of
traditional clinical scores in predicting AF recurrence and highlight the
potential of ML-based approaches, particularly our LTM model.

</details>


### [352] [Explainable AI for Securing Healthcare in IoT-Integrated 6G Wireless Networks](https://arxiv.org/abs/2505.14659)
*Navneet Kaur,Lav Gupta*

Main category: cs.LG

TL;DR: 论文探讨了在6G医疗应用中，如何利用可解释AI技术（如SHAP、LIME和DiCE）提升安全性和透明度。


<details>
  <summary>Details</summary>
Motivation: 随着医疗系统越来越多地采用先进的无线网络和连接设备，医疗应用的安全问题变得至关重要。物联网医疗设备的集成虽然提升了患者护理，但也带来了严重的安全风险，如手术错误、设备故障和数据泄露。

Method: 论文采用了可解释AI技术（SHAP、LIME和DiCE）来识别漏洞并加强防御，同时通过实验分析验证方法的有效性。

Result: 实验分析展示了可解释AI技术在提升6G医疗应用安全性和透明度方面的潜力，并取得了积极的结果。

Conclusion: 可解释AI技术能够有效增强6G医疗应用的安全防御，并提高信任和透明度，为未来医疗系统的安全发展提供了重要方向。

Abstract: As healthcare systems increasingly adopt advanced wireless networks and
connected devices, securing medical applications has become critical. The
integration of Internet of Medical Things devices, such as robotic surgical
tools, intensive care systems, and wearable monitors has enhanced patient care
but introduced serious security risks. Cyberattacks on these devices can lead
to life threatening consequences, including surgical errors, equipment failure,
and data breaches. While the ITU IMT 2030 vision highlights 6G's transformative
role in healthcare through AI and cloud integration, it also raises new
security concerns. This paper explores how explainable AI techniques like SHAP,
LIME, and DiCE can uncover vulnerabilities, strengthen defenses, and improve
trust and transparency in 6G enabled healthcare. We support our approach with
experimental analysis and highlight promising results.

</details>


### [353] [Quartet: Native FP4 Training Can Be Optimal for Large Language Models](https://arxiv.org/abs/2505.14669)
*Roberto L. Castro,Andrei Panferov,Soroush Tabesh,Oliver Sieberling,Jiale Chen,Mahdi Nikdan,Saleh Ashkboos,Dan Alistarh*

Main category: cs.LG

TL;DR: 论文提出Quartet方法，实现全FP4精度的大语言模型训练，在保持精度的同时显著提升计算效率。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型训练的计算需求急剧增长，低精度算术训练可提高计算吞吐和能效，但现有FP4训练方法存在精度下降问题。

Method: 系统研究硬件支持的FP4训练，提出Quartet方法，通过优化的CUDA内核实现端到端FP4训练，并发现低精度缩放定律。

Result: Quartet在Llama类模型上实现FP4精度的最先进准确率，成功训练十亿级模型，证明全FP4训练是标准精度和FP8训练的有力替代。

Conclusion: Quartet展示了全FP4训练的可行性，为高效大语言模型训练提供了新方向。

Abstract: The rapid advancement of large language models (LLMs) has been paralleled by
unprecedented increases in computational demands, with training costs for
state-of-the-art models doubling every few months. Training models directly in
low-precision arithmetic offers a solution, by improving both computational
throughput and energy efficiency. Specifically, NVIDIA's recent Blackwell
architecture facilitates extremely low-precision operations, specifically FP4
variants, promising substantial efficiency gains. Yet, current algorithms for
training LLMs in FP4 precision face significant accuracy degradation and often
rely on mixed-precision fallbacks. In this paper, we systematically investigate
hardware-supported FP4 training and introduce Quartet, a new approach enabling
accurate, end-to-end FP4 training with all the major computations (in e.g.
linear layers) being performed in low precision. Through extensive evaluations
on Llama-type models, we reveal a new low-precision scaling law that quantifies
performance trade-offs across varying bit-widths and allows us to identify a
"near-optimal" low-precision training technique in terms of
accuracy-vs-computation, called Quartet. We implement Quartet using optimized
CUDA kernels tailored for NVIDIA Blackwell GPUs, and show that it can achieve
state-of-the-art accuracy for FP4 precision, successfully training
billion-scale models. Our method demonstrates that fully FP4-based training is
a competitive alternative to standard-precision and FP8 training. Our code is
available at https://github.com/IST-DASLab/Quartet.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [354] [Autonomous nanoparticle synthesis by design](https://arxiv.org/abs/2505.13571)
*Andy S. Anker,Jonas H. Jensen,Miguel Gonzalez-Duque,Rodrigo Moreno,Aleksandra Smolska,Mikkel Juelsholt,Vincent Hardion,Mads R. V. Jorgensen,Andres Faina,Jonathan Quinson,Kasper Stoy,Tejs Vegge*

Main category: cond-mat.mtrl-sci

TL;DR: 该论文提出了一种自主设计纳米颗粒合成协议的方法，通过实时匹配实验数据与模拟目标模式，无需先验知识，成功合成了两种不同结构的金纳米颗粒。


<details>
  <summary>Details</summary>
Motivation: 传统材料合成依赖试错法，纳米颗粒因其复杂参数调控尤为困难。本文旨在开发一种自主方法，直接针对原子级结构进行合成，以克服这一挑战。

Method: 利用实时实验总散射（TS）和对分布函数（PDF）数据，与模拟目标模式匹配，自主设计合成协议。

Result: 在同步辐射实验中成功合成了5纳米十面体和10纳米面心立方结构的金纳米颗粒，验证了方法的有效性。

Conclusion: 该方法通过指定目标散射模式实现定制原子结构合成，为材料设计提供了革命性蓝图，ScatterLab框架可推广至多种系统和应用。

Abstract: Controlled synthesis of materials with specified atomic structures underpins
technological advances yet remains reliant on iterative, trial-and-error
approaches. Nanoparticles (NPs), whose atomic arrangement dictates their
emergent properties, are particularly challenging to synthesise due to numerous
tunable parameters. Here, we introduce an autonomous approach explicitly
targeting synthesis of atomic-scale structures. Our method autonomously designs
synthesis protocols by matching real time experimental total scattering (TS)
and pair distribution function (PDF) data to simulated target patterns, without
requiring prior synthesis knowledge. We demonstrate this capability at a
synchrotron, successfully synthesising two structurally distinct gold NPs: 5 nm
decahedral and 10 nm face-centred cubic structures. Ultimately, specifying a
simulated target scattering pattern, thus representing a bespoke atomic
structure, and obtaining both the synthesised material and its reproducible
synthesis protocol on demand may revolutionise materials design. Thus,
ScatterLab provides a generalisable blueprint for autonomous, atomic
structure-targeted synthesis across diverse systems and applications.

</details>


### [355] [Path-integral molecular dynamics with actively-trained and universal machine learning force fields](https://arxiv.org/abs/2505.14245)
*A. A. Solovykh,N. E. Rybin,I. S. Novikov,A. V. Shapeev*

Main category: cond-mat.mtrl-sci

TL;DR: 该论文开发了一个接口，将MLIP-2软件包中的矩张量势（MTP）与i-PI软件包结合，用于路径积分分子动力学（PIMD）计算，以研究核量子效应对材料性质的影响，并展示了其高精度和有效性。


<details>
  <summary>Details</summary>
Motivation: 核量子效应（NQEs）在有限温度下会显著改变材料性质。传统方法中，经验势计算速度快但精度不足，而量子力学计算精度高但计算成本大。机器学习原子间势能提供了一种兼顾精度和效率的解决方案。

Method: 开发了一个接口，将MLIP-2中的矩张量势（MTP）集成到i-PI软件包中，用于PIMD计算。该方法应用于锂氢化物（LiH）和硅（Si）系统的活性学习势能，并研究了NQEs对晶格参数、热膨胀系数和径向分布函数的影响。

Result: 通过与实验数据、准谐近似计算和MatterSim机器学习力场的预测比较，验证了MTP-PIMD方法的高精度和有效性。

Conclusion: MTP-PIMD方法在计算效率和精度之间取得了良好平衡，为研究核量子效应提供了一种有效的工具。

Abstract: Accounting for nuclear quantum effects (NQEs) can significantly alter
material properties at finite temperatures. Atomic modeling using the
path-integral molecular dynamics (PIMD) method can fully account for such
effects, but requires computationally efficient and accurate models of
interatomic interactions. Empirical potentials are fast but may lack sufficient
accuracy, whereas quantum-mechanical calculations are highly accurate but
computationally expensive. Machine-learned interatomic potentials offer a
solution to this challenge, providing near-quantum-mechanical accuracy while
maintaining high computational efficiency compared to density functional theory
(DFT) calculations. In this context, an interface was developed to integrate
moment tensor potentials (MTPs) from the MLIP-2 software package into PIMD
calculations using the i-PI software package. This interface was then applied
to active learning of potentials and to investigate the influence of NQEs on
material properties, namely the temperature dependence of lattice parameters
and thermal expansion coefficients, as well as radial distribution functions,
for lithium hydride (LiH) and silicon (Si) systems. The results were compared
with experimental data, quasi-harmonic approximation calculations, and
predictions from the universal machine learning force field MatterSim. These
comparisons demonstrated the high accuracy and effectiveness of the MTP-PIMD
approach.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [356] [Low-Cost FlashAttention with Fused Exponential and Multiplication Hardware Operators](https://arxiv.org/abs/2505.14314)
*Kosmas Alexandridis,Vasileios Titopoulos,Giorgos Dimitrakopoulos*

Main category: cs.AR

TL;DR: 该论文提出了一种通过硬件优化改进FlashAttention内核的方法，使用新的硬件操作符ExpMul融合指数和向量乘法运算，显著降低了面积和功耗。


<details>
  <summary>Details</summary>
Motivation: 随着序列长度的增加，注意力机制的计算需求急剧上升，现有的硬件架构在计算指数和向量乘法时存在效率瓶颈，因此需要优化硬件设计以提升性能并降低成本。

Method: 论文提出了一种新的硬件操作符ExpMul，将指数运算（e^x）和向量乘法（V）融合在一起，优化了FlashAttention内核的计算流程，减少了内存访问和硬件资源消耗。

Result: 在28nm ASIC技术中实现时，与现有硬件架构相比，ExpMul操作符平均减少了28.8%的面积和17.6%的功耗。

Conclusion: 通过硬件优化，特别是融合指数和向量乘法运算，可以显著提升FlashAttention内核的效率，为长序列处理提供更高效的硬件加速方案。

Abstract: Attention mechanisms, particularly within Transformer architectures and large
language models (LLMs), have revolutionized sequence modeling in machine
learning and artificial intelligence applications. To compute attention for
increasingly long sequences, specialized accelerators have been proposed to
execute key attention steps directly in hardware. Among the various recently
proposed architectures, those based on variants of the FlashAttention
algorithm, originally designed for GPUs, stand out due to their optimized
computation, tiling capabilities, and reduced memory traffic. In this work, we
focus on optimizing the kernel of floating-point-based FlashAttention using new
hardware operators that fuse the computation of exponentials and vector
multiplications, e.g., e^x, V. The proposed ExpMul hardware operators
significantly reduce the area and power costs of FlashAttention-based hardware
accelerators. When implemented in a 28nm ASIC technology, they achieve
improvements of 28.8% in area and 17.6% in power, on average, compared to
state-of-the-art hardware architectures with separate exponentials and vector
multiplications hardware operators.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [357] [Optimizing Binary and Ternary Neural Network Inference on RRAM Crossbars using CIM-Explorer](https://arxiv.org/abs/2505.14303)
*Rebecca Pelke,José Cubero-Cascante,Nils Bosbach,Niklas Degener,Florian Idrizi,Lennart M. Reimann,Jan Moritz Joseph,Rainer Leupers*

Main category: cs.ET

TL;DR: 论文提出了CIM-Explorer工具包，用于优化基于RRAM存内计算架构的二元和三元神经网络推理，解决了现有工具单一功能及8位量化限制的问题。


<details>
  <summary>Details</summary>
Motivation: 现有RRAM存内计算软件工具功能单一（仅支持编译、模拟或设计空间探索之一），且多依赖传统8位量化，无法充分利用RRAM二元模式的高效性。

Method: 开发模块化工具包CIM-Explorer，集成端到端编译器、多映射选项和模拟器，支持从早期精度评估到芯片部署的全流程设计空间探索。

Result: 通过案例研究展示了不同映射方式和交叉棒参数下的预期精度，工具已开源在GitHub。

Conclusion: CIM-Explorer为二元/三元神经网络在RRAM交叉棒上的高效部署提供了全流程解决方案，填补了现有工具链的空白。

Abstract: Using Resistive Random Access Memory (RRAM) crossbars in Computing-in-Memory
(CIM) architectures offers a promising solution to overcome the von Neumann
bottleneck. Due to non-idealities like cell variability, RRAM crossbars are
often operated in binary mode, utilizing only two states: Low Resistive State
(LRS) and High Resistive State (HRS). Binary Neural Networks (BNNs) and Ternary
Neural Networks (TNNs) are well-suited for this hardware due to their efficient
mapping. Existing software projects for RRAM-based CIM typically focus on only
one aspect: compilation, simulation, or Design Space Exploration (DSE).
Moreover, they often rely on classical 8 bit quantization. To address these
limitations, we introduce CIM-Explorer, a modular toolkit for optimizing BNN
and TNN inference on RRAM crossbars. CIM-Explorer includes an end-to-end
compiler stack, multiple mapping options, and simulators, enabling a DSE flow
for accuracy estimation across different crossbar parameters and mappings.
CIM-Explorer can accompany the entire design process, from early accuracy
estimation for specific crossbar parameters, to selecting an appropriate
mapping, and compiling BNNs and TNNs for a finalized crossbar chip. In DSE case
studies, we demonstrate the expected accuracy for various mappings and crossbar
parameters. CIM-Explorer can be found on GitHub.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [358] [Performance Optimization of Energy-Harvesting Underlay Cognitive Radio Networks Using Reinforcement Learning](https://arxiv.org/abs/2505.14581)
*Deemah H. Tashman,Soumaya Cherkaoui,Walaa Hamouda*

Main category: eess.SP

TL;DR: 该论文采用强化学习技术优化认知无线电网络性能，通过能量收集和深度Q网络决策实现次级用户数据速率最大化。


<details>
  <summary>Details</summary>
Motivation: 在存在主用户的情况下，次级用户需要高效利用有限能量资源进行通信，同时避免干扰主用户。

Method: 结合时间切换能量收集和深度Q网络，动态选择能量来源（主用户干扰或环境射频）并优化传输功率决策。

Result: 所提方法在平均数据速率上优于基线策略，并展现出良好的收敛性。

Conclusion: 强化学习方法能有效解决认知无线电网络中能量受限设备的资源分配问题。

Abstract: In this paper, a reinforcement learning technique is employed to maximize the
performance of a cognitive radio network (CRN). In the presence of primary
users (PUs), it is presumed that two secondary users (SUs) access the licensed
band within underlay mode. In addition, the SU transmitter is assumed to be an
energy-constrained device that requires harvesting energy in order to transmit
signals to their intended destination. Therefore, we propose that there are two
main sources of energy; the interference of PUs' transmissions and ambient
radio frequency (RF) sources. The SU will select whether to gather energy from
PUs or only from ambient sources based on a predetermined threshold. The
process of energy harvesting from the PUs' messages is accomplished via the
time switching approach. In addition, based on a deep Q-network (DQN) approach,
the SU transmitter determines whether to collect energy or transmit messages
during each time slot as well as selects the suitable transmission power in
order to maximize its average data rate. Our approach outperforms a baseline
strategy and converges, as shown by our findings.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [359] [Federated prediction for scalable and privacy-preserved knowledge-based planning in radiotherapy](https://arxiv.org/abs/2505.14507)
*Jingyun Chen,David Horowitz,Yading Yuan*

Main category: cs.DC

TL;DR: 该论文提出了FedKBP+，一个用于放射治疗计划预测任务的联邦学习平台，旨在解决数据稀缺和隐私问题，通过集中式和去中心化策略实现高效通信。


<details>
  <summary>Details</summary>
Motivation: 深度学习在放射治疗计划中有潜力提高效率和一致性，但由于数据稀缺和机构间异质性，临床采用受限。数据共享因隐私和技术障碍难以实现，因此需要一种无需集中数据的解决方案。

Method: 开发了FedKBP+平台，基于gRPC实现统一通信栈，支持集中式和完全去中心化的联邦学习策略，参与者可直接通过点对点通信交换模型权重。使用SA-Net作为预测模型，在三个预测任务上进行了评估。

Result: FedKBP+表现出高效性、有效性和鲁棒性，证明了其作为放射治疗联邦学习平台的巨大潜力。

Conclusion: FedKBP+通过创新的通信和协作机制，成功解决了数据隐私和共享的挑战，为放射治疗计划的预测任务提供了可行的解决方案。

Abstract: Background: Deep learning has potential to improve the efficiency and
consistency of radiation therapy planning, but clinical adoption is hindered by
the limited model generalizability due to data scarcity and heterogeneity among
institutions. Although aggregating data from different institutions could
alleviate this problem, data sharing is a practical challenge due to concerns
about patient data privacy and other technical obstacles. Purpose: This work
aims to address this dilemma by developing FedKBP+, a comprehensive federated
learning (FL) platform for predictive tasks in real-world applications in
radiotherapy treatment planning. Methods: We implemented a unified
communication stack based on Google Remote Procedure Call (gRPC) to support
communication between participants whether located on the same workstation or
distributed across multiple workstations. In addition to supporting the
centralized FL strategies commonly available in existing open-source
frameworks, FedKBP+ also provides a fully decentralized FL model where
participants directly exchange model weights to each other through Peer-to-Peer
communication. We evaluated FedKBP+ on three predictive tasks using
scale-attention network (SA-Net) as the predictive model. Conclusions: Our
results demonstrate that FedKBP+ is highly effective, efficient and robust,
showing great potential as a federated learning platform for radiation therapy.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [360] [HALO: Hierarchical Autonomous Logic-Oriented Orchestration for Multi-Agent LLM Systems](https://arxiv.org/abs/2505.13516)
*Zhipeng Hou,Junyi Tang,Yipeng Wang*

Main category: cs.MA

TL;DR: HALO是一个基于分层推理架构的多智能体协作框架，通过任务分解、角色设计和推理执行提升复杂任务处理能力，性能显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有多智能体系统依赖预定义角色和静态通信结构，在复杂交互环境中适应性差，难以处理高度专业化和专家级任务。

Method: HALO采用分层架构：高层规划代理分解任务，中层角色设计代理实例化子任务，底层推理代理执行子任务，并结合蒙特卡洛树搜索优化推理轨迹。

Result: 在代码生成、通用推理和数学推理基准测试中，HALO平均性能提升14.4%，在专业子领域最高提升19.6%。

Conclusion: HALO通过动态协作和自适应提示优化，显著提升了多智能体系统在复杂任务中的表现。

Abstract: Recent advancements in Multi-Agent Systems (MAS) powered by Large Language
Models (LLMs) have demonstrated tremendous potential in diverse task scenarios.
Nonetheless, existing agentic systems typically rely on predefined agent-role
design spaces and static communication structures, limiting their adaptability
as well as flexibility in complex interaction environments and leading to
subpar performance on highly specialized and expert-level tasks. To address
these issues, we introduce HALO, a multi-agent collaboration framework based on
a hierarchical reasoning architecture. Specifically, we incorporate a
high-level planning agent for task decomposition, mid-level role-design agents
for subtask-specific agent instantiation, and low-level inference agents for
subtask execution. Particularly, subtask execution is reformulated as a
structured workflow search problem, where Monte Carlo Tree Search (MCTS)
systematically explores the agentic action space to construct optimal reasoning
trajectories. Additionally, as the majority of users lack expertise in prompt
engineering, we leverage an Adaptive Prompt Refinement module to transform raw
queries into task-specific prompts. Empirical evaluations on Code Generation
(HumanEval), General Reasoning (MMLU), and Arithmetic Reasoning (MATH)
benchmark datasets highlight the effectiveness of HALO, yielding a 14.4%
average improvement over state-of-the-art baselines. Notably, HALO achieves up
to 13.3% performance gain on the Moral Scenarios subject in the MMLU benchmark
and up to 19.6% performance gain on the Algebra subarea in the MATH benchmark,
indicating its advanced proficiency in tackling highly specialized and
expert-level tasks. The code repository is available at
https://github.com/23japhone/HALO.

</details>


### [361] [ACPs: Agent Collaboration Protocols for the Internet of Agents](https://arxiv.org/abs/2505.13523)
*Jun Liu,Ke Yu,Keliang Chen,Ke Li,Yuxinyue Qian,Xiaolian Guo,Haozhe Song,Yinming Li*

Main category: cs.MA

TL;DR: 论文提出Agent Collaboration Protocols (ACPs)，旨在解决异构智能体间的互操作性、可扩展性和协作问题，通过标准化协议支持可信访问、能力编排和工作流构建。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能的快速发展，自主智能体的激增带来了互操作性、可扩展性和协调性方面的挑战。现有通信协议（如MCP、A2A、ANP）分散且局限于特定场景，无法满足智能体互联网（IoA）的需求。

Method: 提出ACP协议套件，包含注册、发现、交互和工具协议，支持可信访问、能力编排和工作流构建，并通过餐厅预订场景验证其有效性。

Result: ACP为构建安全、开放、可扩展的智能体互联网基础设施奠定了基础，并在协作场景中展示了其功能。

Conclusion: ACP通过标准化协议解决了智能体协作中的关键问题，为未来智能体互联网的发展提供了可行方案。

Abstract: With the rapid advancement of artificial intelligence, the proliferation of
autonomous agents has introduced new challenges in interoperability,
scalability, and coordination. The Internet of Agents (IoA) aims to
interconnect heterogeneous agents through standardized communication protocols,
enabling seamless collaboration and intelligent task execution. However,
existing agent communication protocols such as MCP, A2A, and ANP remain
fragmented and scenario-specific. To address this gap, we propose Agent
Collaboration Protocols (ACPs), a comprehensive protocol suite for the IoA.
ACPs include registration, discovery, interaction, and tooling protocols to
support trustable access, capability orchestration, and workflow construction.
We present the architecture, key technologies, and application workflows of
ACPs, and demonstrate its effectiveness in a collaborative restaurant booking
scenario. ACPs lay the foundation for building a secure, open, and scalable
agent internet infrastructure.

</details>


### [362] [MLZero: A Multi-Agent System for End-to-end Machine Learning Automation](https://arxiv.org/abs/2505.13941)
*Haoyang Fang,Boran Han,Nick Erickson,Xiyuan Zhang,Su Zhou,Anirudh Dagar,Jiani Zhang,Ali Caner Turkmen,Cuixiong Hu,Huzefa Rangwala,Ying Nian Wu,Bernie Wang,George Karypis*

Main category: cs.MA

TL;DR: MLZero是一个基于LLM的多智能体框架，实现了跨模态数据的端到端自动化机器学习，性能显著优于现有方案。


<details>
  <summary>Details</summary>
Motivation: 现有AutoML系统在处理多模态数据时仍依赖大量人工配置，需要更自动化的解决方案。

Method: 采用认知感知模块转换原始数据，通过语义记忆和情景记忆增强LLM的代码生成能力。

Result: 在MLE-Bench Lite上获得6项金牌，在多模态基准测试中成功率提升263.6%，平均排名2.28。

Conclusion: MLZero以轻量级LLM实现超越全尺寸系统的性能，显著推进了AutoML的自动化水平。

Abstract: Existing AutoML systems have advanced the automation of machine learning
(ML); however, they still require substantial manual configuration and expert
input, particularly when handling multimodal data. We introduce MLZero, a novel
multi-agent framework powered by Large Language Models (LLMs) that enables
end-to-end ML automation across diverse data modalities with minimal human
intervention. A cognitive perception module is first employed, transforming raw
multimodal inputs into perceptual context that effectively guides the
subsequent workflow. To address key limitations of LLMs, such as hallucinated
code generation and outdated API knowledge, we enhance the iterative code
generation process with semantic and episodic memory. MLZero demonstrates
superior performance on MLE-Bench Lite, outperforming all competitors in both
success rate and solution quality, securing six gold medals. Additionally, when
evaluated on our Multimodal AutoML Agent Benchmark, which includes 25 more
challenging tasks spanning diverse data modalities, MLZero outperforms the
competing methods by a large margin with a success rate of 0.92 (+263.6\%) and
an average rank of 2.28. Our approach maintains its robust effectiveness even
with a compact 8B LLM, outperforming full-size systems from existing solutions.

</details>


### [363] [Personalized and Resilient Distributed Learning Through Opinion Dynamics](https://arxiv.org/abs/2505.14081)
*Luca Ballotta,Nicola Bastianello,Riccardo M. G. Ferrari,Karl H. Johansson*

Main category: cs.MA

TL;DR: 该论文提出了一种结合分布式梯度下降和意见动态模型的算法，以解决多智能体网络系统中的个性化和韧性挑战。


<details>
  <summary>Details</summary>
Motivation: 多智能体网络系统中，智能体需要学习适合自身数据和任务的本地模型（个性化），同时学习过程需要抵御网络攻击或异常数据（韧性）。这两种需求在概念上具有相似性，因此作者希望设计一种算法同时满足它们。

Method: 作者设计了一种分布式学习算法，结合了分布式梯度下降和Friedkin-Johnsen意见动态模型，以同时实现个性化和韧性。

Result: 算法在合成和真实世界的分布式学习任务中表现出色，既能实现高精度的个性化模型，又能有效抵御恶意智能体的干扰。

Conclusion: 该算法通过调整参数可以灵活控制个性化和韧性的平衡，为多智能体系统的分布式学习提供了有效解决方案。

Abstract: In this paper, we address two practical challenges of distributed learning in
multi-agent network systems, namely personalization and resilience.
Personalization is the need of heterogeneous agents to learn local models
tailored to their own data and tasks, while still generalizing well; on the
other hand, the learning process must be resilient to cyberattacks or anomalous
training data to avoid disruption. Motivated by a conceptual affinity between
these two requirements, we devise a distributed learning algorithm that
combines distributed gradient descent and the Friedkin-Johnsen model of opinion
dynamics to fulfill both of them. We quantify its convergence speed and the
neighborhood that contains the final learned models, which can be easily
controlled by tuning the algorithm parameters to enforce a more
personalized/resilient behavior. We numerically showcase the effectiveness of
our algorithm on synthetic and real-world distributed learning tasks, where it
achieves high global accuracy both for personalized models and with malicious
agents compared to standard strategies.

</details>


<div id='cs.CG'></div>

# cs.CG [[Back]](#toc)

### [364] [EuLearn: A 3D database for learning Euler characteristics](https://arxiv.org/abs/2505.13539)
*Rodrigo Fritz,Pablo Suárez-Serrato,Victor Mijangos,Anayanzi D. Martinez-Hernandez,Eduardo Ivan Velazquez Richards*

Main category: cs.CG

TL;DR: EuLearn是首个公平代表多种拓扑类型的表面数据集，通过随机节点生成不同属的嵌入表面，为机器学习系统提供拓扑特征识别的训练数据。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏能够公平代表多种拓扑类型的表面数据集，限制了机器学习系统在拓扑特征识别上的训练效果。

Method: 利用随机节点生成不同属的嵌入表面，开发了非欧几里得统计采样方法，并改进了PointNet和Transformer架构以适应图与流形数据。

Result: 实验表明，将拓扑信息融入深度学习流程显著提升了在EuLearn数据集上的分类性能。

Conclusion: EuLearn数据集及配套方法有效提升了机器学习系统在拓扑特征识别上的表现，为非欧几里得数据处理提供了新思路。

Abstract: We present EuLearn, the first surface datasets equitably representing a
diversity of topological types. We designed our embedded surfaces of uniformly
varying genera relying on random knots, thus allowing our surfaces to knot with
themselves. EuLearn contributes new topological datasets of meshes, point
clouds, and scalar fields in 3D. We aim to facilitate the training of machine
learning systems that can discern topological features. We experimented with
specific emblematic 3D neural network architectures, finding that their vanilla
implementations perform poorly on genus classification. To enhance performance,
we developed a novel, non-Euclidean, statistical sampling method adapted to
graph and manifold data. We also introduce adjacency-informed adaptations of
PointNet and Transformer architectures that rely on our non-Euclidean sampling
strategy. Our results demonstrate that incorporating topological information
into deep learning workflows significantly improves performance on these
otherwise challenging EuLearn datasets.

</details>


### [365] [Towards Non-Euclidean Foundation Models: Advancing AI Beyond Euclidean Frameworks](https://arxiv.org/abs/2505.14417)
*Menglin Yang,Yifei Zhang,Jialin Chen,Melanie Weber,Rex Ying*

Main category: cs.CG

TL;DR: 非欧几里得学习在大型语言模型时代展现出潜力，尤其在处理复杂网络数据时表现更优。


<details>
  <summary>Details</summary>
Motivation: 当前机器学习架构主要基于欧几里得空间，但存在固有局限性。非欧几里得空间（如双曲、球面空间）能更高效地表征复杂数据结构，适用于社交网络、推荐系统等场景。

Method: 探索非欧几里得几何与基础模型的结合（NEGEL），研究其在网络技术中的潜在应用。

Result: 非欧几里得表示可提升搜索、推荐和内容理解等任务的性能。

Conclusion: 非欧几里得基础模型是未来重要研究方向，需进一步解决挑战并探索应用前景。

Abstract: In the era of foundation models and Large Language Models (LLMs), Euclidean
space is the de facto geometric setting of our machine learning architectures.
However, recent literature has demonstrated that this choice comes with
fundamental limitations. To that end, non-Euclidean learning is quickly gaining
traction, particularly in web-related applications where complex relationships
and structures are prevalent. Non-Euclidean spaces, such as hyperbolic,
spherical, and mixed-curvature spaces, have been shown to provide more
efficient and effective representations for data with intrinsic geometric
properties, including web-related data like social network topology,
query-document relationships, and user-item interactions. Integrating
foundation models with non-Euclidean geometries has great potential to enhance
their ability to capture and model the underlying structures, leading to better
performance in search, recommendations, and content understanding. This
workshop focuses on the intersection of Non-Euclidean Foundation Models and
Geometric Learning (NEGEL), exploring its potential benefits, including the
potential benefits for advancing web-related technologies, challenges, and
future directions. Workshop page:
[https://hyperboliclearning.github.io/events/www2025workshop](https://hyperboliclearning.github.io/events/www2025workshop)

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [366] [LODGE: Joint Hierarchical Task Planning and Learning of Domain Models with Grounded Execution](https://arxiv.org/abs/2505.13497)
*Claudius Kienle,Benjamin Alt,Oleg Arenz,Jan Peters*

Main category: cs.RO

TL;DR: 该论文提出了一种分层学习领域模型的方法，结合仿真验证和错误推理器，显著提升了长程规划的准确性和成功率。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型（LLMs）的规划方法常产生有缺陷的计划，且依赖大量人工反馈。论文旨在通过分层领域学习和自动化验证减少人工干预，提升长程规划效果。

Method: 采用分层领域学习，将低级谓词和动作组合为高级结构，并通过仿真验证前提条件和效果；引入中央错误推理器确保规划层级间的一致性。

Result: 在国际规划竞赛（IPC）领域和机器人长程操作任务中，规划成功率和领域模型质量均优于现有领域合成和LLM规划方法。

Conclusion: 分层领域学习与自动化验证的结合有效解决了LLM在长程规划中的局限性，为复杂任务提供了可靠解决方案。

Abstract: Large Language Models (LLMs) enable planning from natural language
instructions using implicit world knowledge, but often produce flawed plans
that require refinement. Instead of directly predicting plans, recent methods
aim to learn a problem domain that can be solved for different goal states
using classical planners. However, these approaches require significant human
feedback to obtain useful models. We address this shortcoming by learning
hierarchical domains, where low-level predicates and actions are composed into
higher-level counterparts, and by leveraging simulation to validate their
preconditions and effects. This hierarchical approach is particularly powerful
for long-horizon planning, where LLM-based planning approaches typically
struggle. Furthermore, we introduce a central error reasoner to ensure
consistency among the different planning levels. Evaluation on two challenging
International Planning Competition (IPC) domains and a long-horizon robot
manipulation task demonstrates higher planning success rates than
state-of-the-art domain synthesis and LLM-modulo planning methods, while
constructing high-quality models of the domain. Resources, videos and detailed
experiment results are available at https://claudius-kienle.github.io/lodge/.

</details>


### [367] [Distributional Soft Actor-Critic with Harmonic Gradient for Safe and Efficient Autonomous Driving in Multi-lane Scenarios](https://arxiv.org/abs/2505.13532)
*Feihong Zhang,Guojian Zhan,Bin Shuai,Tianyi Zhang,Jingliang Duan,Shengbo Eben Li*

Main category: cs.RO

TL;DR: 提出了一种名为谐波策略迭代（HPI）的安全导向训练技术，结合DSAC算法开发了DSAC-H，在保证高效驾驶的同时几乎不违反安全约束。


<details>
  <summary>Details</summary>
Motivation: 强化学习在自动驾驶领域具有潜力，但现有算法在处理约束条件时面临挑战，特别是在实际应用中。

Method: 采用谐波策略迭代（HPI）技术，计算与高效驾驶和安全约束相关的两个策略梯度，生成谐波梯度以更新策略，减少冲突，并与DSAC算法结合形成DSAC-H。

Result: 在多车道场景的模拟中，DSAC-H实现了高效驾驶性能，且几乎不违反安全约束。

Conclusion: DSAC-H通过谐波策略迭代技术，在强化学习中有效平衡了驾驶效率与安全性，为自动驾驶系统提供了一种可行的训练方法。

Abstract: Reinforcement learning (RL), known for its self-evolution capability, offers
a promising approach to training high-level autonomous driving systems.
However, handling constraints remains a significant challenge for existing RL
algorithms, particularly in real-world applications. In this paper, we propose
a new safety-oriented training technique called harmonic policy iteration
(HPI). At each RL iteration, it first calculates two policy gradients
associated with efficient driving and safety constraints, respectively. Then, a
harmonic gradient is derived for policy updating, minimizing conflicts between
the two gradients and consequently enabling a more balanced and stable training
process. Furthermore, we adopt the state-of-the-art DSAC algorithm as the
backbone and integrate it with our HPI to develop a new safe RL algorithm,
DSAC-H. Extensive simulations in multi-lane scenarios demonstrate that DSAC-H
achieves efficient driving performance with near-zero safety constraint
violations.

</details>


### [368] [SayCoNav: Utilizing Large Language Models for Adaptive Collaboration in Decentralized Multi-Robot Navigation](https://arxiv.org/abs/2505.13729)
*Abhinav Rajvanshi,Pritish Sahu,Tixiao Shan,Karan Sikka,Han-Pang Chiu*

Main category: cs.RO

TL;DR: SayCoNav利用大型语言模型为机器人团队生成自适应协作策略，提升多目标导航任务效率。


<details>
  <summary>Details</summary>
Motivation: 在未知大规模环境中，机器人团队需要根据各自技能和状态动态调整协作策略以完成复杂导航任务。

Method: 基于LLM的SayCoNav框架：1) 集中生成协作策略 2) 分布式生成个体计划 3) 通过信息共享实时更新行动。

Result: 实验表明：1) 搜索效率最高提升44.28% 2) 能适应异构团队组合 3) 支持动态环境变化。

Conclusion: SayCoNav证明了LLM在机器人协作规划中的有效性，特别适用于需要互补优势的多目标搜索场景。

Abstract: Adaptive collaboration is critical to a team of autonomous robots to perform
complicated navigation tasks in large-scale unknown environments. An effective
collaboration strategy should be determined and adapted according to each
robot's skills and current status to successfully achieve the shared goal. We
present SayCoNav, a new approach that leverages large language models (LLMs)
for automatically generating this collaboration strategy among a team of
robots. Building on the collaboration strategy, each robot uses the LLM to
generate its plans and actions in a decentralized way. By sharing information
to each other during navigation, each robot also continuously updates its
step-by-step plans accordingly. We evaluate SayCoNav on Multi-Object Navigation
(MultiON) tasks, that require the team of the robots to utilize their
complementary strengths to efficiently search multiple different objects in
unknown environments. By validating SayCoNav with varied team compositions and
conditions against baseline methods, our experimental results show that
SayCoNav can improve search efficiency by at most 44.28% through effective
collaboration among heterogeneous robots. It can also dynamically adapt to the
changing conditions during task execution.

</details>


### [369] [Toward Real-World Cooperative and Competitive Soccer with Quadrupedal Robot Teams](https://arxiv.org/abs/2505.13834)
*Zhi Su,Yuman Gao,Emily Lukas,Yunfei Li,Jiaze Cai,Faris Tulbah,Fei Gao,Chao Yu,Zhongyu Li,Yi Wu,Koushil Sreenath*

Main category: cs.RO

TL;DR: 本文提出了一种分层多智能体强化学习框架，用于实现四足机器人在足球比赛中的自主协作与决策。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于解决腿式机器人在动态、竞争性和多智能体交互环境中的团队协作问题，机器人足球是一个理想的测试平台。

Method: 方法包括训练低层动态技能（如行走、带球和踢球）和高层战略规划策略（使用MAPPO和FSP），实现完全自主和去中心化的决策。

Result: 实验结果表明，该方法在合作与竞争的多智能体足球游戏中表现出显著优势，并成功部署到真实机器人上，支持自主机器人与人类比赛。

Conclusion: 结论是提出的分层学习框架有效提升了四足机器人在复杂环境中的团队协作能力，为多智能体系统研究提供了新思路。

Abstract: Achieving coordinated teamwork among legged robots requires both fine-grained
locomotion control and long-horizon strategic decision-making. Robot soccer
offers a compelling testbed for this challenge, combining dynamic, competitive,
and multi-agent interactions. In this work, we present a hierarchical
multi-agent reinforcement learning (MARL) framework that enables fully
autonomous and decentralized quadruped robot soccer. First, a set of highly
dynamic low-level skills is trained for legged locomotion and ball
manipulation, such as walking, dribbling, and kicking. On top of these, a
high-level strategic planning policy is trained with Multi-Agent Proximal
Policy Optimization (MAPPO) via Fictitious Self-Play (FSP). This learning
framework allows agents to adapt to diverse opponent strategies and gives rise
to sophisticated team behaviors, including coordinated passing, interception,
and dynamic role allocation. With an extensive ablation study, the proposed
learning method shows significant advantages in the cooperative and competitive
multi-agent soccer game. We deploy the learned policies to real quadruped
robots relying solely on onboard proprioception and decentralized localization,
with the resulting system supporting autonomous robot-robot and robot-human
soccer matches on indoor and outdoor soccer courts.

</details>


### [370] [Enhancing Robot Navigation Policies with Task-Specific Uncertainty Managements](https://arxiv.org/abs/2505.13837)
*Gokul Puthumanaillam,Paulo Padrao,Jose Fuentes,Leonardo Bobadilla,Melkior Ornik*

Main category: cs.RO

TL;DR: GUIDE框架通过任务特定不确定性地图(TSUMs)和强化学习，使机器人能根据任务需求动态调整不确定性管理，显著提升导航性能。


<details>
  <summary>Details</summary>
Motivation: 机器人在复杂环境中导航时面临传感器噪声、环境变化和信息不完整等不确定性，不同任务在不同区域对精度的需求各异，需要一种能自适应管理不确定性的方法。

Method: 提出GUIDE框架，利用任务特定不确定性地图(TSUMs)为不同位置分配可接受的不确定性水平，并结合强化学习自动学习平衡任务完成和不确定性管理的策略。

Result: 实际测试表明，GUIDE相比缺乏任务特定不确定性感知的方法，性能有显著提升。

Conclusion: GUIDE通过集成任务特定需求和自适应不确定性管理，提供了一种高效且无需复杂奖励设计的机器人导航解决方案。

Abstract: Robots navigating complex environments must manage uncertainty from sensor
noise, environmental changes, and incomplete information, with different tasks
requiring varying levels of precision in different areas. For example, precise
localization may be crucial near obstacles but less critical in open spaces. We
present GUIDE (Generalized Uncertainty Integration for Decision-Making and
Execution), a framework that integrates these task-specific requirements into
navigation policies via Task-Specific Uncertainty Maps (TSUMs). By assigning
acceptable uncertainty levels to different locations, TSUMs enable robots to
adapt uncertainty management based on context. When combined with reinforcement
learning, GUIDE learns policies that balance task completion and uncertainty
management without extensive reward engineering. Real-world tests show
significant performance gains over methods lacking task-specific uncertainty
awareness.

</details>


### [371] [Safety2Drive: Safety-Critical Scenario Benchmark for the Evaluation of Autonomous Driving](https://arxiv.org/abs/2505.13872)
*Jingzheng Li,Tiancheng Wang,Xingyu Peng,Jiacheng Chen,Zhijun Chen,Bing Li,Xianglong Liu*

Main category: cs.RO

TL;DR: 提出Safety2Drive，一个用于评估自动驾驶系统安全性的关键场景库，填补现有数据集在闭环测试和真实事故场景覆盖上的不足。


<details>
  <summary>Details</summary>
Motivation: 当前自动驾驶数据集缺乏符合法规的闭环测试场景库，且真实事故场景代表性不足，导致安全性评估不充分，影响实际部署。

Method: 构建Safety2Drive场景库，覆盖70项标准测试项目，支持自然环境和对抗攻击等安全威胁注入，并支持多维感知任务评估。

Result: Safety2Drive提供了从场景构建到验证的标准化框架，全面评估自动驾驶系统及感知任务的安全性。

Conclusion: Safety2Drive为自动驾驶安全部署建立了标准化测试范式，解决了现有数据集在安全关键场景评估上的缺陷。

Abstract: Autonomous Driving (AD) systems demand the high levels of safety assurance.
Despite significant advancements in AD demonstrated on open-source benchmarks
like Longest6 and Bench2Drive, existing datasets still lack
regulatory-compliant scenario libraries for closed-loop testing to
comprehensively evaluate the functional safety of AD. Meanwhile, real-world AD
accidents are underrepresented in current driving datasets. This scarcity leads
to inadequate evaluation of AD performance, posing risks to safety validation
and practical deployment. To address these challenges, we propose Safety2Drive,
a safety-critical scenario library designed to evaluate AD systems.
Safety2Drive offers three key contributions. (1) Safety2Drive comprehensively
covers the test items required by standard regulations and contains 70 AD
function test items. (2) Safety2Drive supports the safety-critical scenario
generalization. It has the ability to inject safety threats such as natural
environment corruptions and adversarial attacks cross camera and LiDAR sensors.
(3) Safety2Drive supports multi-dimensional evaluation. In addition to the
evaluation of AD systems, it also supports the evaluation of various perception
tasks, such as object detection and lane detection. Safety2Drive provides a
paradigm from scenario construction to validation, establishing a standardized
test framework for the safe deployment of AD.

</details>


### [372] [APEX: Empowering LLMs with Physics-Based Task Planning for Real-time Insight](https://arxiv.org/abs/2505.13921)
*Wanjing Huang,Weixiang Yan,Zhen Zhang,Ambuj Singh*

Main category: cs.RO

TL;DR: APEX框架通过物理驱动的预见性增强LLMs的实时任务规划能力，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法在物理交互建模方面存在局限，无法捕捉动态对象交互或需要任务特定训练，限制了实际应用。

Method: APEX构建结构化图来建模环境中的动态交互，并提供低延迟的前向模拟，使LLMs能基于预测结果选择最优策略。

Result: 在三个基准测试中，APEX显著优于标准LLMs和基于VLM的模型，证明了显式物理推理的必要性。

Conclusion: APEX通过显式物理推理缩小了语言智能与现实任务执行之间的差距。

Abstract: Large Language Models (LLMs) demonstrate strong reasoning and task planning
capabilities but remain fundamentally limited in physical interaction modeling.
Existing approaches integrate perception via Vision-Language Models (VLMs) or
adaptive decision-making through Reinforcement Learning (RL), but they fail to
capture dynamic object interactions or require task-specific training, limiting
their real-world applicability. We introduce APEX (Anticipatory
Physics-Enhanced Execution), a framework that equips LLMs with physics-driven
foresight for real-time task planning. APEX constructs structured graphs to
identify and model the most relevant dynamic interactions in the environment,
providing LLMs with explicit physical state updates. Simultaneously, APEX
provides low-latency forward simulations of physically feasible actions,
allowing LLMs to select optimal strategies based on predictive outcomes rather
than static observations. We evaluate APEX on three benchmarks designed to
assess perception, prediction, and decision-making: (1) Physics Reasoning
Benchmark, testing causal inference and object motion prediction; (2) Tetris,
evaluating whether physics-informed prediction enhances decision-making
performance in long-horizon planning tasks; (3) Dynamic Obstacle Avoidance,
assessing the immediate integration of perception and action feasibility
analysis. APEX significantly outperforms standard LLMs and VLM-based models,
demonstrating the necessity of explicit physics reasoning for bridging the gap
between language-based intelligence and real-world task execution. The source
code and experiment setup are publicly available at
https://github.com/hwj20/APEX_EXP .

</details>


### [373] [Hypothesis on the Functional Advantages of the Selection-Broadcast Cycle Structure: Global Workspace Theory and Dealing with a Real-Time World](https://arxiv.org/abs/2505.13969)
*Junya Nakanishi,Jun Baba,Yuichiro Yoshikawa,Hiroko Kamide,Hiroshi Ishiguro*

Main category: cs.RO

TL;DR: 该论文探讨了基于人类意识启发的全局工作空间理论（GWT）中'选择-广播循环'结构的功能优势，特别关注其在动态实时场景下对人工智能和机器人技术的适用性。


<details>
  <summary>Details</summary>
Motivation: 以往研究通常独立分析选择和广播过程，而本研究旨在强调二者结合的循环结构及其对实时认知系统的综合效益。

Method: 通过理论分析GWT框架中的选择-广播循环机制，重点研究其在动态环境中的适应性表现。

Result: 识别出三大核心优势：动态思维适应、基于经验的适应以及即时实时适应能力。

Conclusion: GWT展现出作为认知架构的潜力，可为无监督动态环境中的复杂决策和自适应系统提供新方向。

Abstract: This paper discusses the functional advantages of the Selection-Broadcast
Cycle structure proposed by Global Workspace Theory (GWT), inspired by human
consciousness, particularly focusing on its applicability to artificial
intelligence and robotics in dynamic, real-time scenarios. While previous
studies often examined the Selection and Broadcast processes independently,
this research emphasizes their combined cyclic structure and the resulting
benefits for real-time cognitive systems. Specifically, the paper identifies
three primary benefits: Dynamic Thinking Adaptation, Experience-Based
Adaptation, and Immediate Real-Time Adaptation. This work highlights GWT's
potential as a cognitive architecture suitable for sophisticated
decision-making and adaptive performance in unsupervised, dynamic environments.
It suggests new directions for the development and implementation of robust,
general-purpose AI and robotics systems capable of managing complex, real-world
tasks.

</details>


### [374] [Learning Collision Risk from Naturalistic Driving with Generalised Surrogate Safety Measures](https://arxiv.org/abs/2505.13556)
*Yiru Jiao,Simeon C. Calvert,Sander van Cranenburgh,Hans van Lint*

Main category: cs.RO

TL;DR: 该论文提出了一种名为GSSM的新型方法，通过无监督学习从自然驾驶数据中识别交通交互中的碰撞风险，无需事故或风险标签，显著提升了预警准确性和及时性。


<details>
  <summary>Details</summary>
Motivation: 当前驾驶员或自动化系统的碰撞预警方法存在标注成本高、难以适应多样化交互场景或仅适用于特定场景的问题，亟需一种通用、可扩展的解决方案。

Method: 提出广义替代安全度量（GSSM），利用神经网络学习正常驾驶的交互模式，通过极端值理论计算上下文自适应的风险评分，可整合运动学、天气等多维环境因素。

Result: 在SHRP2 NDS数据集测试中，基础版GSSM的AUPRC达0.9，中位预警时间提前2.6秒；在追尾、变道等场景下性能均优于基线方法。

Conclusion: GSSM为交通交互风险量化提供了可扩展、上下文感知的通用框架，无需人工标注即可实现主动式碰撞预警。

Abstract: Accurate and timely alerts for drivers or automated systems to unfolding
collisions remains a challenge in road safety, particularly in highly
interactive urban traffic. Existing approaches require labour-intensive
annotation of sparse risk, struggle to consider varying interaction context, or
are useful only in the scenarios they are designed for. To address these
limits, this study introduces the generalised surrogate safety measure (GSSM),
a new approach that learns exclusively from naturalistic driving without crash
or risk labels. GSSM captures the patterns of normal driving and estimates the
extent to which a traffic interaction deviates from the norm towards unsafe
extreme. Utilising neural networks, normal interactions are characterised by
context-conditioned distributions of multi-directional spacing between road
users. In the same interaction context, a spacing closer than normal entails
higher risk of potential collision. Then a context-adaptive risk score and its
associated probability can be calculated based on the theory of extreme values.
Any measurable factors, such as motion kinematics, weather, lighting, can serve
as part of the context, allowing for diverse coverage of safety-critical
interactions. Multiple public driving datasets are used to train GSSMs, which
are tested with 4,875 real-world crashes and near-crashes reconstructed from
the SHRP2 NDS. A vanilla GSSM using only instantaneous states achieves AUPRC of
0.9 and secures a median time advance of 2.6 seconds to prevent potential
collisions. Additional data and contextual factors provide further performance
gains. Across various interaction types such as rear-end, merging, and
crossing, the accuracy and timeliness of GSSM consistently outperforms existing
baselines. GSSM therefore establishes a scalable, context-aware, and
generalisable foundation to proactively quantify collision risk in traffic
interactions.

</details>


### [375] [Certifiably Safe Manipulation of Deformable Linear Objects via Joint Shape and Tension Prediction](https://arxiv.org/abs/2505.13889)
*Yiting Zhang,Shichen Li*

Main category: cs.RO

TL;DR: 提出了一种可证明安全的运动规划与控制框架，用于可变形线性物体的操作，确保在接触密集环境中的安全性。


<details>
  <summary>Details</summary>
Motivation: 现有模型大多仅关注形状预测，忽略了接触和张力的约束，可能导致物体和机器人的损坏。

Method: 核心是一个预测模型，联合估计物体的未来形状和张力，并结合基于多项式zonotopes的实时轨迹优化器。

Result: 在模拟线束装配任务中，相比现有方法，实现了更高的任务成功率且无安全违规。

Conclusion: 该方法在接触密集环境中实现了稳健且安全的可变形线性物体操作。

Abstract: Manipulating deformable linear objects (DLOs) is challenging due to their
complex dynamics and the need for safe interaction in contact-rich
environments. Most existing models focus on shape prediction alone and fail to
account for contact and tension constraints, which can lead to damage to both
the DLO and the robot. In this work, we propose a certifiably safe motion
planning and control framework for DLO manipulation. At the core of our method
is a predictive model that jointly estimates the DLO's future shape and
tension. These predictions are integrated into a real-time trajectory optimizer
based on polynomial zonotopes, allowing us to enforce safety constraints
throughout the execution. We evaluate our framework on a simulated wire harness
assembly task using a 7-DOF robotic arm. Compared to state-of-the-art methods,
our approach achieves a higher task success rate while avoiding all safety
violations. The results demonstrate that our method enables robust and safe DLO
manipulation in contact-rich environments.

</details>


### [376] [Time Reversal Symmetry for Efficient Robotic Manipulations in Deep Reinforcement Learning](https://arxiv.org/abs/2505.13925)
*Yunpeng Jiang,Jianshu Hu,Paul Weng,Yutong Ban*

Main category: cs.RO

TL;DR: 论文提出TR-DRL框架，利用时间反转对称性提升深度强化学习的样本效率，在Robosuite和MetaWorld基准测试中表现优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注空间对称性（如反射、旋转、平移），而忽略了时间对称性。本文旨在填补这一空白，探索时间反转对称性在机器人任务（如开门/关门）中的应用。

Method: 提出TR-DRL框架，结合轨迹反转增强和时间反转引导的奖励塑形：1) 通过动态一致性过滤器识别完全可逆的转移并生成反转轨迹以扩充数据；2) 对部分可逆转移应用奖励塑形，依据反转任务的成功轨迹指导学习。

Result: 在Robosuite和MetaWorld基准测试中，TR-DRL在单任务和多任务场景下均展现出更高的样本效率和更强的最终性能。

Conclusion: 时间反转对称性是提升DRL效率的有效途径，TR-DRL为处理时间对称任务提供了通用框架。

Abstract: Symmetry is pervasive in robotics and has been widely exploited to improve
sample efficiency in deep reinforcement learning (DRL). However, existing
approaches primarily focus on spatial symmetries, such as reflection, rotation,
and translation, while largely neglecting temporal symmetries. To address this
gap, we explore time reversal symmetry, a form of temporal symmetry commonly
found in robotics tasks such as door opening and closing. We propose Time
Reversal symmetry enhanced Deep Reinforcement Learning (TR-DRL), a framework
that combines trajectory reversal augmentation and time reversal guided reward
shaping to efficiently solve temporally symmetric tasks. Our method generates
reversed transitions from fully reversible transitions, identified by a
proposed dynamics-consistent filter, to augment the training data. For
partially reversible transitions, we apply reward shaping to guide learning,
according to successful trajectories from the reversed task. Extensive
experiments on the Robosuite and MetaWorld benchmarks demonstrate that TR-DRL
is effective in both single-task and multi-task settings, achieving higher
sample efficiency and stronger final performance compared to baseline methods.

</details>


### [377] [NavBench: A Unified Robotics Benchmark for Reinforcement Learning-Based Autonomous Navigation](https://arxiv.org/abs/2505.14526)
*Matteo El-Hariry,Antoine Richard,Ricard M. Castan,Luis F. W. Batista,Matthieu Geist,Cedric Pradalier,Miguel Olivares-Mendez*

Main category: cs.RO

TL;DR: NavBench是一个多领域基准测试框架，用于训练和评估不同机器人平台的RL导航策略，解决了现有基准测试的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试通常局限于特定平台，难以实现跨平台泛化和公平比较，限制了RL导航策略的通用性。

Method: 基于IsaacLab构建标准化任务定义，支持统一跨介质基准测试、模块化设计及仿真到现实的鲁棒验证。

Result: NavBench成功将策略迁移至多种真实机器人（如卫星模拟器、无人水面艇、轮式地面车辆），验证了其有效性。

Conclusion: NavBench通过标准化和模块化设计，简化了适应性RL导航策略的开发，并支持广泛的应用场景。

Abstract: Autonomous robots must navigate and operate in diverse environments, from
terrestrial and aquatic settings to aerial and space domains. While
Reinforcement Learning (RL) has shown promise in training policies for specific
autonomous robots, existing benchmarks are often constrained to unique
platforms, limiting generalization and fair comparisons across different
mobility systems. In this paper, we present NavBench, a multi-domain benchmark
for training and evaluating RL-based navigation policies across diverse robotic
platforms and operational environments. Built on IsaacLab, our framework
standardizes task definitions, enabling different robots to tackle various
navigation challenges without the need for ad-hoc task redesigns or custom
evaluation metrics. Our benchmark addresses three key challenges: (1) Unified
cross-medium benchmarking, enabling direct evaluation of diverse actuation
methods (thrusters, wheels, water-based propulsion) in realistic environments;
(2) Scalable and modular design, facilitating seamless robot-task
interchangeability and reproducible training pipelines; and (3) Robust
sim-to-real validation, demonstrated through successful policy transfer to
multiple real-world robots, including a satellite robotic simulator, an
unmanned surface vessel, and a wheeled ground vehicle. By ensuring consistency
between simulation and real-world deployment, NavBench simplifies the
development of adaptable RL-based navigation strategies. Its modular design
allows researchers to easily integrate custom robots and tasks by following the
framework's predefined templates, making it accessible for a wide range of
applications. Our code is publicly available at NavBench.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [378] [PandaGuard: Systematic Evaluation of LLM Safety in the Era of Jailbreaking Attacks](https://arxiv.org/abs/2505.13862)
*Guobin Shen,Dongcheng Zhao,Linghao Feng,Xiang He,Jihang Wang,Sicheng Shen,Haibo Tong,Yiting Dong,Jindong Li,Xiang Zheng,Yi Zeng*

Main category: cs.CR

TL;DR: PandaGuard是一个统一模块化框架，通过多智能体系统评估LLM的对抗性攻击与防御，发现现有防御措施存在局限性。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型（LLM）安全研究评估零散，缺乏系统性分析，需统一框架来评估对抗性攻击与防御。

Method: 提出PandaGuard框架，包含攻击者、防御者和裁判多智能体系统，支持19种攻击方法、12种防御机制及多种判断策略。

Result: 评估49个LLM发现，无单一防御措施在所有维度最优，裁判不一致性导致安全评估存在显著差异。

Conclusion: LLM安全需综合考虑攻击、防御与判断策略，PandaGuard框架为透明可复现研究提供支持。

Abstract: Large language models (LLMs) have achieved remarkable capabilities but remain
vulnerable to adversarial prompts known as jailbreaks, which can bypass safety
alignment and elicit harmful outputs. Despite growing efforts in LLM safety
research, existing evaluations are often fragmented, focused on isolated attack
or defense techniques, and lack systematic, reproducible analysis. In this
work, we introduce PandaGuard, a unified and modular framework that models LLM
jailbreak safety as a multi-agent system comprising attackers, defenders, and
judges. Our framework implements 19 attack methods and 12 defense mechanisms,
along with multiple judgment strategies, all within a flexible plugin
architecture supporting diverse LLM interfaces, multiple interaction modes, and
configuration-driven experimentation that enhances reproducibility and
practical deployment. Built on this framework, we develop PandaBench, a
comprehensive benchmark that evaluates the interactions between these
attack/defense methods across 49 LLMs and various judgment approaches,
requiring over 3 billion tokens to execute. Our extensive evaluation reveals
key insights into model vulnerabilities, defense cost-performance trade-offs,
and judge consistency. We find that no single defense is optimal across all
dimensions and that judge disagreement introduces nontrivial variance in safety
assessments. We release the code, configurations, and evaluation results to
support transparent and reproducible research in LLM safety.

</details>


### [379] [Beyond Text: Unveiling Privacy Vulnerabilities in Multi-modal Retrieval-Augmented Generation](https://arxiv.org/abs/2505.13957)
*Jiankun Zhang,Shenglai Zeng,Jie Ren,Tianqi Zheng,Hui Liu,Xianfeng Tang,Hui Liu,Yi Chang*

Main category: cs.CR

TL;DR: 该论文首次系统分析了多模态检索增强生成(MRAG)系统的隐私漏洞，通过黑盒攻击实验展示了攻击者如何通过查询操作提取私有信息，揭示了现有技术的隐私风险。


<details>
  <summary>Details</summary>
Motivation: 尽管基于文本的检索增强生成(RAG)隐私风险已有研究，但多模态数据（如视觉-语言和语音-语言）带来的独特隐私挑战尚未被探索。论文旨在填补这一空白。

Method: 采用新颖的组合式结构化提示攻击方法，在黑盒设置下通过操纵查询来测试多模态LMMs的隐私泄露情况。

Result: 实验表明，LMMs不仅会直接生成类似检索内容的输出，还会通过描述间接暴露敏感信息，证实了MRAG系统存在严重隐私漏洞。

Conclusion: 研究结果凸显了开发鲁棒的多模态隐私保护技术的紧迫性，为未来MRAG系统的安全设计提供了重要依据。

Abstract: Multimodal Retrieval-Augmented Generation (MRAG) systems enhance LMMs by
integrating external multimodal databases, but introduce unexplored privacy
vulnerabilities. While text-based RAG privacy risks have been studied,
multimodal data presents unique challenges. We provide the first systematic
analysis of MRAG privacy vulnerabilities across vision-language and
speech-language modalities. Using a novel compositional structured prompt
attack in a black-box setting, we demonstrate how attackers can extract private
information by manipulating queries. Our experiments reveal that LMMs can both
directly generate outputs resembling retrieved content and produce descriptions
that indirectly expose sensitive information, highlighting the urgent need for
robust privacy-preserving MRAG techniques.

</details>


### [380] [Is Your Prompt Safe? Investigating Prompt Injection Attacks Against Open-Source LLMs](https://arxiv.org/abs/2505.14368)
*Jiawen Wang,Pritha Gupta,Ivan Habernal,Eyke Hüllermeier*

Main category: cs.CR

TL;DR: 研究发现主流开源大语言模型易受提示注入攻击，提出新评估指标ASP并开发催眠攻击方法，成功突破14种模型防线。


<details>
  <summary>Details</summary>
Motivation: 现有研究对开源和闭源大语言模型的提示注入攻击脆弱性缺乏系统评估，且传统指标无法反映攻击可行性中的不确定性。

Method: 在5个基准测试上对14种流行开源LLMs进行攻击实验，提出ASP指标量化攻击成功率，开发催眠攻击和忽略前缀攻击方法。

Result: 催眠攻击使Stablelm2等对齐模型产生违规行为（ASP达90%），忽略前缀攻击突破全部14种模型（ASP超60%），发现中等知名度模型最脆弱。

Conclusion: 提示注入攻击对开源LLMs构成严重威胁，需提升公众防范意识并优先开发缓解策略，ASP指标能更全面评估攻击有效性。

Abstract: Recent studies demonstrate that Large Language Models (LLMs) are vulnerable
to different prompt-based attacks, generating harmful content or sensitive
information. Both closed-source and open-source LLMs are underinvestigated for
these attacks. This paper studies effective prompt injection attacks against
the $\mathbf{14}$ most popular open-source LLMs on five attack benchmarks.
Current metrics only consider successful attacks, whereas our proposed Attack
Success Probability (ASP) also captures uncertainty in the model's response,
reflecting ambiguity in attack feasibility. By comprehensively analyzing the
effectiveness of prompt injection attacks, we propose a simple and effective
hypnotism attack; results show that this attack causes aligned language models,
including Stablelm2, Mistral, Openchat, and Vicuna, to generate objectionable
behaviors, achieving around $90$% ASP. They also indicate that our ignore
prefix attacks can break all $\mathbf{14}$ open-source LLMs, achieving over
$60$% ASP on a multi-categorical dataset. We find that moderately well-known
LLMs exhibit higher vulnerability to prompt injection attacks, highlighting the
need to raise public awareness and prioritize efficient mitigation strategies.

</details>


### [381] [CSAGC-IDS: A Dual-Module Deep Learning Network Intrusion Detection Model for Complex and Imbalanced Data](https://arxiv.org/abs/2505.14027)
*Yifan Zeng*

Main category: cs.CR

TL;DR: 本文提出了一种基于深度学习的网络入侵检测模型CSAGC-IDS，通过生成高质量数据和改进特征提取方法，有效解决了高维复杂流量数据和不平衡类别的挑战。


<details>
  <summary>Details</summary>
Motivation: 随着计算机网络的普及，网络入侵的严重性日益增加，网络入侵检测系统对保障安全至关重要。然而，现有的深度学习模型在处理高维复杂流量模式和不平衡数据类别方面存在挑战。

Method: CSAGC-IDS整合了SC-CGAN（一种自注意力增强的卷积条件生成对抗网络）来生成高质量数据以缓解类别不平衡问题，并采用CSCA-CNN（一种通过成本敏感学习和通道注意力机制增强的卷积神经网络）来提取复杂流量数据的特征以实现精确检测。

Result: 在NSL-KDD数据集上的实验表明，CSAGC-IDS在五分类任务中达到了84.55%的准确率和84.52%的F1分数，在二分类任务中达到了91.09%的准确率和92.04%的F1分数。此外，本文还通过SHAP和LIME对模型的决策机制进行了解释性分析。

Conclusion: CSAGC-IDS模型在提高网络入侵检测的准确性和解释性方面表现出色，为解决高维复杂数据和不平衡类别问题提供了有效的方法。

Abstract: As computer networks proliferate, the gravity of network intrusions has
escalated, emphasizing the criticality of network intrusion detection systems
for safeguarding security. While deep learning models have exhibited promising
results in intrusion detection, they face challenges in managing
high-dimensional, complex traffic patterns and imbalanced data categories. This
paper presents CSAGC-IDS, a network intrusion detection model based on deep
learning techniques. CSAGC-IDS integrates SC-CGAN, a self-attention-enhanced
convolutional conditional generative adversarial network that generates
high-quality data to mitigate class imbalance. Furthermore, CSAGC-IDS
integrates CSCA-CNN, a convolutional neural network enhanced through cost
sensitive learning and channel attention mechanism, to extract features from
complex traffic data for precise detection. Experiments conducted on the
NSL-KDD dataset. CSAGC-IDS achieves an accuracy of 84.55% and an F1-score of
84.52% in five-class classification task, and an accuracy of 91.09% and an F1
score of 92.04% in binary classification task.Furthermore, this paper provides
an interpretability analysis of the proposed model, using SHAP and LIME to
explain the decision-making mechanisms of the model.

</details>


### [382] [Traceable Black-box Watermarks for Federated Learning](https://arxiv.org/abs/2505.13651)
*Jiahao Xu,Rui Hu,Olivera Kotevska,Zikai Zhang*

Main category: cs.CR

TL;DR: 该论文提出了一种名为TraMark的服务器端水印方法，用于在联邦学习系统中注入可追踪的黑盒水印，以保护模型知识产权并验证模型泄露。


<details>
  <summary>Details</summary>
Motivation: 由于联邦学习系统的分布式特性，每个本地客户端都能访问全局模型，存在模型泄露的风险。现有方法主要关注不可追踪水印或可追踪但白盒水印，缺乏对可追踪黑盒水印的正式定义和问题描述。

Method: 论文首先形式化了在联邦学习中注入可追踪黑盒水印的问题，并提出TraMark方法。该方法将模型参数空间划分为主任务区域和水印区域，通过仅聚合主任务区域构建个性化全局模型，并在水印区域使用独特水印数据集学习唯一水印。

Result: 实验结果表明，TraMark能够在保持主任务性能的同时，确保所有水印模型的可追踪性。

Conclusion: TraMark方法有效解决了联邦学习中的模型泄露问题，为模型知识产权保护提供了可行的解决方案。

Abstract: Due to the distributed nature of Federated Learning (FL) systems, each local
client has access to the global model, posing a critical risk of model leakage.
Existing works have explored injecting watermarks into local models to enable
intellectual property protection. However, these methods either focus on
non-traceable watermarks or traceable but white-box watermarks. We identify a
gap in the literature regarding the formal definition of traceable black-box
watermarking and the formulation of the problem of injecting such watermarks
into FL systems. In this work, we first formalize the problem of injecting
traceable black-box watermarks into FL. Based on the problem, we propose a
novel server-side watermarking method, $\mathbf{TraMark}$, which creates a
traceable watermarked model for each client, enabling verification of model
leakage in black-box settings. To achieve this, $\mathbf{TraMark}$ partitions
the model parameter space into two distinct regions: the main task region and
the watermarking region. Subsequently, a personalized global model is
constructed for each client by aggregating only the main task region while
preserving the watermarking region. Each model then learns a unique watermark
exclusively within the watermarking region using a distinct watermark dataset
before being sent back to the local client. Extensive results across various FL
systems demonstrate that $\mathbf{TraMark}$ ensures the traceability of all
watermarked models while preserving their main task performance.

</details>


### [383] [Optimal Client Sampling in Federated Learning with Client-Level Heterogeneous Differential Privacy](https://arxiv.org/abs/2505.13655)
*Jiahao Xu,Rui Hu,Olivera Kotevska*

Main category: cs.CR

TL;DR: 论文提出GDPFed和GDPFed$^+$方法，通过分组客户隐私预算和模型稀疏化，在保证客户级差分隐私的同时提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 传统DP-FedAvg等方法在客户隐私需求异构时，需统一采用最严格的隐私级别，导致过度噪声和模型性能下降。现有方法多假设服务器可信且缺乏理论支持，难以在实际攻击模型下优化性能。

Method: GDPFed按隐私预算将客户分组，组内实现客户级差分隐私以减少隐私预算浪费；GDPFed$^+$进一步引入模型稀疏化消除不必要噪声，并优化组内客户采样比例以最小化收敛误差。

Result: 在多个基准数据集上的实验表明，GDPFed$^+$相比现有方法显著提升模型性能，同时满足严格的隐私保护要求。

Conclusion: GDPFed$^+$通过分组隐私预算和模型稀疏化，在诚实但好奇的服务器和客户攻击模型下，实现了隐私保护与模型效用的最优平衡。

Abstract: Federated Learning with client-level differential privacy (DP) provides a
promising framework for collaboratively training models while rigorously
protecting clients' privacy. However, classic approaches like DP-FedAvg
struggle when clients have heterogeneous privacy requirements, as they must
uniformly enforce the strictest privacy level across clients, leading to
excessive DP noise and significant model utility degradation. Existing methods
to improve the model utility in such heterogeneous privacy settings often
assume a trusted server and are largely heuristic, resulting in suboptimal
performance and lacking strong theoretical underpinnings. In this work, we
address these challenges under a practical attack model where both clients and
the server are honest-but-curious. We propose GDPFed, which partitions clients
into groups based on their privacy budgets and achieves client-level DP within
each group to reduce the privacy budget waste and hence improve the model
utility. Based on the privacy and convergence analysis of GDPFed, we find that
the magnitude of DP noise depends on both model dimensionality and the
per-group client sampling ratios. To further improve the performance of GDPFed,
we introduce GDPFed$^+$, which integrates model sparsification to eliminate
unnecessary noise and optimizes per-group client sampling ratios to minimize
convergence error. Extensive empirical evaluations on multiple benchmark
datasets demonstrate the effectiveness of GDPFed$^+$, showing substantial
performance gains compared with state-of-the-art methods.

</details>


### [384] [AudioJailbreak: Jailbreak Attacks against End-to-End Large Audio-Language Models](https://arxiv.org/abs/2505.14103)
*Guangke Chen,Fu Song,Zhe Zhao,Xiaojun Jia,Yang Liu,Yanchen Qiao,Weizhe Zhang*

Main category: cs.CR

TL;DR: 本文提出了一种新型音频越狱攻击AudioJailbreak，具有异步性、通用性、隐蔽性和空中鲁棒性，显著提升了攻击效果和适用场景。


<details>
  <summary>Details</summary>
Motivation: 现有的大型音频语言模型（LALMs）越狱攻击效果不佳，且假设攻击者能完全操控用户提示，实际应用受限。

Method: 通过设计后缀越狱音频、多提示扰动生成、意图隐藏策略及房间脉冲响应模拟，实现异步、通用、隐蔽且空中鲁棒的音频越狱攻击。

Result: 实验表明，AudioJailbreak在多种LALMs上高效有效，且适用于无法完全操控用户提示的攻击场景。

Conclusion: AudioJailbreak揭示了LALMs的音频越狱安全风险，为提升其安全鲁棒性提供了现实依据。

Abstract: Jailbreak attacks to Large audio-language models (LALMs) are studied
recently, but they achieve suboptimal effectiveness, applicability, and
practicability, particularly, assuming that the adversary can fully manipulate
user prompts. In this work, we first conduct an extensive experiment showing
that advanced text jailbreak attacks cannot be easily ported to end-to-end
LALMs via text-to speech (TTS) techniques. We then propose AudioJailbreak, a
novel audio jailbreak attack, featuring (1) asynchrony: the jailbreak audio
does not need to align with user prompts in the time axis by crafting suffixal
jailbreak audios; (2) universality: a single jailbreak perturbation is
effective for different prompts by incorporating multiple prompts into
perturbation generation; (3) stealthiness: the malicious intent of jailbreak
audios will not raise the awareness of victims by proposing various intent
concealment strategies; and (4) over-the-air robustness: the jailbreak audios
remain effective when being played over the air by incorporating the
reverberation distortion effect with room impulse response into the generation
of the perturbations. In contrast, all prior audio jailbreak attacks cannot
offer asynchrony, universality, stealthiness, or over-the-air robustness.
Moreover, AudioJailbreak is also applicable to the adversary who cannot fully
manipulate user prompts, thus has a much broader attack scenario. Extensive
experiments with thus far the most LALMs demonstrate the high effectiveness of
AudioJailbreak. We highlight that our work peeks into the security implications
of audio jailbreak attacks against LALMs, and realistically fosters improving
their security robustness. The implementation and audio samples are available
at our website https://audiojailbreak.github.io/AudioJailbreak.

</details>


### [385] [Exploring Jailbreak Attacks on LLMs through Intent Concealment and Diversion](https://arxiv.org/abs/2505.14316)
*Tiehan Cui,Yanxu Mao,Peipei Liu,Congying Liu,Datao You*

Main category: cs.CR

TL;DR: 论文提出了一种名为ICE的新型黑盒越狱方法，通过意图隐藏和转移有效绕过LLM的安全限制，并创建了BiSceneEval数据集评估模型鲁棒性。实验表明ICE在单次查询下实现高攻击成功率，揭示了当前防御机制的漏洞。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型（LLM）取得显著进展，但其安全性仍是紧迫问题。现有越狱攻击方法存在迭代查询过多和跨模型泛化性差的问题，且评估数据集缺乏对文本生成任务的关注。

Method: 提出ICE（意图隐藏和转移）黑盒越狱方法，以及BiSceneEval评估数据集，涵盖问答和文本生成任务。

Result: ICE在单次查询中实现高攻击成功率，显著提升效率和跨模型迁移能力；BiSceneEval数据集全面评估了LLM鲁棒性。

Conclusion: 研究强调需要结合预定义安全机制和实时语义分解的混合安全策略，以增强LLM安全性。

Abstract: Although large language models (LLMs) have achieved remarkable advancements,
their security remains a pressing concern. One major threat is jailbreak
attacks, where adversarial prompts bypass model safeguards to generate harmful
or objectionable content. Researchers study jailbreak attacks to understand
security and robustness of LLMs. However, existing jailbreak attack methods
face two main challenges: (1) an excessive number of iterative queries, and (2)
poor generalization across models. In addition, recent jailbreak evaluation
datasets focus primarily on question-answering scenarios, lacking attention to
text generation tasks that require accurate regeneration of toxic content. To
tackle these challenges, we propose two contributions: (1) ICE, a novel
black-box jailbreak method that employs Intent Concealment and divErsion to
effectively circumvent security constraints. ICE achieves high attack success
rates (ASR) with a single query, significantly improving efficiency and
transferability across different models. (2) BiSceneEval, a comprehensive
dataset designed for assessing LLM robustness in question-answering and
text-generation tasks. Experimental results demonstrate that ICE outperforms
existing jailbreak techniques, revealing critical vulnerabilities in current
defense mechanisms. Our findings underscore the necessity of a hybrid security
strategy that integrates predefined security mechanisms with real-time semantic
decomposition to enhance the security of LLMs.

</details>


### [386] [Vulnerability of Transfer-Learned Neural Networks to Data Reconstruction Attacks in Small-Data Regime](https://arxiv.org/abs/2505.14323)
*Tomasz Maciążek,Robert Allison*

Main category: cs.CR

TL;DR: 论文提出一种针对迁移学习模型的小数据训练集重建攻击，证明差分隐私防御在数据量不足时会严重损害模型精度，并改进重建效果评估指标。


<details>
  <summary>Details</summary>
Motivation: 现有训练数据重建攻击依赖攻击者掌握大部分训练数据，且差分隐私防御在大数据量下有效。但现实中攻击者可能仅知道小数据集的分布，且迁移学习模型在小数据场景下隐私保护不足。

Method: 使用重构神经网络逆向映射模型权重与训练数据，在仅知数据分布和小数据集条件下攻击迁移学习分类器，采用Neyman-Pearson准则改进评估指标。

Result: 在MNIST/CIFAR-10/CelebA数据集上成功攻击VGG/EfficientNet/ResNet模型，证明DP-SGD防御会显著降低小数据场景下的分类准确率。

Conclusion: 当训练数据保护至关重要时，当前迁移学习分类器存在严重隐私风险，需重新评估其适用性。

Abstract: Training data reconstruction attacks enable adversaries to recover portions
of a released model's training data. We consider the attacks where a
reconstructor neural network learns to invert the (random) mapping between
training data and model weights. Prior work has shown that an informed
adversary with access to released model's weights and all but one training data
point can achieve high-quality reconstructions in this way. However,
differential privacy can defend against such an attack with little to no loss
in model's utility when the amount of training data is sufficiently large. In
this work we consider a more realistic adversary who only knows the
distribution from which a small training dataset has been sampled and who
attacks a transfer-learned neural network classifier that has been trained on
this dataset. We exhibit an attack that works in this realistic threat model
and demonstrate that in the small-data regime it cannot be defended against by
DP-SGD without severely damaging the classifier accuracy. This raises
significant concerns about the use of such transfer-learned classifiers when
protection of training-data is paramount. We demonstrate the effectiveness and
robustness of our attack on VGG, EfficientNet and ResNet image classifiers
transfer-learned on MNIST, CIFAR-10 and CelebA respectively. Additionally, we
point out that the commonly used (true-positive) reconstruction success rate
metric fails to reliably quantify the actual reconstruction effectiveness.
Instead, we make use of the Neyman-Pearson lemma to construct the receiver
operating characteristic curve and consider the associated true-positive
reconstruction rate at a fixed level of the false-positive reconstruction rate.

</details>


### [387] [Can Large Language Models Really Recognize Your Name?](https://arxiv.org/abs/2505.14549)
*Dzung Pham,Peter Kairouz,Niloofar Mireshghallah,Eugene Bagdasarian,Chau Minh Pham,Amir Houmansadr*

Main category: cs.CR

TL;DR: 研究发现大语言模型在隐私保护任务中存在系统性缺陷，尤其容易忽略模糊上下文中的姓名，导致隐私泄露风险。


<details>
  <summary>Details</summary>
Motivation: 当前基于大语言模型的隐私解决方案假设模型能可靠识别敏感信息，但实际存在未被充分探索的失败模式。

Method: 提出AMBENCH基准数据集，通过包含模糊姓名和良性提示注入的短文本片段测试模型表现。

Result: 实验显示模型对模糊姓名的召回率下降20-40%，且在隐私摘要中忽略概率增加4倍。

Conclusion: 仅依赖大语言模型保护隐私存在风险，需系统研究其失败模式。

Abstract: Large language models (LLMs) are increasingly being used to protect sensitive
user data. However, current LLM-based privacy solutions assume that these
models can reliably detect personally identifiable information (PII),
particularly named entities. In this paper, we challenge that assumption by
revealing systematic failures in LLM-based privacy tasks. Specifically, we show
that modern LLMs regularly overlook human names even in short text snippets due
to ambiguous contexts, which cause the names to be misinterpreted or
mishandled. We propose AMBENCH, a benchmark dataset of seemingly ambiguous
human names, leveraging the name regularity bias phenomenon, embedded within
concise text snippets along with benign prompt injections. Our experiments on
modern LLMs tasked to detect PII as well as specialized tools show that recall
of ambiguous names drops by 20--40% compared to more recognizable names.
Furthermore, ambiguous human names are four times more likely to be ignored in
supposedly privacy-preserving summaries generated by LLMs when benign prompt
injections are present. These findings highlight the underexplored risks of
relying solely on LLMs to safeguard user privacy and underscore the need for a
more systematic investigation into their privacy failure modes.

</details>


### [388] [Lessons from Defending Gemini Against Indirect Prompt Injections](https://arxiv.org/abs/2505.14534)
*Chongyang Shi,Sharon Lin,Shuang Song,Jamie Hayes,Ilia Shumailov,Itay Yona,Juliette Pluto,Aneesh Pappu,Christopher A. Choquette-Choo,Milad Nasr,Chawin Sitawarin,Gena Gibson,Andreas Terzis,John "Four" Flynn*

Main category: cs.CR

TL;DR: Google DeepMind评估了Gemini模型对抗恶意指令的鲁棒性，通过持续对抗测试提升模型安全性。


<details>
  <summary>Details</summary>
Motivation: Gemini模型在处理非可信数据时可能受到恶意指令操控，导致偏离用户预期或误用权限，需评估其对抗攻击的鲁棒性。

Method: 采用自适应攻击技术构建对抗评估框架，持续测试Gemini各版本模型。

Result: 持续对抗测试有效增强了Gemini模型抵御恶意指令的能力。

Conclusion: 通过动态对抗评估可显著提升Gemini模型的安全性，使其更抗操控。

Abstract: Gemini is increasingly used to perform tasks on behalf of users, where
function-calling and tool-use capabilities enable the model to access user
data. Some tools, however, require access to untrusted data introducing risk.
Adversaries can embed malicious instructions in untrusted data which cause the
model to deviate from the user's expectations and mishandle their data or
permissions. In this report, we set out Google DeepMind's approach to
evaluating the adversarial robustness of Gemini models and describe the main
lessons learned from the process. We test how Gemini performs against a
sophisticated adversary through an adversarial evaluation framework, which
deploys a suite of adaptive attack techniques to run continuously against past,
current, and future versions of Gemini. We describe how these ongoing
evaluations directly help make Gemini more resilient against manipulation.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [389] [An agentic system with reinforcement-learned subsystem improvements for parsing form-like documents](https://arxiv.org/abs/2505.13504)
*Ayesha Amjad,Saurav Sthapit,Tahir Qasim Syed*

Main category: cs.IR

TL;DR: 提出了一种基于多智能体框架的自适应系统，用于从表单类文档中提取字母数字数据，通过强化学习驱动自我改进。


<details>
  <summary>Details</summary>
Motivation: 传统OCR和单一学习算法在提取表单类文档数据时存在系统性改进潜力有限的问题，需要一种能够自我纠正和适应的解决方案。

Method: 采用多智能体框架，结合大型语言模型（LLM）和强化学习（RL）驱动代理，通过任务特定提示和奖惩策略实现自我改进。

Result: 在SOIRE和CORD两个基准数据集上的实验结果显示了该框架的潜力。

Conclusion: 该自适应系统能够处理多样化的文档和格式，无需人工干预即可实现准确的信息提取。

Abstract: Extracting alphanumeric data from form-like documents such as invoices,
purchase orders, bills, and financial documents is often performed via vision
(OCR) and learning algorithms or monolithic pipelines with limited potential
for systemic improvements. We propose an agentic AI system that leverages Large
Language Model (LLM) agents and a reinforcement learning (RL) driver agent to
automate consistent, self-improving extraction under LLM inference uncertainty.
Our work highlights the limitations of monolithic LLM-based extraction and
introduces a modular, multi-agent framework with task-specific prompts and an
RL policy of rewards and penalties to guide a meta-prompting agent to learn
from past errors and improve prompt-based actor agents. This self-corrective
adaptive system handles diverse documents, file formats, layouts, and LLMs,
aiming to automate accurate information extraction without the need for human
intervention. Results as reported on two benchmark datasets of SOIRE, and CORD,
are promising for the agentic AI framework.

</details>


### [390] [Beyond Retrieval: Joint Supervision and Multimodal Document Ranking for Textbook Question Answering](https://arxiv.org/abs/2505.13520)
*Hessa Alawwad,Usman Naseem,Areej Alhothali,Ali Alkhathlan,Amani Jamal*

Main category: cs.IR

TL;DR: 提出JETRTQA模型，通过多目标联合训练增强语义表示，提升教科书问答中复杂多模态文档检索的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在教育场景中难以实现精确的语义对齐和任务特定文档检索，影响问答效果。

Method: 基于检索-生成架构，结合排序监督和隐式答案监督，优化问题和文档的语义表示。

Result: 在CK12-QA数据集上验证集准确率提升2.4%，测试集提升11.1%，显著优于现有方法。

Conclusion: JETRTQA有效提升复杂教育语境下的文档检索相关性，推动多模态问答技术进步。

Abstract: Textbook question answering (TQA) is a complex task, requiring the
interpretation of complex multimodal context. Although recent advances have
improved overall performance, they often encounter difficulties in educational
settings where accurate semantic alignment and task-specific document retrieval
are essential. In this paper, we propose a novel approach to multimodal
textbook question answering by introducing a mechanism for enhancing semantic
representations through multi-objective joint training. Our model, Joint
Embedding Training With Ranking Supervision for Textbook Question Answering
(JETRTQA), is a multimodal learning framework built on a retriever--generator
architecture that uses a retrieval-augmented generation setup, in which a
multimodal large language model generates answers. JETRTQA is designed to
improve the relevance of retrieved documents in complex educational contexts.
Unlike traditional direct scoring approaches, JETRTQA learns to refine the
semantic representations of questions and documents through a supervised signal
that combines pairwise ranking and implicit supervision derived from answers.
We evaluate our method on the CK12-QA dataset and demonstrate that it
significantly improves the discrimination between informative and irrelevant
documents, even when they are long, complex, and multimodal. JETRTQA
outperforms the previous state of the art, achieving a 2.4\% gain in accuracy
on the validation set and 11.1\% on the test set.

</details>


### [391] [Geography-Aware Large Language Models for Next POI Recommendation](https://arxiv.org/abs/2505.13526)
*Zhao Liu,Wei Liu,Huajie Zhu,Jianxing Yu,Jian Yin,Wang-Chien Lee,Shun Wang*

Main category: cs.IR

TL;DR: 论文提出GA-LLM框架，通过地理坐标注入和POI对齐模块增强大语言模型在POI推荐任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在POI推荐任务中难以精确建模空间上下文和POI转移关系，需针对性解决这两个挑战。

Method: GA-LLM包含地理坐标注入模块（GCIM）和POI对齐模块（PAM），分别处理空间特征编码和POI转移关系建模。

Result: 在三个真实数据集上的实验表明，GA-LLM达到了最先进的性能。

Conclusion: GA-LLM有效提升了大语言模型在空间任务中的表现，为POI推荐提供了新解决方案。

Abstract: The next Point-of-Interest (POI) recommendation task aims to predict users'
next destinations based on their historical movement data and plays a key role
in location-based services and personalized applications. Accurate next POI
recommendation depends on effectively modeling geographic information and POI
transition relations, which are crucial for capturing spatial dependencies and
user movement patterns. While Large Language Models (LLMs) exhibit strong
capabilities in semantic understanding and contextual reasoning, applying them
to spatial tasks like next POI recommendation remains challenging. First, the
infrequent nature of specific GPS coordinates makes it difficult for LLMs to
model precise spatial contexts. Second, the lack of knowledge about POI
transitions limits their ability to capture potential POI-POI relationships. To
address these issues, we propose GA-LLM (Geography-Aware Large Language Model),
a novel framework that enhances LLMs with two specialized components. The
Geographic Coordinate Injection Module (GCIM) transforms GPS coordinates into
spatial representations using hierarchical and Fourier-based positional
encoding, enabling the model to understand geographic features from multiple
perspectives. The POI Alignment Module (PAM) incorporates POI transition
relations into the LLM's semantic space, allowing it to infer global POI
relationships and generalize to unseen POIs. Experiments on three real-world
datasets demonstrate the state-of-the-art performance of GA-LLM.

</details>


### [392] [LLM-Based User Simulation for Low-Knowledge Shilling Attacks on Recommender Systems](https://arxiv.org/abs/2505.13528)
*Shengkang Gu,Jiahao Liu,Dongsheng Li,Guangping Zhang,Mingzhe Han,Hansu Gu,Peng Zhang,Ning Gu,Li Shang,Tun Lu*

Main category: cs.IR

TL;DR: 论文提出Agent4SR框架，利用基于大语言模型的智能体进行低知识、高影响的推荐系统攻击，通过生成评分和评论实现高效且隐蔽的操纵。


<details>
  <summary>Details</summary>
Motivation: 传统推荐系统攻击方法依赖简单启发式规则，需访问内部数据且忽视文本评论的操纵潜力。针对这些局限，研究探索如何利用LLM智能体实现更真实的攻击。

Method: 开发Agent4SR框架：通过定向画像构建、混合记忆检索和评论攻击策略（将目标物品特征扩散至无关评论），模拟真实用户行为生成评分和文本。

Result: 在多数据集和推荐架构上的实验表明，Agent4SR在攻击效果和隐蔽性上均优于现有低知识基线方法。

Conclusion: LLM驱动的智能体构成新型威胁，凸显现代推荐系统亟需加强防御机制以应对此类攻击。

Abstract: Recommender systems (RS) are increasingly vulnerable to shilling attacks,
where adversaries inject fake user profiles to manipulate system outputs.
Traditional attack strategies often rely on simplistic heuristics, require
access to internal RS data, and overlook the manipulation potential of textual
reviews. In this work, we introduce Agent4SR, a novel framework that leverages
Large Language Model (LLM)-based agents to perform low-knowledge, high-impact
shilling attacks through both rating and review generation. Agent4SR simulates
realistic user behavior by orchestrating adversarial interactions, selecting
items, assigning ratings, and crafting reviews, while maintaining behavioral
plausibility. Our design includes targeted profile construction, hybrid memory
retrieval, and a review attack strategy that propagates target item features
across unrelated reviews to amplify manipulation. Extensive experiments on
multiple datasets and RS architectures demonstrate that Agent4SR outperforms
existing low-knowledge baselines in both effectiveness and stealth. Our
findings reveal a new class of emergent threats posed by LLM-driven agents,
underscoring the urgent need for enhanced defenses in modern recommender
systems.

</details>


### [393] [Information Extraction from Visually Rich Documents using LLM-based Organization of Documents into Independent Textual Segments](https://arxiv.org/abs/2505.13535)
*Aniket Bhattacharyya,Anurag Tripathi,Ujjal Das,Archan Karmakar,Amit Pathak,Maneesh Gupta*

Main category: cs.IR

TL;DR: BLOCKIE提出了一种基于LLM的新方法，通过将视觉丰富文档组织成可重用的语义块，提升了信息提取的性能和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有非LLM方法缺乏推理能力且泛化性差，而基于LLM的方法在理解文档布局方面表现不佳。

Method: 将文档分割为独立处理的语义块，进行聚焦和泛化推理。

Result: 在公共VRD基准测试中F1分数提升1-3%，对未见文档格式具有鲁棒性。

Conclusion: BLOCKIE在信息提取任务中表现出色，具备处理隐含信息和未知格式的能力。

Abstract: Information extraction (IE) from Visually Rich Documents (VRDs) containing
layout features along with text is a critical and well-studied task.
Specialized non-LLM NLP-based solutions typically involve training models using
both textual and geometric information to label sequences/tokens as named
entities or answers to specific questions. However, these approaches lack
reasoning, are not able to infer values not explicitly present in documents,
and do not generalize well to new formats. Generative LLM-based approaches
proposed recently are capable of reasoning, but struggle to comprehend clues
from document layout especially in previously unseen document formats, and do
not show competitive performance in heterogeneous VRD benchmark datasets. In
this paper, we propose BLOCKIE, a novel LLM-based approach that organizes VRDs
into localized, reusable semantic textual segments called $\textit{semantic
blocks}$, which are processed independently. Through focused and more
generalizable reasoning,our approach outperforms the state-of-the-art on public
VRD benchmarks by 1-3% in F1 scores, is resilient to document formats
previously not encountered and shows abilities to correctly extract information
not explicitly present in documents.

</details>


### [394] [RAGXplain: From Explainable Evaluation to Actionable Guidance of RAG Pipelines](https://arxiv.org/abs/2505.13538)
*Dvir Cohen,Lin Burg,Gilad Barkan*

Main category: cs.IR

TL;DR: RAGXplain是一个评估框架，通过将RAG系统的量化评分转化为可操作的改进建议，帮助用户理解和优化AI系统。


<details>
  <summary>Details</summary>
Motivation: 传统RAG评估方法仅提供量化分数，缺乏对复杂流程的具体指导，难以帮助用户优化系统。

Method: RAGXplain利用LLM推理将原始评分转化为清晰叙述，识别性能差距并提供针对性改进建议。

Result: 实验表明，RAGXplain的评估与人类判断高度一致，其建议能显著提升系统性能。

Conclusion: RAGXplain架起了量化评估与实际优化之间的桥梁，增强了用户对AI系统的理解和信任。

Abstract: Retrieval-Augmented Generation (RAG) systems show promise by coupling large
language models with external knowledge, yet traditional RAG evaluation methods
primarily report quantitative scores while offering limited actionable guidance
for refining these complex pipelines. In this paper, we introduce RAGXplain, an
evaluation framework that quantifies RAG performance and translates these
assessments into clear insights that clarify the workings of its complex,
multi-stage pipeline and offer actionable recommendations. Using LLM reasoning,
RAGXplain converts raw scores into coherent narratives identifying performance
gaps and suggesting targeted improvements. By providing transparent
explanations for AI decision-making, our framework fosters user trust-a key
challenge in AI adoption. Our LLM-based metric assessments show strong
alignment with human judgments, and experiments on public question-answering
datasets confirm that applying RAGXplain's actionable recommendations
measurably improves system performance. RAGXplain thus bridges quantitative
evaluation and practical optimization, empowering users to understand, trust,
and enhance their AI systems.

</details>


### [395] [Know Or Not: a library for evaluating out-of-knowledge base robustness](https://arxiv.org/abs/2505.13545)
*Jessica Foo,Pradyumna Shyama Prasad,Shaun Khoo*

Main category: cs.IR

TL;DR: 该论文提出了一种评估大型语言模型在检索增强生成（RAG）设置中对超出知识库问题的鲁棒性的新方法，并开发了开源工具knowornot来支持这一评估。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在高风险应用中的使用受到幻觉风险的限制，尤其是在面对超出知识库的问题时。为了确保LLMs在缺乏足够上下文时能够拒绝回答，需要一种系统化的评估方法。

Method: 论文提出了一种无需人工标注黄金标准答案的方法，用于评估LLMs在RAG设置中对超出知识库问题的鲁棒性，并开发了开源工具knowornot，提供统一的API、模块化架构、严格的数据建模设计以及定制化工具。

Result: 通过开发一个名为PolicyBench的具有挑战性的基准测试，论文展示了knowornot的实用性，该基准测试涵盖了四个关于政府政策的问答聊天机器人，并分析了其超出知识库的鲁棒性。

Conclusion: knowornot工具为评估LLMs在RAG设置中的鲁棒性提供了一种有效且灵活的方法，有助于在高风险应用中减少幻觉风险。

Abstract: While the capabilities of large language models (LLMs) have progressed
significantly, their use in high-stakes applications have been limited due to
risks of hallucination. One key approach in reducing hallucination is
retrieval-augmented generation (RAG), but even in such setups, LLMs may still
hallucinate when presented with questions outside of the knowledge base. Such
behavior is unacceptable in high-stake applications where LLMs are expected to
abstain from answering queries it does not have sufficient context on. In this
work, we present a novel methodology for systematically evaluating
out-of-knowledge base (OOKB) robustness of LLMs (whether LLMs know or do not
know) in the RAG setting, without the need for manual annotation of gold
standard answers. We implement our methodology in knowornot, an open-source
library that enables users to develop their own customized evaluation data and
pipelines for OOKB robustness. knowornot comprises four main features. Firstly,
it provides a unified, high-level API that streamlines the process of setting
up and running robustness benchmarks. Secondly, its modular architecture
emphasizes extensibility and flexibility, allowing users to easily integrate
their own LLM clients and RAG settings. Thirdly, its rigorous data modeling
design ensures experiment reproducibility, reliability and traceability.
Lastly, it implements a comprehensive suite of tools for users to customize
their pipelines. We demonstrate the utility of knowornot by developing a
challenging benchmark, PolicyBench, which spans four Question-Answer (QA)
chatbots on government policies, and analyze its OOKB robustness. The source
code of knowornot is available
https://github.com/govtech-responsibleai/KnowOrNot.

</details>


### [396] [JIR-Arena: The First Benchmark Dataset for Just-in-time Information Recommendation](https://arxiv.org/abs/2505.13550)
*Ke Yang,Kevin Ros,Shankar Kumar Senthil Kumar,ChengXiang Zhai*

Main category: cs.IR

TL;DR: 该论文首次定义了即时信息推荐（JIR）任务并建立了评估框架，提出了JIR-Arena多模态基准数据集，并开发了一个基线JIR系统进行验证。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏对即时信息推荐（JIR）任务的系统性定义和评估框架，尽管技术进步使其成为可能。本文旨在填补这一空白。

Method: 提出JIR任务的数学定义和评估指标，创建JIR-Arena多模态基准数据集，并开发一个基线JIR系统处理实时信息流。

Result: 基于基础模型的JIR系统在模拟用户需求时精度合理，但在召回率和内容检索效率方面存在挑战。

Conclusion: JIR-Arena为未来研究提供了基准和工具，尽管现有系统仍有改进空间，特别是在召回率和内容检索方面。

Abstract: Just-in-time Information Recommendation (JIR) is a service designed to
deliver the most relevant information precisely when users need it, ,
addressing their knowledge gaps with minimal effort and boosting
decision-making and efficiency in daily life. Advances in device-efficient
deployment of foundation models and the growing use of intelligent wearable
devices have made always-on JIR assistants feasible. However, there has been no
systematic effort to formally define JIR tasks or establish evaluation
frameworks. To bridge this gap, we present the first mathematical definition of
JIR tasks and associated evaluation metrics. Additionally, we introduce
JIR-Arena, a multimodal benchmark dataset featuring diverse,
information-request-intensive scenarios to evaluate JIR systems across critical
dimensions: i) accurately inferring user information needs, ii) delivering
timely and relevant recommendations, and iii) avoiding irrelevant content that
may distract users.
  Developing a JIR benchmark dataset poses challenges due to subjectivity in
estimating user information needs and uncontrollable system variables affecting
reproducibility. To address these, JIR-Arena: i) combines input from multiple
humans and large AI models to approximate information need distributions; ii)
assesses JIR quality through information retrieval outcomes using static
knowledge base snapshots; and iii) employs a multi-turn, multi-entity
validation framework to improve objectivity and generality. Furthermore, we
implement a baseline JIR system capable of processing real-time information
streams aligned with user inputs. Our evaluation of this baseline system on
JIR-Arena indicates that while foundation model-based JIR systems simulate user
needs with reasonable precision, they face challenges in recall and effective
content retrieval. To support future research in this new area, we fully
release our code and data.

</details>


### [397] [AMAQA: A Metadata-based QA Dataset for RAG Systems](https://arxiv.org/abs/2505.13557)
*Davide Bruni,Marco Avvenuti,Nicola Tonellotto,Maurizio Tesconi*

Main category: cs.IR

TL;DR: AMAQA是一个新的开放访问QA数据集，旨在评估结合文本和元数据的任务，特别适用于需要快速分析大量数据的领域，如网络安全和情报。


<details>
  <summary>Details</summary>
Motivation: 当前的检索增强生成（RAG）系统在QA任务中广泛使用，但现有基准缺乏元数据集成，限制了在需要结合文本和外部信息场景下的评估。

Method: AMAQA数据集包含约110万条来自26个公共Telegram群的英文消息，并丰富了时间戳、主题、情感基调等元数据，以及450个高质量QA对。

Result: 利用元数据可将准确率从0.12提升至0.61，展示了结构化上下文的价值。通过迭代上下文和丰富噪声文档，进一步提升了性能。

Conclusion: AMAQA是首个集成元数据和消息标签的单跳QA基准，为未来研究设立了新标准，并显著提升了QA系统的性能。

Abstract: Retrieval-augmented generation (RAG) systems are widely used in
question-answering (QA) tasks, but current benchmarks lack metadata
integration, hindering evaluation in scenarios requiring both textual data and
external information. To address this, we present AMAQA, a new open-access QA
dataset designed to evaluate tasks combining text and metadata. The integration
of metadata is especially important in fields that require rapid analysis of
large volumes of data, such as cybersecurity and intelligence, where timely
access to relevant information is critical. AMAQA includes about 1.1 million
English messages collected from 26 public Telegram groups, enriched with
metadata such as timestamps, topics, emotional tones, and toxicity indicators,
which enable precise and contextualized queries by filtering documents based on
specific criteria. It also includes 450 high-quality QA pairs, making it a
valuable resource for advancing research on metadata-driven QA and RAG systems.
To the best of our knowledge, AMAQA is the first single-hop QA benchmark to
incorporate metadata and labels such as topics covered in the messages. We
conduct extensive tests on the benchmark, establishing a new standard for
future research. We show that leveraging metadata boosts accuracy from 0.12 to
0.61, highlighting the value of structured context. Building on this, we
explore several strategies to refine the LLM input by iterating over provided
context and enriching it with noisy documents, achieving a further 3-point gain
over the best baseline and a 14-point improvement over simple metadata
filtering. The dataset is available at
https://anonymous.4open.science/r/AMAQA-5D0D/

</details>


### [398] [MedEIR: A Specialized Medical Embedding Model for Enhanced Information Retrieval](https://arxiv.org/abs/2505.13482)
*Anand Selvadurai,Jasheen Shaik,Girish Chandrasekar,ShriRadhaKrishnan Balamurugan,Eswara Reddy*

Main category: cs.IR

TL;DR: MedEIR是一种新型嵌入模型和分词器，专为医疗和通用NLP任务联合优化，支持长达8,192个token的长文本处理，在多个基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 当前嵌入模型在医疗文档语义捕捉、长文本处理等方面存在局限，通用领域分词器对医疗词汇理解不足，需要更通用的解决方案。

Method: MedEIR结合了ALiBi长文本处理技术，预训练仅使用60亿token，随后在300万句对上进行微调。

Result: MedEIR在MTEB基准测试中全面超越Jina V2和MiniLM，在ArguAna、NFCorpus、MedicalQARetrieval等任务中取得最高分。

Conclusion: MedEIR在通用和特定领域任务中均表现出色，成为高效嵌入模型的潜力选择。

Abstract: Embedding models have become essential for retrieval-augmented generation
(RAG) tasks, semantic clustering, and text re-ranking. But despite their
growing use, many of these come with notable limitations. For example, Jina
fails to capture the semantic content of medical documents, while models such
as MiniLM often perform poorly on long-form documents. Domain-adapted models,
while specialized, often underperform in general-purpose tasks, reducing their
overall applicability. General-domain tokenizers often misinterpret medical
vocabulary. The limitations of current embedding models, whether in
tokenization accuracy, domain comprehension, or handling long sequences,
highlight the need for more versatile solutions. In this work, we present
MedEIR, a novel embedding model and tokenizer jointly optimized for both
medical and general NLP tasks, incorporating ALiBi-based long-context
processing to support sequences of up to 8,192 tokens. MedEIR was pre-trained
on only 6 billion tokens, significantly fewer than Jina's, followed by
fine-tuning on 3 million sentence pairs. MedEIR consistently outperforms Jina
V2 and MiniLM across MTEB benchmarks, achieving top scores on ArguAna (55.24),
NFCorpus (38.44), MedicalQARetrieval (74.25), SciFact (72.04), and TRECCOVID
(79.56). These results highlight the potential of MedEIR as a highly effective
embedding model, demonstrating strong performance across both general-purpose
and domain-specific tasks and outperforming existing models on multiple
benchmarks.

</details>


### [399] [RAR: Setting Knowledge Tripwires for Retrieval Augmented Rejection](https://arxiv.org/abs/2505.13581)
*Tommaso Mario Buonocore,Enea Parimbelli*

Main category: cs.IR

TL;DR: 论文提出RAR方法，通过检索增强生成架构动态拒绝不安全查询，无需重新训练模型，性能媲美嵌入式审核且更灵活。


<details>
  <summary>Details</summary>
Motivation: 大语言模型的内容审核需快速响应新威胁，现有方法缺乏实时定制能力。

Method: 在向量数据库中插入标记的恶意文档，基于检索结果触发简单拒绝机制。

Result: RAR性能接近Claude 3.5 Sonnet的嵌入式审核，同时具备实时定制优势。

Conclusion: RAR为现有RAG系统提供零架构修改的安全增强方案。

Abstract: Content moderation for large language models (LLMs) remains a significant
challenge, requiring flexible and adaptable solutions that can quickly respond
to emerging threats. This paper introduces Retrieval Augmented Rejection (RAR),
a novel approach that leverages a retrieval-augmented generation (RAG)
architecture to dynamically reject unsafe user queries without model
retraining. By strategically inserting and marking malicious documents into the
vector database, the system can identify and reject harmful requests when these
documents are retrieved. Our preliminary results show that RAR achieves
comparable performance to embedded moderation in LLMs like Claude 3.5 Sonnet,
while offering superior flexibility and real-time customization capabilities, a
fundamental feature to timely address critical vulnerabilities. This approach
introduces no architectural changes to existing RAG systems, requiring only the
addition of specially crafted documents and a simple rejection mechanism based
on retrieval results.

</details>


### [400] [Boosting LLM-based Relevance Modeling with Distribution-Aware Robust Learning](https://arxiv.org/abs/2412.12504)
*Hong Liu,Saisai Gong,Yixin Ji,Kaixin Wu,Jia Xu,Jinjie Gu*

Main category: cs.IR

TL;DR: 论文提出了一种名为DaRL的新框架，通过改进损失函数和样本增强技术，提升大语言模型在支付宝搜索中的相关性建模能力，解决了细粒度区分和数据分布偏移的问题。


<details>
  <summary>Details</summary>
Motivation: 当前基于大语言模型（LLM）的相关性建模存在两个主要问题：一是难以区分细粒度的相关性等级（如强相关、弱相关、无关），二是面对真实场景中的数据分布偏移时性能显著下降。

Method: 提出Distribution-Aware Robust Learning（DaRL）框架，包括设计新的损失函数以增强细粒度相关性区分能力，以及Distribution-Aware Sample Augmentation（DASA）模块，通过OOD检测技术选择未被训练集充分覆盖的样本进行微调，并采用多阶段微调策略同步提升ID和OOD性能。

Result: DaRL框架已成功部署于支付宝的保险产品搜索服务中，显著提升了相关性建模的性能和泛化能力。

Conclusion: DaRL框架有效解决了LLM在相关性建模中的细粒度区分和数据分布偏移问题，为实际应用提供了可靠解决方案。

Abstract: With the rapid advancement of pre-trained large language models (LLMs),
recent endeavors have leveraged the capabilities of LLMs in relevance modeling,
resulting in enhanced performance. This is usually done through the process of
fine-tuning LLMs on specifically annotated datasets to determine the relevance
between queries and items. However, there are two limitations when LLMs are
naively employed for relevance modeling through fine-tuning and inference.
First, it is not inherently efficient for performing nuanced tasks beyond
simple yes or no answers, such as assessing search relevance. It may therefore
tend to be overconfident and struggle to distinguish fine-grained degrees of
relevance (e.g., strong relevance, weak relevance, irrelevance) used in search
engines. Second, it exhibits significant performance degradation when
confronted with data distribution shift in real-world scenarios. In this paper,
we propose a novel Distribution-Aware Robust Learning framework (DaRL) for
relevance modeling in Alipay Search. Specifically, we design an effective loss
function to enhance the discriminability of LLM-based relevance modeling across
various fine-grained degrees of query-item relevance. To improve the
generalizability of LLM-based relevance modeling, we first propose the
Distribution-Aware Sample Augmentation (DASA) module. This module utilizes
out-of-distribution (OOD) detection techniques to actively select appropriate
samples that are not well covered by the original training set for model
fine-tuning. Furthermore, we adopt a multi-stage fine-tuning strategy to
simultaneously improve in-distribution (ID) and OOD performance, bridging the
performance gap between them. DaRL has been deployed online to serve the
Alipay's insurance product search...

</details>


### [401] [LLM-Based Compact Reranking with Document Features for Scientific Retrieval](https://arxiv.org/abs/2505.13757)
*Runchu Tian,Xueqiang Xu,Bowen Jin,SeongKu Kang,Jiawei Han*

Main category: cs.IR

TL;DR: 论文提出了一种名为CoRank的无训练、模型无关的框架，通过紧凑文档表示和两阶段重排序提升科学文献检索性能。


<details>
  <summary>Details</summary>
Motivation: 科学文献检索中，传统基于LLM的列表重排序方法面临两大挑战：初检结果质量不高导致相关文档排名靠后，以及全文重排序的上下文窗口限制候选文档数量。

Method: CoRank框架采用三阶段设计：(1)离线提取文档级语义特征（如类别、章节、关键词）；(2)基于紧凑表示的粗粒度重排序扩大候选池；(3)对筛选出的Top文档进行全文细粒度重排序。

Result: 在LitSearch和CSFCube数据集上，CoRank将nDCG@10从32.0提升至39.7，显著优于基线方法且适配不同LLM骨干网络。

Conclusion: 研究表明，结合语义特征提取的混合重排序策略能有效突破科学检索中的上下文限制，同时兼顾语义抽象与精确排序需求。

Abstract: Scientific retrieval is essential for advancing academic discovery. Within
this process, document reranking plays a critical role by refining first-stage
retrieval results. However, large language model (LLM) listwise reranking faces
unique challenges in the scientific domain. First-stage retrieval is often
suboptimal in the scientific domain, so relevant documents are ranked lower.
Moreover, conventional listwise reranking uses the full text of candidate
documents in the context window, limiting the number of candidates that can be
considered. As a result, many relevant documents are excluded before reranking,
which constrains overall retrieval performance. To address these challenges, we
explore compact document representations based on semantic features such as
categories, sections, and keywords, and propose a training-free, model-agnostic
reranking framework for scientific retrieval called CoRank. The framework
involves three stages: (i) offline extraction of document-level features, (ii)
coarse reranking using these compact representations, and (iii) fine-grained
reranking on full texts of the top candidates from stage (ii). This hybrid
design provides a high-level abstraction of document semantics, expands
candidate coverage, and retains critical details required for precise ranking.
Experiments on LitSearch and CSFCube show that CoRank significantly improves
reranking performance across different LLM backbones, increasing nDCG@10 from
32.0 to 39.7. Overall, these results highlight the value of information
extraction for reranking in scientific retrieval.

</details>


### [402] [Field Matters: A lightweight LLM-enhanced Method for CTR Prediction](https://arxiv.org/abs/2505.14057)
*Yu Cui,Feng Liu,Jiawei Chen,Xingyu Lou,Changwang Zhang,Jun Wang,Yuegang Sun,Xiaohu Yang,Can Wang*

Main category: cs.IR

TL;DR: LLaCTR是一种轻量级的LLM增强CTR预测方法，通过字段级语义知识提升效果与效率。


<details>
  <summary>Details</summary>
Motivation: 现有LLM增强CTR方法需处理大规模文本描述，计算开销大，需轻量化解决方案。

Method: LLaCTR利用LLM从小规模特征字段蒸馏轻量语义知识，增强特征表示与交互。

Result: 在四个数据集上，LLaCTR在六种CTR模型中均表现优于现有LLM增强方法。

Conclusion: LLaCTR以更低计算成本实现高效CTR预测，为推荐系统提供实用增强方案。

Abstract: Click-through rate (CTR) prediction is a fundamental task in modern
recommender systems. In recent years, the integration of large language models
(LLMs) has been shown to effectively enhance the performance of traditional CTR
methods. However, existing LLM-enhanced methods often require extensive
processing of detailed textual descriptions for large-scale instances or
user/item entities, leading to substantial computational overhead. To address
this challenge, this work introduces LLaCTR, a novel and lightweight
LLM-enhanced CTR method that employs a field-level enhancement paradigm.
Specifically, LLaCTR first utilizes LLMs to distill crucial and lightweight
semantic knowledge from small-scale feature fields through self-supervised
field-feature fine-tuning. Subsequently, it leverages this field-level semantic
knowledge to enhance both feature representation and feature interactions. In
our experiments, we integrate LLaCTR with six representative CTR models across
four datasets, demonstrating its superior performance in terms of both
effectiveness and efficiency compared to existing LLM-enhanced methods. Our
code is available at https://anonymous.4open.science/r/LLaCTR-EC46.

</details>


### [403] [Rank-K: Test-Time Reasoning for Listwise Reranking](https://arxiv.org/abs/2505.14432)
*Eugene Yang,Andrew Yates,Kathryn Ricci,Orion Weller,Vivek Chari,Benjamin Van Durme,Dawn Lawrie*

Main category: cs.IR

TL;DR: Rank-K是一种新型的列表式段落重排序模型，利用大型语言模型的推理能力，显著提升了检索效率，尤其在多语言环境下表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有的神经重排序模型虽然有效，但计算资源消耗大，即使经过优化也难以满足实际需求。因此，需要一种更高效且能处理复杂查询的重排序方法。

Method: Rank-K是一种列表式段落重排序模型，利用大型语言模型的推理能力，在查询时提供可扩展性，特别适用于处理复杂查询。

Result: Rank-K在BM25初始排序列表上的检索效率比当前最先进的列表式重排序模型RankZephyr提高了23%，在SPLADE-v3的强检索结果上提高了19%。此外，Rank-K在多语言检索中表现同样出色。

Conclusion: Rank-K通过利用大型语言模型的推理能力，显著提升了检索效率，并在多语言环境下表现出色，为高效重排序提供了新的解决方案。

Abstract: Retrieve-and-rerank is a popular retrieval pipeline because of its ability to
make slow but effective rerankers efficient enough at query time by reducing
the number of comparisons. Recent works in neural rerankers take advantage of
large language models for their capability in reasoning between queries and
passages and have achieved state-of-the-art retrieval effectiveness. However,
such rerankers are resource-intensive, even after heavy optimization. In this
work, we introduce Rank-K, a listwise passage reranking model that leverages
the reasoning capability of the reasoning language model at query time that
provides test time scalability to serve hard queries. We show that Rank-K
improves retrieval effectiveness by 23\% over the RankZephyr, the
state-of-the-art listwise reranker, when reranking a BM25 initial ranked list
and 19\% when reranking strong retrieval results by SPLADE-v3. Since Rank-K is
inherently a multilingual model, we found that it ranks passages based on
queries in different languages as effectively as it does in monolingual
retrieval.

</details>


### [404] [TranSUN: A Preemptive Paradigm to Eradicate Retransformation Bias Intrinsically from Regression Models in Recommender Systems](https://arxiv.org/abs/2505.13881)
*Jiahao Yu,Haozhuang Liu,Yeqiu Yang,Lu Chen,Wu Jian,Yuning Jiang,Bo Zheng*

Main category: cs.IR

TL;DR: 该论文提出了一种名为TranSUN的新方法，通过模型内部微调解决推荐系统中回归模型的再转换偏差问题，并进一步推广为通用回归模型家族GTS，在实际工业推荐场景中验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 推荐系统中的回归模型存在再转换偏差问题，现有方法多为事后修正，难以实际应用。本文旨在从模型内部消除偏差，提出更有效的解决方案。

Method: 提出TranSUN方法，采用联合偏差学习方式，确保理论上的无偏性；并推广为通用回归模型家族GTS，提供灵活开发无偏模型的框架。

Result: 实验结果表明，该方法在多个领域数据上表现优越，并成功应用于淘宝App首页的猜你喜欢业务场景，服务于主要在线流量。

Conclusion: TranSUN和GTS不仅解决了再转换偏差问题，还提供了理论保证和实际应用价值，在工业推荐系统中具有广泛适用性。

Abstract: Regression models are crucial in recommender systems. However,
retransformation bias problem has been conspicuously neglected within the
community. While many works in other fields have devised effective bias
correction methods, all of them are post-hoc cures externally to the model,
facing practical challenges when applied to real-world recommender systems.
Hence, we propose a preemptive paradigm to eradicate the bias intrinsically
from the models via minor model refinement. Specifically, a novel TranSUN
method is proposed with a joint bias learning manner to offer theoretically
guaranteed unbiasedness under empirical superior convergence. It is further
generalized into a novel generic regression model family, termed Generalized
TranSUN (GTS), which not only offers more theoretical insights but also serves
as a generic framework for flexibly developing various bias-free models.
Comprehensive experimental results demonstrate the superiority of our methods
across data from various domains, which have been successfully deployed in two
real-world industrial recommendation scenarios, i.e. product and short video
recommendation scenarios in Guess What You Like business domain in the homepage
of Taobao App (a leading e-commerce platform), to serve the major online
traffic. Codes will be released after this paper is published.

</details>


### [405] [NExT-Search: Rebuilding User Feedback Ecosystem for Generative AI Search](https://arxiv.org/abs/2505.14680)
*Sunhao Dai,Wenjie Wang,Liang Pang,Jun Xu,See-Kiong Ng,Ji-Rong Wen,Tat-Seng Chua*

Main category: cs.IR

TL;DR: 生成式AI搜索通过端到端答案改变信息检索，但缺乏细粒度反馈循环。NExT-Search提出新范式，通过用户调试模式和影子用户模式重新引入细粒度反馈，以持续优化搜索系统。


<details>
  <summary>Details</summary>
Motivation: 生成式AI搜索虽然便捷，但破坏了传统Web搜索依赖的细粒度用户反馈循环，导致难以优化中间环节。论文旨在解决这一反馈断层问题。

Method: 提出NExT-Search框架，包含两种模式：用户调试模式（允许干预关键阶段）和影子用户模式（AI代理模拟用户偏好），并结合在线实时调整与离线模型更新。

Result: NExT-Search通过重建细粒度反馈机制，使生成式AI搜索能持续优化查询分解、检索和生成等环节，形成闭环改进系统。

Conclusion: 通过赋予用户对生成式AI搜索关键环节的控制权，NExT-Search为构建可随人类反馈持续进化的AI搜索系统提供了可行路径。

Abstract: Generative AI search is reshaping information retrieval by offering
end-to-end answers to complex queries, reducing users' reliance on manually
browsing and summarizing multiple web pages. However, while this paradigm
enhances convenience, it disrupts the feedback-driven improvement loop that has
historically powered the evolution of traditional Web search. Web search can
continuously improve their ranking models by collecting large-scale,
fine-grained user feedback (e.g., clicks, dwell time) at the document level. In
contrast, generative AI search operates through a much longer search pipeline,
spanning query decomposition, document retrieval, and answer generation, yet
typically receives only coarse-grained feedback on the final answer. This
introduces a feedback loop disconnect, where user feedback for the final output
cannot be effectively mapped back to specific system components, making it
difficult to improve each intermediate stage and sustain the feedback loop. In
this paper, we envision NExT-Search, a next-generation paradigm designed to
reintroduce fine-grained, process-level feedback into generative AI search.
NExT-Search integrates two complementary modes: User Debug Mode, which allows
engaged users to intervene at key stages; and Shadow User Mode, where a
personalized user agent simulates user preferences and provides AI-assisted
feedback for less interactive users. Furthermore, we envision how these
feedback signals can be leveraged through online adaptation, which refines
current search outputs in real-time, and offline update, which aggregates
interaction logs to periodically fine-tune query decomposition, retrieval, and
generation models. By restoring human control over key stages of the generative
AI search pipeline, we believe NExT-Search offers a promising direction for
building feedback-rich AI search systems that can evolve continuously alongside
human feedback.

</details>


### [406] [Taming Recommendation Bias with Causal Intervention on Evolving Personal Popularity](https://arxiv.org/abs/2505.14310)
*Shiyin Tan,Dongyuan Li,Renhe Jiang,Zhen Wang,Xingtong Yu,Manabu Okumura*

Main category: cs.IR

TL;DR: 论文提出CausalEPP方法，通过量化用户对流行项目的偏好并考虑其时间演化，有效减少推荐系统中的流行度偏差。


<details>
  <summary>Details</summary>
Motivation: 现有方法在减轻流行度偏差时通常对所有用户一视同仁，且未充分考虑用户或项目的时间演化。用户对流行项目的偏好不同且随时间变化，因此需要更精细的方法。

Method: 引入'演化个人流行度'指标量化用户偏好，设计因果图整合演化个人流行度与从众效应，并应用去混淆训练减少偏差。推理时考虑用户与项目的演化一致性。

Result: 实证研究表明，CausalEPP在减少流行度偏差的同时提高了推荐准确性，优于基线方法。

Conclusion: CausalEPP通过考虑用户偏好的时间演化，有效解决了推荐系统中的流行度偏差问题，提升了推荐性能。

Abstract: Popularity bias occurs when popular items are recommended far more frequently
than they should be, negatively impacting both user experience and
recommendation accuracy. Existing debiasing methods mitigate popularity bias
often uniformly across all users and only partially consider the time evolution
of users or items. However, users have different levels of preference for item
popularity, and this preference is evolving over time. To address these issues,
we propose a novel method called CausalEPP (Causal Intervention on Evolving
Personal Popularity) for taming recommendation bias, which accounts for the
evolving personal popularity of users. Specifically, we first introduce a
metric called {Evolving Personal Popularity} to quantify each user's preference
for popular items. Then, we design a causal graph that integrates evolving
personal popularity into the conformity effect, and apply deconfounded training
to mitigate the popularity bias of the causal graph. During inference, we
consider the evolution consistency between users and items to achieve a better
recommendation. Empirical studies demonstrate that CausalEPP outperforms
baseline methods in reducing popularity bias while improving recommendation
accuracy.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [407] [RAG/LLM Augmented Switching Driven Polymorphic Metaheuristic Framework](https://arxiv.org/abs/2505.13808)
*Faramarz Safi Esfahani,Ghassan Beydoun,Morteza Saberi,Brad McCusker,Biswajeet Pradhan*

Main category: cs.NE

TL;DR: 论文提出了一种自适应的多态元启发式框架（PMF），通过实时性能反馈和动态算法选择，提升优化效率，适用于高维、动态和多模态环境。


<details>
  <summary>Details</summary>
Motivation: 传统元启发式算法因固定结构和需要大量调参而效果受限，PMF旨在解决这一问题，实现动态选择和切换算法。

Method: PMF利用多态元启发式代理（PMA）和选择代理（PMSA），基于关键性能指标动态选择和切换算法，确保持续适应。

Result: 实验表明，PMF显著提升了收敛速度、适应性和解的质量，在多种问题场景中优于传统方法。

Conclusion: PMF通过AI驱动决策和自校正机制，为可扩展、智能和自主的优化框架奠定了基础，适用于工程、物流等复杂系统。

Abstract: Metaheuristic algorithms are widely used for solving complex optimization
problems, yet their effectiveness is often constrained by fixed structures and
the need for extensive tuning. The Polymorphic Metaheuristic Framework (PMF)
addresses this limitation by introducing a self-adaptive metaheuristic
switching mechanism driven by real-time performance feedback and dynamic
algorithmic selection. PMF leverages the Polymorphic Metaheuristic Agent (PMA)
and the Polymorphic Metaheuristic Selection Agent (PMSA) to dynamically select
and transition between metaheuristic algorithms based on key performance
indicators, ensuring continuous adaptation. This approach enhances convergence
speed, adaptability, and solution quality, outperforming traditional
metaheuristics in high-dimensional, dynamic, and multimodal environments.
Experimental results on benchmark functions demonstrate that PMF significantly
improves optimization efficiency by mitigating stagnation and balancing
exploration-exploitation strategies across various problem landscapes. By
integrating AI-driven decision-making and self-correcting mechanisms, PMF paves
the way for scalable, intelligent, and autonomous optimization frameworks, with
promising applications in engineering, logistics, and complex decision-making
systems.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [408] [Sat2Sound: A Unified Framework for Zero-Shot Soundscape Mapping](https://arxiv.org/abs/2505.13777)
*Subash Khanal,Srikumar Sastry,Aayush Dhakal,Adeel Ahmad,Nathan Jacobs*

Main category: cs.CV

TL;DR: Sat2Sound是一个多模态表示学习框架，用于预测地球上任意位置的声音分布，通过结合卫星图像和音频数据，并利用视觉语言模型生成丰富的声景描述，实现了跨模态检索的最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖卫星图像和地理标记的音频样本，但往往无法捕捉特定位置声音的多样性。为了解决这一问题，作者提出了Sat2Sound框架，通过增强数据集和跨模态对比学习来改进声景映射。

Method: Sat2Sound利用视觉语言模型生成卫星图像的声景描述，通过对比学习结合音频、音频描述、卫星图像和卫星图像描述，学习共享的声景概念代码本，并将每个样本表示为这些概念的加权平均。

Result: Sat2Sound在GeoSound和SoundingEarth两个数据集上实现了卫星图像与音频之间跨模态检索的最先进性能，并引入了基于位置的声景合成新应用。

Conclusion: Sat2Sound通过多模态表示学习和共享声景概念代码本，显著提升了声景映射的性能，并开辟了沉浸式声景体验的新应用场景。

Abstract: We present Sat2Sound, a multimodal representation learning framework for
soundscape mapping, designed to predict the distribution of sounds at any
location on Earth. Existing methods for this task rely on satellite image and
paired geotagged audio samples, which often fail to capture the diversity of
sound sources at a given location. To address this limitation, we enhance
existing datasets by leveraging a Vision-Language Model (VLM) to generate
semantically rich soundscape descriptions for locations depicted in satellite
images. Our approach incorporates contrastive learning across audio, audio
captions, satellite images, and satellite image captions. We hypothesize that
there is a fixed set of soundscape concepts shared across modalities. To this
end, we learn a shared codebook of soundscape concepts and represent each
sample as a weighted average of these concepts. Sat2Sound achieves
state-of-the-art performance in cross-modal retrieval between satellite image
and audio on two datasets: GeoSound and SoundingEarth. Additionally, building
on Sat2Sound's ability to retrieve detailed soundscape captions, we introduce a
novel application: location-based soundscape synthesis, which enables immersive
acoustic experiences. Our code and models will be publicly available.

</details>


### [409] [An Edge AI Solution for Space Object Detection](https://arxiv.org/abs/2505.13468)
*Wenxuan Zhang,Peng Hu*

Main category: cs.CV

TL;DR: 该论文提出了一种基于深度学习的边缘AI解决方案，用于空间物体检测任务，结合SE层、Vision Transformers和YOLOv9框架，实现了高精度和低延迟的检测。


<details>
  <summary>Details</summary>
Motivation: 随着近地轨道空间资产的增加，实时碰撞评估和避碰变得至关重要。低地球轨道卫星需要高精度、低延迟的空间物体检测能力。

Method: 论文提出了一种基于Squeeze-and-Excitation层、Vision Transformers和YOLOv9框架的深度学习模型，用于空间物体检测任务。

Result: 模型在多种实际场景中表现出色，能够高精度、低延迟地检测多个卫星。

Conclusion: 该边缘AI解决方案在空间物体检测任务中具有高效性和实用性，为实时碰撞评估和避碰提供了有效支持。

Abstract: Effective Edge AI for space object detection (SOD) tasks that can facilitate
real-time collision assessment and avoidance is essential with the increasing
space assets in near-Earth orbits. In SOD, low Earth orbit (LEO) satellites
must detect other objects with high precision and minimal delay. We explore an
Edge AI solution based on deep-learning-based vision sensing for SOD tasks and
propose a deep learning model based on Squeeze-and-Excitation (SE) layers,
Vision Transformers (ViT), and YOLOv9 framework. We evaluate the performance of
these models across various realistic SOD scenarios, demonstrating their
ability to detect multiple satellites with high accuracy and very low latency.

</details>


### [410] [Domain Adaptation of VLM for Soccer Video Understanding](https://arxiv.org/abs/2505.13860)
*Tiancheng Jiang,Henry Wang,Md Sirajus Salekin,Parmida Atighehchian,Shinan Zhang*

Main category: cs.CV

TL;DR: 该研究探索了开源视觉语言模型（VLM）在特定领域（如足球）的适应性，通过课程学习和指令微调显著提升了模型在足球相关任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 当前大多数视频理解VLM研究是领域无关的，缺乏对模型在特定领域迁移学习能力的深入探索。本研究以足球为例，填补了这一空白。

Method: 利用大规模足球数据集和LLM生成指令跟随数据，采用课程学习方式（先学习足球概念，再执行问答任务）迭代微调通用VLM。

Result: 最终模型在足球视觉问答任务上相对提升37.5%，足球动作分类任务准确率从11.8%提升至63.5%。

Conclusion: 研究表明领域适配方法能有效提升VLM在专业场景的性能，为其他垂直领域的迁移学习提供了参考。

Abstract: Vision Language Models (VLMs) have demonstrated strong performance in
multi-modal tasks by effectively aligning visual and textual representations.
However, most video understanding VLM research has been domain-agnostic,
leaving the understanding of their transfer learning capability to specialized
domains under-explored. In this work, we address this by exploring the
adaptability of open-source VLMs to specific domains, and focusing on soccer as
an initial case study. Our approach uses large-scale soccer datasets and LLM to
create instruction-following data, and use them to iteratively fine-tune the
general-domain VLM in a curriculum learning fashion (first teaching the model
key soccer concepts to then question answering tasks). The final adapted model,
trained using a curated dataset of 20k video clips, exhibits significant
improvement in soccer-specific tasks compared to the base model, with a 37.5%
relative improvement for the visual question-answering task and an accuracy
improvement from 11.8% to 63.5% for the downstream soccer action classification
task.

</details>


### [411] [RADAR: Enhancing Radiology Report Generation with Supplementary Knowledge Injection](https://arxiv.org/abs/2505.14318)
*Wenjun Hou,Yi Cheng,Kaishuai Xu,Heng Li,Yan Hu,Wenjie Li,Jiang Liu*

Main category: cs.CV

TL;DR: 本文提出RADAR框架，通过结合大语言模型内部知识与外部检索信息，提升放射学报告生成的准确性和信息量。


<details>
  <summary>Details</summary>
Motivation: 现有方法在利用多模态大语言模型生成放射学报告时，常忽略模型内部已嵌入的知识，导致信息冗余和低效利用。

Method: RADAR框架首先提取模型内部与专家图像分类输出一致的知识，再检索外部补充知识，最后整合两者生成报告。

Result: 在MIMIC-CXR、CheXpert-Plus和IU X-ray数据集上的实验表明，RADAR在语言质量和临床准确性上优于现有方法。

Conclusion: RADAR通过系统整合内部和外部知识，显著提升了放射学报告生成的质量。

Abstract: Large language models (LLMs) have demonstrated remarkable capabilities in
various domains, including radiology report generation. Previous approaches
have attempted to utilize multimodal LLMs for this task, enhancing their
performance through the integration of domain-specific knowledge retrieval.
However, these approaches often overlook the knowledge already embedded within
the LLMs, leading to redundant information integration and inefficient
utilization of learned representations. To address this limitation, we propose
RADAR, a framework for enhancing radiology report generation with supplementary
knowledge injection. RADAR improves report generation by systematically
leveraging both the internal knowledge of an LLM and externally retrieved
information. Specifically, it first extracts the model's acquired knowledge
that aligns with expert image-based classification outputs. It then retrieves
relevant supplementary knowledge to further enrich this information. Finally,
by aggregating both sources, RADAR generates more accurate and informative
radiology reports. Extensive experiments on MIMIC-CXR, CheXpert-Plus, and IU
X-ray demonstrate that our model outperforms state-of-the-art LLMs in both
language quality and clinical accuracy

</details>


### [412] [AppleGrowthVision: A large-scale stereo dataset for phenological analysis, fruit detection, and 3D reconstruction in apple orchards](https://arxiv.org/abs/2505.14029)
*Laura-Sophia von Hirschhausen,Jannes S. Magnusson,Mykyta Kovalenko,Fredrik Boye,Tanay Rawat,Peter Eisert,Anna Hilsmann,Sebastian Pretzsch,Sebastian Bosse*

Main category: cs.CV

TL;DR: 该论文提出了AppleGrowthVision数据集，解决了苹果园监测中数据不足的问题，提升了果实检测和生长阶段预测的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有苹果园监测数据集缺乏多样性和真实性，忽略了不同生长阶段和立体图像，限制了3D建模和果实定位等任务的精度。

Method: 作者构建了AppleGrowthVision数据集，包含9,317张高分辨率立体图像和1,125张密集标注图像，覆盖六个农业验证的生长阶段。

Result: 使用该数据集提升了YOLOv8和Faster R-CNN的性能，生长阶段预测准确率超过95%。

Conclusion: AppleGrowthVision填补了农业科学与计算机视觉之间的空白，未来将改进标注和3D重建，扩展多模态分析。

Abstract: Deep learning has transformed computer vision for precision agriculture, yet
apple orchard monitoring remains limited by dataset constraints. The lack of
diverse, realistic datasets and the difficulty of annotating dense,
heterogeneous scenes. Existing datasets overlook different growth stages and
stereo imagery, both essential for realistic 3D modeling of orchards and tasks
like fruit localization, yield estimation, and structural analysis. To address
these gaps, we present AppleGrowthVision, a large-scale dataset comprising two
subsets. The first includes 9,317 high resolution stereo images collected from
a farm in Brandenburg (Germany), covering six agriculturally validated growth
stages over a full growth cycle. The second subset consists of 1,125 densely
annotated images from the same farm in Brandenburg and one in Pillnitz
(Germany), containing a total of 31,084 apple labels. AppleGrowthVision
provides stereo-image data with agriculturally validated growth stages,
enabling precise phenological analysis and 3D reconstructions. Extending
MinneApple with our data improves YOLOv8 performance by 7.69 % in terms of
F1-score, while adding it to MinneApple and MAD boosts Faster R-CNN F1-score by
31.06 %. Additionally, six BBCH stages were predicted with over 95 % accuracy
using VGG16, ResNet152, DenseNet201, and MobileNetv2. AppleGrowthVision bridges
the gap between agricultural science and computer vision, by enabling the
development of robust models for fruit detection, growth modeling, and 3D
analysis in precision agriculture. Future work includes improving annotation,
enhancing 3D reconstruction, and extending multimodal analysis across all
growth stages.

</details>


### [413] [RAVENEA: A Benchmark for Multimodal Retrieval-Augmented Visual Culture Understanding](https://arxiv.org/abs/2505.14462)
*Jiaang Li,Yifei Yuan,Wenyan Li,Mohammad Aliannejadi,Daniel Hershcovich,Anders Søgaard,Ivan Vulić,Wenxuan Zhang,Paul Pu Liang,Yang Deng,Serge Belongie*

Main category: cs.CV

TL;DR: RAVENEA是一个新的基准测试，通过检索增强方法提升视觉语言模型对文化细微差别的理解能力，在文化视觉问答和图像描述任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 随着视觉语言模型在日常生活中的广泛应用，准确理解视觉文化变得至关重要。然而，这些模型在解释文化细微差别时往往表现不佳。

Method: 研究团队引入了RAVENEA基准测试，整合了超过10,000篇经过人工标注的维基百科文档，训练并评估了七种多模态检索器，测量了检索增强输入对14种先进视觉语言模型的影响。

Result: 实验结果表明，经过文化感知检索增强的轻量级视觉语言模型在文化视觉问答和图像描述任务中的表现优于未增强的模型，分别提升了至少3.2%和6.2%。

Conclusion: 检索增强方法和文化包容性基准测试对提升多模态理解能力具有重要价值。

Abstract: As vision-language models (VLMs) become increasingly integrated into daily
life, the need for accurate visual culture understanding is becoming critical.
Yet, these models frequently fall short in interpreting cultural nuances
effectively. Prior work has demonstrated the effectiveness of
retrieval-augmented generation (RAG) in enhancing cultural understanding in
text-only settings, while its application in multimodal scenarios remains
underexplored. To bridge this gap, we introduce RAVENEA (Retrieval-Augmented
Visual culturE uNdErstAnding), a new benchmark designed to advance visual
culture understanding through retrieval, focusing on two tasks: culture-focused
visual question answering (cVQA) and culture-informed image captioning (cIC).
RAVENEA extends existing datasets by integrating over 10,000 Wikipedia
documents curated and ranked by human annotators. With RAVENEA, we train and
evaluate seven multimodal retrievers for each image query, and measure the
downstream impact of retrieval-augmented inputs across fourteen
state-of-the-art VLMs. Our results show that lightweight VLMs, when augmented
with culture-aware retrieval, outperform their non-augmented counterparts (by
at least 3.2% absolute on cVQA and 6.2% absolute on cIC). This highlights the
value of retrieval-augmented methods and culturally inclusive benchmarks for
multimodal understanding.

</details>


### [414] [Unify Graph Learning with Text: Unleashing LLM Potentials for Session Search](https://arxiv.org/abs/2505.14156)
*Songhao Wu,Quan Tu,Hong Liu,Jia Xu,Zhongyi Liu,Guannan Zhang,Ran Wang,Xiuying Chen,Rui Yan*

Main category: cs.CV

TL;DR: 提出Symbolic Graph Ranker (SGR)，结合文本和图结构方法，利用大语言模型优化会话搜索。


<details>
  <summary>Details</summary>
Motivation: 当前会话搜索方法侧重序列建模，忽视交互图结构；或虽关注结构但忽略词级语义建模。

Method: 通过符号语法规则将会话图转为文本，结合自监督任务（链接预测、节点生成等）增强LLM的图结构理解能力。

Result: 在AOL和Tiangong-ST数据集上验证了方法的优越性。

Conclusion: SGR为传统搜索策略与现代LLM的融合提供了有效范式。

Abstract: Session search involves a series of interactive queries and actions to
fulfill user's complex information need. Current strategies typically
prioritize sequential modeling for deep semantic understanding, overlooking the
graph structure in interactions. While some approaches focus on capturing
structural information, they use a generalized representation for documents,
neglecting the word-level semantic modeling. In this paper, we propose Symbolic
Graph Ranker (SGR), which aims to take advantage of both text-based and
graph-based approaches by leveraging the power of recent Large Language Models
(LLMs). Concretely, we first introduce a set of symbolic grammar rules to
convert session graph into text. This allows integrating session history,
interaction process, and task instruction seamlessly as inputs for the LLM.
Moreover, given the natural discrepancy between LLMs pre-trained on textual
corpora, and the symbolic language we produce using our graph-to-text grammar,
our objective is to enhance LLMs' ability to capture graph structures within a
textual format. To achieve this, we introduce a set of self-supervised symbolic
learning tasks including link prediction, node content generation, and
generative contrastive learning, to enable LLMs to capture the topological
information from coarse-grained to fine-grained. Experiment results and
comprehensive analysis on two benchmark datasets, AOL and Tiangong-ST, confirm
the superiority of our approach. Our paradigm also offers a novel and effective
methodology that bridges the gap between traditional search strategies and
modern LLMs.

</details>


### [415] [Dual Precision Quantization for Efficient and Accurate Deep Neural Networks Inference](https://arxiv.org/abs/2505.14638)
*Tomer Gafni,Asaf Karnieli,Yair Hanani*

Main category: cs.CV

TL;DR: 提出了一种硬件高效的4比特权重存储、8比特浮点计算的量化推理方案(W4A8)，通过双精度量化算法(DPQ)在保证精度的同时提升速度和内存利用率。


<details>
  <summary>Details</summary>
Motivation: 随着深度学习模型规模增大，延迟和内存效率成为瓶颈，后训练量化是解决这一问题的有效途径。

Method: 采用W4A8量化方案（4比特权重存储+8比特浮点计算），并开发了双精度量化算法(DPQ)来减少精度损失。

Result: 实验表明该方案在保持可接受精度损失的同时，显著提高了吞吐量和内存利用率。

Conclusion: W4A8量化方案在多种现代加速器上实现了性能与精度的良好平衡。

Abstract: Deep neural networks have achieved state-of-the-art results in a wide range
of applications, from natural language processing and computer vision to speech
recognition. However, as tasks become increasingly complex, model sizes
continue to grow, posing challenges in latency and memory efficiency. To meet
these constraints, post-training quantization has emerged as a promising
solution. In this paper, we propose a novel hardware-efficient quantization and
inference scheme that exploits hardware advantages with minimal accuracy
degradation. Specifically, we introduce a W4A8 scheme, where weights are
quantized and stored using 4-bit integer precision, and inference computations
are performed using 8-bit floating-point arithmetic, demonstrating significant
speedups and improved memory utilization compared to 16-bit operations,
applicable on various modern accelerators. To mitigate accuracy loss, we
develop a novel quantization algorithm, dubbed Dual Precision Quantization
(DPQ), that leverages the unique structure of our scheme without introducing
additional inference overhead. Experimental results demonstrate improved
performance (i.e., increased throughput) while maintaining tolerable accuracy
degradation relative to the full-precision model.

</details>


### [416] [Beyond Words: Multimodal LLM Knows When to Speak](https://arxiv.org/abs/2505.14654)
*Zikai Liao,Yi Ouyang,Yi-Lun Lee,Chen-Ping Yu,Yi-Hsuan Tsai,Zhaozheng Yin*

Main category: cs.CV

TL;DR: 论文提出多模态模型MM-When2Speak，通过整合视觉、听觉和文本信息，显著提升了对话AI的响应时机准确性。


<details>
  <summary>Details</summary>
Motivation: 现有基于大型语言模型的聊天机器人在实时对话中难以把握简短反应的时机，主要因为缺乏真实对话中的多模态上下文线索。

Method: 构建包含视觉、听觉和文本对齐的多模态数据集，并提出MM-When2Speak模型，自适应整合多模态信息预测响应时机和类型。

Result: MM-When2Speak在响应时机准确率上比现有最佳单模态和LLM基线提升高达4倍。

Conclusion: 多模态输入对实现自然、及时的对话AI至关重要，MM-When2Speak验证了其有效性。

Abstract: While large language model (LLM)-based chatbots have demonstrated strong
capabilities in generating coherent and contextually relevant responses, they
often struggle with understanding when to speak, particularly in delivering
brief, timely reactions during ongoing conversations. This limitation arises
largely from their reliance on text input, lacking the rich contextual cues in
real-world human dialogue. In this work, we focus on real-time prediction of
response types, with an emphasis on short, reactive utterances that depend on
subtle, multimodal signals across vision, audio, and text. To support this, we
introduce a new multimodal dataset constructed from real-world conversational
videos, containing temporally aligned visual, auditory, and textual streams.
This dataset enables fine-grained modeling of response timing in dyadic
interactions. Building on this dataset, we propose MM-When2Speak, a multimodal
LLM-based model that adaptively integrates visual, auditory, and textual
context to predict when a response should occur, and what type of response is
appropriate. Experiments show that MM-When2Speak significantly outperforms
state-of-the-art unimodal and LLM-based baselines, achieving up to a 4x
improvement in response timing accuracy over leading commercial LLMs. These
results underscore the importance of multimodal inputs for producing timely,
natural, and engaging conversational AI.

</details>


### [417] [VoQA: Visual-only Question Answering](https://arxiv.org/abs/2505.14227)
*Luyang Jiang,Jianing An,Jie Luo,Wenjun Wu,Lei Huang*

Main category: cs.CV

TL;DR: 论文提出VoQA任务，要求模型仅通过视觉输入回答嵌入图像中的问题，并开发GRT-SFT微调策略提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有大型视觉语言模型在纯视觉问答任务中表现不佳，需提升模型对视觉嵌入文本的理解和推理能力。

Method: 采用GRT-SFT结构化微调策略，引导模型基于视觉输入进行逐步推理。

Result: GRT-SFT显著提高了模型在VoQA任务上的表现。

Conclusion: 该研究增强了模型在复杂多模态场景中类人视觉理解的能力。

Abstract: We propose Visual-only Question Answering (VoQA), a novel multimodal task in
which questions are visually embedded within images, without any accompanying
textual input. This requires models to locate, recognize, and reason over
visually embedded textual questions, posing challenges for existing large
vision-language models (LVLMs), which show notable performance drops even with
carefully designed prompts. To bridge this gap, we introduce Guided Response
Triggering Supervised Fine-tuning (GRT-SFT), a structured fine-tuning strategy
that guides the model to perform step-by-step reasoning purely based on visual
input, significantly improving model performance. Our work enhances models'
capacity for human-like visual understanding in complex multimodal scenarios,
where information, including language, is perceived visually.

</details>


### [418] [Visual Agentic Reinforcement Fine-Tuning](https://arxiv.org/abs/2505.14246)
*Ziyu Liu,Yuhang Zang,Yushan Zou,Zijian Liang,Xiaoyi Dong,Yuhang Cao,Haodong Duan,Dahua Lin,Jiaqi Wang*

Main category: cs.CV

TL;DR: 该论文提出Visual-ARFT方法，通过强化微调提升大型视觉语言模型的多模态代理能力，使其能实时搜索信息并操作图像，性能超越GPT-4o。


<details>
  <summary>Details</summary>
Motivation: 当前开源社区在纯语言代理能力（如函数调用）上进展显著，但涉及图像的多模态代理能力及其基准测试仍待探索。

Method: 采用Visual-ARFT（视觉代理强化微调）方法，赋予模型实时网页浏览和图像处理（裁剪、旋转等）的编码能力，并设计MAT多模态工具评测集。

Result: Visual-ARFT在MAT-Coding和MAT-Search任务上分别超越基线18.6%/13.0%和10.3%/8.7%，且在2Wiki等多跳问答基准上提升29.3%/25.9%。

Conclusion: Visual-ARFT为构建鲁棒、可泛化的多模态代理提供了有效路径，其性能超越现有最先进模型。

Abstract: A key trend in Large Reasoning Models (e.g., OpenAI's o3) is the native
agentic ability to use external tools such as web browsers for searching and
writing/executing code for image manipulation to think with images. In the
open-source research community, while significant progress has been made in
language-only agentic abilities such as function calling and tool integration,
the development of multi-modal agentic capabilities that involve truly thinking
with images, and their corresponding benchmarks, are still less explored. This
work highlights the effectiveness of Visual Agentic Reinforcement Fine-Tuning
(Visual-ARFT) for enabling flexible and adaptive reasoning abilities for Large
Vision-Language Models (LVLMs). With Visual-ARFT, open-source LVLMs gain the
ability to browse websites for real-time information updates and write code to
manipulate and analyze input images through cropping, rotation, and other image
processing techniques. We also present a Multi-modal Agentic Tool Bench (MAT)
with two settings (MAT-Search and MAT-Coding) designed to evaluate LVLMs'
agentic search and coding abilities. Our experimental results demonstrate that
Visual-ARFT outperforms its baseline by +18.6% F1 / +13.0% EM on MAT-Coding and
+10.3% F1 / +8.7% EM on MAT-Search, ultimately surpassing GPT-4o. Visual-ARFT
also achieves +29.3 F1% / +25.9% EM gains on existing multi-hop QA benchmarks
such as 2Wiki and HotpotQA, demonstrating strong generalization capabilities.
Our findings suggest that Visual-ARFT offers a promising path toward building
robust and generalizable multimodal agents.

</details>


### [419] [CONSIGN: Conformal Segmentation Informed by Spatial Groupings via Decomposition](https://arxiv.org/abs/2505.14113)
*Bruno Viti,Elias Karabelas,Martin Holler*

Main category: cs.CV

TL;DR: 提出CONSIGN方法，通过结合空间相关性改进图像分割中的不确定性量化，生成具有统计保证的预测集。


<details>
  <summary>Details</summary>
Motivation: 现有图像分割模型的置信度分数缺乏严格的统计保证，且忽略像素间空间相关性，导致不确定性估计保守且难以解释。

Method: 提出CONSIGN方法，基于共形预测框架，利用空间分组分解整合像素间相关性，兼容多种预训练分割模型。

Result: 在三个医学影像数据集和两个COCO子集上验证，CONSIGN显著提升多指标性能，改善不确定性估计质量。

Conclusion: 考虑空间结构能有效提升分割任务的不确定性量化性能，CONSIGN为高风险领域提供可靠统计保证。

Abstract: Most machine learning-based image segmentation models produce pixel-wise
confidence scores - typically derived from softmax outputs - that represent the
model's predicted probability for each class label at every pixel. While this
information can be particularly valuable in high-stakes domains such as medical
imaging, these (uncalibrated) scores are heuristic in nature and do not
constitute rigorous quantitative uncertainty estimates. Conformal prediction
(CP) provides a principled framework for transforming heuristic confidence
scores into statistically valid uncertainty estimates. However, applying CP
directly to image segmentation ignores the spatial correlations between pixels,
a fundamental characteristic of image data. This can result in overly
conservative and less interpretable uncertainty estimates. To address this, we
propose CONSIGN (Conformal Segmentation Informed by Spatial Groupings via
Decomposition), a CP-based method that incorporates spatial correlations to
improve uncertainty quantification in image segmentation. Our method generates
meaningful prediction sets that come with user-specified, high-probability
error guarantees. It is compatible with any pre-trained segmentation model
capable of generating multiple sample outputs - such as those using dropout,
Bayesian modeling, or ensembles. We evaluate CONSIGN against a standard
pixel-wise CP approach across three medical imaging datasets and two COCO
dataset subsets, using three different pre-trained segmentation models. Results
demonstrate that accounting for spatial structure significantly improves
performance across multiple metrics and enhances the quality of uncertainty
estimates.

</details>


### [420] [Speculative Decoding Reimagined for Multimodal Large Language Models](https://arxiv.org/abs/2505.14260)
*Luxi Lin,Zhihang Lin,Zhanpeng Zeng,Rongrong Ji*

Main category: cs.CV

TL;DR: 本文提出多模态推测解码(MSD)方法，通过分离文本/视觉标记处理与两阶段训练策略，显著加速多模态大语言模型推理速度。


<details>
  <summary>Details</summary>
Motivation: 现有推测解码方法在多模态大语言模型(MLLM)上的加速效果不及纯文本模型(LLM)，需针对MLLM特性重新设计解码策略。

Method: 1) 在草案模型中解耦文本与视觉标记处理；2) 采用两阶段训练：先文本指令调优增强语言能力，再渐进引入多模态数据提升视觉感知。

Result: 实验显示MSD将LLaVA-1.5-7B/13B的推理速度分别提升2.29倍和2.46倍，且保持模型精度。

Conclusion: MSD通过针对性设计有效解决了MLLM加速难题，代码已开源。

Abstract: This paper introduces Multimodal Speculative Decoding (MSD) to accelerate
Multimodal Large Language Models (MLLMs) inference. Speculative decoding has
been shown to accelerate Large Language Models (LLMs) without sacrificing
accuracy. However, current speculative decoding methods for MLLMs fail to
achieve the same speedup as they do for LLMs. To address this, we reimagine
speculative decoding specifically for MLLMs. Our analysis of MLLM
characteristics reveals two key design principles for MSD: (1) Text and visual
tokens have fundamentally different characteristics and need to be processed
separately during drafting. (2) Both language modeling ability and visual
perception capability are crucial for the draft model. For the first principle,
MSD decouples text and visual tokens in the draft model, allowing each to be
handled based on its own characteristics. For the second principle, MSD uses a
two-stage training strategy: In stage one, the draft model is trained on
text-only instruction-tuning datasets to improve its language modeling ability.
In stage two, MSD gradually introduces multimodal data to enhance the visual
perception capability of the draft model. Experiments show that MSD boosts
inference speed by up to $2.29\times$ for LLaVA-1.5-7B and up to $2.46\times$
for LLaVA-1.5-13B on multimodal benchmarks, demonstrating its effectiveness.
Our code is available at https://github.com/Lyn-Lucy/MSD.

</details>


### [421] [Handloom Design Generation Using Generative Networks](https://arxiv.org/abs/2505.14330)
*Rajat Kanti Bhattacharjee,Meghali Nandi,Amrit Jha,Gunajit Kalita,Ferdous Ahmed Barbhuiya*

Main category: cs.CV

TL;DR: 本文提出利用深度学习技术生成手织布料设计，探讨了相关挑战及应用，并通过用户评分评估了多种生成模型和风格迁移算法的表现，同时发布了新数据集NeuralLoom。


<details>
  <summary>Details</summary>
Motivation: 生成式神经网络模型在理解和合成艺术设计方面的能力尚未充分探索，特别是在手织布料设计领域。

Method: 结合当前最先进的生成模型和风格迁移算法，采用多种方法进行研究，并通过用户评分评估结果。

Result: 通过用户评分评估了不同生成模型和风格迁移算法的表现，并发布了新数据集NeuralLoom。

Conclusion: 本文展示了深度学习在手织布料设计生成中的潜力，并提供了新的数据集和评估方法。

Abstract: This paper proposes deep learning techniques of generating designs for
clothing, focused on handloom fabric and discusses the associated challenges
along with its application. The capability of generative neural network models
in understanding artistic designs and synthesizing those is not yet explored
well. In this work, multiple methods are employed incorporating the current
state of the art generative models and style transfer algorithms to study and
observe their performance for the task. The results are then evaluated through
user score. This work also provides a new dataset NeuralLoom for the task of
the design generation.

</details>


### [422] [Replace in Translation: Boost Concept Alignment in Counterfactual Text-to-Image](https://arxiv.org/abs/2505.14341)
*Sifan Li,Ming Tao,Hao Zhao,Ling Shao,Hao Tang*

Main category: cs.CV

TL;DR: 该论文提出了一种通过潜在空间逐步替换对象的方法，结合显式逻辑叙述提示（ELNP）和DeepSeek语言模型，提升反事实文本到图像生成中的概念对齐能力。


<details>
  <summary>Details</summary>
Motivation: 当前文本到图像生成技术在常见场景中表现良好，但在反事实场景（如违反物理规律或现实中不可能发生的场景）中，生成图像的概念对齐和真实感仍有待提升。

Method: 利用可控文本到图像模型，在潜在空间中逐步替换合成图像中的对象，将其从常见场景转变为反事实场景，并通过DeepSeek语言模型生成显式逻辑叙述提示（ELNP）指导替换过程。

Result: 实验和定性比较表明，该方法能有效提升反事实文本到图像生成中的概念对齐，并通过新设计的指标验证了生成图像对提示中所需概念的覆盖程度。

Conclusion: 提出的策略显著提升了反事实文本到图像生成的概念对齐能力，为更通用的AIGC体验提供了新思路。

Abstract: Text-to-Image (T2I) has been prevalent in recent years, with most common
condition tasks having been optimized nicely. Besides, counterfactual
Text-to-Image is obstructing us from a more versatile AIGC experience. For
those scenes that are impossible to happen in real world and anti-physics, we
should spare no efforts in increasing the factual feel, which means
synthesizing images that people think very likely to be happening, and concept
alignment, which means all the required objects should be in the same frame. In
this paper, we focus on concept alignment. As controllable T2I models have
achieved satisfactory performance for real applications, we utilize this
technology to replace the objects in a synthesized image in latent space
step-by-step to change the image from a common scene to a counterfactual scene
to meet the prompt. We propose a strategy to instruct this replacing process,
which is called as Explicit Logical Narrative Prompt (ELNP), by using the newly
SoTA language model DeepSeek to generate the instructions. Furthermore, to
evaluate models' performance in counterfactual T2I, we design a metric to
calculate how many required concepts in the prompt can be covered averagely in
the synthesized images. The extensive experiments and qualitative comparisons
demonstrate that our strategy can boost the concept alignment in counterfactual
T2I.

</details>


### [423] [Plane Geometry Problem Solving with Multi-modal Reasoning: A Survey](https://arxiv.org/abs/2505.14340)
*Seunghyuk Cho,Zhenyue Qin,Yang Liu,Youngbin Choi,Seungbeom Lee,Dongwoo Kim*

Main category: cs.CV

TL;DR: 本文综述了平面几何问题求解（PGPS）领域的研究进展，提出了编码器-解码器框架的分类方法，并讨论了当前面临的挑战和未来方向。


<details>
  <summary>Details</summary>
Motivation: 平面几何问题求解（PGPS）作为评估多模态推理能力的基准，近年来受到广泛关注，但缺乏系统性综述。本文旨在填补这一空白。

Method: 通过编码器-解码器框架对PGPS方法进行分类，总结了编码器和解码器的输出格式，并分析了它们的架构设计。

Result: 本文系统梳理了PGPS领域的研究现状，提出了编码器-解码器框架的分类方法，并指出了当前研究中的幻觉问题和数据泄露问题。

Conclusion: PGPS领域仍面临编码阶段的幻觉问题和基准数据泄露等挑战，未来研究应关注这些问题以推动领域发展。

Abstract: Plane geometry problem solving (PGPS) has recently gained significant
attention as a benchmark to assess the multi-modal reasoning capabilities of
large vision-language models. Despite the growing interest in PGPS, the
research community still lacks a comprehensive overview that systematically
synthesizes recent work in PGPS. To fill this gap, we present a survey of
existing PGPS studies. We first categorize PGPS methods into an encoder-decoder
framework and summarize the corresponding output formats used by their encoders
and decoders. Subsequently, we classify and analyze these encoders and decoders
according to their architectural designs. Finally, we outline major challenges
and promising directions for future research. In particular, we discuss the
hallucination issues arising during the encoding phase within encoder-decoder
architectures, as well as the problem of data leakage in current PGPS
benchmarks.

</details>


### [424] [Vid2World: Crafting Video Diffusion Models to Interactive World Models](https://arxiv.org/abs/2505.14357)
*Siqiao Huang,Jialong Wu,Qixing Zhou,Shangchen Miao,Mingsheng Long*

Main category: cs.CV

TL;DR: Vid2World利用预训练视频扩散模型构建交互式世界模型，通过因果化架构和动作引导机制提升预测质量与控制性。


<details>
  <summary>Details</summary>
Motivation: 现有世界模型需大量领域特定训练且预测粗糙，而视频扩散模型能生成高质量视频但缺乏交互性。本文旨在结合两者优势。

Method: 提出Vid2World方法：1) 对预训练视频扩散模型进行因果化改造以支持自回归生成；2) 引入因果动作引导机制增强控制性。

Result: 在机器人操控和游戏仿真领域的实验表明，该方法能有效将视频扩散模型转化为可交互的世界模型。

Conclusion: Vid2World为复用强大视频扩散模型提供可扩展方案，显著提升了世界模型的预测质量与交互能力。

Abstract: World models, which predict transitions based on history observation and
action sequences, have shown great promise in improving data efficiency for
sequential decision making. However, existing world models often require
extensive domain-specific training and still produce low-fidelity, coarse
predictions, limiting their applicability in complex environments. In contrast,
video diffusion models trained on large, internet-scale datasets have
demonstrated impressive capabilities in generating high-quality videos that
capture diverse real-world dynamics. In this work, we present Vid2World, a
general approach for leveraging and transferring pre-trained video diffusion
models into interactive world models. To bridge the gap, Vid2World performs
casualization of a pre-trained video diffusion model by crafting its
architecture and training objective to enable autoregressive generation.
Furthermore, it introduces a causal action guidance mechanism to enhance action
controllability in the resulting interactive world model. Extensive experiments
in robot manipulation and game simulation domains show that our method offers a
scalable and effective approach for repurposing highly capable video diffusion
models to interactive world models.

</details>


### [425] [Enhancing Interpretability of Sparse Latent Representations with Class Information](https://arxiv.org/abs/2505.14476)
*Farshad Sangari Abiz,Reshad Hosseini,Babak N. Araabi*

Main category: cs.CV

TL;DR: 本文提出了一种新方法，通过确保同一类别样本在潜在空间中激活的维度一致，增强变分自编码器（VAE）潜在空间的可解释性。


<details>
  <summary>Details</summary>
Motivation: 标准VAE生成的潜在空间分散且无结构，限制了其可解释性。变分稀疏编码（VSC）虽然通过稀疏潜在表示提高了可解释性，但未能确保同一类别样本间激活维度的一致性。

Method: 引入一种新的损失函数，鼓励同一类别样本共享相似的激活维度，从而在潜在空间中形成结构化表示。

Result: 该方法创建了更具结构化和可解释性的潜在空间，每个共享维度对应一个高级概念或“因子”，同时捕捉全局和类别特定因子。

Conclusion: 通过增强潜在空间的结构化和可解释性，该方法提升了潜在表示的实用性和解释能力。

Abstract: Variational Autoencoders (VAEs) are powerful generative models for learning
latent representations. Standard VAEs generate dispersed and unstructured
latent spaces by utilizing all dimensions, which limits their interpretability,
especially in high-dimensional spaces. To address this challenge, Variational
Sparse Coding (VSC) introduces a spike-and-slab prior distribution, resulting
in sparse latent representations for each input. These sparse representations,
characterized by a limited number of active dimensions, are inherently more
interpretable. Despite this advantage, VSC falls short in providing structured
interpretations across samples within the same class. Intuitively, samples from
the same class are expected to share similar attributes while allowing for
variations in those attributes. This expectation should manifest as consistent
patterns of active dimensions in their latent representations, but VSC does not
enforce such consistency.
  In this paper, we propose a novel approach to enhance the latent space
interpretability by ensuring that the active dimensions in the latent space are
consistent across samples within the same class. To achieve this, we introduce
a new loss function that encourages samples from the same class to share
similar active dimensions. This alignment creates a more structured and
interpretable latent space, where each shared dimension corresponds to a
high-level concept, or "factor." Unlike existing disentanglement-based methods
that primarily focus on global factors shared across all classes, our method
captures both global and class-specific factors, thereby enhancing the utility
and interpretability of latent representations.

</details>


### [426] [Instance Segmentation for Point Sets](https://arxiv.org/abs/2505.14583)
*Abhimanyu Talwar,Julien Laasri*

Main category: cs.CV

TL;DR: 该论文提出两种基于采样的方法，用于解决SGPN在实例分割中内存消耗大的问题，随机采样策略在速度和内存使用上表现最佳。


<details>
  <summary>Details</summary>
Motivation: SGPN在实例分割中使用内存密集的相似性矩阵，导致内存消耗随点数平方增长，亟需优化。

Method: 采用两种采样方法：在子采样点集上计算实例分割，并通过最近邻方法将标签扩展到完整点集。

Result: 两种方法在大子样本上表现相当，但随机采样策略在速度和内存使用上提升最显著。

Conclusion: 随机采样策略有效解决了SGPN的内存问题，同时保持了实例分割的准确性。

Abstract: Recently proposed neural network architectures like PointNet [QSMG16] and
PointNet++ [QYSG17] have made it possible to apply Deep Learning to 3D point
sets. The feature representations of shapes learned by these two networks
enabled training classifiers for Semantic Segmentation, and more recently for
Instance Segmentation via the Similarity Group Proposal Network (SGPN)
[WYHN17]. One area of improvement which has been highlighted by SGPN's authors,
pertains to use of memory intensive similarity matrices which occupy memory
quadratic in the number of points. In this report, we attempt to tackle this
issue through use of two sampling based methods, which compute Instance
Segmentation on a sub-sampled Point Set, and then extrapolate labels to the
complete set using the nearest neigbhour approach. While both approaches
perform equally well on large sub-samples, the random-based strategy gives the
most improvements in terms of speed and memory usage.

</details>


### [427] [CAD-Coder: An Open-Source Vision-Language Model for Computer-Aided Design Code Generation](https://arxiv.org/abs/2505.14646)
*Anna C. Doris,Md Ferdous Alam,Amin Heyrani Nobari,Faez Ahmed*

Main category: cs.CV

TL;DR: CAD-Coder是一个开源视觉语言模型，通过微调直接从视觉输入生成可编辑的CAD代码，显著提升工程设计的效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 当前手动创建CAD模型耗时且需要专业知识，现有AI模型在CAD操作表示、泛化能力和输出准确性方面存在局限。

Method: 利用新构建的GenCAD-Code数据集（包含16.3万CAD模型图像和代码对），微调视觉语言模型生成CadQuery Python代码。

Result: CAD-Coder在语法有效率和3D实体相似度上超越GPT-4.5等基线模型，并能处理未见过的CAD操作和真实世界图像。

Conclusion: CAD-Coder展示了基于代码微调的VLMs在简化CAD工作流程方面的潜力，为工程师和设计师提供了高效工具。

Abstract: Efficient creation of accurate and editable 3D CAD models is critical in
engineering design, significantly impacting cost and time-to-market in product
innovation. Current manual workflows remain highly time-consuming and demand
extensive user expertise. While recent developments in AI-driven CAD generation
show promise, existing models are limited by incomplete representations of CAD
operations, inability to generalize to real-world images, and low output
accuracy. This paper introduces CAD-Coder, an open-source Vision-Language Model
(VLM) explicitly fine-tuned to generate editable CAD code (CadQuery Python)
directly from visual input. Leveraging a novel dataset that we
created--GenCAD-Code, consisting of over 163k CAD-model image and code
pairs--CAD-Coder outperforms state-of-the-art VLM baselines such as GPT-4.5 and
Qwen2.5-VL-72B, achieving a 100% valid syntax rate and the highest accuracy in
3D solid similarity. Notably, our VLM demonstrates some signs of
generalizability, successfully generating CAD code from real-world images and
executing CAD operations unseen during fine-tuning. The performance and
adaptability of CAD-Coder highlights the potential of VLMs fine-tuned on code
to streamline CAD workflows for engineers and designers. CAD-Coder is publicly
available at: https://github.com/anniedoris/CAD-Coder.

</details>


### [428] [3D Reconstruction from Sketches](https://arxiv.org/abs/2505.14621)
*Abhimanyu Talwar,Julien Laasri*

Main category: cs.CV

TL;DR: 提出了一种从多张草图重建3D场景的流程，包括草图拼接、CycleGAN转换和深度图估计，尽管拼接步骤泛化性有限，但整体流程在单草图重建上表现良好。


<details>
  <summary>Details</summary>
Motivation: 研究如何从手绘草图中高效准确地重建3D场景，解决传统方法在草图处理上的局限性。

Method: 1) 通过对应点拼接多张草图；2) 使用CycleGAN将拼接图转为真实图像；3) 用MegaDepth网络估计深度图。构建了图像-草图对数据集用于训练。

Result: 拼接步骤对真实草图泛化性较差，但单草图到3D重建的流程在多样化草图上表现优异。

Conclusion: 该流程为草图到3D重建提供了有效方案，尤其在单草图输入场景下具有实用价值，未来需改进拼接泛化能力。

Abstract: We consider the problem of reconstructing a 3D scene from multiple sketches.
We propose a pipeline which involves (1) stitching together multiple sketches
through use of correspondence points, (2) converting the stitched sketch into a
realistic image using a CycleGAN, and (3) estimating that image's depth-map
using a pre-trained convolutional neural network based architecture called
MegaDepth. Our contribution includes constructing a dataset of image-sketch
pairs, the images for which are from the Zurich Building Database, and sketches
have been generated by us. We use this dataset to train a CycleGAN for our
pipeline's second step. We end up with a stitching process that does not
generalize well to real drawings, but the rest of the pipeline that creates a
3D reconstruction from a single sketch performs quite well on a wide variety of
drawings.

</details>


### [429] [AKRMap: Adaptive Kernel Regression for Trustworthy Visualization of Cross-Modal Embeddings](https://arxiv.org/abs/2505.14664)
*Yilin Ye,Junchao Huang,Xingchen Zeng,Jiazhi Xia,Wei Zeng*

Main category: cs.CV

TL;DR: 本文提出了一种新的降维技术AKRMap，用于更准确地可视化跨模态嵌入度量，通过在学习投影空间中学习度量景观的核回归来实现。


<details>
  <summary>Details</summary>
Motivation: 现有的降维技术（如PCA和t-SNE）主要关注单模态内的特征分布，而未能有效整合跨模态的度量（如CLIPScore）。因此，需要一种新的方法来更准确地可视化跨模态嵌入度量。

Method: AKRMap通过构建一个由投影后核回归损失指导的监督投影网络，并采用可联合优化的自适应广义核，从而能够高效生成捕捉复杂度量分布的可视化结果。

Result: 定量实验表明，AKRMap在生成更准确和可信的可视化结果方面优于现有的降维方法，并在文本到图像模型的跨模态嵌入可视化中展示了其有效性。

Conclusion: AKRMap是一种有效的降维技术，能够更准确地可视化跨模态嵌入度量，并支持交互式探索功能，为多模态模型的可解释性提供了新的工具。

Abstract: Cross-modal embeddings form the foundation for multi-modal models. However,
visualization methods for interpreting cross-modal embeddings have been
primarily confined to traditional dimensionality reduction (DR) techniques like
PCA and t-SNE. These DR methods primarily focus on feature distributions within
a single modality, whilst failing to incorporate metrics (e.g., CLIPScore)
across multiple modalities.This paper introduces AKRMap, a new DR technique
designed to visualize cross-modal embeddings metric with enhanced accuracy by
learning kernel regression of the metric landscape in the projection space.
Specifically, AKRMap constructs a supervised projection network guided by a
post-projection kernel regression loss, and employs adaptive generalized
kernels that can be jointly optimized with the projection. This approach
enables AKRMap to efficiently generate visualizations that capture complex
metric distributions, while also supporting interactive features such as zoom
and overlay for deeper exploration. Quantitative experiments demonstrate that
AKRMap outperforms existing DR methods in generating more accurate and
trustworthy visualizations. We further showcase the effectiveness of AKRMap in
visualizing and comparing cross-modal embeddings for text-to-image models. Code
and demo are available at https://github.com/yilinye/AKRMap.

</details>


### [430] [Training-Free Watermarking for Autoregressive Image Generation](https://arxiv.org/abs/2505.14673)
*Yu Tong,Zihao Pan,Shuai Yang,Kaiyang Zhou*

Main category: cs.CV

TL;DR: 提出IndexMark，一种无需训练的自动回归图像生成模型水印框架，利用码本冗余性嵌入水印且不影响图像质量，验证精度高且抗干扰能力强。


<details>
  <summary>Details</summary>
Motivation: 现有生成水印方法主要针对扩散模型，而自动回归图像生成模型的水印技术研究不足，需要一种有效保护图像所有权的方法。

Method: 基于码本冗余性，通过匹配替换方法选择水印标记并嵌入，利用Index Encoder提高验证精度，并引入辅助验证方案增强抗裁剪攻击能力。

Result: IndexMark在图像质量和验证精度上达到最优，且对裁剪、噪声、模糊等多种干扰具有鲁棒性。

Conclusion: IndexMark为自动回归图像生成模型提供了一种高效、鲁棒的水印解决方案，具有实际应用价值。

Abstract: Invisible image watermarking can protect image ownership and prevent
malicious misuse of visual generative models. However, existing generative
watermarking methods are mainly designed for diffusion models while
watermarking for autoregressive image generation models remains largely
underexplored. We propose IndexMark, a training-free watermarking framework for
autoregressive image generation models. IndexMark is inspired by the redundancy
property of the codebook: replacing autoregressively generated indices with
similar indices produces negligible visual differences. The core component in
IndexMark is a simple yet effective match-then-replace method, which carefully
selects watermark tokens from the codebook based on token similarity, and
promotes the use of watermark tokens through token replacement, thereby
embedding the watermark without affecting the image quality. Watermark
verification is achieved by calculating the proportion of watermark tokens in
generated images, with precision further improved by an Index Encoder.
Furthermore, we introduce an auxiliary validation scheme to enhance robustness
against cropping attacks. Experiments demonstrate that IndexMark achieves
state-of-the-art performance in terms of image quality and verification
accuracy, and exhibits robustness against various perturbations, including
cropping, noises, Gaussian blur, random erasing, color jittering, and JPEG
compression.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [431] [Uncertainty Quantification for Prior-Data Fitted Networks using Martingale Posteriors](https://arxiv.org/abs/2505.11325)
*Thomas Nagler,David Rügamer*

Main category: stat.ME

TL;DR: 该论文提出了一种基于Martingale后验的采样方法，为PFNs提供了贝叶斯不确定性量化。


<details>
  <summary>Details</summary>
Motivation: PFNs虽然在表格数据预测中表现出色，但缺乏对预测均值、分位数等的不确定性量化。

Method: 提出了一种基于Martingale后验的采样方法，构建贝叶斯后验分布。

Result: 通过模拟和真实数据验证了该方法在不确定性量化方面的有效性。

Conclusion: 该方法为PFNs提供了可靠的不确定性量化，适用于推理应用。

Abstract: Prior-data fitted networks (PFNs) have emerged as promising foundation models
for prediction from tabular data sets, achieving state-of-the-art performance
on small to moderate data sizes without tuning. While PFNs are motivated by
Bayesian ideas, they do not provide any uncertainty quantification for
predictive means, quantiles, or similar quantities. We propose a principled and
efficient sampling procedure to construct Bayesian posteriors for such
estimates based on Martingale posteriors, and prove its convergence. Several
simulated and real-world data examples showcase the uncertainty quantification
of our method in inference applications.

</details>


<div id='econ.EM'></div>

# econ.EM [[Back]](#toc)

### [432] [CATS: Clustering-Aggregated and Time Series for Business Customer Purchase Intention Prediction](https://arxiv.org/abs/2505.13558)
*Yingjie Kuang,Tianchen Zhang,Zhen-Wei Huang,Zhongjie Zeng,Zhe-Yuan Li,Ling Huang,Yuefang Gao*

Main category: econ.EM

TL;DR: 该论文提出了一种结合聚类和注意力机制的GRU模型（CAGRU），用于预测客户购买意图，特别关注客户回购行为的不平衡分布问题。


<details>
  <summary>Details</summary>
Motivation: 当前研究多集中于预测客户未来可能购买的产品类型，而忽视了客户是否会进行回购这一关键因素。由于客户群体中存在大量偶尔购买者和少量忠实客户，这种不平衡分布使得传统时间序列预测方法在处理此类问题时存在局限性。

Method: 论文提出CAGRU模型，首先根据客户特征进行聚类，然后使用GRU神经网络提取不同客户群体的时间序列特征，并引入注意力机制捕捉序列位置的重要性。针对客户群体的不平衡分布，模型对每个客户群体分别训练。

Result: 通过构建四个数据集并进行大量实验，证明了CAGRU方法的优越性。

Conclusion: CAGRU模型能够更准确地捕捉不同客户群体间的行为差异以及同一群体内的相似特征，有效解决了客户回购预测中的不平衡分布问题。

Abstract: Accurately predicting customers' purchase intentions is critical to the
success of a business strategy. Current researches mainly focus on analyzing
the specific types of products that customers are likely to purchase in the
future, little attention has been paid to the critical factor of whether
customers will engage in repurchase behavior. Predicting whether a customer
will make the next purchase is a classic time series forecasting task. However,
in real-world purchasing behavior, customer groups typically exhibit imbalance
- i.e., there are a large number of occasional buyers and a small number of
loyal customers. This head-to-tail distribution makes traditional time series
forecasting methods face certain limitations when dealing with such problems.
To address the above challenges, this paper proposes a unified Clustering and
Attention mechanism GRU model (CAGRU) that leverages multi-modal data for
customer purchase intention prediction. The framework first performs customer
profiling with respect to the customer characteristics and clusters the
customers to delineate the different customer clusters that contain similar
features. Then, the time series features of different customer clusters are
extracted by GRU neural network and an attention mechanism is introduced to
capture the significance of sequence locations. Furthermore, to mitigate the
head-to-tail distribution of customer segments, we train the model separately
for each customer segment, to adapt and capture more accurately the differences
in behavioral characteristics between different customer segments, as well as
the similar characteristics of the customers within the same customer segment.
We constructed four datasets and conducted extensive experiments to demonstrate
the superiority of the proposed CAGRU approach.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [433] [FreeMesh: Boosting Mesh Generation with Coordinates Merging](https://arxiv.org/abs/2505.13573)
*Jian Liu,Haohan Weng,Biwen Lei,Xianghui Yang,Zibo Zhao,Zhuo Chen,Song Guo,Tao Han,Chunchao Guo*

Main category: cs.GR

TL;DR: 提出新指标PTME评估网格序列化方法，并设计坐标合并技术提升压缩率。


<details>
  <summary>Details</summary>
Motivation: 现有网格序列化方法缺乏高效评估指标，需理论指导优化。

Method: 引入PTME指标理论评估tokenizer，提出坐标合并技术优化高频模式。

Result: 实验验证方法在MeshXL等tokenizer上有效提升压缩率。

Conclusion: PTME和坐标合并技术可推动原生网格生成领域发展。

Abstract: The next-coordinate prediction paradigm has emerged as the de facto standard
in current auto-regressive mesh generation methods. Despite their
effectiveness, there is no efficient measurement for the various tokenizers
that serialize meshes into sequences. In this paper, we introduce a new metric
Per-Token-Mesh-Entropy (PTME) to evaluate the existing mesh tokenizers
theoretically without any training. Building upon PTME, we propose a
plug-and-play tokenization technique called coordinate merging. It further
improves the compression ratios of existing tokenizers by rearranging and
merging the most frequent patterns of coordinates. Through experiments on
various tokenization methods like MeshXL, MeshAnything V2, and Edgerunner, we
further validate the performance of our method. We hope that the proposed PTME
and coordinate merging can enhance the existing mesh tokenizers and guide the
further development of native mesh generation.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [434] [Learning Wavelet-Sparse FDK for 3D Cone-Beam CT Reconstruction](https://arxiv.org/abs/2505.13579)
*Yipeng Sun,Linda-Sophie Schneider,Chengze Ye,Mingxuan Gu,Siyuan Mei,Siming Bayer,Andreas Maier*

Main category: eess.IV

TL;DR: 提出一种改进的FDK神经网络方法，通过选择性集成可训练元素并利用小波变换降低参数数量，在保持传统算法可解释性的同时提升图像质量。


<details>
  <summary>Details</summary>
Motivation: FDK算法在CBCT重建中效率高但易受噪声和伪影影响，现有深度学习方法虽提升质量但增加计算复杂度和失去可解释性。

Method: 在FDK算法的余弦加权和滤波阶段选择性集成可训练元素，并利用小波变换创建稀疏表示以减少参数数量。

Result: 参数数量减少93.75%，保持计算成本不变，提升抗噪能力且易于集成到现有CT重建流程中。

Conclusion: 该方法在保持FDK算法优势的同时显著提升性能，适用于计算资源有限的临床环境。

Abstract: Cone-Beam Computed Tomography (CBCT) is essential in medical imaging, and the
Feldkamp-Davis-Kress (FDK) algorithm is a popular choice for reconstruction due
to its efficiency. However, FDK is susceptible to noise and artifacts. While
recent deep learning methods offer improved image quality, they often increase
computational complexity and lack the interpretability of traditional methods.
In this paper, we introduce an enhanced FDK-based neural network that maintains
the classical algorithm's interpretability by selectively integrating trainable
elements into the cosine weighting and filtering stages. Recognizing the
challenge of a large parameter space inherent in 3D CBCT data, we leverage
wavelet transformations to create sparse representations of the cosine weights
and filters. This strategic sparsification reduces the parameter count by
$93.75\%$ without compromising performance, accelerates convergence, and
importantly, maintains the inference computational cost equivalent to the
classical FDK algorithm. Our method not only ensures volumetric consistency and
boosts robustness to noise, but is also designed for straightforward
integration into existing CT reconstruction pipelines. This presents a
pragmatic enhancement that can benefit clinical applications, particularly in
environments with computational limitations.

</details>


### [435] [XDementNET: An Explainable Attention Based Deep Convolutional Network to Detect Alzheimer Progression from MRI data](https://arxiv.org/abs/2505.13906)
*Soyabul Islam Lincoln,Mirza Mohd Shahriar Maswood*

Main category: eess.IV

TL;DR: 该论文提出了一种结合多残差块、空间注意力机制和多种注意力机制的深度学习架构，用于阿尔茨海默病的MRI图像分类，并在多个公开数据集上取得了极高的分类准确率。


<details>
  <summary>Details</summary>
Motivation: 随着医疗费用上涨和人工智能在医疗诊断中的应用增加，阿尔茨海默病作为一种常见的神经退行性疾病，需要更精确的诊断和高效的治疗方法。

Method: 论文提出了一种新型深度学习架构，结合了多残差块、专用空间注意力块、分组查询注意力和多头注意力机制，并在多个公开数据集上进行了测试。

Result: 在Kaggle数据集上，4分类、3分类和2分类的准确率分别达到99.66%、99.63%和100%；在OASIS数据集上分别为99.92%、99.90%和99.95%；在ADNI-1数据集的不同平面上也取得了99%以上的准确率。

Conclusion: 该深度学习模型在阿尔茨海默病阶段分类中表现出色，能够有效从MRI图像中提取重要信息，其性能优于当前最先进的方法。

Abstract: A common neurodegenerative disease, Alzheimer's disease requires a precise
diagnosis and efficient treatment, particularly in light of escalating
healthcare expenses and the expanding use of artificial intelligence in medical
diagnostics. Many recent studies shows that the combination of brain Magnetic
Resonance Imaging (MRI) and deep neural networks have achieved promising
results for diagnosing AD. Using deep convolutional neural networks, this paper
introduces a novel deep learning architecture that incorporates multiresidual
blocks, specialized spatial attention blocks, grouped query attention, and
multi-head attention. The study assessed the model's performance on four
publicly accessible datasets and concentrated on identifying binary and
multiclass issues across various categories. This paper also takes into account
of the explainability of AD's progression and compared with state-of-the-art
methods namely Gradient Class Activation Mapping (GradCAM), Score-CAM, Faster
Score-CAM, and XGRADCAM. Our methodology consistently outperforms current
approaches, achieving 99.66\% accuracy in 4-class classification, 99.63\% in
3-class classification, and 100\% in binary classification using Kaggle
datasets. For Open Access Series of Imaging Studies (OASIS) datasets the
accuracies are 99.92\%, 99.90\%, and 99.95\% respectively. The Alzheimer's
Disease Neuroimaging Initiative-1 (ADNI-1) dataset was used for experiments in
three planes (axial, sagittal, and coronal) and a combination of all planes.
The study achieved accuracies of 99.08\% for axis, 99.85\% for sagittal, 99.5\%
for coronal, and 99.17\% for all axis, and 97.79\% and 8.60\% respectively for
ADNI-2. The network's ability to retrieve important information from MRI images
is demonstrated by its excellent accuracy in categorizing AD stages.

</details>


### [436] [Bronchovascular Tree-Guided Weakly Supervised Learning Method for Pulmonary Segment Segmentation](https://arxiv.org/abs/2505.13911)
*Ruijie Zhao,Zuopeng Tan,Xiao Xue,Longfei Zhao,Bing Li,Zicheng Liao,Ying Ming,Jiaru Wang,Ran Xiao,Sirong Piao,Rui Zhao,Qiqi Xu,Wei Song*

Main category: eess.IV

TL;DR: 提出了一种基于解剖层次监督学习的弱监督方法（AHSL），用于肺段分割，通过结合临床解剖定义和支气管血管树先验信息，有效解决了医学图像中边界模糊的问题。


<details>
  <summary>Details</summary>
Motivation: 肺段分割对于癌症定位和手术规划至关重要，但由于医学图像中肺段边界难以区分，像素级标注耗时耗力。因此，需要一种弱监督学习方法，利用临床解剖定义来指导分割。

Method: 提出AHSL方法，通过段级和叶级监督确保肺段准确包含支气管血管树并位于对应肺叶内；采用两阶段分割策略结合支气管血管先验信息，并引入一致性损失和边界平滑度评估指标。

Result: 在私有数据集上的实验表明，该方法在视觉检查和评估指标上均表现出色，有效提升了肺段分割的准确性和边界平滑度。

Conclusion: AHSL方法通过弱监督学习和解剖层次监督，显著提高了肺段分割的效果，为临床实践提供了可靠的工具。

Abstract: Pulmonary segment segmentation is crucial for cancer localization and
surgical planning. However, the pixel-wise annotation of pulmonary segments is
laborious, as the boundaries between segments are indistinguishable in medical
images. To this end, we propose a weakly supervised learning (WSL) method,
termed Anatomy-Hierarchy Supervised Learning (AHSL), which consults the precise
clinical anatomical definition of pulmonary segments to perform pulmonary
segment segmentation. Since pulmonary segments reside within the lobes and are
determined by the bronchovascular tree, i.e., artery, airway and vein, the
design of the loss function is founded on two principles. First, segment-level
labels are utilized to directly supervise the output of the pulmonary segments,
ensuring that they accurately encompass the appropriate bronchovascular tree.
Second, lobe-level supervision indirectly oversees the pulmonary segment,
ensuring their inclusion within the corresponding lobe. Besides, we introduce a
two-stage segmentation strategy that incorporates bronchovascular priori
information. Furthermore, a consistency loss is proposed to enhance the
smoothness of segment boundaries, along with an evaluation metric designed to
measure the smoothness of pulmonary segment boundaries. Visual inspection and
evaluation metrics from experiments conducted on a private dataset demonstrate
the effectiveness of our method.

</details>


### [437] [NOVA: A Benchmark for Anomaly Localization and Clinical Reasoning in Brain MRI](https://arxiv.org/abs/2505.14064)
*Cosmin I. Bercea,Jun Li,Philipp Raffler,Evamaria O. Riedel,Lena Schmitzer,Angela Kurz,Felix Bitzer,Paula Roßmüller,Julian Canisius,Mirjam L. Beyrle,Che Liu,Wenjia Bai,Bernhard Kainz,Julia A. Schnabel,Benedikt Wiestler*

Main category: eess.IV

TL;DR: 论文提出了NOVA基准测试，用于评估模型在罕见病理和异构数据上的泛化能力，发现现有模型性能显著下降。


<details>
  <summary>Details</summary>
Motivation: 现实应用中，模型常遇到训练数据分布外的输入，现有评估方法仅针对常见异常类型，掩盖了模型在罕见或新情况下的失败。

Method: 构建NOVA基准测试，包含约900个脑MRI扫描，涵盖281种罕见病理和异构采集协议，提供临床叙述和专家标注。

Result: 领先的视觉-语言模型在NOVA上表现显著下降，表明其在真正未知异常上的检测、定位和推理能力不足。

Conclusion: NOVA是一个严格的测试平台，可推动模型在分布外泛化能力上的进步。

Abstract: In many real-world applications, deployed models encounter inputs that differ
from the data seen during training. Out-of-distribution detection identifies
whether an input stems from an unseen distribution, while open-world
recognition flags such inputs to ensure the system remains robust as
ever-emerging, previously $unknown$ categories appear and must be addressed
without retraining. Foundation and vision-language models are pre-trained on
large and diverse datasets with the expectation of broad generalization across
domains, including medical imaging. However, benchmarking these models on test
sets with only a few common outlier types silently collapses the evaluation
back to a closed-set problem, masking failures on rare or truly novel
conditions encountered in clinical use.
  We therefore present $NOVA$, a challenging, real-life $evaluation-only$
benchmark of $\sim$900 brain MRI scans that span 281 rare pathologies and
heterogeneous acquisition protocols. Each case includes rich clinical
narratives and double-blinded expert bounding-box annotations. Together, these
enable joint assessment of anomaly localisation, visual captioning, and
diagnostic reasoning. Because NOVA is never used for training, it serves as an
$extreme$ stress-test of out-of-distribution generalisation: models must bridge
a distribution gap both in sample appearance and in semantic space. Baseline
results with leading vision-language models (GPT-4o, Gemini 2.0 Flash, and
Qwen2.5-VL-72B) reveal substantial performance drops across all tasks,
establishing NOVA as a rigorous testbed for advancing models that can detect,
localize, and reason about truly unknown anomalies.

</details>


### [438] [Automated Fetal Biometry Assessment with Deep Ensembles using Sparse-Sampling of 2D Intrapartum Ultrasound Images](https://arxiv.org/abs/2505.14572)
*Jayroop Ramesh,Valentin Bacher,Mark C. Eid,Hoda Kalabizadeh,Christian Rupprecht,Ana IL Namburete,Pak-Hei Yeung,Madeleine K. Wyburd,Nicola K. Dinsdale*

Main category: eess.IV

TL;DR: 该论文提出了一种自动化胎儿生物测量流程，用于减少观察者间差异并提高测量可靠性，通过分类、分割和计算关键参数来监测分娩进程。


<details>
  <summary>Details</summary>
Motivation: 国际超声协会提倡使用分娩期超声成像监测分娩进程，但存在观察者间差异和测量可靠性问题。本研究旨在通过自动化流程解决这些问题。

Method: 研究提出三阶段流程：1) 从超声视频中分类标准平面；2) 分割胎儿头部和耻骨联合；3) 计算关键参数（AoP和HSD）。采用稀疏采样和集成深度学习方法提高泛化能力。

Result: 在4名患者和224帧超声图像的测试集上，模型取得了高精度指标（如ACC: 0.9452, DSC: 0.918），参数测量误差较小（ΔAoP: 8.90, ΔHSD: 14.35）。

Conclusion: 自动化流程能有效提升分娩停滞原因的分析能力，并为临床风险分层工具开发提供支持，从而实现更高效精准的产前护理。

Abstract: The International Society of Ultrasound advocates Intrapartum Ultrasound (US)
Imaging in Obstetrics and Gynecology (ISUOG) to monitor labour progression
through changes in fetal head position. Two reliable ultrasound-derived
parameters that are used to predict outcomes of instrumental vaginal delivery
are the angle of progression (AoP) and head-symphysis distance (HSD). In this
work, as part of the Intrapartum Ultrasounds Grand Challenge (IUGC) 2024, we
propose an automated fetal biometry measurement pipeline to reduce intra- and
inter-observer variability and improve measurement reliability. Our pipeline
consists of three key tasks: (i) classification of standard planes (SP) from US
videos, (ii) segmentation of fetal head and pubic symphysis from the detected
SPs, and (iii) computation of the AoP and HSD from the segmented regions. We
perform sparse sampling to mitigate class imbalances and reduce spurious
correlations in task (i), and utilize ensemble-based deep learning methods for
task (i) and (ii) to enhance generalizability under different US acquisition
settings. Finally, to promote robustness in task iii) with respect to the
structural fidelity of measurements, we retain the largest connected components
and apply ellipse fitting to the segmentations. Our solution achieved ACC:
0.9452, F1: 0.9225, AUC: 0.983, MCC: 0.8361, DSC: 0.918, HD: 19.73, ASD: 5.71,
$\Delta_{AoP}$: 8.90 and $\Delta_{HSD}$: 14.35 across an unseen hold-out set of
4 patients and 224 US frames. The results from the proposed automated pipeline
can improve the understanding of labour arrest causes and guide the development
of clinical risk stratification tools for efficient and effective prenatal
care.

</details>


<div id='astro-ph.EP'></div>

# astro-ph.EP [[Back]](#toc)

### [439] [ThermoONet -- a deep learning-based small body thermophysical network: applications to modelling water activity of comets](https://arxiv.org/abs/2505.14016)
*Shunjing Zhao,Xian Shi,Hanlun Lei*

Main category: astro-ph.EP

TL;DR: 该论文提出了一种名为ThermoONet的机器学习模型，用于高效预测彗星的地下温度和水冰升华通量，显著降低了计算时间并保持了高精度。


<details>
  <summary>Details</summary>
Motivation: 传统的彗星热物理模型数值解算计算成本高昂，限制了高分辨率或重复建模的研究需求。

Method: 采用机器学习方法开发了ThermoONet神经网络，用于预测彗星温度和水冰升华通量。

Result: ThermoONet在保持约2%的平均误差的同时，将计算时间减少了近六个数量级，并成功拟合了67P和21P彗星的水生成率曲线。

Conclusion: ThermoONet结合全局优化算法，能够高效准确地反演目标天体的物理特性，展示了其在彗星研究中的有效性和高效性。

Abstract: Cometary activity is a compelling subject of study, with thermophysical
models playing a pivotal role in its understanding. However, traditional
numerical solutions for small body thermophysical models are computationally
intensive, posing challenges for investigations requiring high-resolution or
repetitive modeling. To address this limitation, we employed a machine learning
approach to develop ThermoONet - a neural network designed to predict the
temperature and water ice sublimation flux of comets. Performance evaluations
indicate that ThermoONet achieves a low average error in subsurface temperature
of approximately 2% relative to the numerical simulation, while reducing
computational time by nearly six orders of magnitude. We applied ThermoONet to
model the water activity of comets 67P/Churyumov-Gerasimenko and
21P/Giacobini-Zinner. By successfully fitting the water production rate curves
of these comets, as obtained by the Rosetta mission and the SOHO telescope,
respectively, we demonstrate the network's effectiveness and efficiency.
Furthermore, when combined with a global optimization algorithm, ThermoONet
proves capable of retrieving the physical properties of target bodies.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [440] [When Bias Backfires: The Modulatory Role of Counterfactual Explanations on the Adoption of Algorithmic Bias in XAI-Supported Human Decision-Making](https://arxiv.org/abs/2505.14377)
*Ulrike Kuhl,Annika Bush*

Main category: cs.HC

TL;DR: 研究发现，带有偏见的AI推荐会潜移默化影响人类决策，但反事实解释可逆转这种影响。


<details>
  <summary>Details</summary>
Motivation: 探讨AI偏见如何通过推荐系统影响人类决策，尤其是当AI带有性别偏见时，人类是否会被其同化。

Method: 通过模拟招聘实验，分三阶段（无AI推荐、带偏见AI推荐、无AI推荐）观察294名参与者的决策变化，测试反事实解释的效果。

Result: 70%情况下参与者会采纳AI建议，仅8人发现性别偏见。接触偏见AI后，无解释组决策被同化，有解释组则逆转偏见。信任度无差异，但信心水平受微妙影响。

Conclusion: 需谨慎设计可解释AI(XAI)以避免偏见传播，反事实解释能有效抵消AI偏见的长期影响。

Abstract: Although the integration of artificial intelligence (AI) into everyday tasks
improves efficiency and objectivity, it also risks transmitting bias to human
decision-making. In this study, we conducted a controlled experiment that
simulated hiring decisions to examine how biased AI recommendations - augmented
with or without counterfactual explanations - influence human judgment over
time. Participants, acting as hiring managers, completed 60 decision trials
divided into a baseline phase without AI, followed by a phase with biased (X)AI
recommendations (favoring either male or female candidates), and a final
post-interaction phase without AI. Our results indicate that the participants
followed the AI recommendations 70% of the time when the qualifications of the
given candidates were comparable. Yet, only a fraction of participants detected
the gender bias (8 out of 294). Crucially, exposure to biased AI altered
participants' inherent preferences: in the post-interaction phase,
participants' independent decisions aligned with the bias when no
counterfactual explanations were provided before, but reversed the bias when
explanations were given. Reported trust did not differ significantly across
conditions. Confidence varied throughout the study phases after exposure to
male-biased AI, indicating nuanced effects of AI bias on decision certainty.
Our findings point to the importance of calibrating XAI to avoid unintended
behavioral shifts in order to safeguard equitable decision-making and prevent
the adoption of algorithmic bias.

</details>


### [441] [How Managers Perceive AI-Assisted Conversational Training for Workplace Communication](https://arxiv.org/abs/2505.14452)
*Lance T Wilhelm,Xiaohan Ding,Kirk McInnis Knutsen,Buse Carik,Eugenia H Rho*

Main category: cs.HC

TL;DR: 研究探讨管理者如何利用AI辅助系统提升职场沟通技能，强调个性化与适应性训练的重要性。


<details>
  <summary>Details</summary>
Motivation: 管理者缺乏定制化沟通培训，AI辅助系统可能提供可扩展的解决方案，但管理者对AI角色的预期尚不明确。

Method: 设计对话角色扮演系统CommCoach，通过半结构化访谈了解管理者对AI辅助沟通训练的期望。

Result: 管理者重视自适应、低风险模拟训练，期待人机协作、透明反馈及对AI生成角色的控制。

Conclusion: AI辅助沟通训练需平衡个性化与结构化目标，同时解决反馈一致性、偏见等挑战。

Abstract: Effective workplace communication is essential for managerial success, yet
many managers lack access to tailored and sustained training. Although
AI-assisted communication systems may offer scalable training solutions, little
is known about how managers envision the role of AI in helping them improve
their communication skills. To investigate this, we designed a conversational
role-play system, CommCoach, as a functional probe to understand how managers
anticipate using AI to practice their communication skills. Through
semi-structured interviews, participants emphasized the value of adaptive,
low-risk simulations for practicing difficult workplace conversations. They
also highlighted opportunities, including human-AI teaming, transparent and
context-aware feedback, and greater control over AI-generated personas.
AI-assisted communication training should balance personalization, structured
learning objectives, and adaptability to different user styles and contexts.
However, achieving this requires carefully navigating tensions between adaptive
and consistent AI feedback, realism and potential bias, and the open-ended
nature of AI conversations versus structured workplace discourse.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [442] [Pel, A Programming Language for Orchestrating AI Agents](https://arxiv.org/abs/2505.13453)
*Behnam Mohammadi*

Main category: cs.PL

TL;DR: 论文提出了一种名为Pel的新型编程语言，旨在解决大型语言模型（LLMs）在复杂控制和编排方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型的控制方法（如函数调用和直接代码生成）在表达能力、可扩展性、成本、安全性和细粒度控制方面存在不足，需要一种更高效、安全的解决方案。

Method: Pel语言结合了Lisp、Elixir、Gleam和Haskell的优点，提供语法简单、语义丰富的平台，支持复杂操作、控制流和代理间通信。

Result: Pel通过语法级能力控制、线性组合机制、闭包支持、自然语言条件评估和自动并行化等特性，显著提升了LLM的编排能力。

Conclusion: Pel为LLM编排提供了更强大、安全和表达丰富的范式，为构建复杂可靠的AI代理框架奠定了基础。

Abstract: The proliferation of Large Language Models (LLMs) has opened new frontiers in
computing, yet controlling and orchestrating their capabilities beyond simple
text generation remains a challenge. Current methods, such as function/tool
calling and direct code generation, suffer from limitations in expressiveness,
scalability, cost, security, and the ability to enforce fine-grained control.
This paper introduces Pel, a novel programming language specifically designed
to bridge this gap. Inspired by the strengths of Lisp, Elixir, Gleam, and
Haskell, Pel provides a syntactically simple, homoiconic, and semantically rich
platform for LLMs to express complex actions, control flow, and inter-agent
communication safely and efficiently. Pel's design emphasizes a minimal, easily
modifiable grammar suitable for constrained LLM generation, eliminating the
need for complex sandboxing by enabling capability control at the syntax level.
Key features include a powerful piping mechanism for linear composition,
first-class closures enabling easy partial application and functional patterns,
built-in support for natural language conditions evaluated by LLMs, and an
advanced Read-Eval-Print-Loop (REPeL) with Common Lisp-style restarts and
LLM-powered helper agents for automated error correction. Furthermore, Pel
incorporates automatic parallelization of independent operations via static
dependency analysis, crucial for performant agentic systems. We argue that Pel
offers a more robust, secure, and expressive paradigm for LLM orchestration,
paving the way for more sophisticated and reliable AI agentic frameworks.

</details>


### [443] [RTL++: Graph-enhanced LLM for RTL Code Generation](https://arxiv.org/abs/2505.13479)
*Mohammad Akyash,Kimia Azar,Hadi Kamali*

Main category: cs.PL

TL;DR: 论文提出RTL++方法，利用图结构增强LLM生成RTL代码的质量，解决了传统方法的局限性和开源模型的质量问题。


<details>
  <summary>Details</summary>
Motivation: 随着硬件设计复杂性增加，传统RTL设计方法手动、耗时且易错，商业LLM存在安全和隐私问题，开源模型因缺乏高质量RTL代码数据而表现不佳。

Method: RTL++通过将RTL代码编码为文本化的控制流图（CFG）和数据流图（DFG），增强LLM对代码结构和依赖关系的理解，从而提升生成代码的质量。

Result: 实验结果表明，RTL++在VerilogEval基准测试和RTLLM1.1模型评估中优于现有最先进的RTL生成模型。

Conclusion: RTL++通过图增强上下文有效提升了LLM辅助RTL代码生成的能力，为自动化硬件设计提供了新思路。

Abstract: As hardware design complexity escalates, there is an urgent need for advanced
automation in electronic design automation (EDA). Traditional register transfer
level (RTL) design methods are manual, time-consuming, and prone to errors.
While commercial (instruction-tuned) large language models (LLMs) shows
promising performance for automation, they pose security and privacy concerns.
Open-source models offer alternatives; however, they frequently fall short in
quality/correctness, largely due to limited, high-quality RTL code data
essential for effective training and generalization. This paper proposes RTL++,
a first-of-its-kind LLM-assisted method for RTL code generation that utilizes
graph representations of code structures to enhance the quality of generated
code. By encoding RTL code into a textualized control flowgraphs (CFG) and data
flow graphs (DFG), RTL++ captures the inherent hierarchy, dependencies, and
relationships within the code. This structured graph-based approach enhances
the context available to LLMs, enabling them to better understand and generate
instructions. By focusing on data generation through graph representations,
RTL++ addresses the limitations of previous approaches that rely solely on code
and suffer from lack of diversity. Experimental results demonstrate that RTL++
outperforms state-of-the-art models fine-tuned for RTL generation, as evaluated
using the VerilogEval benchmark's Pass@1/5/10 metric, as well as the RTLLM1.1
model, which highlight the effectiveness of graph-enhanced context in advancing
the capabilities of LLM-assisted RTL code generation.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [444] [Exploring Emotional Synchrony in Dyadic Interactions: The Role of Speech Conditions in Facial and Vocal Affective Alignment](https://arxiv.org/abs/2505.13455)
*Von Ralph Dane Marquez Herbuela,Yukie Nagai*

Main category: eess.AS

TL;DR: 研究探讨非重叠和重叠对话如何影响面部表情与语音在多模态情感同步中的时空对齐，发现非重叠对话能带来更稳定、可预测的情感同步。


<details>
  <summary>Details</summary>
Motivation: 理解人类如何通过多种沟通渠道（特别是面部表情和语音）表达和同步情感，对情感识别系统和人机交互有重要意义。研究特别关注非重叠对话促进情感协调，而重叠对话破坏同步的现象。

Method: 使用IEMOCAP数据集中的双人对话，通过EmoNet（面部视频）和基于Wav2Vec2的模型（语音音频）提取连续情感估计。根据语音重叠对片段分类，并采用皮尔逊相关、滞后调整分析和动态时间规整（DTW）评估情感对齐。

Result: 非重叠对话比重叠对话表现出更稳定和可预测的情感同步。非重叠对话的滞后调整相关性和最佳滞后分布显示出更清晰、一致的时间对齐，而重叠对话则表现出更高的变异性和更平坦的滞后分布。面部表情在轮流发言时更常先于语音，而在同时发声时语音则领先。

Conclusion: 对话结构在调节情感沟通中起重要作用，研究为现实世界互动中多模态情感对齐的时空动态提供了新见解。

Abstract: Understanding how humans express and synchronize emotions across multiple
communication channels particularly facial expressions and speech has
significant implications for emotion recognition systems and human computer
interaction. Motivated by the notion that non-overlapping speech promotes
clearer emotional coordination, while overlapping speech disrupts synchrony,
this study examines how these conversational dynamics shape the spatial and
temporal alignment of arousal and valence across facial and vocal modalities.
Using dyadic interactions from the IEMOCAP dataset, we extracted continuous
emotion estimates via EmoNet (facial video) and a Wav2Vec2-based model (speech
audio). Segments were categorized based on speech overlap, and emotional
alignment was assessed using Pearson correlation, lag adjusted analysis, and
Dynamic Time Warping (DTW). Across analyses, non overlapping speech was
associated with more stable and predictable emotional synchrony than
overlapping speech. While zero-lag correlations were low and not statistically
different, non overlapping speech showed reduced variability, especially for
arousal. Lag adjusted correlations and best-lag distributions revealed clearer,
more consistent temporal alignment in these segments. In contrast, overlapping
speech exhibited higher variability and flatter lag profiles, though DTW
indicated unexpectedly tighter alignment suggesting distinct coordination
strategies. Notably, directionality patterns showed that facial expressions
more often preceded speech during turn-taking, while speech led during
simultaneous vocalizations. These findings underscore the importance of
conversational structure in regulating emotional communication and provide new
insight into the spatial and temporal dynamics of multimodal affective
alignment in real world interaction.

</details>


### [445] [Direction-Aware Neural Acoustic Fields for Few-Shot Interpolation of Ambisonic Impulse Responses](https://arxiv.org/abs/2505.13617)
*Christopher Ick,Gordon Wichern,Yoshiki Masuyama,François Germain,Jonathan Le Roux*

Main category: eess.AS

TL;DR: 提出方向感知神经场（DANF），通过Ambisonic格式的RIR更精确捕捉声场方向特性，并探索其在新房间的适应能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于神经场的方法主要针对单耳全向或双耳听众，无法精确捕捉单点真实声场的方向特性。

Method: 提出方向感知神经场（DANF），结合Ambisonic格式RIR显式整合方向信息，并引入方向感知损失函数。

Result: DANF能更精确建模声场方向特性，并通过低秩适应等方式展示在新房间的适应潜力。

Conclusion: DANF为声场建模提供了更精确的方向感知能力，并具备跨房间场景的泛化潜力。

Abstract: The characteristics of a sound field are intrinsically linked to the
geometric and spatial properties of the environment surrounding a sound source
and a listener. The physics of sound propagation is captured in a time-domain
signal known as a room impulse response (RIR). Prior work using neural fields
(NFs) has allowed learning spatially-continuous representations of RIRs from
finite RIR measurements. However, previous NF-based methods have focused on
monaural omnidirectional or at most binaural listeners, which does not
precisely capture the directional characteristics of a real sound field at a
single point. We propose a direction-aware neural field (DANF) that more
explicitly incorporates the directional information by Ambisonic-format RIRs.
While DANF inherently captures spatial relations between sources and listeners,
we further propose a direction-aware loss. In addition, we investigate the
ability of DANF to adapt to new rooms in various ways including low-rank
adaptation.

</details>


### [446] [Articulatory Feature Prediction from Surface EMG during Speech Production](https://arxiv.org/abs/2505.13814)
*Jihwan Lee,Kevin Huang,Kleanthis Avramidis,Simon Pistrosch,Monica Gonzalez-Machorro,Yoonjeong Lee,Björn Schuller,Louis Goldstein,Shrikanth Narayanan*

Main category: eess.AS

TL;DR: 提出一种通过表面肌电信号预测发音特征并合成语音的新方法，预测准确率约0.9，首次实现基于肌电信号-发音特征-语音波形的解码。


<details>
  <summary>Details</summary>
Motivation: 探索利用表面肌电信号（EMG）预测发音特征并合成语音的新途径，为肌电语音合成提供创新方法。

Method: 结合卷积层和Transformer模块构建预测模型，针对不同发音特征设计独立预测器，并分析电极位置对预测的影响。

Result: 多数发音特征预测相关性达0.9，成功解码出可理解语音，公开了源代码和合成样本。

Conclusion: 该方法首次实现肌电信号→发音特征→语音的端到端解码，为优化肌电电极配置提供理论依据。

Abstract: We present a model for predicting articulatory features from surface
electromyography (EMG) signals during speech production. The proposed model
integrates convolutional layers and a Transformer block, followed by separate
predictors for articulatory features. Our approach achieves a high prediction
correlation of approximately 0.9 for most articulatory features. Furthermore,
we demonstrate that these predicted articulatory features can be decoded into
intelligible speech waveforms. To our knowledge, this is the first method to
decode speech waveforms from surface EMG via articulatory features, offering a
novel approach to EMG-based speech synthesis. Additionally, we analyze the
relationship between EMG electrode placement and articulatory feature
predictability, providing knowledge-driven insights for optimizing EMG
electrode configurations. The source code and decoded speech samples are
publicly available.

</details>


### [447] [SPIRIT: Patching Speech Language Models against Jailbreak Attacks](https://arxiv.org/abs/2505.13541)
*Amirbek Djanibekov,Nurdaulet Mukhituly,Kentaro Inui,Hanan Aldarmaki,Nils Lukas*

Main category: eess.AS

TL;DR: 语音语言模型(SLMs)因语音信号更丰富而面临更高安全风险，研究发现其对越狱攻击极为脆弱，但通过后修补防御可显著提升安全性且不影响实用性。


<details>
  <summary>Details</summary>
Motivation: 语音语言模型通过语音交互更精准捕捉用户意图，但语音信号的复杂性也带来了新的安全风险，尤其是对抗性攻击可能绕过安全机制。

Method: 提出后修补防御方法，在推理时干预模型激活值，无需重新训练即可提升模型鲁棒性，并通过消融研究优化防御效果。

Result: 防御方法将模型对抗攻击的鲁棒性提升至99%，攻击成功率在某些情况下可从100%大幅降低，且对模型实用性影响可忽略不计。

Conclusion: 后修补防御能有效解决SLMs的安全漏洞，在保持实用性的同时显著提升抗攻击能力，为语音模型安全提供了可行方案。

Abstract: Speech Language Models (SLMs) enable natural interactions via spoken
instructions, which more effectively capture user intent by detecting nuances
in speech. The richer speech signal introduces new security risks compared to
text-based models, as adversaries can better bypass safety mechanisms by
injecting imperceptible noise to speech. We analyze adversarial attacks and
find that SLMs are substantially more vulnerable to jailbreak attacks, which
can achieve a perfect 100% attack success rate in some instances. To improve
security, we propose post-hoc patching defenses used to intervene during
inference by modifying the SLM's activations that improve robustness up to 99%
with (i) negligible impact on utility and (ii) without any re-training. We
conduct ablation studies to maximize the efficacy of our defenses and improve
the utility/security trade-off, validated with large-scale benchmarks unique to
SLMs.

</details>


### [448] [Pairwise Evaluation of Accent Similarity in Speech Synthesis](https://arxiv.org/abs/2505.14410)
*Jinzuomu Zhong,Suyuan Liu,Dan Wells,Korin Richmond*

Main category: eess.AS

TL;DR: 本文提出改进语音合成中口音相似性评估的方法，包括主观和客观评估，并指出常用指标在评估少数口音时的局限性。


<details>
  <summary>Details</summary>
Motivation: 当前语音合成中对口音相似性的评估方法不足，尤其是主观评估的统计显著性和成本问题，以及客观评估中常用指标的局限性。

Method: 主观上改进XAB听力测试，提供文本转录并让听者标注口音差异；客观上使用基于元音共振峰和语音后验图的发音相关指标。

Result: 实验表明，所提指标能有效评估口音生成，同时发现常用词错误率在评估少数口音时存在显著局限性。

Conclusion: 改进后的主观和客观评估方法能更准确评估口音相似性，为语音合成研究提供更可靠的评估工具。

Abstract: Despite growing interest in generating high-fidelity accents, evaluating
accent similarity in speech synthesis has been underexplored. We aim to enhance
both subjective and objective evaluation methods for accent similarity.
Subjectively, we refine the XAB listening test by adding components that
achieve higher statistical significance with fewer listeners and lower costs.
Our method involves providing listeners with transcriptions, having them
highlight perceived accent differences, and implementing meticulous screening
for reliability. Objectively, we utilise pronunciation-related metrics, based
on distances between vowel formants and phonetic posteriorgrams, to evaluate
accent generation. Comparative experiments reveal that these metrics, alongside
accent similarity, speaker similarity, and Mel Cepstral Distortion, can be
used. Moreover, our findings underscore significant limitations of common
metrics like Word Error Rate in assessing underrepresented accents.

</details>


### [449] [Mitigating Subgroup Disparities in Multi-Label Speech Emotion Recognition: A Pseudo-Labeling and Unsupervised Learning Approach](https://arxiv.org/abs/2505.14449)
*Yi-Cheng Lin,Huang-Cheng Chou,Hung-yi Lee*

Main category: eess.AS

TL;DR: 该论文提出了一种隐式人口统计推断（IDI）模块，通过伪标签和无监督学习减少语音情感识别中的偏见，显著提高了公平性指标。


<details>
  <summary>Details</summary>
Motivation: 当前语音情感识别（SER）中的公平性问题研究不足，且现有方法依赖难以获取的显式人口统计标签。

Method: 结合预训练模型的伪标签和无监督k-means聚类，开发了隐式人口统计推断（IDI）模块。

Result: 伪标签IDI将公平性指标提高33%以上，SER准确率下降不到3%；无监督IDI公平性指标提高26%以上，SER性能下降不到4%。

Conclusion: 无监督IDI能有效缓解种族和年龄偏见，适用于缺乏显式人口统计信息的场景。

Abstract: While subgroup disparities and performance bias are increasingly studied in
computational research, fairness in categorical Speech Emotion Recognition
(SER) remains underexplored. Existing methods often rely on explicit
demographic labels, which are difficult to obtain due to privacy concerns. To
address this limitation, we introduce an Implicit Demography Inference (IDI)
module that leverages pseudo-labeling from a pre-trained model and unsupervised
learning using k-means clustering to mitigate bias in SER. Our experiments show
that pseudo-labeling IDI reduces subgroup disparities, improving fairness
metrics by over 33% with less than a 3% decrease in SER accuracy. Also, the
unsupervised IDI yields more than a 26% improvement in fairness metrics with a
drop of less than 4% in SER performance. Further analyses reveal that the
unsupervised IDI consistently mitigates race and age disparities, demonstrating
its potential in scenarios where explicit demographic information is
unavailable.

</details>


### [450] [Teaching Audio-Aware Large Language Models What Does Not Hear: Mitigating Hallucinations through Synthesized Negative Samples](https://arxiv.org/abs/2505.14518)
*Chun-Yi Kuan,Hung-yi Lee*

Main category: eess.AS

TL;DR: 论文提出LISTEN方法，通过对比训练增强音频感知大语言模型区分真实与虚构声音的能力，无需修改模型参数且高效。


<details>
  <summary>Details</summary>
Motivation: 当前音频感知大语言模型在处理音频输入时易产生声音幻觉，降低实际应用可靠性，需解决此问题。

Method: 采用LISTEN对比式训练方法，利用轻量适配器集成音频表示，通过合成数据增强模型区分能力。

Result: 实验表明LISTEN有效减少幻觉，保持现有音频问答基准性能，同时数据与计算效率更高。

Conclusion: LISTEN为音频感知大语言模型提供了一种高效、无需参数修改的幻觉抑制解决方案。

Abstract: Recent advancements in audio-aware large language models (ALLMs) enable them
to process and understand audio inputs. However, these models often hallucinate
non-existent sound events, reducing their reliability in real-world
applications. To address this, we propose LISTEN (Learning to Identify Sounds
Through Extended Negative Samples), a contrastive-like training method that
enhances ALLMs' ability to distinguish between present and absent sounds using
synthesized data from the backbone LLM. Unlike prior approaches, our method
requires no modification to LLM parameters and efficiently integrates audio
representations via a lightweight adapter. Experiments show that LISTEN
effectively mitigates hallucinations while maintaining impressive performance
on existing audio question and reasoning benchmarks. At the same time, it is
more efficient in both data and computation.

</details>


### [451] [FlowTSE: Target Speaker Extraction with Flow Matching](https://arxiv.org/abs/2505.14465)
*Aviv Navon,Aviv Shamsian,Yael Segal-Feldman,Neta Glazer,Gil Hetz,Joseph Keshet*

Main category: eess.AS

TL;DR: FlowTSE提出了一种基于条件流匹配的简单有效目标说话人提取方法，通过改进相位估计，在标准基准测试中达到或超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有生成式目标说话人提取方法依赖复杂流程和预训练组件，计算开销大且研究不足，需更简洁高效的解决方案。

Method: 使用条件流匹配模型处理注册音频和混合语音的梅尔谱，并设计新型声码器优化相位重建。

Result: 在标准TSE基准测试中，FlowTSE性能匹配或优于现有强基线模型。

Conclusion: FlowTSE通过流匹配框架简化了生成式TSE流程，同时通过相位条件声码器提升重建质量，为领域提供新思路。

Abstract: Target speaker extraction (TSE) aims to isolate a specific speaker's speech
from a mixture using speaker enrollment as a reference. While most existing
approaches are discriminative, recent generative methods for TSE achieve strong
results. However, generative methods for TSE remain underexplored, with most
existing approaches relying on complex pipelines and pretrained components,
leading to computational overhead. In this work, we present FlowTSE, a simple
yet effective TSE approach based on conditional flow matching. Our model
receives an enrollment audio sample and a mixed speech signal, both represented
as mel-spectrograms, with the objective of extracting the target speaker's
clean speech. Furthermore, for tasks where phase reconstruction is crucial, we
propose a novel vocoder conditioned on the complex STFT of the mixed signal,
enabling improved phase estimation. Experimental results on standard TSE
benchmarks show that FlowTSE matches or outperforms strong baselines.

</details>


### [452] [Steering Deep Non-Linear Spatially Selective Filters for Weakly Guided Extraction of Moving Speakers in Dynamic Scenarios](https://arxiv.org/abs/2505.14517)
*Jakob Kienegger,Timo Gerkmann*

Main category: eess.AS

TL;DR: 论文提出了一种弱引导的说话人提取方法，仅依赖目标初始位置处理动态场景，结合深度跟踪算法和联合训练策略，性能优于强引导方法。


<details>
  <summary>Details</summary>
Motivation: 现有深度非线性空间滤波方法在目标方向已知且静态时表现优异，但在动态场景中（如说话人移动交叉）因时变空间特征和模糊性而面临挑战。手动跟踪移动说话人不切实际，需摆脱对精确时变方向线索的依赖。

Method: 提出弱引导提取方法，仅需目标初始位置；结合自研深度跟踪算法，并在合成数据集上开发联合训练策略。

Result: 该方法能有效解决空间模糊性问题，其性能甚至超过不匹配的强引导提取方法。

Conclusion: 弱引导方法在动态场景中具有实际可行性，通过初始位置和深度跟踪即可实现优越的说话人提取效果。

Abstract: Recent speaker extraction methods using deep non-linear spatial filtering
perform exceptionally well when the target direction is known and stationary.
However, spatially dynamic scenarios are considerably more challenging due to
time-varying spatial features and arising ambiguities, e.g. when moving
speakers cross. While in a static scenario it may be easy for a user to point
to the target's direction, manually tracking a moving speaker is impractical.
Instead of relying on accurate time-dependent directional cues, which we refer
to as strong guidance, in this paper we propose a weakly guided extraction
method solely depending on the target's initial position to cope with spatial
dynamic scenarios. By incorporating our own deep tracking algorithm and
developing a joint training strategy on a synthetic dataset, we demonstrate the
proficiency of our approach in resolving spatial ambiguities and even
outperform a mismatched, but strongly guided extraction method.

</details>


### [453] [SSPS: Self-Supervised Positive Sampling for Robust Self-Supervised Speaker Verification](https://arxiv.org/abs/2505.14561)
*Theo Lepage,Reda Dehak*

Main category: eess.AS

TL;DR: 该论文提出了一种新的自监督正采样技术SSPS，用于改进说话人验证任务，显著降低了说话人内部差异，性能优于现有自监督学习方法。


<details>
  <summary>Details</summary>
Motivation: 现有的自监督学习框架在说话人验证任务中主要依赖同语句的正采样和数据增强，导致模型主要编码了录音条件中的通道信息，而忽略了说话人身份的一致性。

Method: 论文提出了一种名为SSPS的自监督正采样技术，通过在潜在空间中使用聚类分配和正嵌入的记忆队列，为给定锚点寻找同一说话人但不同录音条件的正样本。

Result: SSPS在SimCLR和DINO框架上均取得了显著改进，在VoxCeleb1-O数据集上分别达到了2.57%和2.53%的EER，性能优于现有自监督学习方法。

Conclusion: SSPS技术有效降低了说话人内部差异，显著提升了说话人验证的性能，为自监督学习在说话人验证任务中的应用提供了新的思路。

Abstract: Self-Supervised Learning (SSL) has led to considerable progress in Speaker
Verification (SV). The standard framework uses same-utterance positive sampling
and data-augmentation to generate anchor-positive pairs of the same speaker.
This is a major limitation, as this strategy primarily encodes channel
information from the recording condition, shared by the anchor and positive. We
propose a new positive sampling technique to address this bottleneck:
Self-Supervised Positive Sampling (SSPS). For a given anchor, SSPS aims to find
an appropriate positive, i.e., of the same speaker identity but a different
recording condition, in the latent space using clustering assignments and a
memory queue of positive embeddings. SSPS improves SV performance for both
SimCLR and DINO, reaching 2.57% and 2.53% EER, outperforming SOTA SSL methods
on VoxCeleb1-O. In particular, SimCLR-SSPS achieves a 58% EER reduction by
lowering intra-speaker variance, providing comparable performance to DINO-SSPS.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [454] [Robust learning of halfspaces under log-concave marginals](https://arxiv.org/abs/2505.13708)
*Jane Lange,Arsen Vasilyan*

Main category: cs.DS

TL;DR: 该论文提出了一种高效学习具有小边界体积假设的算法，通过增强多项式回归方法，实现了对抗鲁棒性分类器的学习。


<details>
  <summary>Details</summary>
Motivation: 研究如何在计算效率高的情况下学习具有小边界体积的假设，特别是在输入服从亚高斯各向同性对数凹分布时。线性阈值函数具有对抗鲁棒性，但一般的多项式阈值函数可能边界体积较大。

Method: 算法通过三个步骤增强多项式回归：a) 在噪声敏感度约束下进行ℓ1误差回归，b) 结构化分区和舍入步骤，c) 使用局部校正器平滑低噪声敏感度函数。

Result: 算法能够以d^O(1/ε²)的时间和样本复杂度，学习边界体积为O(r+ε)的分类器，与多项式回归复杂度匹配。

Conclusion: 该算法成功实现了对抗鲁棒性分类器的高效学习，为小边界体积假设的学习提供了有效解决方案。

Abstract: We say that a classifier is \emph{adversarially robust} to perturbations of
norm $r$ if, with high probability over a point $x$ drawn from the input
distribution, there is no point within distance $\le r$ from $x$ that is
classified differently. The \emph{boundary volume} is the probability that a
point falls within distance $r$ of a point with a different label. This work
studies the task of computationally efficient learning of hypotheses with small
boundary volume, where the input is distributed as a subgaussian isotropic
log-concave distribution over $\mathbb{R}^d$.
  Linear threshold functions are adversarially robust; they have boundary
volume proportional to $r$. Such concept classes are efficiently learnable by
polynomial regression, which produces a polynomial threshold function (PTF),
but PTFs in general may have boundary volume $\Omega(1)$, even for $r \ll 1$.
  We give an algorithm that agnostically learns linear threshold functions and
returns a classifier with boundary volume $O(r+\varepsilon)$ at radius of
perturbation $r$. The time and sample complexity of
$d^{\tilde{O}(1/\varepsilon^2)}$ matches the complexity of polynomial
regression.
  Our algorithm augments the classic approach of polynomial regression with
three additional steps: a) performing the $\ell_1$-error regression under noise
sensitivity constraints, b) a structured partitioning and rounding step that
returns a Boolean classifier with error $\textsf{opt} + O(\varepsilon)$ and
noise sensitivity $O(r+\varepsilon)$ simultaneously, and c) a local corrector
that ``smooths'' a function with low noise sensitivity into a function that is
adversarially robust.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [455] [InterFeat: An Automated Pipeline for Finding Interesting Hypotheses in Structured Biomedical Data](https://arxiv.org/abs/2505.13534)
*Dan Ofer,Michal Linial,Dafna Shahaf*

Main category: q-bio.QM

TL;DR: 该论文提出了一种自动化发现生物医学数据中有趣假设的集成管道，结合机器学习、知识图谱和大型语言模型，成功预测了疾病风险因素。


<details>
  <summary>Details</summary>
Motivation: 科学发现的核心是寻找有趣现象，但这一过程通常是手动且定义模糊的。论文旨在自动化这一过程，特别是在生物医学数据中发现简单但有趣的假设。

Method: 论文提出了一种集成管道，结合机器学习、知识图谱、文献搜索和大型语言模型，将“有趣性”形式化为新颖性、实用性和合理性的组合。

Result: 在英国生物银行的8种主要疾病数据上，该管道能够提前数年发现风险因素。40-53%的顶级候选假设被验证为有趣，远高于基线方法的0-7%。

Conclusion: 该管道能够可扩展地操作“有趣性”，并为任何目标提供支持。论文还公开了数据和代码，促进了进一步研究。

Abstract: Finding interesting phenomena is the core of scientific discovery, but it is
a manual, ill-defined concept. We present an integrative pipeline for
automating the discovery of interesting simple hypotheses (feature-target
relations with effect direction and a potential underlying mechanism) in
structured biomedical data. The pipeline combines machine learning, knowledge
graphs, literature search and Large Language Models. We formalize
"interestingness" as a combination of novelty, utility and plausibility. On 8
major diseases from the UK Biobank, our pipeline consistently recovers risk
factors years before their appearance in the literature. 40--53% of our top
candidates were validated as interesting, compared to 0--7% for a SHAP-based
baseline. Overall, 28% of 109 candidates were interesting to medical experts.
The pipeline addresses the challenge of operationalizing "interestingness"
scalably and for any target. We release data and code:
https://github.com/LinialLab/InterFeat

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [456] [Learning to Program Quantum Measurements for Machine Learning](https://arxiv.org/abs/2505.13525)
*Samual Yen-Chi Chen,Huan-Hsin Tseng,Hsin-Yi Lin,Shinjae Yoo*

Main category: quant-ph

TL;DR: 该论文提出了一种创新框架，通过使量子系统的可观测量可训练，动态优化量子机器学习模型的性能，显著提升了分类准确率等指标。


<details>
  <summary>Details</summary>
Motivation: 量子机器学习（QML）的发展面临数据编码策略和参数化量子电路设计的挑战，且现有模型的测量方案往往与问题需求不匹配。论文旨在解决这些问题，提升QML模型的性能。

Method: 论文提出了一种端到端可微分学习框架，动态编程量子可观测量参数，同时优化神经网络和标准量子电路参数，使可观测量能根据输入数据流实时调整。

Result: 数值模拟表明，该方法在变分量子电路中有效动态编程可观测量，相比现有方法取得了更优的结果，如更高的分类准确率。

Conclusion: 该框架显著提升了量子机器学习模型的整体效能，为QML的广泛应用提供了新的解决方案。

Abstract: The rapid advancements in quantum computing (QC) and machine learning (ML)
have sparked significant interest, driving extensive exploration of quantum
machine learning (QML) algorithms to address a wide range of complex
challenges. The development of high-performance QML models requires
expert-level expertise, presenting a key challenge to the widespread adoption
of QML. Critical obstacles include the design of effective data encoding
strategies and parameterized quantum circuits, both of which are vital for the
performance of QML models. Furthermore, the measurement process is often
neglected-most existing QML models employ predefined measurement schemes that
may not align with the specific requirements of the targeted problem. We
propose an innovative framework that renders the observable of a quantum
system-specifically, the Hermitian matrix-trainable. This approach employs an
end-to-end differentiable learning framework, enabling simultaneous
optimization of the neural network used to program the parameterized
observables and the standard quantum circuit parameters. Notably, the quantum
observable parameters are dynamically programmed by the neural network,
allowing the observables to adapt in real time based on the input data stream.
Through numerical simulations, we demonstrate that the proposed method
effectively programs observables dynamically within variational quantum
circuits, achieving superior results compared to existing approaches. Notably,
it delivers enhanced performance metrics, such as higher classification
accuracy, thereby significantly improving the overall effectiveness of QML
models.

</details>


### [457] [Benchmarking data encoding methods in Quantum Machine Learning](https://arxiv.org/abs/2505.14295)
*Orlane Zang,Grégoire Barrué,Tony Quertier*

Main category: quant-ph

TL;DR: 该论文研究了量子机器学习中数据编码的重要性，比较了不同编码方法在不同数据集上的表现。


<details>
  <summary>Details</summary>
Motivation: 量子机器学习中的数据编码步骤对模型性能有显著影响，但目前缺乏选择合适编码方法的通用规则，因此需要系统研究不同编码方法的适用性。

Method: 研究并比较了使用不同量子逻辑门的常见编码方法，并在多个数据集上进行了基准测试。

Result: 通过实验评估了各种编码方法在不同数据集上的表现，为选择合适的数据编码方法提供了参考依据。

Conclusion: 量子数据编码方法的选择应根据具体数据集特性进行，该研究为量子机器学习中的编码选择提供了实践指导。

Abstract: Data encoding plays a fundamental and distinctive role in Quantum Machine
Learning (QML). While classical approaches process data directly as vectors,
QML may require transforming classical data into quantum states through
encoding circuits, known as quantum feature maps or quantum embeddings. This
step leverages the inherently high-dimensional and non-linear nature of Hilbert
space, enabling more efficient data separation in complex feature spaces that
may be inaccessible to classical methods. This encoding part significantly
affects the performance of the QML model, so it is important to choose the
right encoding method for the dataset to be encoded. However, this choice is
generally arbitrary, since there is no "universal" rule for knowing which
encoding to choose based on a specific set of data. There are currently a
variety of encoding methods using different quantum logic gates. We studied the
most commonly used types of encoding methods and benchmarked them using
different datasets.

</details>


### [458] [QSVM-QNN: Quantum Support Vector Machine Based Quantum Neural Network Learning Algorithm for Brain-Computer Interfacing Systems](https://arxiv.org/abs/2505.14192)
*Bikash K. Behera,Saif Al-Kuwari,Ahmed Farouk*

Main category: quant-ph

TL;DR: 该研究提出了一种名为QSVM-QNN的新型混合量子学习模型，结合量子支持向量机（QSVM）和量子神经网络（QNN），用于提升脑机接口（BCI）任务中的分类准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 尽管脑机接口系统在辅助技术和人机交互方面具有巨大潜力，但仍面临信号变异性、分类效率低以及难以实时适应用户等挑战。

Method: 研究提出了一种混合量子学习模型QSVM-QNN，结合了QSVM的决策边界能力和QNN的表达学习能力，并在两个基准EEG数据集上进行了评估。

Result: QSVM-QNN在两个数据集上分别达到了0.990和0.950的高准确率，优于经典和独立的量子模型，并且在六种量子噪声模型下表现出稳定的性能。

Conclusion: QSVM-QNN不仅适用于脑机接口任务，还可推广到其他生物医学和时间序列分类任务，为下一代神经技术系统提供了可扩展且抗噪声的解决方案。

Abstract: A brain-computer interface (BCI) system enables direct communication between
the brain and external devices, offering significant potential for assistive
technologies and advanced human-computer interaction. Despite progress, BCI
systems face persistent challenges, including signal variability,
classification inefficiency, and difficulty adapting to individual users in
real time. In this study, we propose a novel hybrid quantum learning model,
termed QSVM-QNN, which integrates a Quantum Support Vector Machine (QSVM) with
a Quantum Neural Network (QNN), to improve classification accuracy and
robustness in EEG-based BCI tasks. Unlike existing models, QSVM-QNN combines
the decision boundary capabilities of QSVM with the expressive learning power
of QNN, leading to superior generalization performance. The proposed model is
evaluated on two benchmark EEG datasets, achieving high accuracies of 0.990 and
0.950, outperforming both classical and standalone quantum models. To
demonstrate real-world viability, we further validated the robustness of QNN,
QSVM, and QSVM-QNN against six realistic quantum noise models, including bit
flip and phase damping. These experiments reveal that QSVM-QNN maintains stable
performance under noisy conditions, establishing its applicability for
deployment in practical, noisy quantum environments. Beyond BCI, the proposed
hybrid quantum architecture is generalizable to other biomedical and
time-series classification tasks, offering a scalable and noise-resilient
solution for next-generation neurotechnological systems.

</details>


### [459] [Quantum Optimization via Gradient-Based Hamiltonian Descent](https://arxiv.org/abs/2505.14670)
*Jiaqi Leng,Bin Shi*

Main category: quant-ph

TL;DR: 该论文提出了一种基于梯度信息的量子哈密顿下降（QHD）改进方法，显著提升了收敛速度和全局解发现能力。


<details>
  <summary>Details</summary>
Motivation: 量子哈密顿下降（QHD）虽然能通过量子隧穿逃离鞍点和局部极小值，但其收敛速度较慢且在高度非凸问题中鲁棒性不足。受经典方法中高分辨率微分方程启发，作者希望通过引入梯度信息来改进QHD。

Method: 提出梯度基QHD（gradient-based QHD），在原有QHD框架中融入梯度信息，从而加速收敛并增强全局优化能力。

Result: 数值模拟表明，梯度基QHD在复杂问题实例上的性能优于现有量子与经典方法至少一个数量级。

Conclusion: 梯度信息的引入有效解决了QHD的局限性，为连续优化问题提供了更高效的量子算法解决方案。

Abstract: With rapid advancements in machine learning, first-order algorithms have
emerged as the backbone of modern optimization techniques, owing to their
computational efficiency and low memory requirements. Recently, the connection
between accelerated gradient methods and damped heavy-ball motion, particularly
within the framework of Hamiltonian dynamics, has inspired the development of
innovative quantum algorithms for continuous optimization. One such algorithm,
Quantum Hamiltonian Descent (QHD), leverages quantum tunneling to escape saddle
points and local minima, facilitating the discovery of global solutions in
complex optimization landscapes. However, QHD faces several challenges,
including slower convergence rates compared to classical gradient methods and
limited robustness in highly non-convex problems due to the non-local nature of
quantum states. Furthermore, the original QHD formulation primarily relies on
function value information, which limits its effectiveness. Inspired by
insights from high-resolution differential equations that have elucidated the
acceleration mechanisms in classical methods, we propose an enhancement to QHD
by incorporating gradient information, leading to what we call gradient-based
QHD. Gradient-based QHD achieves faster convergence and significantly increases
the likelihood of identifying global solutions. Numerical simulations on
challenging problem instances demonstrate that gradient-based QHD outperforms
existing quantum and classical methods by at least an order of magnitude.

</details>


<div id='cond-mat.soft'></div>

# cond-mat.soft [[Back]](#toc)

### [460] [Polymer Data Challenges in the AI Era: Bridging Gaps for Next-Generation Energy Materials](https://arxiv.org/abs/2505.13494)
*Ying Zhao,Guanhua Chen,Jie Liu*

Main category: cond-mat.soft

TL;DR: 聚合物科学数据碎片化阻碍能源技术发展，需通过技术创新与协作解决。


<details>
  <summary>Details</summary>
Motivation: 聚合物科学在能源技术（如光伏、固态电池、氢存储）中的应用因数据碎片化而受阻，缺乏可互操作的数据库和一致的测试方法，限制了机器学习的应用和新材料的发现。

Method: 采用自然语言处理（NLP）工具从文献中提取结构化数据，利用高通量机器人平台生成自洽数据集，并遵循FAIR原则确保数据的可查找、可访问、可互操作和可重用。

Result: 通过技术创新和协作治理，聚合物科学社区能够克服数据碎片化问题，加速新材料发现。

Conclusion: 未来突破依赖于开放科学的文化转变，结合去中心化数据市场和自主实验室，将数据瓶颈转化为加速器。

Abstract: The pursuit of advanced polymers for energy technologies, spanning
photovoltaics, solid-state batteries, and hydrogen storage, is hindered by
fragmented data ecosystems that fail to capture the hierarchical complexity of
these materials. Polymer science lacks interoperable databases, forcing
reliance on disconnected literature and legacy records riddled with
unstructured formats and irreproducible testing protocols. This fragmentation
stifles machine learning (ML) applications and delays the discovery of
materials critical for global decarbonization. Three systemic barriers compound
the challenge. First, academic-industrial data silos restrict access to
proprietary industrial datasets, while academic publications often omit
critical synthesis details. Second, inconsistent testing methods undermine
cross-study comparability. Third, incomplete metadata in existing databases
limits their utility for training reliable ML models. Emerging solutions
address these gaps through technological and collaborative innovation. Natural
language processing (NLP) tools extract structured polymer data from decades of
literature, while high-throughput robotic platforms generate self-consistent
datasets via autonomous experimentation. Central to these advances is the
adoption of FAIR (Findable, Accessible, Interoperable, Reusable) principles,
adapted to polymer-specific ontologies, ensuring machine-readability and
reproducibility. Future breakthroughs hinge on cultural shifts toward open
science, accelerated by decentralized data markets and autonomous laboratories
that merge robotic experimentation with real-time ML validation. By addressing
data fragmentation through technological innovation, collaborative governance,
and ethical stewardship, the polymer community can transform bottlenecks into
accelerants.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [461] [LLM Context Conditioning and PWP Prompting for Multimodal Validation of Chemical Formulas](https://arxiv.org/abs/2505.12257)
*Evgeny Markhasin*

Main category: cs.CY

TL;DR: 该研究探索了通过结构化上下文调节（基于PWP原则）来改进通用大语言模型（如Gemini 2.5 Pro和ChatGPT Plus o3）在科学文档中识别技术错误的能力，初步结果显示该方法能提升文本和图像公式错误的检测效果。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在复杂科学文档中识别细微技术错误时存在困难，尤其是涉及多模态内容（如图像中的公式）时，其固有的纠错倾向可能掩盖错误。本研究旨在探索如何通过结构化上下文调节来改进这一能力。

Method: 研究采用基于Persistent Workflow Prompting (PWP)原则的结构化上下文调节方法，通过标准聊天界面（无需API或模型修改）对Gemini 2.5 Pro和ChatGPT Plus o3进行测试，重点验证化学公式中的文本和图像错误。

Result: 初步结果显示，基础提示方法不可靠，而基于PWP的结构化调节能提升文本错误识别能力。Gemini 2.5 Pro成功识别出图像中的公式错误，而ChatGPT Plus o3未能做到。

Conclusion: 研究表明，PWP-informed的上下文调节是一种有前景且易于实施的技术，可提升LLMs在科学文档中细致错误检测的可靠性，但需进一步验证其广泛适用性。

Abstract: Identifying subtle technical errors within complex scientific and technical
documents, especially those requiring multimodal interpretation (e.g., formulas
in images), presents a significant hurdle for Large Language Models (LLMs)
whose inherent error-correction tendencies can mask inaccuracies. This
exploratory proof-of-concept (PoC) study investigates structured LLM context
conditioning, informed by Persistent Workflow Prompting (PWP) principles, as a
methodological strategy to modulate this LLM behavior at inference time. The
approach is designed to enhance the reliability of readily available,
general-purpose LLMs (specifically Gemini 2.5 Pro and ChatGPT Plus o3) for
precise validation tasks, crucially relying only on their standard chat
interfaces without API access or model modifications. To explore this
methodology, we focused on validating chemical formulas within a single,
complex test paper with known textual and image-based errors. Several prompting
strategies were evaluated: while basic prompts proved unreliable, an approach
adapting PWP structures to rigorously condition the LLM's analytical mindset
appeared to improve textual error identification with both models. Notably,
this method also guided Gemini 2.5 Pro to repeatedly identify a subtle
image-based formula error previously overlooked during manual review, a task
where ChatGPT Plus o3 failed in our tests. These preliminary findings highlight
specific LLM operational modes that impede detail-oriented validation and
suggest that PWP-informed context conditioning offers a promising and highly
accessible technique for developing more robust LLM-driven analytical
workflows, particularly for tasks requiring meticulous error detection in
scientific and technical documents. Extensive validation beyond this limited
PoC is necessary to ascertain broader applicability.

</details>


### [462] [Algorithmic Tradeoffs in Fair Lending: Profitability, Compliance, and Long-Term Impact](https://arxiv.org/abs/2505.13469)
*Aayam Bansal,Harsh Vardhan Narsaria*

Main category: cs.CY

TL;DR: 论文探讨了在贷款决策中平衡算法公平性与盈利性的问题，发现忽略受保护属性的方法在公平性和盈利性上表现最佳。


<details>
  <summary>Details</summary>
Motivation: 随着金融机构越来越多地依赖机器学习模型自动化贷款决策，算法公平性问题日益突出。本文旨在研究公平性约束与贷款机构盈利性之间的权衡。

Method: 通过模拟反映现实贷款模式的合成数据，量化不同公平性干预措施对利润率和违约率的影响。

Result: 结果表明，平等机会约束的利润成本低于人口统计平等约束，而忽略受保护属性的方法在公平性和盈利性上均优于显式公平性干预。

Conclusion: 研究为设计平衡伦理考量和商业目标的贷款算法提供了实用指导，并识别了公平贷款变得有利可图的具体经济条件。

Abstract: As financial institutions increasingly rely on machine learning models to
automate lending decisions, concerns about algorithmic fairness have risen.
This paper explores the tradeoff between enforcing fairness constraints (such
as demographic parity or equal opportunity) and maximizing lender
profitability. Through simulations on synthetic data that reflects real-world
lending patterns, we quantify how different fairness interventions impact
profit margins and default rates. Our results demonstrate that equal
opportunity constraints typically impose lower profit costs than demographic
parity, but surprisingly, removing protected attributes from the model
(fairness through unawareness) outperforms explicit fairness interventions in
both fairness and profitability metrics. We further identify the specific
economic conditions under which fair lending becomes profitable and analyze the
feature-specific drivers of unfairness. These findings offer practical guidance
for designing lending algorithms that balance ethical considerations with
business objectives.

</details>


### [463] [AdAEM: An Adaptively and Automated Extensible Measurement of LLMs' Value Difference](https://arxiv.org/abs/2505.13531)
*Shitong Duan,Xiaoyuan Yi,Peng Zhang,Dongkuan Xu,Jing Yao,Tun Lu,Ning Gu,Xing Xie*

Main category: cs.CY

TL;DR: 论文提出AdAEM框架，通过自适应生成测试问题来动态评估不同大语言模型的价值差异，解决了现有方法信息量不足的问题。


<details>
  <summary>Details</summary>
Motivation: 当前评估大语言模型价值差异的数据集存在信息量不足的问题，测试问题过时、污染或过于通用，导致结果饱和且缺乏区分度。

Method: 提出AdAEM框架，通过上下文优化的方式自动生成和扩展测试问题，最大化信息理论目标，提取最新或有文化争议的主题。

Result: 生成12,310个基于Schwartz价值理论的问题，对16个大语言模型进行了广泛分析，验证了方法的有效性和区分度。

Conclusion: AdAEM能够与大语言模型共同进化，持续追踪其价值动态，为价值研究提供了更好的基础。

Abstract: Assessing Large Language Models (LLMs)' underlying value differences enables
comprehensive comparison of their misalignment, cultural adaptability, and
biases. Nevertheless, current value measurement datasets face the
informativeness challenge: with often outdated, contaminated, or generic test
questions, they can only capture the shared value orientations among different
LLMs, leading to saturated and thus uninformative results. To address this
problem, we introduce AdAEM, a novel, self-extensible assessment framework for
revealing LLMs' inclinations. Distinct from previous static benchmarks, AdAEM
can automatically and adaptively generate and extend its test questions. This
is achieved by probing the internal value boundaries of a diverse set of LLMs
developed across cultures and time periods in an in-context optimization
manner. The optimization process theoretically maximizes an
information-theoretic objective to extract the latest or culturally
controversial topics, providing more distinguishable and informative insights
about models' value differences. In this way, AdAEM is able to co-evolve with
the development of LLMs, consistently tracking their value dynamics. Using
AdAEM, we generate 12,310 questions grounded in Schwartz Value Theory, conduct
an extensive analysis to manifest our method's validity and effectiveness, and
benchmark the values of 16 LLMs, laying the groundwork for better value
research.

</details>


### [464] [Aligning Trustworthy AI with Democracy: A Dual Taxonomy of Opportunities and Risks](https://arxiv.org/abs/2505.13565)
*Oier Mentxaka,Natalia Díaz-Rodríguez,Mark Coeckelbergh,Marcos López de Prado,Emilia Gómez,David Fernández Llorca,Enrique Herrera-Viedma,Francisco Herrera*

Main category: cs.CY

TL;DR: 本文提出双重分类法评估AI对民主的影响：AIRD分类法识别AI威胁民主原则的风险，AIPD分类法强调AI促进民主的潜力，并基于欧盟伦理框架提出缓解策略。


<details>
  <summary>Details</summary>
Motivation: 探讨AI对民主治理的双重影响（风险与机遇），为研究、政策制定和技术设计提供规范性框架，弥合伦理目标与实际操作间的差距。

Method: 基于欧盟可信AI要求，建立AIRD（风险）和AIPD（贡献）双重分类法，结合民主理论与欧盟治理工具进行规范性分析。

Result: 提出透明度和社会福利在风险缓解中的核心作用，提供可操作框架以协调AI系统与民主价值，支持包容性、问责制的民主体系。

Conclusion: 整合民主理论与治理工具，为学者、政策制定者和技术人员提供评估、监管和设计AI系统的结构化基础，推动算法时代的民主韧性。

Abstract: Artificial Intelligence (AI) poses both significant risks and valuable
opportunities for democratic governance. This paper introduces a dual taxonomy
to evaluate AI's complex relationship with democracy: the AI Risks to Democracy
(AIRD) taxonomy, which identifies how AI can undermine core democratic
principles such as autonomy, fairness, and trust; and the AI's Positive
Contributions to Democracy (AIPD) taxonomy, which highlights AI's potential to
enhance transparency, participation, efficiency, and evidence-based
policymaking.
  Grounded in the European Union's approach to ethical AI governance, and
particularly the seven Trustworthy AI requirements proposed by the European
Commission's High-Level Expert Group on AI, each identified risk is aligned
with mitigation strategies based on EU regulatory and normative frameworks. Our
analysis underscores the transversal importance of transparency and societal
well-being across all risk categories and offers a structured lens for aligning
AI systems with democratic values.
  By integrating democratic theory with practical governance tools, this paper
offers a normative and actionable framework to guide research, regulation, and
institutional design to support trustworthy, democratic AI. It provides
scholars with a conceptual foundation to evaluate the democratic implications
of AI, equips policymakers with structured criteria for ethical oversight, and
helps technologists align system design with democratic principles. In doing
so, it bridges the gap between ethical aspirations and operational realities,
laying the groundwork for more inclusive, accountable, and resilient democratic
systems in the algorithmic age.

</details>


### [465] [Fuck the Algorithm: Conceptual Issues in Algorithmic Bias](https://arxiv.org/abs/2505.13509)
*Catherine Stinson*

Main category: cs.CY

TL;DR: 该论文探讨了算法偏见的争议，澄清了算法本身是否可能具有偏见的问题，并分析了偏见的不同含义及其道德影响。


<details>
  <summary>Details</summary>
Motivation: 近年来，算法偏见引发了广泛争议。为了澄清争议并推动问题解决，论文旨在更好地理解相关概念，特别是算法本身是否可能具有偏见。

Method: 论文通过分析“算法本身”的定义和“偏见”的多重含义，结合统计偏见与道德偏见的关联，以及政治工具和压迫性事物的概念，探讨算法偏见的来源。

Result: 论文指出，算法确实可能成为偏见的源头，例如在推荐系统、学术搜索引擎和英国2020年A-level成绩算法中。识别算法的偏见对明确责任和防止歧视至关重要。

Conclusion: 论文得出结论，算法本身可以具有偏见，这一认识对于明确责任和防止算法中介的歧视具有重要意义。

Abstract: Algorithmic bias has been the subject of much recent controversy. To clarify
what is at stake and to make progress resolving the controversy, a better
understanding of the concepts involved would be helpful. The discussion here
focuses on the disputed claim that algorithms themselves cannot be biased. To
clarify this claim we need to know what kind of thing 'algorithms themselves'
are, and to disambiguate the several meanings of 'bias' at play. This further
involves showing how bias of moral import can result from statistical biases,
and drawing connections to previous conceptual work about political artifacts
and oppressive things. Data bias has been identified in domains like hiring,
policing and medicine. Examples where algorithms themselves have been
pinpointed as the locus of bias include recommender systems that influence
media consumption, academic search engines that influence citation patterns,
and the 2020 UK algorithmically-moderated A-level grades. Recognition that
algorithms are a kind of thing that can be biased is key to making decisions
about responsibility for harm, and preventing algorithmically mediated
discrimination.

</details>


### [466] [Upgrading Democracies with Fairer Voting Methods](https://arxiv.org/abs/2505.14349)
*Evangelos Pournaras,Srijoni Majumdar,Thomas Wellings,Joshua C. Yang,Fatemeh B. Heravan,Regula Hänggli Fricker,Dirk Helbing*

Main category: cs.CY

TL;DR: 论文探讨了如何通过改进投票方法来提升民主制度的效能，特别是在多元现代社会中。


<details>
  <summary>Details</summary>
Motivation: 当前许多民主国家仍使用过时的投票方法，这些方法不适应现代多元社会，缺乏社会创新。

Method: 研究采用了替代性优先投票方法，如累积投票和比例代表制方法，并在瑞士阿劳市进行了实证评估。

Result: 研究发现，公平投票方法能带来更多获胜项目、更广泛的地理和偏好代表性，尤其惠及以往被忽视的选民。

Conclusion: 公民更倾向于比例投票方法，这些方法具有强合法性且无需复杂解释，展现了支持公平投票的民主价值观。

Abstract: Voting methods are instrumental design element of democracies. Citizens use
them to express and aggregate their preferences to reach a collective decision.
However, voting outcomes can be as sensitive to voting rules as they are to
people's voting choices. Despite the significance and inter-disciplinary
scientific progress on voting methods, several democracies keep relying on
outdated voting methods that do not fit modern, pluralistic societies well,
while lacking social innovation. Here, we demonstrate how one can upgrade
real-world democracies, namely by using alternative preferential voting methods
such as cumulative voting and the method of equal shares designed for a
proportional representation of voters' preferences. By rigorously assessing a
new participatory budgeting approach applied in the city of Aarau, Switzerland,
we unravel the striking voting outcomes of fair voting methods: more winning
projects with the same budget and broader geographic and preference
representation of citizens by the elected projects, in particular for voters
who used to be under-represented, while promoting novel project ideas. We
provide profound causal evidence showing that citizens prefer proportional
voting methods, which possess strong legitimacy without the need of very
technical specialized explanations. We also reveal strong underlying democratic
values exhibited by citizens who support fair voting methods such as altruism
and compromise. These findings come with a global momentum to unleash a new and
long-awaited participation blueprint of how to upgrade democracies.

</details>


### [467] [Choosing a Model, Shaping a Future: Comparing LLM Perspectives on Sustainability and its Relationship with AI](https://arxiv.org/abs/2505.14435)
*Annika Bush,Meltem Aksoy,Markus Pauly,Greta Ontrup*

Main category: cs.CY

TL;DR: 研究发现不同大语言模型对可持续性和AI关系的理解存在显著差异，模型选择可能影响组织的可持续性策略。


<details>
  <summary>Details</summary>
Motivation: 随着组织越来越多地依赖AI系统进行可持续性决策支持，了解大语言模型中固有的偏见和观点变得至关重要。

Method: 研究对五种先进的大语言模型（Claude、DeepSeek、GPT、LLaMA和Mistral）进行了系统调查，每种模型各进行了100次经过验证的心理测量问卷调查。

Result: 结果显示模型之间存在显著差异：例如，GPT对AI与可持续性的兼容性持怀疑态度，而LLaMA表现出极端的技术乐观主义。模型在归因AI和可持续性整合的机构责任方面也存在分歧。

Conclusion: 研究结果表明，模型选择可能对组织的可持续性策略产生重大影响，强调了在部署大语言模型进行可持续性相关决策时需要意识到模型特定的偏见。

Abstract: As organizations increasingly rely on AI systems for decision support in
sustainability contexts, it becomes critical to understand the inherent biases
and perspectives embedded in Large Language Models (LLMs). This study
systematically investigates how five state-of-the-art LLMs -- Claude, DeepSeek,
GPT, LLaMA, and Mistral - conceptualize sustainability and its relationship
with AI. We administered validated, psychometric sustainability-related
questionnaires - each 100 times per model -- to capture response patterns and
variability. Our findings revealed significant inter-model differences: For
example, GPT exhibited skepticism about the compatibility of AI and
sustainability, whereas LLaMA demonstrated extreme techno-optimism with perfect
scores for several Sustainable Development Goals (SDGs). Models also diverged
in attributing institutional responsibility for AI and sustainability
integration, a results that holds implications for technology governance
approaches. Our results demonstrate that model selection could substantially
influence organizational sustainability strategies, highlighting the need for
awareness of model-specific biases when deploying LLMs for
sustainability-related decision-making.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [468] [Advancing Software Quality: A Standards-Focused Review of LLM-Based Assurance Techniques](https://arxiv.org/abs/2505.13766)
*Avinash Patil*

Main category: cs.SE

TL;DR: 本文探讨了如何利用大语言模型（LLM）增强软件质量保证（SQA）流程，同时确保符合国际标准，并分析了其应用、挑战及未来方向。


<details>
  <summary>Details</summary>
Motivation: 软件质量保证（SQA）对交付可靠、安全和高效的软件产品至关重要。随着大语言模型（LLM）的发展，为自动化SQA任务（如需求分析、代码审查等）提供了新机遇，同时需确保其符合ISO/IEC等国际标准。

Method: 论文首先回顾了软件质量标准和LLM的技术基础，接着探讨了LLM在SQA中的应用（如需求验证、缺陷检测等），并将这些应用映射到关键软件质量框架中，辅以实证案例和开源项目验证。

Result: 研究表明，LLM能够有效支持传统SQA流程，提升效率并满足标准要求，但也面临数据隐私、模型偏差等挑战，需加强治理和审计。

Conclusion: 未来应关注自适应学习、隐私保护部署、多模态分析及AI驱动的软件质量标准演进，以进一步优化LLM在SQA中的应用。

Abstract: Software Quality Assurance (SQA) is critical for delivering reliable, secure,
and efficient software products. The Software Quality Assurance Process aims to
provide assurance that work products and processes comply with predefined
provisions and plans. Recent advancements in Large Language Models (LLMs)
present new opportunities to enhance existing SQA processes by automating tasks
like requirement analysis, code review, test generation, and compliance checks.
Simultaneously, established standards such as ISO/IEC 12207, ISO/IEC 25010,
ISO/IEC 5055, ISO 9001/ISO/IEC 90003, CMMI, and TMM provide structured
frameworks for ensuring robust quality practices. This paper surveys the
intersection of LLM-based SQA methods and these recognized standards,
highlighting how AI-driven solutions can augment traditional approaches while
maintaining compliance and process maturity. We first review the foundational
software quality standards and the technical fundamentals of LLMs in software
engineering. Next, we explore various LLM-based SQA applications, including
requirement validation, defect detection, test generation, and documentation
maintenance. We then map these applications to key software quality frameworks,
illustrating how LLMs can address specific requirements and metrics within each
standard. Empirical case studies and open-source initiatives demonstrate the
practical viability of these methods. At the same time, discussions on
challenges (e.g., data privacy, model bias, explainability) underscore the need
for deliberate governance and auditing. Finally, we propose future directions
encompassing adaptive learning, privacy-focused deployments, multimodal
analysis, and evolving standards for AI-driven software quality.

</details>


### [469] [Guided Search Strategies in Non-Serializable Environments with Applications to Software Engineering Agents](https://arxiv.org/abs/2505.13652)
*Karina Zainullina,Alexander Golubev,Maria Trofimova,Sergei Polezhaev,Ibragim Badertdinov,Daria Litvintseva,Simon Karasik,Filipp Fisin,Sergei Skvortsov,Maksim Nekrashevich,Anton Shevtsov,Boris Yangel*

Main category: cs.SE

TL;DR: 论文提出两种搜索策略（1步前瞻和轨迹选择）提升大模型在不可序列化环境中的性能，在SWE-bench基准上将Qwen-72B成功率翻倍至40.8%，并验证策略可迁移至GPT-4o。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在多步任务中表现不稳定，传统搜索方法（如MCTS）无法适用于不可序列化环境（如Docker容器），需开发新搜索策略提升性能一致性。

Method: 采用两种搜索策略：1）1-step lookahead（单步前瞻）；2）trajectory selection（轨迹选择），均通过学习的动作价值函数估计器引导。

Result: 在SWE-bench基准上，Qwen-72B模型平均成功率提升至40.8%（原方法翻倍），创开源模型新纪录；方法可迁移至GPT-4o带来类似提升。

Conclusion: 针对不可序列化环境设计的搜索策略显著提升大模型性能，且具备跨模型迁移能力，为复杂任务中的稳定性提供了有效解决方案。

Abstract: Large language models (LLMs) have recently achieved remarkable results in
complex multi-step tasks, such as mathematical reasoning and agentic software
engineering. However, they often struggle to maintain consistent performance
across multiple solution attempts. One effective approach to narrow the gap
between average-case and best-case performance is guided test-time search,
which explores multiple solution paths to identify the most promising one.
Unfortunately, effective search techniques (e.g. MCTS) are often unsuitable for
non-serializable RL environments, such as Docker containers, where intermediate
environment states cannot be easily saved and restored. We investigate two
complementary search strategies applicable to such environments: 1-step
lookahead and trajectory selection, both guided by a learned action-value
function estimator. On the SWE-bench Verified benchmark, a key testbed for
agentic software engineering, we find these methods to double the average
success rate of a fine-tuned Qwen-72B model, achieving 40.8%, the new
state-of-the-art for open-weights models. Additionally, we show that these
techniques are transferable to more advanced closed models, yielding similar
improvements with GPT-4o.

</details>


### [470] [Selective Code Generation for Functional Guarantees](https://arxiv.org/abs/2505.13553)
*Jaewoo Jeong,Taesoo Kim,Sangdon Park*

Main category: cs.SE

TL;DR: 论文提出了一种通过动态代码分析工具自动生成单元测试的方法，以解决代码生成模型的幻觉问题，并提出了选择性代码生成器来控制幻觉率。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）及其代码生成模型在复杂任务中表现出色，但幻觉问题限制了其在需要高安全标准的系统中的应用。代码的复杂性使得判断生成代码的功能正确性变得困难，而传统的单元测试方法扩展成本高昂。

Method: 论文提出使用动态代码分析工具自动生成单元测试，利用代码的可执行性来评估生成代码的功能正确性。此外，还提出了一种选择性代码生成器，通过学习避免对不确定的生成进行回答，从而控制非拒绝回答中的代码幻觉率。

Result: 实验表明，选择性代码生成器在开放和封闭代码生成器上均表现出色，能够有效控制代码幻觉率，并显示出合理的选择效率。

Conclusion: 通过自动生成单元测试和选择性代码生成器，论文提供了一种可信赖的代码生成方法，并提出了名为FuzzEval的评估范式，为代码生成模型的可靠性提供了保障。

Abstract: Large language models (LLMs) show human-level performance and their
specialized descendants, code generation models, play core roles in solving
complex tasks, including mathematical reasoning and software development. On
the downside, the hallucination of LLMs mainly hinders their applicability to
systems requiring higher safety standards, thus drawing the attention of the AI
community. However, the hallucination of code generation models is rarely
considered. One critical bottleneck in considering code hallucination is the
intricate property of code to identify whether generated code has the intended
functionality due to its un-natural form, different to natural languages.
Handful of unit tests have been considered to address this issue, but
scaling-up its size is extremely expensive. We address this core bottleneck by
automatically generating unit tests using dynamic code analysis tools, which
leverages the \emph{executable nature} of code. Given generated unit tests from
true code for measuring functional correctness of generated code, we propose to
learn a \emph{selective code generator}, which abstains from answering for
unsure generation, to control the rate of code hallucination among
non-abstaining answers in terms of a false discovery rate. This learning
algorithm provides a controllability guarantee, providing trustworthiness of
code generation. Finally, we propose to use generated unit tests in evaluation
as well as in learning for precise code evaluation, calling this evaluation
paradigm \emph{FuzzEval}. We demonstrate the efficacy of our selective code
generator over open and closed code generators, showing clear benefit of
leveraging generated unit tests along with the controllability of code
hallucination and reasonable selection efficiency via our selective code
generator.

</details>


### [471] [HarmonE: A Self-Adaptive Approach to Architecting Sustainable MLOps](https://arxiv.org/abs/2505.13693)
*Hiya Bhatt,Shaunak Biswas,Srinivasan Rakhunathan,Karthik Vaidhyanathan*

Main category: cs.SE

TL;DR: 论文提出HarmonE架构，通过MAPE-K循环增强MLOps管道的自适应能力，以应对机器学习系统在多维可持续性（技术、经济、环境、社会）上的挑战，并在智能交通系统的数字孪生中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 机器学习系统在动态环境中运行时面临数据漂移和模型退化等不确定性，传统MLOps仅解决技术维度问题，频繁重新训练带来高能耗，亟需兼顾多维度可持续性的解决方案。

Method: 提出HarmonE架构，允许设计时定义可持续目标与适应阈值，运行时监测精度、能耗等指标，通过MAPE-K循环触发自适应策略。

Result: 在交通流预测的数字孪生实验中，HarmonE能在保持精度的同时动态适应环境变化，满足可持续目标。

Conclusion: HarmonE为ML系统提供了兼顾性能与多维度可持续性的自适应框架，实验证明其在动态环境中的有效性。

Abstract: Machine Learning Enabled Systems (MLS) are becoming integral to real-world
applications, but ensuring their sustainable performance over time remains a
significant challenge. These systems operate in dynamic environments and face
runtime uncertainties like data drift and model degradation, which affect the
sustainability of MLS across multiple dimensions: technical, economical,
environmental, and social. While Machine Learning Operations (MLOps) addresses
the technical dimension by streamlining the ML model lifecycle, it overlooks
other dimensions. Furthermore, some traditional practices, such as frequent
retraining, incur substantial energy and computational overhead, thus
amplifying sustainability concerns. To address them, we introduce HarmonE, an
architectural approach that enables self-adaptive capabilities in MLOps
pipelines using the MAPE-K loop. HarmonE allows system architects to define
explicit sustainability goals and adaptation thresholds at design time, and
performs runtime monitoring of key metrics, such as prediction accuracy, energy
consumption, and data distribution shifts, to trigger appropriate adaptation
strategies. We validate our approach using a Digital Twin (DT) of an
Intelligent Transportation System (ITS), focusing on traffic flow prediction as
our primary use case. The DT employs time series ML models to simulate
real-time traffic and assess various flow scenarios. Our results show that
HarmonE adapts effectively to evolving conditions while maintaining accuracy
and meeting sustainability goals.

</details>


<div id='q-bio.GN'></div>

# q-bio.GN [[Back]](#toc)

### [472] [OmniGenBench: A Modular Platform for Reproducible Genomic Foundation Models Benchmarking](https://arxiv.org/abs/2505.14402)
*Heng Yang,Jack Cole,Yuan Li,Renzhi Chen,Geyong Min,Ke Li*

Main category: q-bio.GN

TL;DR: OmniGenBench是一个模块化基准测试平台，旨在统一基因组基础模型的数据、模型、基准测试和可解释性层，解决可重复性挑战。


<details>
  <summary>Details</summary>
Motivation: 随着基因组基础模型（GFMs）的兴起，基因组学领域亟需一个严格且可重复的评估方法，以应对数据透明度、模型互操作性等问题。

Method: 开发了OmniGenBench平台，通过自动化流程和社区可扩展功能，标准化评估GFMs，并整合了31个开源模型。

Result: OmniGenBench能够通过单一命令在五个基准测试套件中评估任何GFM，解决了可重复性挑战。

Conclusion: OmniGenBench旨在作为可重复基因组AI研究的基础设施，加速基因组规模建模时代的可信发现和协作创新。

Abstract: The code of nature, embedded in DNA and RNA genomes since the origin of life,
holds immense potential to impact both humans and ecosystems through genome
modeling. Genomic Foundation Models (GFMs) have emerged as a transformative
approach to decoding the genome. As GFMs scale up and reshape the landscape of
AI-driven genomics, the field faces an urgent need for rigorous and
reproducible evaluation. We present OmniGenBench, a modular benchmarking
platform designed to unify the data, model, benchmarking, and interpretability
layers across GFMs. OmniGenBench enables standardized, one-command evaluation
of any GFM across five benchmark suites, with seamless integration of over 31
open-source models. Through automated pipelines and community-extensible
features, the platform addresses critical reproducibility challenges, including
data transparency, model interoperability, benchmark fragmentation, and
black-box interpretability. OmniGenBench aims to serve as foundational
infrastructure for reproducible genomic AI research, accelerating trustworthy
discovery and collaborative innovation in the era of genome-scale modeling.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [473] [Trustworthy Reputation Games and Applications to Proof-of-Reputation Blockchains](https://arxiv.org/abs/2505.14551)
*Petros Drineas,Rohit Nema,Rafail Ostrovsky,Vassilis Zikas*

Main category: cs.GT

TL;DR: 本文提出了一种新的可信声誉系统模型，通过博弈论方法确保用户真实报告对服务器可信度的评估，并应用于PoR区块链。


<details>
  <summary>Details</summary>
Motivation: 在互联网时代，声誉系统帮助人们决定信任对象，但现有去中心化账本中的声誉系统易受操纵，缺乏经济稳健性的博弈论支持。

Method: 提出“可信声誉博弈”模型，要求用户按真实信念报告服务器可信度，并设计满足最佳响应和可估计性的效用函数与解码方法。

Result: 该模型确保理性用户遵循真实策略，且观察者可据此估计服务器的相对可信度，同时与PageRank算法建立关联。

Conclusion: 可信声誉博弈为PoR区块链提供了理论基础，兼具独立研究价值与应用潜力。

Abstract: Reputation systems play an essential role in the Internet era, as they enable
people to decide whom to trust, by collecting and aggregating data about users'
behavior. Recently, several works proposed the use of reputation for the design
and scalability improvement of decentralized (blockchain) ledgers; however,
such systems are prone to manipulation and to our knowledge no game-theoretic
treatment exists that can support their economic robustness.
  In this work we put forth a new model for the design of what we call, {\em
trustworthy reputation systems}. Concretely, we describe a class of games,
which we term {\em trustworthy reputation games}, that enable a set of users to
report a function of their beliefs about the trustworthiness of each server in
a set -- i.e., their estimate of the probability that this server will behave
according to its specified strategy -- in a way that satisfies the following
properties:
  1. It is $(\epsilon$-)best response for any rational user in the game to play
a prescribed (truthful) strategy according to their true belief.
  2. Assuming that the users' beliefs are not too far from the {\em true}
trustworthiness of the servers, playing the above ($\epsilon-$)Nash equilibrium
allows anyone who observes the users' strategies to estimate the relative
trustworthiness of any two servers.
  Our utilities and decoding function build on a connection between the well
known PageRank algorithm and the problem of trustworthiness discovery, which
can be of independent interest. Finally, we show how the above games are
motivated by and can be leveraged in proof-of-reputation (PoR) blockchains.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [474] [Sobolev Gradient Ascent for Optimal Transport: Barycenter Optimization and Convergence Analysis](https://arxiv.org/abs/2505.13660)
*Kaheon Kim,Bohan Zhou,Changbo Zhu,Xiaohui Chen*

Main category: math.OC

TL;DR: 本文提出了一种新的无约束凹对偶公式来计算Wasserstein重心，并基于Sobolev几何设计了高效的SGA算法，显著简化了计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 现有计算Wasserstein重心的对偶方法需要复杂的c-凹投影操作，导致计算效率低下。本文旨在通过无约束对偶公式和Sobolev几何优化，简化计算流程并提升效率。

Method: 提出基于Sobolev几何的梯度上升算法（SGA），避免了传统方法中计算成本高的c-凹投影操作，从而简化了计算过程。

Result: SGA算法在理论上达到了与欧氏空间中非光滑凸函数优化相同的收敛速率，实验表明其性能优于现有最优传输重心求解器。

Conclusion: SGA算法通过简化计算步骤和理论分析，显著提升了Wasserstein重心的计算效率，为相关领域提供了更优的解决方案。

Abstract: This paper introduces a new constraint-free concave dual formulation for the
Wasserstein barycenter. Tailoring the vanilla dual gradient ascent algorithm to
the Sobolev geometry, we derive a scalable Sobolev gradient ascent (SGA)
algorithm to compute the barycenter for input distributions supported on a
regular grid. Despite the algorithmic simplicity, we provide a global
convergence analysis that achieves the same rate as the classical subgradient
descent methods for minimizing nonsmooth convex functions in the Euclidean
space. A central feature of our SGA algorithm is that the computationally
expensive $c$-concavity projection operator enforced on the Kantorovich dual
potentials is unnecessary to guarantee convergence, leading to significant
algorithmic and theoretical simplifications over all existing primal and dual
methods for computing the exact barycenter. Our numerical experiments
demonstrate the superior empirical performance of SGA over the existing optimal
transport barycenter solvers.

</details>


### [475] [Sequential QCQP for Bilevel Optimization with Line Search](https://arxiv.org/abs/2505.14647)
*Sina Sharifi,Erfan Yazdandoost Hamedani,Mahyar Fazlyab*

Main category: math.OC

TL;DR: 提出一种无需调参的单循环双层优化算法，保证任意时间可行性和上层目标下降，具有O(1/k)收敛速度。


<details>
  <summary>Details</summary>
Motivation: 解决双层优化中层级间复杂依赖关系，传统方法需调参且难以保证实时可行性。

Method: 通过闭式解QCQP确定搜索方向，结合控制屏障函数回溯线搜索确保步长安全。

Result: 算法可扩展性强，在局部正则条件下收敛，并在典型任务中验证有效性。

Conclusion: 该调优自由算法为双层优化提供了高效可靠的解决方案。

Abstract: Bilevel optimization involves a hierarchical structure where one problem is
nested within another, leading to complex interdependencies between levels. We
propose a single-loop, tuning-free algorithm that guarantees anytime
feasibility, i.e., approximate satisfaction of the lower-level optimality
condition, while ensuring descent of the upper-level objective. At each
iteration, a convex quadratically-constrained quadratic program (QCQP) with a
closed-form solution yields the search direction, followed by a backtracking
line search inspired by control barrier functions to ensure safe, uniformly
positive step sizes. The resulting method is scalable, requires no
hyperparameter tuning, and converges under mild local regularity assumptions.
We establish an O(1/k) ergodic convergence rate and demonstrate the algorithm's
effectiveness on representative bilevel tasks.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [476] [VocalAgent: Large Language Models for Vocal Health Diagnostics with Safety-Aware Evaluation](https://arxiv.org/abs/2505.13577)
*Yubin Kim,Taehan Kim,Wonjune Kang,Eugene Park,Joonsik Yoon,Dongjae Lee,Xin Liu,Daniel McDuff,Hyeonhoon Lee,Cynthia Breazeal,Hae Won Park*

Main category: cs.SD

TL;DR: 论文提出VocalAgent，一个基于音频大语言模型的工具，用于便捷诊断嗓音健康问题，并在多语言和安全性评估中表现优异。


<details>
  <summary>Details</summary>
Motivation: 嗓音健康对人们的交流至关重要，但全球范围内嗓音障碍诊断和治疗的可及性不足，需要一种便捷的解决方案。

Method: 利用Qwen-Audio-Chat模型，基于医院患者的三组数据集进行微调，并通过多维度评估框架验证模型性能。

Result: VocalAgent在嗓音障碍分类上准确率优于现有基准方法，并展示了跨语言和安全性方面的优势。

Conclusion: VocalAgent为嗓音健康诊断提供了可扩展的解决方案，同时强调了伦理和技术验证的重要性。

Abstract: Vocal health plays a crucial role in peoples' lives, significantly impacting
their communicative abilities and interactions. However, despite the global
prevalence of voice disorders, many lack access to convenient diagnosis and
treatment. This paper introduces VocalAgent, an audio large language model
(LLM) to address these challenges through vocal health diagnosis. We leverage
Qwen-Audio-Chat fine-tuned on three datasets collected in-situ from hospital
patients, and present a multifaceted evaluation framework encompassing a safety
assessment to mitigate diagnostic biases, cross-lingual performance analysis,
and modality ablation studies. VocalAgent demonstrates superior accuracy on
voice disorder classification compared to state-of-the-art baselines. Its
LLM-based method offers a scalable solution for broader adoption of health
diagnostics, while underscoring the importance of ethical and technical
validation.

</details>


### [477] [ClapFM-EVC: High-Fidelity and Flexible Emotional Voice Conversion with Dual Control from Natural Language and Speech](https://arxiv.org/abs/2505.13805)
*Yu Pan,Yanni Hu,Yuguang Yang,Jixun Yao,Jianhao Ye,Hongbin Zhou,Lei Ma,Jianjun Zhao*

Main category: cs.SD

TL;DR: ClapFM-EVC提出了一种新型情感语音转换框架，通过自然语言提示或参考语音驱动，实现高质量语音转换，并支持情感强度调节。


<details>
  <summary>Details</summary>
Motivation: 当前情感语音转换（EVC）在实现高保真、灵活且可解释的控制方面仍面临挑战。

Method: 提出EVC-CLAP模型进行情感对比语言-音频预训练，结合FuEncoder和自适应强度门融合情感特征，使用流匹配模型重建梅尔频谱。

Result: 主观和客观评估验证了ClapFM-EVC的有效性。

Conclusion: ClapFM-EVC能够生成高质量的情感转换语音，并支持灵活的情感控制。

Abstract: Despite great advances, achieving high-fidelity emotional voice conversion
(EVC) with flexible and interpretable control remains challenging. This paper
introduces ClapFM-EVC, a novel EVC framework capable of generating high-quality
converted speech driven by natural language prompts or reference speech with
adjustable emotion intensity. We first propose EVC-CLAP, an emotional
contrastive language-audio pre-training model, guided by natural language
prompts and categorical labels, to extract and align fine-grained emotional
elements across speech and text modalities. Then, a FuEncoder with an adaptive
intensity gate is presented to seamless fuse emotional features with Phonetic
PosteriorGrams from a pre-trained ASR model. To further improve emotion
expressiveness and speech naturalness, we propose a flow matching model
conditioned on these captured features to reconstruct Mel-spectrogram of source
speech. Subjective and objective evaluations validate the effectiveness of
ClapFM-EVC.

</details>


### [478] [Forensic deepfake audio detection using segmental speech features](https://arxiv.org/abs/2505.13847)
*Tianle Yang,Chengzhe Sun,Siwei Lyu,Phil Rose*

Main category: cs.SD

TL;DR: 该研究探讨了利用语音片段的声学特征检测深度伪造音频的潜力，发现某些片段特征在识别伪造音频中有效，而全局特征价值较低。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于利用与人类发音过程密切相关的声学特征，这些特征具有高度可解释性且难以被深度伪造模型复制，从而提升伪造音频检测的准确性。

Method: 方法是通过分析语音片段的声学特征，特别是常用于司法语音比对的特征，来检测深度伪造音频。

Result: 结果表明，某些片段特征能有效识别伪造音频，而全局特征作用有限。

Conclusion: 结论指出，司法语音比对中的音频伪造检测需采用不同方法，并强调了片段特征在此领域的应用潜力。

Abstract: This study explores the potential of using acoustic features of segmental
speech sounds to detect deepfake audio. These features are highly interpretable
because of their close relationship with human articulatory processes and are
expected to be more difficult for deepfake models to replicate. The results
demonstrate that certain segmental features commonly used in forensic voice
comparison are effective in identifying deep-fakes, whereas some global
features provide little value. These findings underscore the need to approach
audio deepfake detection differently for forensic voice comparison and offer a
new perspective on leveraging segmental features for this purpose.

</details>


### [479] [The Multimodal Information Based Speech Processing (MISP) 2025 Challenge: Audio-Visual Diarization and Recognition](https://arxiv.org/abs/2505.13971)
*Ming Gao,Shilong Wu,Hang Chen,Jun Du,Chin-Hui Lee,Shinji Watanabe,Jingdong Chen,Siniscalchi Sabato Marco,Odette Scharenborg*

Main category: cs.SD

TL;DR: MISP 2025挑战赛聚焦多模态会议转录，结合视频提升音频处理性能，最佳系统在说话人日志、语音识别等任务上显著超越基线。


<details>
  <summary>Details</summary>
Motivation: 会议场景因复杂声学条件对语音应用构成挑战，需通过多模态（音频+视频）提升转录准确性。

Method: 设立三项任务（AVSD/AVSR/AVDR），提供数据集和基线系统，汇总参赛者的多模态解决方案。

Result: 最优系统性能提升显著：AVSD错误率降低7.43%，AVSR字符错误率降低10.62%，AVDR综合错误率降低72.49%。

Conclusion: 多模态方法能有效改善会议转录任务，视频模态的引入带来突破性进展。

Abstract: Meetings are a valuable yet challenging scenario for speech applications due
to complex acoustic conditions. This paper summarizes the outcomes of the MISP
2025 Challenge, hosted at Interspeech 2025, which focuses on multi-modal,
multi-device meeting transcription by incorporating video modality alongside
audio. The tasks include Audio-Visual Speaker Diarization (AVSD), Audio-Visual
Speech Recognition (AVSR), and Audio-Visual Diarization and Recognition (AVDR).
We present the challenge's objectives, tasks, dataset, baseline systems, and
solutions proposed by participants. The best-performing systems achieved
significant improvements over the baseline: the top AVSD model achieved a
Diarization Error Rate (DER) of 8.09%, improving by 7.43%; the top AVSR system
achieved a Character Error Rate (CER) of 9.48%, improving by 10.62%; and the
best AVDR system achieved a concatenated minimum-permutation Character Error
Rate (cpCER) of 11.56%, improving by 72.49%.

</details>


### [480] [FMSD-TTS: Few-shot Multi-Speaker Multi-Dialect Text-to-Speech Synthesis for Ü-Tsang, Amdo and Kham Speech Dataset Generation](https://arxiv.org/abs/2505.14351)
*Yutong Liu,Ziyue Zhang,Ban Ma-bao,Yuqing Cai,Yongbin Yu,Renzeng Duojie,Xiangxiang Wang,Fan Gao,Cheng Huang,Nyima Tashi*

Main category: cs.SD

TL;DR: 提出FMSD-TTS框架，解决藏语多方言语音合成资源匮乏问题，通过少量参考音频和方言标签实现高质量合成。


<details>
  <summary>Details</summary>
Motivation: 藏语作为低资源语言，三大方言（卫藏、安多、康巴）缺乏平行语音语料库，限制了语音建模进展。

Method: 采用说话人-方言融合模块和方言专用动态路由网络（DSDR-Net），捕捉方言间细粒度声学差异并保持说话人身份。

Result: 主客观评估显示，FMSD-TTS在方言表现力和说话人相似度上显著优于基线，并构建了大规模合成语音库及开源评测工具包。

Conclusion: FMSD-TTS为藏语多方言语音合成提供了高效解决方案，公开语料和工具推动相关研究标准化。

Abstract: Tibetan is a low-resource language with minimal parallel speech corpora
spanning its three major dialects-\"U-Tsang, Amdo, and Kham-limiting progress
in speech modeling. To address this issue, we propose FMSD-TTS, a few-shot,
multi-speaker, multi-dialect text-to-speech framework that synthesizes parallel
dialectal speech from limited reference audio and explicit dialect labels. Our
method features a novel speaker-dialect fusion module and a Dialect-Specialized
Dynamic Routing Network (DSDR-Net) to capture fine-grained acoustic and
linguistic variations across dialects while preserving speaker identity.
Extensive objective and subjective evaluations demonstrate that FMSD-TTS
significantly outperforms baselines in both dialectal expressiveness and
speaker similarity. We further validate the quality and utility of the
synthesized speech through a challenging speech-to-speech dialect conversion
task. Our contributions include: (1) a novel few-shot TTS system tailored for
Tibetan multi-dialect speech synthesis, (2) the public release of a large-scale
synthetic Tibetan speech corpus generated by FMSD-TTS, and (3) an open-source
evaluation toolkit for standardized assessment of speaker similarity, dialect
consistency, and audio quality.

</details>


### [481] [PersonaTAB: Predicting Personality Traits using Textual, Acoustic, and Behavioral Cues in Fully-Duplex Speech Dialogs](https://arxiv.org/abs/2505.14356)
*Sho Inoue,Shai Wang,Haizhou Li*

Main category: cs.SD

TL;DR: 提出了一种处理原始音频数据并标注人格特征的对话系统，利用ASR和LLM预测对话人格，比现有方法更符合人类判断。


<details>
  <summary>Details</summary>
Motivation: 当前神经对话系统在人格感知方面进展有限，主要因为语音数据缺乏人格标注。

Method: 通过ASR提取文本和时间戳，生成对话级标注，结合LLM预测人格特征，并由人工评估验证。

Result: 系统生成的人格标注与人类评估结果一致性优于现有方法。

Conclusion: 该流程有效解决了语音数据人格标注缺失问题，提升了对话系统的人格适应能力。

Abstract: Despite significant progress in neural spoken dialog systems,
personality-aware conversation agents -- capable of adapting behavior based on
personalities -- remain underexplored due to the absence of personality
annotations in speech datasets. We propose a pipeline that preprocesses raw
audio recordings to create a dialogue dataset annotated with timestamps,
response types, and emotion/sentiment labels. We employ an automatic speech
recognition (ASR) system to extract transcripts and timestamps, then generate
conversation-level annotations. Leveraging these annotations, we design a
system that employs large language models to predict conversational
personality. Human evaluators were engaged to identify conversational
characteristics and assign personality labels. Our analysis demonstrates that
the proposed system achieves stronger alignment with human judgments compared
to existing approaches.

</details>


### [482] [S2SBench: A Benchmark for Quantifying Intelligence Degradation in Speech-to-Speech Large Language Models](https://arxiv.org/abs/2505.14438)
*Yuanbo Fang,Haoze Sun,Jun Liu,Tao Zhang,Zenan Zhou,Weipeng Chen,Xiaofen Xing,Xiangmin Xu*

Main category: cs.SD

TL;DR: 该论文提出了S2SBench基准，用于量化语音大语言模型在处理音频输入时的性能下降问题，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 语音大语言模型在直接处理音频输入时，相比文本输入会出现推理和生成性能下降的问题，称为智能退化。为了系统评估这一差距，需要专门的基准工具。

Method: 提出了S2SBench基准，包含针对音频输入的句子延续和常识推理的诊断数据集，并引入了基于困惑度差异的成对评估协议。

Result: 通过应用于Baichuan-Audio的训练过程分析，验证了S2SBench的有效性。

Conclusion: S2SBench能够有效量化语音大语言模型的性能退化问题，为相关研究提供了评估工具。

Abstract: End-to-end speech large language models ((LLMs)) extend the capabilities of
text-based models to directly process and generate audio tokens. However, this
often leads to a decline in reasoning and generation performance compared to
text input, a phenomenon referred to as intelligence degradation. To
systematically evaluate this gap, we propose S2SBench, a benchmark designed to
quantify performance degradation in Speech LLMs. It includes diagnostic
datasets targeting sentence continuation and commonsense reasoning under audio
input. We further introduce a pairwise evaluation protocol based on perplexity
differences between plausible and implausible samples to measure degradation
relative to text input. We apply S2SBench to analyze the training process of
Baichuan-Audio, which further demonstrates the benchmark's effectiveness. All
datasets and evaluation code are available at
https://github.com/undobug/S2SBench.

</details>


### [483] [PAST: Phonetic-Acoustic Speech Tokenizer](https://arxiv.org/abs/2505.14470)
*Nadav Har-Tuv,Or Tal,Yossi Adi*

Main category: cs.SD

TL;DR: PAST是一个新颖的端到端框架，联合建模语音信息和信号重建，无需依赖外部预训练模型，并在实时语音应用中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖预训练的自监督模型，PAST旨在通过监督语音数据直接整合领域知识，提升语音表示和重建的效果。

Method: PAST通过辅助任务将语音知识直接集成到标记化过程中，并提出了可流式处理的因果变体，支持实时应用。

Result: PAST在语音表示和重建指标上超越现有基线，作为语音语言模型的表示也表现优异。

Conclusion: PAST是一个高效的语音表示基础框架，适用于语音生成任务，并已开源促进进一步研究。

Abstract: We present PAST, a novel end-to-end framework that jointly models phonetic
information alongside signal reconstruction, eliminating the need for external
pretrained models. Unlike previous approaches that rely on pretrained
self-supervised models, PAST employs supervised phonetic data, directly
integrating domain knowledge into the tokenization process via auxiliary tasks.
Additionally, we introduce a streamable, causal variant of PAST, enabling
real-time speech applications. Results demonstrate that PAST surpasses existing
evaluated baseline tokenizers across common evaluation metrics, including
phonetic representation and speech reconstruction. Notably, PAST also achieves
superior performance when serving as a speech representation for speech
language models, further highlighting its effectiveness as a foundation for
spoken language generation. To foster further research, we release the full
implementation. For code, model checkpoints, and samples see:
https://pages.cs.huji.ac.il/adiyoss-lab/PAST

</details>


### [484] [Score-Based Training for Energy-Based TTS Models](https://arxiv.org/abs/2505.13771)
*Wanli Sun,Anton Ragni*

Main category: cs.SD

TL;DR: 论文提出了一种新准则，用于训练更适合一阶优化方案的分数，对比了噪声对比估计(NCE)和切片分数匹配(SSM)在训练能量模型(EBM)中的表现。


<details>
  <summary>Details</summary>
Motivation: NCE和SSM方法在训练能量模型时忽视了对数似然函数的形式，而能量模型和扩散模型在推理时使用一阶优化，这可能导致问题。因此，需要一种新方法来学习更适合一阶优化方案的分数。

Method: 论文提出了一种新准则，通过学习更适合一阶优化方案的分数，改进了NCE和SSM方法在训练能量模型时的表现。

Result: 实验结果表明，新方法在训练能量模型时表现优于NCE和SSM方法。

Conclusion: 论文提出的新准则能够有效学习更适合一阶优化方案的分数，为训练能量模型提供了更好的方法。

Abstract: Noise contrastive estimation (NCE) is a popular method for training
energy-based models (EBM) with intractable normalisation terms. The key idea of
NCE is to learn by comparing unnormalised log-likelihoods of the reference and
noisy samples, thus avoiding explicitly computing normalisation terms. However,
NCE critically relies on the quality of noisy samples. Recently, sliced score
matching (SSM) has been popularised by closely related diffusion models (DM).
Unlike NCE, SSM learns a gradient of log-likelihood, or score, by learning
distribution of its projections on randomly chosen directions. However, both
NCE and SSM disregard the form of log-likelihood function, which is problematic
given that EBMs and DMs make use of first-order optimisation during inference.
This paper proposes a new criterion that learns scores more suitable for
first-order schemes. Experiments contrasts these approaches for training EBMs.

</details>


### [485] [AquaSignal: An Integrated Framework for Robust Underwater Acoustic Analysis](https://arxiv.org/abs/2505.14285)
*Eirini Panteli,Paulo E. Santos,Nabil Humphrey*

Main category: cs.SD

TL;DR: AquaSignal是一个模块化、可扩展的水下声学信号处理管道，结合深度学习技术提升信号分析的准确性和可靠性，在真实海洋环境中表现出色。


<details>
  <summary>Details</summary>
Motivation: 水下声学信号分析在嘈杂多变的海洋环境中面临挑战，需要一种高效、可靠的方法来处理和分类声学信号，并检测异常信号。

Method: AquaSignal采用U-Net去噪、ResNet18分类和AutoEncoder异常检测，结合Deepship和ONC数据集进行验证。

Result: 系统在分类任务中达到71%准确率，异常检测准确率为91%，信号清晰度和任务性能显著提升。

Conclusion: AquaSignal展示了在科学、环境和海事领域实时水下声学监测的强大潜力，尽管分类性能略低于某些先进模型。

Abstract: This paper presents AquaSignal, a modular and scalable pipeline for
preprocessing, denoising, classification, and novelty detection of underwater
acoustic signals. Designed to operate effectively in noisy and dynamic marine
environments, AquaSignal integrates state-of-the-art deep learning
architectures to enhance the reliability and accuracy of acoustic signal
analysis. The system is evaluated on a combined dataset from the Deepship and
Ocean Networks Canada (ONC) benchmarks, providing a diverse set of real-world
underwater scenarios. AquaSignal employs a U-Net architecture for denoising, a
ResNet18 convolutional neural network for classifying known acoustic events,
and an AutoEncoder-based model for unsupervised detection of novel or anomalous
signals. To our knowledge, this is the first comprehensive study to apply and
evaluate this combination of techniques on maritime vessel acoustic data.
Experimental results show that AquaSignal improves signal clarity and task
performance, achieving 71% classification accuracy and 91% accuracy in novelty
detection. Despite slightly lower classification performance compared to some
state-of-the-art models, differences in data partitioning strategies limit
direct comparisons. Overall, AquaSignal demonstrates strong potential for
real-time underwater acoustic monitoring in scientific, environmental, and
maritime domains.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [486] [Q${}^2$Forge: Minting Competency Questions and SPARQL Queries for Question-Answering Over Knowledge Graphs](https://arxiv.org/abs/2505.13572)
*Yousouf Taghzouti,Franck Michel,Tao Jiang,Louis-Félix Nothias,Fabien Gandon*

Main category: cs.DB

TL;DR: Q²Forge是一个开源工具，通过迭代验证和LLM辅助生成知识图谱的SPARQL查询与能力问题，支持创建参考查询集。


<details>
  <summary>Details</summary>
Motivation: SPARQL查询对非专家用户具有挑战性，且知识图谱的示例查询通常有限。需要一种方法自动生成高质量的查询与问题对。

Method: Q²Forge采用模块化设计，结合能力问题生成、查询生成和迭代验证（含人类反馈和LLM评估）的流程。

Result: 该工具实现了从自然语言问题到SPARQL查询的完整流水线，可扩展适用于任意知识图谱。

Conclusion: Q²Forge为知识图谱提供了可定制化的查询生成解决方案，缓解了人工编写示例查询的瓶颈问题。

Abstract: The SPARQL query language is the standard method to access knowledge graphs
(KGs). However, formulating SPARQL queries is a significant challenge for
non-expert users, and remains time-consuming for the experienced ones. Best
practices recommend to document KGs with competency questions and example
queries to contextualise the knowledge they contain and illustrate their
potential applications. In practice, however, this is either not the case or
the examples are provided in limited numbers. Large Language Models (LLMs) are
being used in conversational agents and are proving to be an attractive
solution with a wide range of applications, from simple question-answering
about common knowledge to generating code in a targeted programming language.
However, training and testing these models to produce high quality SPARQL
queries from natural language questions requires substantial datasets of
question-query pairs. In this paper, we present Q${}^2$Forge that addresses the
challenge of generating new competency questions for a KG and corresponding
SPARQL queries. It iteratively validates those queries with human feedback and
LLM as a judge. Q${}^2$Forge is open source, generic, extensible and modular,
meaning that the different modules of the application (CQ generation, query
generation and query refinement) can be used separately, as an integrated
pipeline, or replaced by alternative services. The result is a complete
pipeline from competency question formulation to query evaluation, supporting
the creation of reference query sets for any target KG.

</details>


### [487] [Abacus: A Cost-Based Optimizer for Semantic Operator Systems](https://arxiv.org/abs/2505.14661)
*Matthew Russo,Sivaprasad Sudhir,Gerardo Vitagliano,Chunwei Liu,Tim Kraska,Samuel Madden,Michael Cafarella*

Main category: cs.DB

TL;DR: Abacus是一个可扩展的、基于成本的优化器，用于优化由语义操作符组成的LLM数据处理系统，提升质量、降低成本与延迟。


<details>
  <summary>Details</summary>
Motivation: 现有语义操作符系统虽在基准测试中表现良好，但优化困难，缺乏能全局优化质量、成本或延迟的约束优化器。

Method: Abacus通过少量验证样例和先验性能评估，搜索语义操作符系统的最佳实现方案，支持多目标约束优化。

Result: 在生物医学、法律文档处理及多模态问答任务中，Abacus优化后的系统质量提升18.7%-39.2%，成本降低23.6倍，延迟减少4.2倍。

Conclusion: Abacus显著提升了LLM数据处理系统的综合性能，为复杂约束下的语义操作符优化提供了有效解决方案。

Abstract: LLMs enable an exciting new class of data processing applications over large
collections of unstructured documents. Several new programming frameworks have
enabled developers to build these applications by composing them out of
semantic operators: a declarative set of AI-powered data transformations with
natural language specifications. These include LLM-powered maps, filters,
joins, etc. used for document processing tasks such as information extraction,
summarization, and more. While systems of semantic operators have achieved
strong performance on benchmarks, they can be difficult to optimize. An
optimizer for this setting must determine how to physically implement each
semantic operator in a way that optimizes the system globally. Existing
optimizers are limited in the number of optimizations they can apply, and most
(if not all) cannot optimize system quality, cost, or latency subject to
constraint(s) on the other dimensions. In this paper we present Abacus, an
extensible, cost-based optimizer which searches for the best implementation of
a semantic operator system given a (possibly constrained) optimization
objective. Abacus estimates operator performance by leveraging a minimal set of
validation examples and, if available, prior beliefs about operator
performance. We evaluate Abacus on document processing workloads in the
biomedical and legal domains (BioDEX; CUAD) and multi-modal question answering
(MMQA). We demonstrate that systems optimized by Abacus achieve 18.7%-39.2%
better quality and up to 23.6x lower cost and 4.2x lower latency than the next
best system.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [488] [Data Balancing Strategies: A Survey of Resampling and Augmentation Methods](https://arxiv.org/abs/2505.13518)
*Behnam Yousefimehr,Mehdi Ghatee,Mohammad Amin Seifi,Javad Fazli,Sajed Tavakoli,Zahra Rafei,Shervin Ghaffari,Abolfazl Nikahd,Mahdi Razi Gandomani,Alireza Orouji,Ramtin Mahmoudi Kashani,Sarina Heshmati,Negin Sadat Mousavi*

Main category: stat.ML

TL;DR: 该论文综述了机器学习中处理不平衡数据的多种重采样技术，包括过采样、欠采样、生成模型等，并探讨了当前进展和未来方向。


<details>
  <summary>Details</summary>
Motivation: 不平衡数据在机器学习中会导致预测偏差和模型准确性下降，因此需要有效的重采样技术来改善类别分布。

Method: 论文分类并回顾了多种数据平衡方法，包括合成过采样、自适应技术、生成模型、集成策略、混合方法、欠采样和基于邻居的方法。

Result: 论文总结了各种重采样技术的有效性，并通过实际案例验证了它们的应用效果。

Conclusion: 论文提出了未来在不平衡数据领域的研究方向，强调了进一步探索的必要性。

Abstract: Imbalanced data poses a significant obstacle in machine learning, as an
unequal distribution of class labels often results in skewed predictions and
diminished model accuracy. To mitigate this problem, various resampling
strategies have been developed, encompassing both oversampling and
undersampling techniques aimed at modifying class proportions. Conventional
oversampling approaches like SMOTE enhance the representation of the minority
class, whereas undersampling methods focus on trimming down the majority class.
Advances in deep learning have facilitated the creation of more complex
solutions, such as Generative Adversarial Networks (GANs) and Variational
Autoencoders (VAEs), which are capable of producing high-quality synthetic
examples. This paper reviews a broad spectrum of data balancing methods,
classifying them into categories including synthetic oversampling, adaptive
techniques, generative models, ensemble-based strategies, hybrid approaches,
undersampling, and neighbor-based methods. Furthermore, it highlights current
developments in resampling techniques and discusses practical implementations
and case studies that validate their effectiveness. The paper concludes by
offering perspectives on potential directions for future exploration in this
domain.

</details>


### [489] [Continuous Domain Generalization](https://arxiv.org/abs/2505.13519)
*Zekun Cai,Yiheng Yao,Guangji Bai,Renhe Jiang,Xuan Song,Ryosuke Shibasaki,Liang Zhao*

Main category: stat.ML

TL;DR: 该论文提出了连续域泛化（CDG）任务，通过几何和代数理论构建低维流形，并引入神经李传输算子（NeuralLTO）实现结构化参数转换，显著提升了模型在复杂多维变化下的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现实世界的数据分布通常沿多个潜在因素（如时间、地理和社会经济背景）连续变化，而现有域泛化方法仅处理离散或单轴变化，无法捕捉真实世界复杂多维变化的本质。

Method: 论文提出基于几何和代数理论的框架，证明最优模型参数位于低维流形上，并设计神经李传输算子（NeuralLTO）实现几何连续性和代数一致性的参数转换，同时引入门控机制和局部图表策略处理噪声或缺失描述符。

Result: 在合成和真实数据集（包括遥感、科学文献和交通预测）上的实验表明，该方法在泛化准确性和描述符不完美情况下的鲁棒性上显著优于现有基线。

Conclusion: 通过建模连续多维变化的结构化流形，神经李传输算子有效解决了复杂域泛化问题，为现实场景中的模型泛化提供了新思路。

Abstract: Real-world data distributions often shift continuously across multiple latent
factors such as time, geography, and socioeconomic context. However, existing
domain generalization approaches typically treat domains as discrete or
evolving along a single axis (e.g., time), which fails to capture the complex,
multi-dimensional nature of real-world variation. This paper introduces the
task of Continuous Domain Generalization (CDG), which aims to generalize
predictive models to unseen domains defined by arbitrary combinations of
continuous variation descriptors. We present a principled framework grounded in
geometric and algebraic theory, showing that optimal model parameters across
domains lie on a low-dimensional manifold. To model this structure, we propose
a Neural Lie Transport Operator (NeuralLTO), which enables structured parameter
transitions by enforcing geometric continuity and algebraic consistency. To
handle noisy or incomplete domain descriptors, we introduce a gating mechanism
to suppress irrelevant dimensions and a local chart-based strategy for robust
generalization. Extensive experiments on synthetic and real-world
datasets-including remote sensing, scientific documents, and traffic
forecasting-demonstrate that our method significantly outperforms existing
baselines in generalization accuracy and robustness under descriptor
imperfections.

</details>


### [490] [Randomised Optimism via Competitive Co-Evolution for Matrix Games with Bandit Feedback](https://arxiv.org/abs/2505.13562)
*Shishen Lin*

Main category: stat.ML

TL;DR: 该论文提出了一种名为COEBL的新算法，将进化算法与多臂老虎机框架结合，用于解决双人零和矩阵游戏中随机乐观策略的理论空白，并在实验中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有的研究主要集中在确定性乐观策略（如UCB）在矩阵游戏中的应用，而随机乐观策略的潜力尚未得到理论探索。本文旨在填补这一空白。

Method: 提出COEBL算法，通过进化算法的变异算子实现随机乐观策略，并将其整合到老虎机框架中。

Result: 理论证明COEBL能够实现次线性遗憾，与确定性乐观策略性能相当。实验表明其在多种矩阵游戏基准中优于经典老虎机算法。

Conclusion: 进化老虎机学习在博弈论环境中具有潜力，尤其是通过进化算法实现的随机乐观策略表现出显著效果。

Abstract: Learning in games is a fundamental problem in machine learning and artificial
intelligence, with numerous
applications~\citep{silver2016mastering,schrittwieser2020mastering}. This work
investigates two-player zero-sum matrix games with an unknown payoff matrix and
bandit feedback, where each player observes their actions and the corresponding
noisy payoff. Prior studies have proposed algorithms for this
setting~\citep{o2021matrix,maiti2023query,cai2024uncoupled}, with
\citet{o2021matrix} demonstrating the effectiveness of deterministic optimism
(e.g., \ucb) in achieving sublinear regret. However, the potential of
randomised optimism in matrix games remains theoretically unexplored.
  We propose Competitive Co-evolutionary Bandit Learning (\coebl), a novel
algorithm that integrates evolutionary algorithms (EAs) into the bandit
framework to implement randomised optimism through EA variation operators. We
prove that \coebl achieves sublinear regret, matching the performance of
deterministic optimism-based methods. To the best of our knowledge, this is the
first theoretical regret analysis of an evolutionary bandit learning algorithm
in matrix games.
  Empirical evaluations on diverse matrix game benchmarks demonstrate that
\coebl not only achieves sublinear regret but also consistently outperforms
classical bandit algorithms, including \exptr~\citep{auer2002nonstochastic},
the variant \exptrni~\citep{cai2024uncoupled}, and \ucb~\citep{o2021matrix}.
These results highlight the potential of evolutionary bandit learning,
particularly the efficacy of randomised optimism via evolutionary algorithms in
game-theoretic settings.

</details>


### [491] [Scalable Bayesian Monte Carlo: fast uncertainty estimation beyond deep ensembles](https://arxiv.org/abs/2505.13585)
*Xinzhu Liang,Joseph M. Lukens,Sanjaya Lohani,Brian T. Kirby,Thomas A. Searles,Xin Qiu,Kody J. H. Law*

Main category: stat.ML

TL;DR: 本文提出了一种名为可扩展贝叶斯蒙特卡洛（SBMC）的新方法，通过在点估计和后验之间插值，实现了并行化的贝叶斯深度学习算法，性能与串行实现相当，并在不确定性量化方面表现优异。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于开发一种能够在点估计和后验之间进行插值的方法，通过并行实现贝叶斯深度学习算法（如SMC或MCMC），以提高计算效率并保持准确性。

Method: 方法采用可扩展贝叶斯蒙特卡洛（SBMC），通过并行实现序列蒙特卡洛（SMC）或马尔可夫链蒙特卡洛（MCMC）算法，结合点估计和后验插值。

Result: 实验结果表明，并行实现的SMC和MCMC在性能和总成本上与串行实现相当，准确性达到或超过当前最优方法（如深度集成），并在不确定性量化（尤其是认知不确定性）方面表现显著提升。

Conclusion: 尽管并行实现成本较高，但通过锚定点估计可以在保持有价值的不确定性量化的同时恢复准确性，最终以与当前最优方法相当的成本实现跨指标的强性能。

Abstract: This work introduces a new method called scalable Bayesian Monte Carlo
(SBMC). The model interpolates between a point estimator and the posterior, and
the algorithm is a parallel implementation of a consistent (asymptotically
unbiased) Bayesian deep learning algorithm: sequential Monte Carlo (SMC) or
Markov chain Monte Carlo (MCMC). The method is motivated theoretically, and its
utility is demonstrated on practical examples: MNIST, CIFAR, IMDb. A systematic
numerical study reveals that parallel implementations of SMC and MCMC are
comparable to serial implementations in terms of performance and total cost,
and they achieve accuracy at or beyond the state-of-the-art (SOTA) methods like
deep ensembles at convergence, along with substantially improved uncertainty
quantification (UQ)--in particular, epistemic UQ. But even parallel
implementations are expensive, with an irreducible time barrier much larger
than the cost of the MAP estimator. Compressing time further leads to rapid
degradation of accuracy, whereas UQ remains valuable. By anchoring to a point
estimator we can recover accuracy, while retaining valuable UQ, ultimately
delivering strong performance across metrics for a cost comparable to the SOTA.

</details>


### [492] [Backward Conformal Prediction](https://arxiv.org/abs/2505.13732)
*Etienne Gauthier,Francis Bach,Michael I. Jordan*

Main category: stat.ML

TL;DR: 提出了一种名为“反向共形预测”的新方法，通过灵活控制预测集大小来保证共形覆盖，适用于需要小预测集的应用场景。


<details>
  <summary>Details</summary>
Motivation: 传统共形预测方法固定覆盖水平，导致预测集大小不可控，这在某些实际应用（如医学诊断）中不实用。本文旨在解决这一问题。

Method: 结合了Gauthier等人关于e值的后验有效性结果，以及一种新颖的留一法估计器，用于计算边际错误覆盖率。

Result: 理论分析和实证结果表明，该方法在保持可计算覆盖保证的同时，能生成解释性强且大小可控的预测集。

Conclusion: 反向共形预测方法在需要小预测集的应用中表现出色，兼具理论保证和实际可操作性。

Abstract: We introduce $\textit{Backward Conformal Prediction}$, a method that
guarantees conformal coverage while providing flexible control over the size of
prediction sets. Unlike standard conformal prediction, which fixes the coverage
level and allows the conformal set size to vary, our approach defines a rule
that constrains how prediction set sizes behave based on the observed data, and
adapts the coverage level accordingly. Our method builds on two key
foundations: (i) recent results by Gauthier et al. [2025] on post-hoc validity
using e-values, which ensure marginal coverage of the form $\mathbb{P}(Y_{\rm
test} \in \hat C_n^{\tilde{\alpha}}(X_{\rm test})) \ge 1 -
\mathbb{E}[\tilde{\alpha}]$ up to a first-order Taylor approximation for any
data-dependent miscoverage $\tilde{\alpha}$, and (ii) a novel leave-one-out
estimator $\hat{\alpha}^{\rm LOO}$ of the marginal miscoverage
$\mathbb{E}[\tilde{\alpha}]$ based on the calibration set, ensuring that the
theoretical guarantees remain computable in practice. This approach is
particularly useful in applications where large prediction sets are impractical
such as medical diagnosis. We provide theoretical results and empirical
evidence supporting the validity of our method, demonstrating that it maintains
computable coverage guarantees while ensuring interpretable, well-controlled
prediction set sizes.

</details>


### [493] [Graphon Mixtures](https://arxiv.org/abs/2505.13864)
*Sevvandi Kandanaarachchi,Cheng Soon Ong*

Main category: stat.ML

TL;DR: 提出一种生成模型，结合社交网络中的枢纽节点和密集社区结构，通过图混合方法生成稀疏与稠密结合的图序列，并在理论和实验中验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 社交网络中普遍存在少量枢纽节点和大量密集小社区的结构特征，现有模型难以同时捕捉这两种特性，因此需要一种新的生成模型来更准确地描述此类网络。

Method: 基于线图图论的最新成果，提出一种图混合模型（graphon mixture），通过定义稀疏图的最大度条件识别枢纽节点，并估计其归一化度及稀疏成分的图函数。

Result: 理论证明可估计枢纽节点的归一化度及稀疏成分的图函数；在合成数据、引用网络和社交网络上的实验验证了显式建模稀疏图的有效性。

Conclusion: 该模型能同时捕捉网络的枢纽和密集结构，为分析真实网络提供了更灵活的工具，尤其适用于稀疏-稠密混合的图数据。

Abstract: Social networks have a small number of large hubs, and a large number of
small dense communities. We propose a generative model that captures both hub
and dense structures. Based on recent results about graphons on line graphs,
our model is a graphon mixture, enabling us to generate sequences of graphs
where each graph is a combination of sparse and dense graphs. We propose a new
condition on sparse graphs (the max-degree), which enables us to identify hubs.
We show theoretically that we can estimate the normalized degree of the hubs,
as well as estimate the graphon corresponding to sparse components of graph
mixtures. We illustrate our approach on synthetic data, citation graphs, and
social networks, showing the benefits of explicitly modeling sparse graphs.

</details>


### [494] [An Asymptotic Equation Linking WAIC and WBIC in Singular Models](https://arxiv.org/abs/2505.13902)
*Naoki Hayashi,Takuro Kutsuna,Sawa Takamuku*

Main category: stat.ML

TL;DR: 该论文在统计学习中探讨了正则与奇异模型的区别，针对奇异模型传统信息准则失效的问题，通过理论推导连接了WAIC和WBIC，为奇异模型选择提供了理论基础。


<details>
  <summary>Details</summary>
Motivation: 传统信息准则如AIC和BIC在奇异模型（如具有层次结构或潜在变量的模型）中因正态近似失效而无法适用。WAIC和WBIC虽被提出，但需独立后验采样计算。本文旨在建立两者间的理论联系。

Method: 通过理论推导，建立了一个连接WAIC和WBIC的渐近方程，利用WBIC的后验分布给出了WAIC的渐近无偏表达式。

Result: 论文成功推导出WAIC和WBIC之间的渐近关系，阐明了它们在奇异学习理论框架下的结构联系，深化了对两者渐近行为的理解。

Conclusion: 这一理论成果为奇异模型选择中计算效率的未来发展奠定了基础，同时增进了对WAIC和WBIC关系的理解。

Abstract: In statistical learning, models are classified as regular or singular
depending on whether the mapping from parameters to probability distributions
is injective. Most models with hierarchical structures or latent variables are
singular, for which conventional criteria such as the Akaike Information
Criterion and the Bayesian Information Criterion are inapplicable due to the
breakdown of normal approximations for the likelihood and posterior. To address
this, the Widely Applicable Information Criterion (WAIC) and the Widely
Applicable Bayesian Information Criterion (WBIC) have been proposed. Since WAIC
and WBIC are computed using posterior distributions at different temperature
settings, separate posterior sampling is generally required. In this paper, we
theoretically derive an asymptotic equation that links WAIC and WBIC, despite
their dependence on different posteriors. This equation yields an
asymptotically unbiased expression of WAIC in terms of the posterior
distribution used for WBIC. The result clarifies the structural relationship
between these criteria within the framework of singular learning theory, and
deepens understanding of their asymptotic behavior. This theoretical
contribution provides a foundation for future developments in the computational
efficiency of model selection in singular models.

</details>


### [495] [A Probabilistic Perspective on Model Collapse](https://arxiv.org/abs/2505.13947)
*Shirong Xu,Hengzhi He,Guang Cheng*

Main category: stat.ML

TL;DR: 本文从概率角度研究了递归参数模型训练中的模型崩溃问题，提出了防止崩溃的样本量增长条件，并探讨了合成数据训练的潜力。


<details>
  <summary>Details</summary>
Motivation: 近年来，模型崩溃成为语言模型训练中的关键问题，需要深入理解其驱动机制及缓解方法。

Method: 将递归训练过程建模为模型估计的随机游走，分析样本量对步长的影响以及估计过程对方向与偏差的作用。

Result: 理论证明逐步增加样本量可防止模型崩溃（无偏估计需超线性增长，存在偏差时需更快增速），并验证合成数据训练的优越概率。

Conclusion: 通过概率框架和实验验证，提出了防止模型崩溃的样本增长条件，为递归训练提供了理论支持。

Abstract: In recent years, model collapse has become a critical issue in language model
training, making it essential to understand the underlying mechanisms driving
this phenomenon. In this paper, we investigate recursive parametric model
training from a probabilistic perspective, aiming to characterize the
conditions under which model collapse occurs and, crucially, how it can be
mitigated. We conceptualize the recursive training process as a random walk of
the model estimate, highlighting how the sample size influences the step size
and how the estimation procedure determines the direction and potential bias of
the random walk. Under mild conditions, we rigorously show that progressively
increasing the sample size at each training step is necessary to prevent model
collapse. In particular, when the estimation is unbiased, the required growth
rate follows a superlinear pattern. This rate needs to be accelerated even
further in the presence of substantial estimation bias. Building on this
probabilistic framework, we also investigate the probability that recursive
training on synthetic data yields models that outperform those trained solely
on real data. Moreover, we extend these results to general parametric model
family in an asymptotic regime. Finally, we validate our theoretical results
through extensive simulations and a real-world dataset.

</details>


### [496] [Computational Efficiency under Covariate Shift in Kernel Ridge Regression](https://arxiv.org/abs/2505.14083)
*Andrea Della Vecchia,Arnaud Mavakala Watusadisi,Ernesto De Vito,Lorenzo Rosasco*

Main category: stat.ML

TL;DR: 该论文研究了在协变量偏移情况下，利用随机投影在RKHS中实现计算效率与统计精度平衡的方法。


<details>
  <summary>Details</summary>
Motivation: 协变量偏移导致训练与测试数据输入分布不同，增加了学习难度，而核方法虽统计性能优越但计算成本高，难以扩展到大样本。

Method: 采用随机投影技术，在给定RKHS中构建随机子空间作为假设空间，以降低计算复杂度。

Result: 研究表明，即使存在协变量偏移，该方法仍能显著节省计算资源且不损害学习性能。

Conclusion: 通过随机投影在RKHS中有效平衡了计算效率与统计精度，为大规模数据集下的协变量偏移问题提供了可行解决方案。

Abstract: This paper addresses the covariate shift problem in the context of
nonparametric regression within reproducing kernel Hilbert spaces (RKHSs).
Covariate shift arises in supervised learning when the input distributions of
the training and test data differ, presenting additional challenges for
learning. Although kernel methods have optimal statistical properties, their
high computational demands in terms of time and, particularly, memory, limit
their scalability to large datasets. To address this limitation, the main focus
of this paper is to explore the trade-off between computational efficiency and
statistical accuracy under covariate shift. We investigate the use of random
projections where the hypothesis space consists of a random subspace within a
given RKHS. Our results show that, even in the presence of covariate shift,
significant computational savings can be achieved without compromising learning
performance.

</details>


### [497] [High-dimensional Nonparametric Contextual Bandit Problem](https://arxiv.org/abs/2505.14102)
*Shogo Iwazaki,Junpei Komiyama,Masaaki Imaizumi*

Main category: stat.ML

TL;DR: 该论文研究了高维特征空间下的核化上下文赌博机问题，提出了在上下文分布满足随机假设时实现无遗憾学习的方法，并分析了宽松遗憾的收敛速率。


<details>
  <summary>Details</summary>
Motivation: 核化上下文赌博机问题在个性化广告和推荐系统等决策场景中具有广泛应用。现有方法在高斯核和高维特征下性能较差，需要新的理论突破。

Method: 引入上下文分布的随机假设，放宽每轮遗憾限制（Δ>0），分析高维特征下的核化上下文赌博机算法。

Result: 证明了即使特征维度增长到样本数量级仍可实现无遗憾学习，并推导出宽松遗憾关于Δ的收敛速率。

Conclusion: 通过随机假设和宽松遗憾分析，为高维核化上下文赌博机问题提供了新的理论保证和实用算法框架。

Abstract: We consider the kernelized contextual bandit problem with a large feature
space. This problem involves $K$ arms, and the goal of the forecaster is to
maximize the cumulative rewards through learning the relationship between the
contexts and the rewards. It serves as a general framework for various
decision-making scenarios, such as personalized online advertising and
recommendation systems. Kernelized contextual bandits generalize the linear
contextual bandit problem and offers a greater modeling flexibility. Existing
methods, when applied to Gaussian kernels, yield a trivial bound of $O(T)$ when
we consider $\Omega(\log T)$ feature dimensions. To address this, we introduce
stochastic assumptions on the context distribution and show that no-regret
learning is achievable even when the number of dimensions grows up to the
number of samples. Furthermore, we analyze lenient regret, which allows a
per-round regret of at most $\Delta > 0$. We derive the rate of lenient regret
in terms of $\Delta$.

</details>


### [498] [Hybrid Bernstein Normalizing Flows for Flexible Multivariate Density Regression with Interpretable Marginals](https://arxiv.org/abs/2505.14164)
*Marcel Arpogaus,Thomas Kneib,Thomas Nagler,David Rügamer*

Main category: stat.ML

TL;DR: 该论文结合了多元条件转换模型（MCTM）和自回归归一化流（NF），以提升密度回归模型的解释性和灵活性。


<details>
  <summary>Details</summary>
Motivation: 现有的密度回归模型在灵活性（如NF）和可解释性（如MCTM）之间存在权衡。NF虽灵活但难以解释，而MCTM可解释但灵活性不足。

Method: 将MCTM与自回归NF结合，利用MCTM解释边际分布特征效应，同时用NF捕捉联合数据分布中的复杂非线性关系。

Result: 在模拟和真实数据实验中，该方法展现了优于MCTM和其他NF模型的性能，兼具解释性和灵活性。

Conclusion: 该方法成功平衡了密度回归模型的解释性与灵活性，为复杂数据建模提供了新思路。

Abstract: Density regression models allow a comprehensive understanding of data by
modeling the complete conditional probability distribution. While flexible
estimation approaches such as normalizing flows (NF) work particularly well in
multiple dimensions, interpreting the input-output relationship of such models
is often difficult, due to the black-box character of deep learning models. In
contrast, existing statistical methods for multivariate outcomes such as
multivariate conditional transformation models (MCTM) are restricted in
flexibility and are often not expressive enough to represent complex
multivariate probability distributions. In this paper, we combine MCTM with
state-of-the-art and autoregressive NF to leverage the transparency of MCTM for
modeling interpretable feature effects on the marginal distributions in the
first step and the flexibility of neural-network-based NF techniques to account
for complex and non-linear relationships in the joint data distribution. We
demonstrate our method's versatility in various numerical experiments and
compare it with MCTM and other NF models on both simulated and real-world data.

</details>


### [499] [From stability of Langevin diffusion to convergence of proximal MCMC for non-log-concave sampling](https://arxiv.org/abs/2505.14177)
*Marien Renaud,Valentin De Bortoli,Arthur Leclaire,Nicolas Papadakis*

Main category: stat.ML

TL;DR: 本文研究了非凸势能下的采样问题，证明了离散时间ULA在势能强凸条件下的稳定性，并首次给出了PSGLA在非凸势能下的收敛性证明。实验验证了PSGLA在成像逆问题中的高效性。


<details>
  <summary>Details</summary>
Motivation: 在成像逆问题等场景中，势能函数往往是非凸且非光滑的，传统算法难以处理。因此，需要开发能够有效处理此类势能的采样算法。

Method: 结合前向-后向优化算法与ULA步骤，提出PSGLA算法，并利用Moreau包络的性质进行理论分析。

Result: 理论证明了PSGLA在非凸势能下的收敛性，实验显示PSGLA在收敛速度和恢复性能上优于传统算法。

Conclusion: PSGLA在非凸势能下具有理论保证和实际优势，适用于成像逆问题等复杂场景。

Abstract: We consider the problem of sampling distributions stemming from non-convex
potentials with Unadjusted Langevin Algorithm (ULA). We prove the stability of
the discrete-time ULA to drift approximations under the assumption that the
potential is strongly convex at infinity. In many context, e.g. imaging inverse
problems, potentials are non-convex and non-smooth. Proximal Stochastic
Gradient Langevin Algorithm (PSGLA) is a popular algorithm to handle such
potentials. It combines the forward-backward optimization algorithm with a ULA
step. Our main stability result combined with properties of the Moreau envelope
allows us to derive the first proof of convergence of the PSGLA for non-convex
potentials. We empirically validate our methodology on synthetic data and in
the context of imaging inverse problems. In particular, we observe that PSGLA
exhibits faster convergence rates than Stochastic Gradient Langevin Algorithm
for posterior sampling while preserving its restoration properties.

</details>


### [500] [A system identification approach to clustering vector autoregressive time series](https://arxiv.org/abs/2505.14421)
*Zuogong Yue,Xinyi Wang,Victor Solo*

Main category: stat.ML

TL;DR: 该论文提出了一种基于自回归模型的向量时间序列聚类方法k-LMVAR，解决了现有方法忽略自相关特征或依赖领域知识的问题，并通过BIC准则选择聚类数和模型阶数，在仿真实验中表现优异且计算效率高。


<details>
  <summary>Details</summary>
Motivation: 当前时间序列聚类方法大多仅处理标量时间序列、将其视为白噪声或依赖领域知识构建特征，而忽略了自相关模式。论文旨在通过系统辨识方法，显式考虑向量时间序列的自回归动态特性，提升聚类效果。

Method: 首先推导基于混合自回归模型的聚类算法，但因计算问题转而提出其小噪声极限版本k-LMVAR（Limiting Mixture Vector AutoRegression），并开发BIC准则用于选择聚类数和模型阶数。

Result: k-LMVAR算法在比较仿真中表现优异，且计算可扩展性强，能够有效处理向量时间序列的自回归动态聚类问题。

Conclusion: k-LMVAR通过显式建模自回归动态特性，克服了传统方法的局限性，为时间序列聚类提供了高效且可扩展的解决方案。

Abstract: Clustering of time series based on their underlying dynamics is keeping
attracting researchers due to its impacts on assisting complex system
modelling. Most current time series clustering methods handle only scalar time
series, treat them as white noise, or rely on domain knowledge for high-quality
feature construction, where the autocorrelation pattern/feature is mostly
ignored. Instead of relying on heuristic feature/metric construction, the
system identification approach allows treating vector time series clustering by
explicitly considering their underlying autoregressive dynamics. We first
derive a clustering algorithm based on a mixture autoregressive model.
Unfortunately it turns out to have significant computational problems. We then
derive a `small-noise' limiting version of the algorithm, which we call k-LMVAR
(Limiting Mixture Vector AutoRegression), that is computationally manageable.
We develop an associated BIC criterion for choosing the number of clusters and
model order. The algorithm performs very well in comparative simulations and
also scales well computationally.

</details>


### [501] [A simple estimator of the correlation kernel matrix of a determinantal point process](https://arxiv.org/abs/2505.14529)
*Christian Gouriéroux,Yang Lu*

Main category: stat.ML

TL;DR: 本文提出了一种易于实现的DPP核矩阵闭式估计器，并证明了其一致性、渐近正态性及大偏差性质。


<details>
  <summary>Details</summary>
Motivation: 研究DPP核矩阵的估计问题，旨在提供一种简单且有效的估计方法，可作为最大似然估计学习算法的初始值。

Method: 提出了一种闭式估计器来估计DPP的核矩阵，该方法易于实现且适用于学习算法的初始化。

Result: 证明了该估计器的一致性、渐近正态性及其大偏差性质。

Conclusion: 所提出的闭式估计器为DPP核矩阵的估计提供了一种高效且理论保证的方法。

Abstract: The Determinantal Point Process (DPP) is a parameterized model for
multivariate binary variables, characterized by a correlation kernel matrix.
This paper proposes a closed form estimator of this kernel, which is
particularly easy to implement and can also be used as a starting value of
learning algorithms for maximum likelihood estimation. We prove the consistency
and asymptotic normality of our estimator, as well as its large deviation
properties.

</details>


### [502] [High-Dimensional Analysis of Bootstrap Ensemble Classifiers](https://arxiv.org/abs/2505.14587)
*Hamza Cherkaoui,Malik Tiomoko,Mohamed El Amine Seddik,Cosme Louart,Ekkehard Schnoor,Balazs Kegl*

Main category: stat.ML

TL;DR: 本文通过随机矩阵理论分析了Bootstrap方法在高维数据下对LSSVM集成学习性能的影响，并提出了优化子集数量和正则化参数的策略。


<details>
  <summary>Details</summary>
Motivation: 研究Bootstrap方法在大样本和高维特征下对LSSVM集成学习性能的理论影响，以提升分类器性能。

Method: 利用随机矩阵理论分析Bootstrap技术，研究LSSVM集成学习中多个弱分类器的决策函数聚合效果。

Result: 理论分析表明Bootstrap方法在高维设置中有效，实验验证了优化子集数量和正则化参数能显著提升LSSVM性能。

Conclusion: 本文为高维数据下的Bootstrap应用提供了理论支持，并通过实验验证了优化策略的有效性。

Abstract: Bootstrap methods have long been a cornerstone of ensemble learning in
machine learning. This paper presents a theoretical analysis of bootstrap
techniques applied to the Least Square Support Vector Machine (LSSVM) ensemble
in the context of large and growing sample sizes and feature dimensionalities.
Leveraging tools from Random Matrix Theory, we investigate the performance of
this classifier that aggregates decision functions from multiple weak
classifiers, each trained on different subsets of the data. We provide insights
into the use of bootstrap methods in high-dimensional settings, enhancing our
understanding of their impact. Based on these findings, we propose strategies
to select the number of subsets and the regularization parameter that maximize
the performance of the LSSVM. Empirical experiments on synthetic and real-world
datasets validate our theoretical results.

</details>
